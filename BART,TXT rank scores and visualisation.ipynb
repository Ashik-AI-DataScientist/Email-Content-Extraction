{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72ba6f94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in c:\\users\\764883\\appdata\\roaming\\python\\python311\\site-packages (4.36.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\764883\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\users\\764883\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (0.19.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\program files\\python311\\lib\\site-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\program files\\python311\\lib\\site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\program files\\python311\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\764883\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in c:\\program files\\python311\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\764883\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\764883\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (0.4.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\program files\\python311\\lib\\site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\764883\\appdata\\roaming\\python\\python311\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.12.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\program files\\python311\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.6.3)\n",
      "Requirement already satisfied: colorama in c:\\program files\\python311\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\program files\\python311\\lib\\site-packages (from requests->transformers) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\program files\\python311\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\program files\\python311\\lib\\site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\program files\\python311\\lib\\site-packages (from requests->transformers) (2023.5.7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: rouge in c:\\users\\764883\\appdata\\roaming\\python\\python311\\site-packages (1.0.1)\n",
      "Requirement already satisfied: six in c:\\program files\\python311\\lib\\site-packages (from rouge) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting bert_score\n",
      "  Using cached bert_score-0.3.13-py3-none-any.whl (61 kB)\n",
      "Requirement already satisfied: torch>=1.0.0 in c:\\users\\764883\\appdata\\roaming\\python\\python311\\site-packages (from bert_score) (2.1.1)\n",
      "Requirement already satisfied: pandas>=1.0.1 in c:\\program files\\python311\\lib\\site-packages (from bert_score) (1.5.3)\n",
      "Requirement already satisfied: transformers>=3.0.0 in c:\\users\\764883\\appdata\\roaming\\python\\python311\\site-packages (from bert_score) (4.36.1)\n",
      "Requirement already satisfied: numpy in c:\\program files\\python311\\lib\\site-packages (from bert_score) (1.23.5)\n",
      "Requirement already satisfied: requests in c:\\program files\\python311\\lib\\site-packages (from bert_score) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.31.1 in c:\\program files\\python311\\lib\\site-packages (from bert_score) (4.65.0)\n",
      "Requirement already satisfied: matplotlib in c:\\program files\\python311\\lib\\site-packages (from bert_score) (3.7.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\program files\\python311\\lib\\site-packages (from bert_score) (23.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\program files\\python311\\lib\\site-packages (from pandas>=1.0.1->bert_score) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\program files\\python311\\lib\\site-packages (from pandas>=1.0.1->bert_score) (2023.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\764883\\appdata\\roaming\\python\\python311\\site-packages (from torch>=1.0.0->bert_score) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\program files\\python311\\lib\\site-packages (from torch>=1.0.0->bert_score) (4.6.3)\n",
      "Requirement already satisfied: sympy in c:\\program files\\python311\\lib\\site-packages (from torch>=1.0.0->bert_score) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\program files\\python311\\lib\\site-packages (from torch>=1.0.0->bert_score) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\program files\\python311\\lib\\site-packages (from torch>=1.0.0->bert_score) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\764883\\appdata\\roaming\\python\\python311\\site-packages (from torch>=1.0.0->bert_score) (2023.12.2)\n",
      "Requirement already satisfied: colorama in c:\\program files\\python311\\lib\\site-packages (from tqdm>=4.31.1->bert_score) (0.4.6)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\users\\764883\\appdata\\roaming\\python\\python311\\site-packages (from transformers>=3.0.0->bert_score) (0.19.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\program files\\python311\\lib\\site-packages (from transformers>=3.0.0->bert_score) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\764883\\appdata\\roaming\\python\\python311\\site-packages (from transformers>=3.0.0->bert_score) (2023.10.3)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\764883\\appdata\\roaming\\python\\python311\\site-packages (from transformers>=3.0.0->bert_score) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\764883\\appdata\\roaming\\python\\python311\\site-packages (from transformers>=3.0.0->bert_score) (0.4.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\program files\\python311\\lib\\site-packages (from matplotlib->bert_score) (1.0.7)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\program files\\python311\\lib\\site-packages (from matplotlib->bert_score) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\program files\\python311\\lib\\site-packages (from matplotlib->bert_score) (4.39.4)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\program files\\python311\\lib\\site-packages (from matplotlib->bert_score) (1.4.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\program files\\python311\\lib\\site-packages (from matplotlib->bert_score) (9.5.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\program files\\python311\\lib\\site-packages (from matplotlib->bert_score) (3.0.9)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\program files\\python311\\lib\\site-packages (from requests->bert_score) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\program files\\python311\\lib\\site-packages (from requests->bert_score) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\program files\\python311\\lib\\site-packages (from requests->bert_score) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\program files\\python311\\lib\\site-packages (from requests->bert_score) (2023.5.7)\n",
      "Requirement already satisfied: six>=1.5 in c:\\program files\\python311\\lib\\site-packages (from python-dateutil>=2.8.1->pandas>=1.0.1->bert_score) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\program files\\python311\\lib\\site-packages (from jinja2->torch>=1.0.0->bert_score) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\program files\\python311\\lib\\site-packages (from sympy->torch>=1.0.0->bert_score) (1.3.0)\n",
      "Installing collected packages: bert_score\n",
      "Successfully installed bert_score-0.3.13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The scripts bert-score-show.exe and bert-score.exe are installed in 'C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.99-cp311-cp311-win_amd64.whl (977 kB)\n",
      "                                              0.0/977.5 kB ? eta -:--:--\n",
      "     ------------------------------------  972.8/977.5 kB 31.1 MB/s eta 0:00:01\n",
      "     ------------------------------------- 977.5/977.5 kB 31.2 MB/s eta 0:00:00\n",
      "Installing collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.1.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\program files\\python311\\lib\\site-packages (4.12.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\program files\\python311\\lib\\site-packages (from beautifulsoup4) (2.4.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting extract-msg\n",
      "  Downloading extract_msg-0.47.0-py2.py3-none-any.whl (328 kB)\n",
      "                                              0.0/328.2 kB ? eta -:--:--\n",
      "     ----------------------------           245.8/328.2 kB 7.6 MB/s eta 0:00:01\n",
      "     -------------------------------------- 328.2/328.2 kB 6.8 MB/s eta 0:00:00\n",
      "Collecting olefile==0.47 (from extract-msg)\n",
      "  Downloading olefile-0.47-py2.py3-none-any.whl (114 kB)\n",
      "                                              0.0/114.6 kB ? eta -:--:--\n",
      "     ---------------------------------------- 114.6/114.6 kB ? eta 0:00:00\n",
      "Collecting tzlocal<6,>=4.2 (from extract-msg)\n",
      "  Downloading tzlocal-5.2-py3-none-any.whl (17 kB)\n",
      "Collecting compressed-rtf<2,>=1.0.6 (from extract-msg)\n",
      "  Downloading compressed_rtf-1.0.6.tar.gz (5.8 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting ebcdic<2,>=1.1.1 (from extract-msg)\n",
      "  Downloading ebcdic-1.1.1-py2.py3-none-any.whl (128 kB)\n",
      "                                              0.0/128.5 kB ? eta -:--:--\n",
      "     -------------------------------------- 128.5/128.5 kB 7.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: beautifulsoup4<4.13,>=4.11.1 in c:\\program files\\python311\\lib\\site-packages (from extract-msg) (4.12.2)\n",
      "Collecting RTFDE<0.2,>=0.1.1 (from extract-msg)\n",
      "  Downloading RTFDE-0.1.1-py3-none-any.whl (36 kB)\n",
      "Collecting red-black-tree-mod==1.20 (from extract-msg)\n",
      "  Downloading red-black-tree-mod-1.20.tar.gz (28 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\program files\\python311\\lib\\site-packages (from beautifulsoup4<4.13,>=4.11.1->extract-msg) (2.4.1)\n",
      "Collecting lark==1.1.8 (from RTFDE<0.2,>=0.1.1->extract-msg)\n",
      "  Downloading lark-1.1.8-py3-none-any.whl (111 kB)\n",
      "                                              0.0/111.6 kB ? eta -:--:--\n",
      "     -------------------------------------- 111.6/111.6 kB 6.8 MB/s eta 0:00:00\n",
      "Collecting oletools>=0.56 (from RTFDE<0.2,>=0.1.1->extract-msg)\n",
      "  Downloading oletools-0.60.1-py2.py3-none-any.whl (977 kB)\n",
      "                                              0.0/977.2 kB ? eta -:--:--\n",
      "     ------------------                    501.8/977.2 kB 15.4 MB/s eta 0:00:01\n",
      "     ------------------------------------- 977.2/977.2 kB 10.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: tzdata in c:\\program files\\python311\\lib\\site-packages (from tzlocal<6,>=4.2->extract-msg) (2023.3)\n",
      "Collecting pyparsing<3,>=2.1.0 (from oletools>=0.56->RTFDE<0.2,>=0.1.1->extract-msg)\n",
      "  Downloading pyparsing-2.4.7-py2.py3-none-any.whl (67 kB)\n",
      "                                              0.0/67.8 kB ? eta -:--:--\n",
      "     ---------------------------------------- 67.8/67.8 kB ? eta 0:00:00\n",
      "Collecting easygui (from oletools>=0.56->RTFDE<0.2,>=0.1.1->extract-msg)\n",
      "  Downloading easygui-0.98.3-py2.py3-none-any.whl (92 kB)\n",
      "                                              0.0/92.7 kB ? eta -:--:--\n",
      "     ---------------------------------------- 92.7/92.7 kB ? eta 0:00:00\n",
      "Collecting colorclass (from oletools>=0.56->RTFDE<0.2,>=0.1.1->extract-msg)\n",
      "  Downloading colorclass-2.2.2-py2.py3-none-any.whl (18 kB)\n",
      "Collecting pcodedmp>=1.2.5 (from oletools>=0.56->RTFDE<0.2,>=0.1.1->extract-msg)\n",
      "  Downloading pcodedmp-1.2.6-py2.py3-none-any.whl (30 kB)\n",
      "Collecting msoffcrypto-tool (from oletools>=0.56->RTFDE<0.2,>=0.1.1->extract-msg)\n",
      "  Downloading msoffcrypto_tool-5.1.1-py3-none-any.whl (34 kB)\n",
      "Collecting win-unicode-console (from pcodedmp>=1.2.5->oletools>=0.56->RTFDE<0.2,>=0.1.1->extract-msg)\n",
      "  Downloading win_unicode_console-0.5.zip (31 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: cryptography>=35.0 in c:\\program files\\python311\\lib\\site-packages (from msoffcrypto-tool->oletools>=0.56->RTFDE<0.2,>=0.1.1->extract-msg) (41.0.1)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\program files\\python311\\lib\\site-packages (from cryptography>=35.0->msoffcrypto-tool->oletools>=0.56->RTFDE<0.2,>=0.1.1->extract-msg) (1.15.1)\n",
      "Requirement already satisfied: pycparser in c:\\program files\\python311\\lib\\site-packages (from cffi>=1.12->cryptography>=35.0->msoffcrypto-tool->oletools>=0.56->RTFDE<0.2,>=0.1.1->extract-msg) (2.21)\n",
      "Building wheels for collected packages: red-black-tree-mod, compressed-rtf, win-unicode-console\n",
      "  Building wheel for red-black-tree-mod (setup.py): started\n",
      "  Building wheel for red-black-tree-mod (setup.py): finished with status 'done'\n",
      "  Created wheel for red-black-tree-mod: filename=red_black_tree_mod-1.20-py3-none-any.whl size=18628 sha256=39703d0ab19478a1b971463285c88427920fcfedac54c329897e69a85dea19a0\n",
      "  Stored in directory: c:\\users\\764883\\appdata\\local\\pip\\cache\\wheels\\ef\\a1\\4a\\56417ac7cb331323b07533b8e04ec40f698d4f58e33654d743\n",
      "  Building wheel for compressed-rtf (setup.py): started\n",
      "  Building wheel for compressed-rtf (setup.py): finished with status 'done'\n",
      "  Created wheel for compressed-rtf: filename=compressed_rtf-1.0.6-py3-none-any.whl size=6196 sha256=332c813257bfa703ccfe577c0924971d9a56793463e480f2784adeced78e1f36\n",
      "  Stored in directory: c:\\users\\764883\\appdata\\local\\pip\\cache\\wheels\\25\\43\\67\\e46f8b6d3d39fb567b05bf0d01125410665a3e019a468ace25\n",
      "  Building wheel for win-unicode-console (setup.py): started\n",
      "  Building wheel for win-unicode-console (setup.py): finished with status 'done'\n",
      "  Created wheel for win-unicode-console: filename=win_unicode_console-0.5-py3-none-any.whl size=20191 sha256=7788b076c7dfae8f576710f1b4af3bb881003063376dc6c947fde956abea4a40\n",
      "  Stored in directory: c:\\users\\764883\\appdata\\local\\pip\\cache\\wheels\\f8\\1d\\0b\\075724dc0c787a833dd6bb7255e7a14d9dab45eef102b1845a\n",
      "Successfully built red-black-tree-mod compressed-rtf win-unicode-console\n",
      "Installing collected packages: win-unicode-console, red-black-tree-mod, ebcdic, easygui, compressed-rtf, tzlocal, pyparsing, olefile, lark, colorclass, msoffcrypto-tool, pcodedmp, oletools, RTFDE, extract-msg\n",
      "Successfully installed RTFDE-0.1.1 colorclass-2.2.2 compressed-rtf-1.0.6 easygui-0.98.3 ebcdic-1.1.1 extract-msg-0.47.0 lark-1.1.8 msoffcrypto-tool-5.1.1 olefile-0.47 oletools-0.60.1 pcodedmp-1.2.6 pyparsing-2.4.7 red-black-tree-mod-1.20 tzlocal-5.2 win-unicode-console-0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script msoffcrypto-tool.exe is installed in 'C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script pcodedmp.exe is installed in 'C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The scripts ezhexviewer.exe, ftguess.exe, mraptor.exe, msodde.exe, olebrowse.exe, oledir.exe, olefile.exe, oleid.exe, olemap.exe, olemeta.exe, oleobj.exe, oletimes.exe, olevba.exe, pyxswf.exe and rtfobj.exe are installed in 'C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script extract_msg.exe is installed in 'C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting py7zr\n",
      "  Downloading py7zr-0.20.8-py3-none-any.whl (67 kB)\n",
      "                                              0.0/67.0 kB ? eta -:--:--\n",
      "     ---------------------------------------- 67.0/67.0 kB 3.6 MB/s eta 0:00:00\n",
      "Collecting texttable (from py7zr)\n",
      "  Downloading texttable-1.7.0-py2.py3-none-any.whl (10 kB)\n",
      "Collecting pycryptodomex>=3.16.0 (from py7zr)\n",
      "  Downloading pycryptodomex-3.19.0-cp35-abi3-win_amd64.whl (1.7 MB)\n",
      "                                              0.0/1.7 MB ? eta -:--:--\n",
      "     ---------------------------------------  1.7/1.7 MB 54.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 1.7/1.7 MB 36.9 MB/s eta 0:00:00\n",
      "Collecting pyzstd>=0.15.9 (from py7zr)\n",
      "  Downloading pyzstd-0.15.9-cp311-cp311-win_amd64.whl (245 kB)\n",
      "                                              0.0/245.3 kB ? eta -:--:--\n",
      "     ---------------------------------------- 245.3/245.3 kB ? eta 0:00:00\n",
      "Collecting pyppmd<1.2.0,>=1.1.0 (from py7zr)\n",
      "  Downloading pyppmd-1.1.0-cp311-cp311-win_amd64.whl (46 kB)\n",
      "                                              0.0/46.1 kB ? eta -:--:--\n",
      "     ---------------------------------------- 46.1/46.1 kB ? eta 0:00:00\n",
      "Collecting pybcj<1.1.0,>=1.0.0 (from py7zr)\n",
      "  Downloading pybcj-1.0.2-cp311-cp311-win_amd64.whl (24 kB)\n",
      "Collecting multivolumefile>=0.2.3 (from py7zr)\n",
      "  Downloading multivolumefile-0.2.3-py3-none-any.whl (17 kB)\n",
      "Collecting inflate64<1.1.0,>=1.0.0 (from py7zr)\n",
      "  Downloading inflate64-1.0.0-cp311-cp311-win_amd64.whl (35 kB)\n",
      "Collecting brotli>=1.1.0 (from py7zr)\n",
      "  Downloading Brotli-1.1.0-cp311-cp311-win_amd64.whl (357 kB)\n",
      "                                              0.0/357.3 kB ? eta -:--:--\n",
      "     ---------------------------------------- 357.3/357.3 kB ? eta 0:00:00\n",
      "Requirement already satisfied: psutil in c:\\program files\\python311\\lib\\site-packages (from py7zr) (5.9.5)\n",
      "Installing collected packages: texttable, brotli, pyzstd, pyppmd, pycryptodomex, pybcj, multivolumefile, inflate64, py7zr\n",
      "Successfully installed brotli-1.1.0 inflate64-1.0.0 multivolumefile-0.2.3 py7zr-0.20.8 pybcj-1.0.2 pycryptodomex-3.19.0 pyppmd-1.1.0 pyzstd-0.15.9 texttable-1.7.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script py7zr.exe is installed in 'C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in c:\\program files\\python311\\lib\\site-packages (1.5.3)\n",
      "Requirement already satisfied: matplotlib in c:\\program files\\python311\\lib\\site-packages (3.7.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\program files\\python311\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\program files\\python311\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\program files\\python311\\lib\\site-packages (from pandas) (1.23.5)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\program files\\python311\\lib\\site-packages (from matplotlib) (1.0.7)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\program files\\python311\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\program files\\python311\\lib\\site-packages (from matplotlib) (4.39.4)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\program files\\python311\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\program files\\python311\\lib\\site-packages (from matplotlib) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\program files\\python311\\lib\\site-packages (from matplotlib) (9.5.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\764883\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: six>=1.5 in c:\\program files\\python311\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\program files\\python311\\lib\\site-packages (4.12.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\program files\\python311\\lib\\site-packages (from beautifulsoup4) (2.4.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting textblob\n",
      "  Downloading textblob-0.17.1-py2.py3-none-any.whl (636 kB)\n",
      "                                              0.0/636.8 kB ? eta -:--:--\n",
      "     -------------                          225.3/636.8 kB 6.9 MB/s eta 0:00:01\n",
      "     -------------------------              430.1/636.8 kB 5.4 MB/s eta 0:00:01\n",
      "     -------------------------------------  634.9/636.8 kB 5.0 MB/s eta 0:00:01\n",
      "     -------------------------------------- 636.8/636.8 kB 4.0 MB/s eta 0:00:00\n",
      "Collecting nltk>=3.1 (from textblob)\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "                                              0.0/1.5 MB ? eta -:--:--\n",
      "     ----                                     0.2/1.5 MB 3.5 MB/s eta 0:00:01\n",
      "     -----------                              0.4/1.5 MB 5.3 MB/s eta 0:00:01\n",
      "     ------------------                       0.7/1.5 MB 5.0 MB/s eta 0:00:01\n",
      "     --------------------------               1.0/1.5 MB 5.8 MB/s eta 0:00:01\n",
      "     ------------------------------------     1.4/1.5 MB 6.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 1.5/1.5 MB 6.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: click in c:\\program files\\python311\\lib\\site-packages (from nltk>=3.1->textblob) (8.1.3)\n",
      "Requirement already satisfied: joblib in c:\\program files\\python311\\lib\\site-packages (from nltk>=3.1->textblob) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\764883\\appdata\\roaming\\python\\python311\\site-packages (from nltk>=3.1->textblob) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\program files\\python311\\lib\\site-packages (from nltk>=3.1->textblob) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\program files\\python311\\lib\\site-packages (from click->nltk>=3.1->textblob) (0.4.6)\n",
      "Installing collected packages: nltk, textblob\n",
      "Successfully installed nltk-3.8.1 textblob-0.17.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script nltk.exe is installed in 'C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting clean-text\n",
      "  Downloading clean_text-0.6.0-py3-none-any.whl (11 kB)\n",
      "Collecting emoji<2.0.0,>=1.0.0 (from clean-text)\n",
      "  Downloading emoji-1.7.0.tar.gz (175 kB)\n",
      "                                              0.0/175.4 kB ? eta -:--:--\n",
      "     ------------------------------------- 175.4/175.4 kB 10.3 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting ftfy<7.0,>=6.0 (from clean-text)\n",
      "  Downloading ftfy-6.1.3-py3-none-any.whl (53 kB)\n",
      "                                              0.0/53.4 kB ? eta -:--:--\n",
      "     ---------------------------------------- 53.4/53.4 kB ? eta 0:00:00\n",
      "Collecting wcwidth<0.3.0,>=0.2.12 (from ftfy<7.0,>=6.0->clean-text)\n",
      "  Downloading wcwidth-0.2.12-py2.py3-none-any.whl (34 kB)\n",
      "Building wheels for collected packages: emoji\n",
      "  Building wheel for emoji (setup.py): started\n",
      "  Building wheel for emoji (setup.py): finished with status 'done'\n",
      "  Created wheel for emoji: filename=emoji-1.7.0-py3-none-any.whl size=171059 sha256=8aeeaeea9f88cfaef453c827b3424b685ea188dc5b35aa01262d02cda0e40459\n",
      "  Stored in directory: c:\\users\\764883\\appdata\\local\\pip\\cache\\wheels\\bd\\22\\e5\\b69726d5e1a19795ecd3b3e7464b16c0f1d019aa94ff1c8578\n",
      "Successfully built emoji\n",
      "Installing collected packages: wcwidth, emoji, ftfy, clean-text\n",
      "Successfully installed clean-text-0.6.0 emoji-1.7.0 ftfy-6.1.3 wcwidth-0.2.12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script ftfy.exe is installed in 'C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: bert-score in c:\\users\\764883\\appdata\\roaming\\python\\python311\\site-packages (0.3.13)\n",
      "Requirement already satisfied: torch>=1.0.0 in c:\\users\\764883\\appdata\\roaming\\python\\python311\\site-packages (from bert-score) (2.1.1)\n",
      "Requirement already satisfied: pandas>=1.0.1 in c:\\program files\\python311\\lib\\site-packages (from bert-score) (1.5.3)\n",
      "Requirement already satisfied: transformers>=3.0.0 in c:\\users\\764883\\appdata\\roaming\\python\\python311\\site-packages (from bert-score) (4.36.1)\n",
      "Requirement already satisfied: numpy in c:\\program files\\python311\\lib\\site-packages (from bert-score) (1.23.5)\n",
      "Requirement already satisfied: requests in c:\\program files\\python311\\lib\\site-packages (from bert-score) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.31.1 in c:\\program files\\python311\\lib\\site-packages (from bert-score) (4.65.0)\n",
      "Requirement already satisfied: matplotlib in c:\\program files\\python311\\lib\\site-packages (from bert-score) (3.7.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\program files\\python311\\lib\\site-packages (from bert-score) (23.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\program files\\python311\\lib\\site-packages (from pandas>=1.0.1->bert-score) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\program files\\python311\\lib\\site-packages (from pandas>=1.0.1->bert-score) (2023.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\764883\\appdata\\roaming\\python\\python311\\site-packages (from torch>=1.0.0->bert-score) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\program files\\python311\\lib\\site-packages (from torch>=1.0.0->bert-score) (4.6.3)\n",
      "Requirement already satisfied: sympy in c:\\program files\\python311\\lib\\site-packages (from torch>=1.0.0->bert-score) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\program files\\python311\\lib\\site-packages (from torch>=1.0.0->bert-score) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\program files\\python311\\lib\\site-packages (from torch>=1.0.0->bert-score) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\764883\\appdata\\roaming\\python\\python311\\site-packages (from torch>=1.0.0->bert-score) (2023.12.2)\n",
      "Requirement already satisfied: colorama in c:\\program files\\python311\\lib\\site-packages (from tqdm>=4.31.1->bert-score) (0.4.6)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\users\\764883\\appdata\\roaming\\python\\python311\\site-packages (from transformers>=3.0.0->bert-score) (0.19.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\program files\\python311\\lib\\site-packages (from transformers>=3.0.0->bert-score) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\764883\\appdata\\roaming\\python\\python311\\site-packages (from transformers>=3.0.0->bert-score) (2023.10.3)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\764883\\appdata\\roaming\\python\\python311\\site-packages (from transformers>=3.0.0->bert-score) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\764883\\appdata\\roaming\\python\\python311\\site-packages (from transformers>=3.0.0->bert-score) (0.4.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\program files\\python311\\lib\\site-packages (from matplotlib->bert-score) (1.0.7)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\program files\\python311\\lib\\site-packages (from matplotlib->bert-score) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\program files\\python311\\lib\\site-packages (from matplotlib->bert-score) (4.39.4)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\program files\\python311\\lib\\site-packages (from matplotlib->bert-score) (1.4.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\program files\\python311\\lib\\site-packages (from matplotlib->bert-score) (9.5.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\764883\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib->bert-score) (2.4.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\program files\\python311\\lib\\site-packages (from requests->bert-score) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\program files\\python311\\lib\\site-packages (from requests->bert-score) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\program files\\python311\\lib\\site-packages (from requests->bert-score) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\program files\\python311\\lib\\site-packages (from requests->bert-score) (2023.5.7)\n",
      "Requirement already satisfied: six>=1.5 in c:\\program files\\python311\\lib\\site-packages (from python-dateutil>=2.8.1->pandas>=1.0.1->bert-score) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\program files\\python311\\lib\\site-packages (from jinja2->torch>=1.0.0->bert-score) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\program files\\python311\\lib\\site-packages (from sympy->torch>=1.0.0->bert-score) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting datasets\n",
      "  Downloading datasets-2.15.0-py3-none-any.whl (521 kB)\n",
      "                                              0.0/521.2 kB ? eta -:--:--\n",
      "     ------------------------------         419.8/521.2 kB 8.7 MB/s eta 0:00:01\n",
      "     ------------------------------------- 521.2/521.2 kB 10.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\program files\\python311\\lib\\site-packages (from datasets) (1.23.5)\n",
      "Collecting pyarrow>=8.0.0 (from datasets)\n",
      "  Downloading pyarrow-14.0.1-cp311-cp311-win_amd64.whl (24.6 MB)\n",
      "                                              0.0/24.6 MB ? eta -:--:--\n",
      "     -                                        0.8/24.6 MB 17.2 MB/s eta 0:00:02\n",
      "     ---                                      2.1/24.6 MB 22.1 MB/s eta 0:00:02\n",
      "     ------                                   3.8/24.6 MB 26.9 MB/s eta 0:00:01\n",
      "     --------                                 5.4/24.6 MB 31.6 MB/s eta 0:00:01\n",
      "     -------------                            8.3/24.6 MB 35.5 MB/s eta 0:00:01\n",
      "     ------------------                      12.0/24.6 MB 50.4 MB/s eta 0:00:01\n",
      "     --------------------------              16.6/24.6 MB 81.8 MB/s eta 0:00:01\n",
      "     ----------------------------------     22.6/24.6 MB 108.8 MB/s eta 0:00:01\n",
      "     --------------------------------------  24.6/24.6 MB 93.9 MB/s eta 0:00:01\n",
      "     --------------------------------------  24.6/24.6 MB 93.9 MB/s eta 0:00:01\n",
      "     --------------------------------------- 24.6/24.6 MB 46.9 MB/s eta 0:00:00\n",
      "Collecting pyarrow-hotfix (from datasets)\n",
      "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in c:\\program files\\python311\\lib\\site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: pandas in c:\\program files\\python311\\lib\\site-packages (from datasets) (1.5.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\program files\\python311\\lib\\site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\program files\\python311\\lib\\site-packages (from datasets) (4.65.0)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.4.1-cp311-cp311-win_amd64.whl (29 kB)\n",
      "Collecting multiprocess (from datasets)\n",
      "  Downloading multiprocess-0.70.15-py311-none-any.whl (135 kB)\n",
      "                                              0.0/135.4 kB ? eta -:--:--\n",
      "     ---------------------------------------  133.1/135.4 kB ? eta -:--:--\n",
      "     -------------------------------------- 135.4/135.4 kB 2.7 MB/s eta 0:00:00\n",
      "Collecting fsspec[http]<=2023.10.0,>=2023.1.0 (from datasets)\n",
      "  Downloading fsspec-2023.10.0-py3-none-any.whl (166 kB)\n",
      "                                              0.0/166.4 kB ? eta -:--:--\n",
      "     -------------------------------------- 166.4/166.4 kB 9.8 MB/s eta 0:00:00\n",
      "Collecting aiohttp (from datasets)\n",
      "  Downloading aiohttp-3.9.1-cp311-cp311-win_amd64.whl (364 kB)\n",
      "                                              0.0/364.8 kB ? eta -:--:--\n",
      "     ------------------------------------- 364.8/364.8 kB 22.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: huggingface-hub>=0.18.0 in c:\\users\\764883\\appdata\\roaming\\python\\python311\\site-packages (from datasets) (0.19.4)\n",
      "Requirement already satisfied: packaging in c:\\program files\\python311\\lib\\site-packages (from datasets) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\program files\\python311\\lib\\site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\program files\\python311\\lib\\site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
      "  Downloading multidict-6.0.4-cp311-cp311-win_amd64.whl (28 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets)\n",
      "  Downloading yarl-1.9.4-cp311-cp311-win_amd64.whl (76 kB)\n",
      "                                              0.0/76.7 kB ? eta -:--:--\n",
      "     ---------------------------------------- 76.7/76.7 kB ? eta 0:00:00\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
      "  Downloading frozenlist-1.4.0-cp311-cp311-win_amd64.whl (44 kB)\n",
      "                                              0.0/44.9 kB ? eta -:--:--\n",
      "     ---------------------------------------- 44.9/44.9 kB 2.2 MB/s eta 0:00:00\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\764883\\appdata\\roaming\\python\\python311\\site-packages (from huggingface-hub>=0.18.0->datasets) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\program files\\python311\\lib\\site-packages (from huggingface-hub>=0.18.0->datasets) (4.6.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\program files\\python311\\lib\\site-packages (from requests>=2.19.0->datasets) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\program files\\python311\\lib\\site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\program files\\python311\\lib\\site-packages (from requests>=2.19.0->datasets) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\program files\\python311\\lib\\site-packages (from requests>=2.19.0->datasets) (2023.5.7)\n",
      "Requirement already satisfied: colorama in c:\\program files\\python311\\lib\\site-packages (from tqdm>=4.62.1->datasets) (0.4.6)\n",
      "Collecting dill<0.3.8,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
      "                                              0.0/115.3 kB ? eta -:--:--\n",
      "     ---------------------------------------- 115.3/115.3 kB ? eta 0:00:00\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\program files\\python311\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\program files\\python311\\lib\\site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\program files\\python311\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "Installing collected packages: xxhash, pyarrow-hotfix, pyarrow, multidict, fsspec, frozenlist, dill, yarl, multiprocess, aiosignal, aiohttp, datasets\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2023.12.2\n",
      "    Uninstalling fsspec-2023.12.2:\n",
      "      Successfully uninstalled fsspec-2023.12.2\n",
      "Successfully installed aiohttp-3.9.1 aiosignal-1.3.1 datasets-2.15.0 dill-0.3.7 frozenlist-1.4.0 fsspec-2023.10.0 multidict-6.0.4 multiprocess-0.70.15 pyarrow-14.0.1 pyarrow-hotfix-0.6 xxhash-3.4.1 yarl-1.9.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script datasets-cli.exe is installed in 'C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collecting rouge-score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: absl-py in c:\\program files\\python311\\lib\\site-packages (from rouge-score) (1.4.0)\n",
      "Requirement already satisfied: nltk in c:\\users\\764883\\appdata\\roaming\\python\\python311\\site-packages (from rouge-score) (3.8.1)\n",
      "Requirement already satisfied: numpy in c:\\program files\\python311\\lib\\site-packages (from rouge-score) (1.23.5)\n",
      "Requirement already satisfied: six>=1.14.0 in c:\\program files\\python311\\lib\\site-packages (from rouge-score) (1.16.0)\n",
      "Requirement already satisfied: click in c:\\program files\\python311\\lib\\site-packages (from nltk->rouge-score) (8.1.3)\n",
      "Requirement already satisfied: joblib in c:\\program files\\python311\\lib\\site-packages (from nltk->rouge-score) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\764883\\appdata\\roaming\\python\\python311\\site-packages (from nltk->rouge-score) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\program files\\python311\\lib\\site-packages (from nltk->rouge-score) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\program files\\python311\\lib\\site-packages (from click->nltk->rouge-score) (0.4.6)\n",
      "Building wheels for collected packages: rouge-score\n",
      "  Building wheel for rouge-score (setup.py): started\n",
      "  Building wheel for rouge-score (setup.py): finished with status 'done'\n",
      "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24972 sha256=af73cb5dac29a70d71c3ec29eff9f9ee20a753fd9d717c7f1762e61caaa45da5\n",
      "  Stored in directory: c:\\users\\764883\\appdata\\local\\pip\\cache\\wheels\\1e\\19\\43\\8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\n",
      "Successfully built rouge-score\n",
      "Installing collected packages: rouge-score\n",
      "Successfully installed rouge-score-0.1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install rouge\n",
    "!pip install bert_score\n",
    "!pip install sentencepiece\n",
    "!pip install torch\n",
    "!pip install beautifulsoup4\n",
    "!pip install extract-msg\n",
    "!pip install py7zr\n",
    "!pip install pandas matplotlib\n",
    "!pip install textblob\n",
    "!pip install clean-text\n",
    "!pip install bert-score\n",
    "!pip install datasets\n",
    "!pip install rouge-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43d9ea16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in c:\\users\\764883\\appdata\\roaming\\python\\python311\\site-packages (2.1.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\764883\\appdata\\roaming\\python\\python311\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\program files\\python311\\lib\\site-packages (from torch) (4.6.3)\n",
      "Requirement already satisfied: sympy in c:\\program files\\python311\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\program files\\python311\\lib\\site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\program files\\python311\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\764883\\appdata\\roaming\\python\\python311\\site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\program files\\python311\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\program files\\python311\\lib\\site-packages (from sympy->torch) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9998a72f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting rouge_score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: absl-py in c:\\program files\\python311\\lib\\site-packages (from rouge_score) (1.4.0)\n",
      "Requirement already satisfied: nltk in c:\\users\\764883\\appdata\\roaming\\python\\python311\\site-packages (from rouge_score) (3.8.1)\n",
      "Requirement already satisfied: numpy in c:\\program files\\python311\\lib\\site-packages (from rouge_score) (1.23.5)\n",
      "Requirement already satisfied: six>=1.14.0 in c:\\program files\\python311\\lib\\site-packages (from rouge_score) (1.16.0)\n",
      "Requirement already satisfied: click in c:\\program files\\python311\\lib\\site-packages (from nltk->rouge_score) (8.1.3)\n",
      "Requirement already satisfied: joblib in c:\\program files\\python311\\lib\\site-packages (from nltk->rouge_score) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\764883\\appdata\\roaming\\python\\python311\\site-packages (from nltk->rouge_score) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\program files\\python311\\lib\\site-packages (from nltk->rouge_score) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\program files\\python311\\lib\\site-packages (from click->nltk->rouge_score) (0.4.6)\n",
      "Building wheels for collected packages: rouge_score\n",
      "  Building wheel for rouge_score (setup.py): started\n",
      "  Building wheel for rouge_score (setup.py): finished with status 'done'\n",
      "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24972 sha256=f6a71b7a913b1ac770e92823333efa84f5a5e9bd156c96b6ea7418ec54fa0bfe\n",
      "  Stored in directory: c:\\users\\764883\\appdata\\local\\pip\\cache\\wheels\\1e\\19\\43\\8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\n",
      "Successfully built rouge_score\n",
      "Installing collected packages: rouge_score\n",
      "Successfully installed rouge_score-0.1.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install rouge_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "718fa62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7c62120",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from transformers import T5Tokenizer\n",
    "from dateutil.parser import parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9dab8c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\764883\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\764883\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\764883\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\764883\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\764883\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\764883\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\764883\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\764883\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\764883\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\764883\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\764883\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\764883\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "from tqdm.auto import tqdm\n",
    "from time import sleep\n",
    "import email\n",
    "import logging\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk import download\n",
    "download('stopwords')\n",
    "download('punkt')\n",
    "download('brown')\n",
    "download('averaged_perceptron_tagger')\n",
    "from pathlib import Path\n",
    "from cleantext import clean \n",
    "import logging\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "import math\n",
    "import pickle\n",
    "from collections import Counter\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import operator\n",
    "import torch\n",
    "from datetime import datetime\n",
    "import json\n",
    "import os\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import math\n",
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "import extract_msg\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "import re\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from bs4 import BeautifulSoup\n",
    "import email\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from textblob import TextBlob\n",
    "import pandas as pd\n",
    "from cleantext import clean\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from sklearn.cluster import KMeans\n",
    "from transformers import BertForSequenceClassification, AdamW\n",
    "from sklearn.cluster import KMeans\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from transformers import BertForSequenceClassification, AdamW\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset\n",
    "import email\n",
    "import logging\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk import download\n",
    "from string import punctuation\n",
    "from pathlib import Path\n",
    "from cleantext import clean  \n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import pandas as pd\n",
    "import json\n",
    "from transformers import AutoTokenizer, LongT5ForConditionalGeneration\n",
    "import torch\n",
    "import sentencepiece\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset, DatasetDict\n",
    "from time import sleep\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset as HFDataset, load_metric\n",
    "from bert_score import score as bert_score\n",
    "from torch.utils.data import TensorDataset, RandomSampler, DataLoader\n",
    "from transformers import T5ForConditionalGeneration\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import TrainerCallback\n",
    "from rouge_score import rouge_scorer, scoring\n",
    "from bert_score import score\n",
    "import torch\n",
    "from transformers import TrainingArguments, Trainer, T5ForConditionalGeneration, AdamW\n",
    "import re\n",
    "from transformers import T5Tokenizer\n",
    "from dateutil.parser import parse\n",
    "from transformers import TrainerCallback\n",
    "import numpy as np\n",
    "from rouge_score import rouge_scorer, scoring\n",
    "from bert_score import score\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from transformers import AutoTokenizer, T5ForConditionalGeneration\n",
    "import sentencepiece\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict, load_metric\n",
    "from time import sleep\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, TensorDataset\n",
    "from datasets import Dataset, load_metric\n",
    "from transformers import AutoTokenizer, LongT5ForConditionalGeneration\n",
    "import sentencepiece\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import Dataset, DatasetDict\n",
    "from time import sleep\n",
    "from bert_score import score as bert_score\n",
    "from torch.utils.data import TensorDataset, RandomSampler, DataLoader\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import Dataset as HFDataset  \n",
    "import nltk\n",
    "import networkx as nx\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import sentencepiece \n",
    "from rouge_score import rouge_scorer\n",
    "from bert_score import score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5b27a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from transformers import RobertaModel\n",
    "\n",
    "# Suppressing warnings from the transformers library\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ede0328",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "586d886f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddae66ca",
   "metadata": {},
   "source": [
    "## Loading in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a94a517",
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_json('sum_anno_human_proper.json', lines= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8675d1b8",
   "metadata": {},
   "source": [
    "## Preprocessing for input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2b14fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To convert html text to proper\n",
    "\n",
    "def convert_html_to_text(html):\n",
    "    # Use BeautifulSoup to parse HTML and extract text\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    text = soup.get_text(separator=\" \")\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "def standardize_date_time(text):\n",
    "\n",
    "    # Function to standardize dates\n",
    "    def replace_date(match):\n",
    "        try:\n",
    "            date = parse(match.group(), fuzzy=True)\n",
    "            return date.strftime(\"%Y-%m-%d\")\n",
    "        except ValueError:\n",
    "            # Return the original string if parsing fails\n",
    "            return match.group()\n",
    "\n",
    "    # Function to standardize times\n",
    "    def replace_time(match):\n",
    "        try:\n",
    "            time = parse(match.group(), fuzzy=True)\n",
    "            return time.strftime(\"%H:%M\")\n",
    "        except ValueError:\n",
    "            # Return the original string if parsing fails\n",
    "            return match.group()\n",
    "\n",
    "    # Standardize dates\n",
    "    text = re.sub(r'\\b\\d{1,2}[-/]\\d{1,2}[-/]\\d{2,4}\\b', replace_date, text)\n",
    "    # Standardize times\n",
    "    text = re.sub(r'\\b\\d{1,2}:\\d{2}\\b', replace_time, text)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def preprocess_email_for_t5(email_body):\n",
    "    # Convert HTML to text\n",
    "    email_body = convert_html_to_text(email_body)\n",
    "\n",
    "    # Remove extra whitespaces\n",
    "    email_body = ' '.join(email_body.split())\n",
    "\n",
    "    # Remove common greeting texts\n",
    "    greetings_pattern = (\n",
    "    r\"Hi\\s\\w+|\"\n",
    "    r\"Hello\\s\\w+|\"\n",
    "    r\"Dear\\s\\w+|\"\n",
    "    r\"Dear\\s[Mm]r\\.\\s\\w+|\"\n",
    "    r\"Dear\\s[Mm]rs\\.\\s\\w+|\"\n",
    "    r\"Dear\\s[Mm]s\\.\\s\\w+|\"\n",
    "    r\"Dear\\s[MD]r(s)?\\.\\s\\w+|\"\n",
    "    r\"Dear\\sProf\\.\\s\\w+|\"\n",
    "    r\"Dear\\sDoctor\\.\\s\\w+|\"\n",
    "    r\"Greetings|\"\n",
    "    r\"Good\\s[Mm]orning|\"\n",
    "    r\"Good\\s[Aa]fternoon|\"\n",
    "    r\"Good\\s[Ee]vening|\"\n",
    "    r\"Hey\\s\\w+|\"\n",
    "    r\"Hey\\sthere|\"\n",
    "    r\"Hello\\severyone|\"\n",
    "    r\"Hope\\syou\\sare\\sdoing\\swell|\"\n",
    "    r\"Hope\\syou\\sare\\sdoing\\swell|\"\n",
    "    r\"Hope\\sthis\\semail\\sfinds\\syou\\swell|\"\n",
    "    r\"Hope\\sthis\\semail\\sfinds\\syou\\swell|\"\n",
    "    r\"Hope\\sthis\\semail\\sfinds\\syou\\sin\\sgood\\shealth|\"\n",
    "    r\"How\\sare\\syou\\sdoing|\"\n",
    "    r\"How\\sis\\sit\\sgoing|\"\n",
    "    r\"To\\swhom\\sit\\smay\\sconcern|\"\n",
    "    r\"To\\swhom\\sit\\smay\\sconcern|\"\n",
    "    r\"I\\sam\\swriting\\sto\\syou\\sbecause|\"\n",
    "    r\"I\\sam\\swriting\\sto\\syou\\sto|\"\n",
    "    r\"I\\shope\\sthat\\syou|\"\n",
    "    r\"I\\swanted\\sto\\sreach\\sout\\sto|\"\n",
    "    r\"I\\swanted\\sto\\slet\\syou\\sknow|\"\n",
    "    r\"I\\swould\\slike\\sto\\sinform\\syou|\"\n",
    "    r\"It\\spleases\\sme\\sto\\scontact\\syou|\"\n",
    "    r\"It\\shas\\scome\\sto\\smy\\sattention|\"\n",
    "    r\"I\\swas\\sjust\\sthinking\\sabout\\syou\\sand\\s|\"\n",
    "    r\"Allow\\sme\\sto\\sintroduce\\smyself|\"\n",
    "    r\"I\\shope\\sthat\\severything\\sis\\sgoing\\swell|\"\n",
    "    r\"Thank\\syou\\sfor\\syour\\semail|\"\n",
    "    r\"Thank\\syou\\sfor\\sreaching\\sout\")\n",
    "\n",
    "    email_body = re.sub(greetings_pattern, \"\", email_body, flags=re.IGNORECASE)\n",
    "\n",
    "    # Remove common sign-offs\n",
    "    signoffs_pattern = (\n",
    "    r\"Best\\sregards|\"\n",
    "    r\"Best\\s\\w+|\"\n",
    "    r\"Sincerely|\"\n",
    "    r\"Yours\\sfaithfully|\"\n",
    "    r\"Yours\\ssincerely|\"\n",
    "    r\"Regards|\"\n",
    "    r\"Warm\\sregards|\"\n",
    "    r\"Kind\\sregards|\"\n",
    "    r\"Cheers|\"\n",
    "    r\"Thanks\\sand\\sregards|\"\n",
    "    r\"Thank\\syou|\"\n",
    "    r\"Take\\scare|\"\n",
    "    r\"Looking\\sforward|\"\n",
    "    r\"All\\sbest|\"\n",
    "    r\"Best\\swishes|\"\n",
    "    r\"Best|\"\n",
    "    r\"Yours\\struly|\"\n",
    "    r\"Cordially|\"\n",
    "    r\"With\\sappreciation|\"\n",
    "    r\"Respectfully|\"\n",
    "    r\"With\\sregards|\"\n",
    "    r\"Many\\sthanks|\"\n",
    "    r\"Hope\\sto\\shear\\sfrom\\syou\\ssoon|\"\n",
    "    r\"Until\\snext\\stime|\"\n",
    "    r\"Yours\\svery\\struly|\"\n",
    "    r\"Yours|\"\n",
    "    r\"In\\sgratitude|\"\n",
    "    r\"In\\ssympathy|\"\n",
    "    r\"Thoughtfully|\"\n",
    "    r\"With\\saffection|\"\n",
    "    r\"Fond\\sregards|\"\n",
    "    r\"With\\santicipation|\"\n",
    "    r\"Stay\\swell|\"\n",
    "    r\"Stay\\ssafe|\"\n",
    "    r\"Peace|\"\n",
    "    r\"God\\sbless|\"\n",
    "    r\"Love|\"\n",
    "    r\"Sent\\sfrom\\smy\\s\\w+|\"\n",
    "    r\"Sent\\sfrom\\smy\\s\\w+\\s\\w+|\"\n",
    "    r\"Talk\\sto\\syou\\ssoon|\"\n",
    "    r\"See\\syou\\ssoon|\"\n",
    "    r\"See\\sya|\"\n",
    "    r\"Ciao|\"\n",
    "    r\"Adieu|\"\n",
    "    r\"Farewell|\"\n",
    "    r\"Good\\sbye|\"\n",
    "    r\"Bye\\sfor\\snow|\"\n",
    "    r\"Signing\\soff|\"\n",
    "    r\"Out|\"\n",
    "    r\"Yours\\s[in]\\s\\w+|\"\n",
    "    r\"Your\\sfriend|\"\n",
    "    r\"Your\\s\\w+\\s\\w+|\"\n",
    "    r\"Keep\\sin\\stouch|\"\n",
    "    r\"Yours\\sfaithfully|\"\n",
    "    r\"Yours\\ssincerely|\"\n",
    "    r\"Yours\\sobediently|\"\n",
    "    r\"Yours\\saffectionately|\"\n",
    "    r\"Yours\\scordially|\"\n",
    "    r\"Yours\\srespectfully|\"\n",
    "    r\"Yours\\struly|\"\n",
    "    r\"Yours\\sever\")\n",
    "\n",
    "    email_body = re.sub(signoffs_pattern, \"\", email_body, flags=re.IGNORECASE)\n",
    "\n",
    "    # Pattern to remove repetitive characters like dashes or underscores\n",
    "    repetitive_pattern = r\"[-_]{3,}\"  \n",
    "    email_body = re.sub(repetitive_pattern, \"\", email_body)\n",
    "\n",
    "    # Standardize dates and times\n",
    "    email_body = standardize_date_time(email_body)\n",
    "\n",
    "    # Remove email signatures and disclaimers\n",
    "    email_body = re.sub(r\"--\\s*[\\s\\S]*$\", \"\", email_body)\n",
    "\n",
    "    # Standardize email addresses and URLs\n",
    "    email_body = re.sub(r\"\\S+@\\S+\\.\\S+\", \"<email>\", email_body)\n",
    "    email_body = re.sub(r\"http\\S+\", \"<url>\", email_body)\n",
    "\n",
    "    # Remove phrases indicating difficulty in viewing images or links\n",
    "    irrelevant_phrases_pattern = (\n",
    "    r\"difficulty in viewing this image|\"\n",
    "    r\"click here|\"\n",
    "    r\"having trouble viewing this|\"\n",
    "    r\"view this email in your browser|\"\n",
    "    r\"to ensure delivery to your inbox|\"\n",
    "    r\"if you cannot see this message|\"\n",
    "    r\"message not displaying correctly|\"\n",
    "    r\"trouble seeing this email|\"\n",
    "    r\"can't see the images below|\"\n",
    "    r\"email not looking quite right|\"\n",
    "    r\"viewing this email on a mobile device|\"\n",
    "    r\"can't read this email|\"\n",
    "    r\"images not showing up|\"\n",
    "    r\"to view the online version of this email|\"\n",
    "    r\"email doesn't display correctly|\"\n",
    "    r\"problems seeing this email|\"\n",
    "    r\"to unsubscribe or change preferences|\"\n",
    "    r\"this message was sent to <email>|\"\n",
    "    r\"not interested in these emails|\"\n",
    "    r\"you're receiving this email because|\"\n",
    "    r\"to stop receiving these emails|\"\n",
    "    r\"unsubscribe from this list|\"\n",
    "    r\"manage your email preferences\")\n",
    "    email_body = re.sub(irrelevant_phrases_pattern, \"\", email_body, flags=re.IGNORECASE)\n",
    "\n",
    "    \n",
    "\n",
    "    return email_body\n",
    "\n",
    "# Apply the preprocessing function to the 'body'\n",
    "df['Preprocessed_Body'] = df['body'].apply(lambda x: preprocess_email_for_t5(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b66e2f9",
   "metadata": {},
   "source": [
    "## TextRank algorithm graph based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b88a94e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\764883\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\764883\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\764883\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\764883\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download required NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def sentence_similarity(sent1, sent2, stopwords=None):\n",
    "    if stopwords is None:\n",
    "        stopwords = []\n",
    "\n",
    "    sent1 = [w.lower() for w in word_tokenize(sent1)]\n",
    "    sent2 = [w.lower() for w in word_tokenize(sent2)]\n",
    "\n",
    "    all_words = list(set(sent1 + sent2))\n",
    "\n",
    "    vector1 = [0] * len(all_words)\n",
    "    vector2 = [0] * len(all_words)\n",
    "\n",
    "    # Build the vector for the first sentence\n",
    "    for w in sent1:\n",
    "        if w not in stopwords:\n",
    "            vector1[all_words.index(w)] += 1\n",
    "\n",
    "    # Build the vector for the second sentence\n",
    "    for w in sent2:\n",
    "        if w not in stopwords:\n",
    "            vector2[all_words.index(w)] += 1\n",
    "\n",
    "    return cosine_similarity(np.array([vector1, vector2]))[0, 1]\n",
    "\n",
    "def generate_summary(text, summary_ratio=0.3):\n",
    "    stop_words = stopwords.words('english')\n",
    "    sentences = sent_tokenize(text)\n",
    "\n",
    "    # Adjust the number of sentences in the summary based on the length of the text\n",
    "    top_n = max(1, int(len(sentences) * summary_ratio))\n",
    "\n",
    "    # Generate similarity matrix across sentences\n",
    "    similarity_matrix = np.zeros((len(sentences), len(sentences)))\n",
    "    for idx1 in range(len(sentences)):\n",
    "        for idx2 in range(len(sentences)):\n",
    "            if idx1 != idx2:\n",
    "                similarity_matrix[idx1][idx2] = sentence_similarity(sentences[idx1], sentences[idx2], stop_words)\n",
    "\n",
    "    # Rank sentences in similarity matrix\n",
    "    sentence_similarity_graph = nx.from_numpy_array(similarity_matrix)\n",
    "    scores = nx.pagerank(sentence_similarity_graph)\n",
    "\n",
    "    # Sort the rank and pick top sentences\n",
    "    ranked_sentences = sorted(((scores[i], s) for i, s in enumerate(sentences)), reverse=True)\n",
    "    selected_sentences = [ranked_sentences[i][1] for i in range(top_n)]\n",
    "    \n",
    "    return \" \".join(selected_sentences)\n",
    "\n",
    "\n",
    "\n",
    "# Apply TextRank summarization to each row in the DataFrame\n",
    "df['Summary_txt_grph_2'] = df['Preprocessed_Body'].apply(lambda x: generate_summary(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "084f50f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>from</th>\n",
       "      <th>to</th>\n",
       "      <th>date</th>\n",
       "      <th>body</th>\n",
       "      <th>Body_Length</th>\n",
       "      <th>Subject_Length</th>\n",
       "      <th>Cleaned_Body</th>\n",
       "      <th>Cleaned_Subject</th>\n",
       "      <th>BERT_Embeddings</th>\n",
       "      <th>...</th>\n",
       "      <th>summary_TXTRNK_1</th>\n",
       "      <th>Summary</th>\n",
       "      <th>summary_BART</th>\n",
       "      <th>index_number</th>\n",
       "      <th>Tokenized_Email</th>\n",
       "      <th>Entities</th>\n",
       "      <th>Cluster_retrieved</th>\n",
       "      <th>Summary_human</th>\n",
       "      <th>Preprocessed_Body</th>\n",
       "      <th>Summary_txt_grph_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Thank you for attending Microsoft Ignite</td>\n",
       "      <td>Microsoft Team &lt;microsoft@email.microsoft.com&gt;</td>\n",
       "      <td>richard.potter@raddsolutions.co.uk</td>\n",
       "      <td>Wed, 19 Oct 2022 20:31:34 +0100</td>\n",
       "      <td>...</td>\n",
       "      <td>6232</td>\n",
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td>Thank you for attending Microsoft Ignite</td>\n",
       "      <td>[-0.059376951307058334, 0.17135855555534363, 0...</td>\n",
       "      <td>...</td>\n",
       "      <td>microsoft ignite may be over but heres your ch...</td>\n",
       "      <td>microsoft ignite may be over but heres your ch...</td>\n",
       "      <td>Learn how microsoft empowers organisations to ...</td>\n",
       "      <td>1543</td>\n",
       "      <td>{'input_ids': tensor([[  101,  7513, 16270,  4...</td>\n",
       "      <td>[('microsoft', 'ORG'), ('2022', 'DATE'), ('mic...</td>\n",
       "      <td>0</td>\n",
       "      <td>The email discusses post-Microsoft Ignite 2022...</td>\n",
       "      <td>Microsoft Ignite may be over, but heres  cont...</td>\n",
       "      <td>Dive into the new innovations we announced at ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Invitation to learn how Windows 365 Cloud PCs ...</td>\n",
       "      <td>Microsoft &lt;replyto@email.microsoft.com&gt;</td>\n",
       "      <td>richard.potter@raddsolutions.co.uk</td>\n",
       "      <td>Tue, 01 Nov 2022 11:01:50 +0000</td>\n",
       "      <td>Webinar with demos of Windows 365 and vision f...</td>\n",
       "      <td>2943</td>\n",
       "      <td>90</td>\n",
       "      <td>webinar with demos of windows 365 and vision f...</td>\n",
       "      <td>Invitation to learn how Windows 365 Cloud PCs ...</td>\n",
       "      <td>[-0.1439182013273239, 0.22149936854839325, 0.6...</td>\n",
       "      <td>...</td>\n",
       "      <td>No Summary</td>\n",
       "      <td></td>\n",
       "      <td>webinar with demos of windows 365 and vision f...</td>\n",
       "      <td>765</td>\n",
       "      <td>{'input_ids': tensor([[  101,  4773,  3981,  2...</td>\n",
       "      <td>[('thursday 17th', 'DATE'), ('2022 1400  1500'...</td>\n",
       "      <td>0</td>\n",
       "      <td>Webinar Announcement: \"Windows 365 for Your Hy...</td>\n",
       "      <td>Webinar with demos of Windows 365 and vision f...</td>\n",
       "      <td>* Updates from Ignite, including the built-in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dont fall behind  embrace AI with Dell Techn...</td>\n",
       "      <td>Dell Technologies Partner Program &lt;DellTechnol...</td>\n",
       "      <td>richard.potter@raddsolutions.co.uk</td>\n",
       "      <td>Tue, 15 Nov 2022 06:01:17 +0000</td>\n",
       "      <td>&lt;https://click.comm.delltechnologies.com/open...</td>\n",
       "      <td>4498</td>\n",
       "      <td>64</td>\n",
       "      <td>\\n   \\t\\r\\nview online \\n\\n  \\t\\r\\n \\t\\r\\nwhy...</td>\n",
       "      <td>Dont fall behind  embrace AI with Dell Technol...</td>\n",
       "      <td>[-0.15142026543617249, 0.11411778628826141, 0....</td>\n",
       "      <td>...</td>\n",
       "      <td>we are witnessing and living through the rise ...</td>\n",
       "      <td>why ai and why now\\nwe are witnessing and livi...</td>\n",
       "      <td>Artificial intelligence ai market is forecast ...</td>\n",
       "      <td>369</td>\n",
       "      <td>{'input_ids': tensor([[  101,  3193,  3784,  2...</td>\n",
       "      <td>[('500 billion', 'MONEY'), ('20231', 'DATE'), ...</td>\n",
       "      <td>0</td>\n",
       "      <td>The email discusses the rapid growth of artifi...</td>\n",
       "      <td>View Online Why AI and why now ? Why AI and wh...</td>\n",
       "      <td>No. . Servicenow, 2019. IDC, Feb 2022. Reg. De...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Microsoft Envision Season 3 begins December 13</td>\n",
       "      <td>Microsoft Team &lt;microsoft@email.microsoft.com&gt;</td>\n",
       "      <td>richard.potter@raddsolutions.co.uk</td>\n",
       "      <td>Wed, 09 Nov 2022 17:05:09 +0000</td>\n",
       "      <td>Episode 1 airs December 13, 2022 \\r\\nHaving tr...</td>\n",
       "      <td>3476</td>\n",
       "      <td>46</td>\n",
       "      <td>episode 1 airs december 13 2022 \\r\\nhaving tro...</td>\n",
       "      <td>Microsoft Envision Season 3 begins December 13</td>\n",
       "      <td>[-0.36103835701942444, 0.06514844298362732, 0....</td>\n",
       "      <td>...</td>\n",
       "      <td>episode 1 airs december 13 2022\\nregister now ...</td>\n",
       "      <td>episode 1 airs december 13 2022\\nregister now ...</td>\n",
       "      <td>register now for microsoft envision season 3. ...</td>\n",
       "      <td>895</td>\n",
       "      <td>{'input_ids': tensor([[  101,  2792,  1015, 14...</td>\n",
       "      <td>[('1', 'CARDINAL'), ('13 2022', 'DATE'), ('mic...</td>\n",
       "      <td>0</td>\n",
       "      <td>The email announces the premiere of Microsoft ...</td>\n",
       "      <td>Episode 1 airs December 13, 2022  email? | Vie...</td>\n",
       "      <td>Register now for Microsoft Envision Season 3 T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In September, you had 71 users visit your webs...</td>\n",
       "      <td>Google Analytics &lt;analytics-noreply@google.com&gt;</td>\n",
       "      <td>richard.potter@raddsolutions.co.uk</td>\n",
       "      <td>Tue, 11 Oct 2022 06:02:01 +0100</td>\n",
       "      <td>&lt;https://www.google.com/images/branding/googl...</td>\n",
       "      <td>5559</td>\n",
       "      <td>68</td>\n",
       "      <td>\\n \\r\\nuniversal analytics will no longer pr...</td>\n",
       "      <td>In September you had 71 users visit your websi...</td>\n",
       "      <td>[-0.10645909607410431, 0.17022578418254852, 0....</td>\n",
       "      <td>...</td>\n",
       "      <td>81 35 bounce rate\\nbreakdown of visitors acqui...</td>\n",
       "      <td>81 35 bounce rate\\nbreakdown of visitors acqui...</td>\n",
       "      <td>universal analytics will no longer process new...</td>\n",
       "      <td>740</td>\n",
       "      <td>{'input_ids': tensor([[  101,  5415, 25095,  2...</td>\n",
       "      <td>[('2023', 'DATE'), ('4', 'CARDINAL'), ('septem...</td>\n",
       "      <td>0</td>\n",
       "      <td>Starting in 2023, Universal Analytics will no ...</td>\n",
       "      <td>Universal Analytics will no longer process new...</td>\n",
       "      <td>Starting in 2023, Universal Analytics will no ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>949</th>\n",
       "      <td>Our Black Friday offers have landed!</td>\n",
       "      <td>IT Governance &lt;emailsupport@itgovernance.co.uk&gt;</td>\n",
       "      <td>richie.wynne@raddsolutions.co.uk</td>\n",
       "      <td>Mon, 21 Nov 2022 11:05:15 +0000</td>\n",
       "      <td>Youre not going to want to miss these savings...</td>\n",
       "      <td>3228</td>\n",
       "      <td>36</td>\n",
       "      <td>youre not going to want to miss these savings ...</td>\n",
       "      <td>Our Black Friday offers have landed</td>\n",
       "      <td>[0.09008561074733734, 0.16759826242923737, 0.6...</td>\n",
       "      <td>...</td>\n",
       "      <td>use promo code bf25\\nuse promo code bf25\\nunit...</td>\n",
       "      <td>were starting our black friday offers early wi...</td>\n",
       "      <td>were starting our black friday offers early wi...</td>\n",
       "      <td>1130</td>\n",
       "      <td>{'input_ids': tensor([[  101,  2115,  2063,  2...</td>\n",
       "      <td>[('friday', 'DATE'), ('25', 'CARDINAL'), ('25'...</td>\n",
       "      <td>7</td>\n",
       "      <td>The email advertises an early Black Friday off...</td>\n",
       "      <td>Youre not going to want to miss these savings...</td>\n",
       "      <td>Use promo code BF25 Shop now We accept Get in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>950</th>\n",
       "      <td>Jeff Fritz reviews IronPDF vs SyncFusion, iTex...</td>\n",
       "      <td>\"Jacob, Head of Engineering\" &lt;developers@irons...</td>\n",
       "      <td>Richard Potter &lt;richard.potter@raddsolutions.c...</td>\n",
       "      <td>Tue, 13 Dec 2022 15:31:28 +0000</td>\n",
       "      <td>&lt;https://ironsoftware.lt.acemlnb.com/Prod/lin...</td>\n",
       "      <td>8587</td>\n",
       "      <td>55</td>\n",
       "      <td>\\t \\r\\n \\t \\r\\nhi richard\\r\\nimagine spendin...</td>\n",
       "      <td>Jeff Fritz reviews IronPDF vs SyncFusion iText...</td>\n",
       "      <td>[-0.20470425486564636, 0.20281416177749634, 0....</td>\n",
       "      <td>...</td>\n",
       "      <td>would your project fail hopefully not but it n...</td>\n",
       "      <td>imagine spending a lot of money on software li...</td>\n",
       "      <td>iron software is a free open source solution t...</td>\n",
       "      <td>783</td>\n",
       "      <td>{'input_ids': tensor([[  101,  7632,  2957,  5...</td>\n",
       "      <td>[('jeff fritz', 'PERSON'), ('net conf', 'ORG')...</td>\n",
       "      <td>7</td>\n",
       "      <td>Jeff Fritz from .NET Conf reviewed IronPDF aga...</td>\n",
       "      <td>, Imagine spending a lot of money on software ...</td>\n",
       "      <td>Sent to: &lt;email&gt; Iron Software, 205 N. Michiga...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>951</th>\n",
       "      <td>Don't Miss Out: Help to Grow: Digital Ends in ...</td>\n",
       "      <td>Zym &lt;rebecca@zymplify.com&gt;</td>\n",
       "      <td>Richard Potter &lt;richard.potter@raddsolutions.c...</td>\n",
       "      <td>Tue, 17 Jan 2023 12:01:49 +0000</td>\n",
       "      <td>ZYM helps business owners understand their mar...</td>\n",
       "      <td>6966</td>\n",
       "      <td>56</td>\n",
       "      <td>zym helps business owners understand their mar...</td>\n",
       "      <td>Dont Miss Out Help to Grow Digital Ends in 16 ...</td>\n",
       "      <td>[0.0035861318465322256, 0.07786554843187332, 0...</td>\n",
       "      <td>...</td>\n",
       "      <td>you may qualify for the grant meaning you coul...</td>\n",
       "      <td>even better you can try zym for free for 14 da...</td>\n",
       "      <td>zym helps business owners understand their mar...</td>\n",
       "      <td>345</td>\n",
       "      <td>{'input_ids': tensor([[  101,  1062, 24335,  7...</td>\n",
       "      <td>[('uk', 'GPE'), ('up to 5000', 'CARDINAL'), ('...</td>\n",
       "      <td>7</td>\n",
       "      <td>Zym aids business owners in comprehending mark...</td>\n",
       "      <td>ZYM helps business owners understand their mar...</td>\n",
       "      <td>, Zym P.S. Copyright  2023 Zym AI, All rights...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>952</th>\n",
       "      <td>New videos coming in 2023</td>\n",
       "      <td>Clear Measure &lt;clearmeasure@clear-measure.com&gt;</td>\n",
       "      <td>richard.potter@raddsolutions.co.uk</td>\n",
       "      <td>Thu, 29 Dec 2022 10:00:02 +0000</td>\n",
       "      <td>New videos coming in 2023  made to empower yo...</td>\n",
       "      <td>3404</td>\n",
       "      <td>25</td>\n",
       "      <td>new videos coming in 2023  made to empower you...</td>\n",
       "      <td>New videos coming in 2023</td>\n",
       "      <td>[-0.08648061007261276, 0.11852650344371796, 0....</td>\n",
       "      <td>...</td>\n",
       "      <td>our new videos coming in 2023 are made to empo...</td>\n",
       "      <td>our new videos coming in 2023 are made to empo...</td>\n",
       "      <td>new videos coming in 2023  made to empower you...</td>\n",
       "      <td>958</td>\n",
       "      <td>{'input_ids': tensor([[  101,  2047,  6876,  2...</td>\n",
       "      <td>[('2023', 'DATE'), ('2023', 'DATE'), ('10815',...</td>\n",
       "      <td>7</td>\n",
       "      <td>Summary:\\n\\nClear Measure has announced an upc...</td>\n",
       "      <td>New videos coming in 2023  made to empower . ...</td>\n",
       "      <td>Our new videos coming in 2023 are made to empo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>953</th>\n",
       "      <td>Business PCs up to 40% off</td>\n",
       "      <td>Lenovo New beginnings! &lt;lenovo@ecomm.lenovo.com&gt;</td>\n",
       "      <td>richard.potter@raddsolutions.co.uk</td>\n",
       "      <td>Wed, 01 Feb 2023 09:04:42 +0000</td>\n",
       "      <td>&lt;https://trk.ecomm.lenovo.com/ss/o/k8leYzsS8M...</td>\n",
       "      <td>8108</td>\n",
       "      <td>26</td>\n",
       "      <td>\\n \\t\\r\\n\\tview it in browser instead \\n  fre...</td>\n",
       "      <td>Business PCs up to 40 off</td>\n",
       "      <td>[-0.09954569488763809, 0.13386352360248566, 0....</td>\n",
       "      <td>...</td>\n",
       "      <td>workspace refresh with up to 40 discount until...</td>\n",
       "      <td>thinkpad p1 gen 4\\nthinkpad z13 gen 1\\nideapad...</td>\n",
       "      <td>Free shipping on all orders with up to 40% dis...</td>\n",
       "      <td>132</td>\n",
       "      <td>{'input_ids': tensor([[  101,  3193,  2009,  1...</td>\n",
       "      <td>[('up to 40', 'CARDINAL'), ('up to 40', 'CARDI...</td>\n",
       "      <td>7</td>\n",
       "      <td>Email from Lenovo announces a workspace refres...</td>\n",
       "      <td>View it in browser instead Free shipping on al...</td>\n",
       "      <td>Lenovo and the Lenovo logo, ThinkPad and ideap...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>954 rows  23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               subject  \\\n",
       "0             Thank you for attending Microsoft Ignite   \n",
       "1    Invitation to learn how Windows 365 Cloud PCs ...   \n",
       "2    Dont fall behind  embrace AI with Dell Techn...   \n",
       "3       Microsoft Envision Season 3 begins December 13   \n",
       "4    In September, you had 71 users visit your webs...   \n",
       "..                                                 ...   \n",
       "949               Our Black Friday offers have landed!   \n",
       "950  Jeff Fritz reviews IronPDF vs SyncFusion, iTex...   \n",
       "951  Don't Miss Out: Help to Grow: Digital Ends in ...   \n",
       "952                          New videos coming in 2023   \n",
       "953                         Business PCs up to 40% off   \n",
       "\n",
       "                                                  from  \\\n",
       "0       Microsoft Team <microsoft@email.microsoft.com>   \n",
       "1              Microsoft <replyto@email.microsoft.com>   \n",
       "2    Dell Technologies Partner Program <DellTechnol...   \n",
       "3       Microsoft Team <microsoft@email.microsoft.com>   \n",
       "4      Google Analytics <analytics-noreply@google.com>   \n",
       "..                                                 ...   \n",
       "949    IT Governance <emailsupport@itgovernance.co.uk>   \n",
       "950  \"Jacob, Head of Engineering\" <developers@irons...   \n",
       "951                         Zym <rebecca@zymplify.com>   \n",
       "952     Clear Measure <clearmeasure@clear-measure.com>   \n",
       "953   Lenovo New beginnings! <lenovo@ecomm.lenovo.com>   \n",
       "\n",
       "                                                    to  \\\n",
       "0                   richard.potter@raddsolutions.co.uk   \n",
       "1                   richard.potter@raddsolutions.co.uk   \n",
       "2                   richard.potter@raddsolutions.co.uk   \n",
       "3                   richard.potter@raddsolutions.co.uk   \n",
       "4                   richard.potter@raddsolutions.co.uk   \n",
       "..                                                 ...   \n",
       "949                   richie.wynne@raddsolutions.co.uk   \n",
       "950  Richard Potter <richard.potter@raddsolutions.c...   \n",
       "951  Richard Potter <richard.potter@raddsolutions.c...   \n",
       "952                 richard.potter@raddsolutions.co.uk   \n",
       "953                 richard.potter@raddsolutions.co.uk   \n",
       "\n",
       "                                date  \\\n",
       "0    Wed, 19 Oct 2022 20:31:34 +0100   \n",
       "1    Tue, 01 Nov 2022 11:01:50 +0000   \n",
       "2    Tue, 15 Nov 2022 06:01:17 +0000   \n",
       "3    Wed, 09 Nov 2022 17:05:09 +0000   \n",
       "4    Tue, 11 Oct 2022 06:02:01 +0100   \n",
       "..                               ...   \n",
       "949  Mon, 21 Nov 2022 11:05:15 +0000   \n",
       "950  Tue, 13 Dec 2022 15:31:28 +0000   \n",
       "951  Tue, 17 Jan 2023 12:01:49 +0000   \n",
       "952  Thu, 29 Dec 2022 10:00:02 +0000   \n",
       "953  Wed, 01 Feb 2023 09:04:42 +0000   \n",
       "\n",
       "                                                  body  Body_Length  \\\n",
       "0                                                  ...         6232   \n",
       "1    Webinar with demos of Windows 365 and vision f...         2943   \n",
       "2     <https://click.comm.delltechnologies.com/open...         4498   \n",
       "3    Episode 1 airs December 13, 2022 \\r\\nHaving tr...         3476   \n",
       "4     <https://www.google.com/images/branding/googl...         5559   \n",
       "..                                                 ...          ...   \n",
       "949  Youre not going to want to miss these savings...         3228   \n",
       "950   <https://ironsoftware.lt.acemlnb.com/Prod/lin...         8587   \n",
       "951  ZYM helps business owners understand their mar...         6966   \n",
       "952  New videos coming in 2023  made to empower yo...         3404   \n",
       "953   <https://trk.ecomm.lenovo.com/ss/o/k8leYzsS8M...         8108   \n",
       "\n",
       "     Subject_Length                                       Cleaned_Body  \\\n",
       "0                40                                                ...   \n",
       "1                90  webinar with demos of windows 365 and vision f...   \n",
       "2                64   \\n   \\t\\r\\nview online \\n\\n  \\t\\r\\n \\t\\r\\nwhy...   \n",
       "3                46  episode 1 airs december 13 2022 \\r\\nhaving tro...   \n",
       "4                68    \\n \\r\\nuniversal analytics will no longer pr...   \n",
       "..              ...                                                ...   \n",
       "949              36  youre not going to want to miss these savings ...   \n",
       "950              55    \\t \\r\\n \\t \\r\\nhi richard\\r\\nimagine spendin...   \n",
       "951              56  zym helps business owners understand their mar...   \n",
       "952              25  new videos coming in 2023  made to empower you...   \n",
       "953              26   \\n \\t\\r\\n\\tview it in browser instead \\n  fre...   \n",
       "\n",
       "                                       Cleaned_Subject  \\\n",
       "0             Thank you for attending Microsoft Ignite   \n",
       "1    Invitation to learn how Windows 365 Cloud PCs ...   \n",
       "2    Dont fall behind  embrace AI with Dell Technol...   \n",
       "3       Microsoft Envision Season 3 begins December 13   \n",
       "4    In September you had 71 users visit your websi...   \n",
       "..                                                 ...   \n",
       "949                Our Black Friday offers have landed   \n",
       "950  Jeff Fritz reviews IronPDF vs SyncFusion iText...   \n",
       "951  Dont Miss Out Help to Grow Digital Ends in 16 ...   \n",
       "952                          New videos coming in 2023   \n",
       "953                          Business PCs up to 40 off   \n",
       "\n",
       "                                       BERT_Embeddings  ...  \\\n",
       "0    [-0.059376951307058334, 0.17135855555534363, 0...  ...   \n",
       "1    [-0.1439182013273239, 0.22149936854839325, 0.6...  ...   \n",
       "2    [-0.15142026543617249, 0.11411778628826141, 0....  ...   \n",
       "3    [-0.36103835701942444, 0.06514844298362732, 0....  ...   \n",
       "4    [-0.10645909607410431, 0.17022578418254852, 0....  ...   \n",
       "..                                                 ...  ...   \n",
       "949  [0.09008561074733734, 0.16759826242923737, 0.6...  ...   \n",
       "950  [-0.20470425486564636, 0.20281416177749634, 0....  ...   \n",
       "951  [0.0035861318465322256, 0.07786554843187332, 0...  ...   \n",
       "952  [-0.08648061007261276, 0.11852650344371796, 0....  ...   \n",
       "953  [-0.09954569488763809, 0.13386352360248566, 0....  ...   \n",
       "\n",
       "                                      summary_TXTRNK_1  \\\n",
       "0    microsoft ignite may be over but heres your ch...   \n",
       "1                                           No Summary   \n",
       "2    we are witnessing and living through the rise ...   \n",
       "3    episode 1 airs december 13 2022\\nregister now ...   \n",
       "4    81 35 bounce rate\\nbreakdown of visitors acqui...   \n",
       "..                                                 ...   \n",
       "949  use promo code bf25\\nuse promo code bf25\\nunit...   \n",
       "950  would your project fail hopefully not but it n...   \n",
       "951  you may qualify for the grant meaning you coul...   \n",
       "952  our new videos coming in 2023 are made to empo...   \n",
       "953  workspace refresh with up to 40 discount until...   \n",
       "\n",
       "                                               Summary  \\\n",
       "0    microsoft ignite may be over but heres your ch...   \n",
       "1                                                        \n",
       "2    why ai and why now\\nwe are witnessing and livi...   \n",
       "3    episode 1 airs december 13 2022\\nregister now ...   \n",
       "4    81 35 bounce rate\\nbreakdown of visitors acqui...   \n",
       "..                                                 ...   \n",
       "949  were starting our black friday offers early wi...   \n",
       "950  imagine spending a lot of money on software li...   \n",
       "951  even better you can try zym for free for 14 da...   \n",
       "952  our new videos coming in 2023 are made to empo...   \n",
       "953  thinkpad p1 gen 4\\nthinkpad z13 gen 1\\nideapad...   \n",
       "\n",
       "                                          summary_BART index_number  \\\n",
       "0    Learn how microsoft empowers organisations to ...         1543   \n",
       "1    webinar with demos of windows 365 and vision f...          765   \n",
       "2    Artificial intelligence ai market is forecast ...          369   \n",
       "3    register now for microsoft envision season 3. ...          895   \n",
       "4    universal analytics will no longer process new...          740   \n",
       "..                                                 ...          ...   \n",
       "949  were starting our black friday offers early wi...         1130   \n",
       "950  iron software is a free open source solution t...          783   \n",
       "951  zym helps business owners understand their mar...          345   \n",
       "952  new videos coming in 2023  made to empower you...          958   \n",
       "953  Free shipping on all orders with up to 40% dis...          132   \n",
       "\n",
       "                                       Tokenized_Email  \\\n",
       "0    {'input_ids': tensor([[  101,  7513, 16270,  4...   \n",
       "1    {'input_ids': tensor([[  101,  4773,  3981,  2...   \n",
       "2    {'input_ids': tensor([[  101,  3193,  3784,  2...   \n",
       "3    {'input_ids': tensor([[  101,  2792,  1015, 14...   \n",
       "4    {'input_ids': tensor([[  101,  5415, 25095,  2...   \n",
       "..                                                 ...   \n",
       "949  {'input_ids': tensor([[  101,  2115,  2063,  2...   \n",
       "950  {'input_ids': tensor([[  101,  7632,  2957,  5...   \n",
       "951  {'input_ids': tensor([[  101,  1062, 24335,  7...   \n",
       "952  {'input_ids': tensor([[  101,  2047,  6876,  2...   \n",
       "953  {'input_ids': tensor([[  101,  3193,  2009,  1...   \n",
       "\n",
       "                                              Entities  Cluster_retrieved  \\\n",
       "0    [('microsoft', 'ORG'), ('2022', 'DATE'), ('mic...                  0   \n",
       "1    [('thursday 17th', 'DATE'), ('2022 1400  1500'...                  0   \n",
       "2    [('500 billion', 'MONEY'), ('20231', 'DATE'), ...                  0   \n",
       "3    [('1', 'CARDINAL'), ('13 2022', 'DATE'), ('mic...                  0   \n",
       "4    [('2023', 'DATE'), ('4', 'CARDINAL'), ('septem...                  0   \n",
       "..                                                 ...                ...   \n",
       "949  [('friday', 'DATE'), ('25', 'CARDINAL'), ('25'...                  7   \n",
       "950  [('jeff fritz', 'PERSON'), ('net conf', 'ORG')...                  7   \n",
       "951  [('uk', 'GPE'), ('up to 5000', 'CARDINAL'), ('...                  7   \n",
       "952  [('2023', 'DATE'), ('2023', 'DATE'), ('10815',...                  7   \n",
       "953  [('up to 40', 'CARDINAL'), ('up to 40', 'CARDI...                  7   \n",
       "\n",
       "                                         Summary_human  \\\n",
       "0    The email discusses post-Microsoft Ignite 2022...   \n",
       "1    Webinar Announcement: \"Windows 365 for Your Hy...   \n",
       "2    The email discusses the rapid growth of artifi...   \n",
       "3    The email announces the premiere of Microsoft ...   \n",
       "4    Starting in 2023, Universal Analytics will no ...   \n",
       "..                                                 ...   \n",
       "949  The email advertises an early Black Friday off...   \n",
       "950  Jeff Fritz from .NET Conf reviewed IronPDF aga...   \n",
       "951  Zym aids business owners in comprehending mark...   \n",
       "952  Summary:\\n\\nClear Measure has announced an upc...   \n",
       "953  Email from Lenovo announces a workspace refres...   \n",
       "\n",
       "                                     Preprocessed_Body  \\\n",
       "0    Microsoft Ignite may be over, but heres  cont...   \n",
       "1    Webinar with demos of Windows 365 and vision f...   \n",
       "2    View Online Why AI and why now ? Why AI and wh...   \n",
       "3    Episode 1 airs December 13, 2022  email? | Vie...   \n",
       "4    Universal Analytics will no longer process new...   \n",
       "..                                                 ...   \n",
       "949  Youre not going to want to miss these savings...   \n",
       "950  , Imagine spending a lot of money on software ...   \n",
       "951  ZYM helps business owners understand their mar...   \n",
       "952  New videos coming in 2023  made to empower . ...   \n",
       "953  View it in browser instead Free shipping on al...   \n",
       "\n",
       "                                    Summary_txt_grph_2  \n",
       "0    Dive into the new innovations we announced at ...  \n",
       "1    * Updates from Ignite, including the built-in ...  \n",
       "2    No. . Servicenow, 2019. IDC, Feb 2022. Reg. De...  \n",
       "3    Register now for Microsoft Envision Season 3 T...  \n",
       "4    Starting in 2023, Universal Analytics will no ...  \n",
       "..                                                 ...  \n",
       "949  Use promo code BF25 Shop now We accept Get in ...  \n",
       "950  Sent to: <email> Iron Software, 205 N. Michiga...  \n",
       "951  , Zym P.S. Copyright  2023 Zym AI, All rights...  \n",
       "952  Our new videos coming in 2023 are made to empo...  \n",
       "953  Lenovo and the Lenovo logo, ThinkPad and ideap...  \n",
       "\n",
       "[954 rows x 23 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372dc8a0",
   "metadata": {},
   "source": [
    "## Calculating the scores for shortmail extractive summary with Manually annotated summaries as target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ec7f1e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d1fdd6abb004fbb95b401d6c6678413",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\764883\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14fc0cf5686d4d8ea23946b99da62ee3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8ee858120ad49259c7a2c178d8ead15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e74e372f7044e72b9e3c531eee6cadd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "661467df39014a6aa16fe17adb9a052b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Scores:\n",
      "Average ROUGE-1: 0.35\n",
      "Average ROUGE-2: 0.15\n",
      "Average ROUGE-L: 0.25\n",
      "Average ROUGE-Lsum: 0.28\n",
      "Average BERT Precision: -0.47\n",
      "Average BERT Recall: -0.24\n",
      "Average BERT F1: -0.36\n"
     ]
    }
   ],
   "source": [
    "## tO CALCULATE ROUGE AND BERT SCORE\n",
    "def calculate_rouge_and_bert_scores(summary, reference):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL', 'rougeLsum'], use_stemmer=True)\n",
    "    rouge_scores = scorer.score(reference, summary)\n",
    "    formatted_rouge_scores = {key: value.fmeasure for key, value in rouge_scores.items()}\n",
    "\n",
    "    P, R, F1 = score([summary], [reference], lang=\"en\", rescale_with_baseline=True)\n",
    "    bert_scores = {'precision': P.item(), 'recall': R.item(), 'f1': F1.item()}\n",
    "\n",
    "    return formatted_rouge_scores, bert_scores\n",
    "\n",
    "# Initialize lists to store the scores\n",
    "rouge1_scores, rouge2_scores, rougeL_scores, rougeLsum_scores = [], [], [], []\n",
    "bert_precision, bert_recall, bert_f1 = [], [], []\n",
    "\n",
    "# Iterate over DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    summary = row['Summary']\n",
    "    reference = row['Summary_human']\n",
    "    rouge_scores, bert_scores = calculate_rouge_and_bert_scores(summary, reference)\n",
    "\n",
    "    # Append ROUGE scores\n",
    "    rouge1_scores.append(rouge_scores['rouge1'])\n",
    "    rouge2_scores.append(rouge_scores['rouge2'])\n",
    "    rougeL_scores.append(rouge_scores['rougeL'])\n",
    "    rougeLsum_scores.append(rouge_scores['rougeLsum'])\n",
    "\n",
    "    # Append BERTScore\n",
    "    bert_precision.append(bert_scores['precision'])\n",
    "    bert_recall.append(bert_scores['recall'])\n",
    "    bert_f1.append(bert_scores['f1'])\n",
    "\n",
    "# Add scores to DataFrame\n",
    "df['ROUGE-1'] = rouge1_scores\n",
    "df['ROUGE-2'] = rouge2_scores\n",
    "df['ROUGE-L'] = rougeL_scores\n",
    "df['ROUGE-Lsum'] = rougeLsum_scores\n",
    "df['BERT Precision'] = bert_precision\n",
    "df['BERT Recall'] = bert_recall\n",
    "df['BERT F1'] = bert_f1\n",
    "\n",
    "# Calculate average scores\n",
    "average_scores = {\n",
    "    'Average ROUGE-1': df['ROUGE-1'].mean(),\n",
    "    'Average ROUGE-2': df['ROUGE-2'].mean(),\n",
    "    'Average ROUGE-L': df['ROUGE-L'].mean(),\n",
    "    'Average ROUGE-Lsum': df['ROUGE-Lsum'].mean(),\n",
    "    'Average BERT Precision': df['BERT Precision'].mean(),\n",
    "    'Average BERT Recall': df['BERT Recall'].mean(),\n",
    "    'Average BERT F1': df['BERT F1'].mean()\n",
    "}\n",
    "\n",
    "# Display average scores\n",
    "print(\"Average Scores:\")\n",
    "for metric, score in average_scores.items():\n",
    "    print(f\"{metric}: {score:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fde050f",
   "metadata": {},
   "source": [
    "## Calculating the scores for Graph based textrank extractive summary with Manually annotated summaries as target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5194be73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Scores:\n",
      "Average ROUGE-1: 0.28\n",
      "Average ROUGE-2: 0.11\n",
      "Average ROUGE-L: 0.18\n",
      "Average ROUGE-Lsum: 0.19\n",
      "Average BERT Precision: 0.00\n",
      "Average BERT Recall: 0.03\n",
      "Average BERT F1: 0.02\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def calculate_rouge_and_bert_scores(summary, reference):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL', 'rougeLsum'], use_stemmer=True)\n",
    "    rouge_scores = scorer.score(reference, summary)\n",
    "    formatted_rouge_scores = {key: value.fmeasure for key, value in rouge_scores.items()}\n",
    "\n",
    "    P, R, F1 = score([summary], [reference], lang=\"en\", rescale_with_baseline=True)\n",
    "    bert_scores = {'precision': P.item(), 'recall': R.item(), 'f1': F1.item()}\n",
    "\n",
    "    return formatted_rouge_scores, bert_scores\n",
    "\n",
    "# Initialize lists to store the scores\n",
    "rouge1_scores, rouge2_scores, rougeL_scores, rougeLsum_scores = [], [], [], []\n",
    "bert_precision, bert_recall, bert_f1 = [], [], []\n",
    "\n",
    "# Iterate over DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    summary = row['Summary_txt_grph_2']\n",
    "    reference = row['Summary_human']\n",
    "    rouge_scores, bert_scores = calculate_rouge_and_bert_scores(summary, reference)\n",
    "\n",
    "    # Append ROUGE scores\n",
    "    rouge1_scores.append(rouge_scores['rouge1'])\n",
    "    rouge2_scores.append(rouge_scores['rouge2'])\n",
    "    rougeL_scores.append(rouge_scores['rougeL'])\n",
    "    rougeLsum_scores.append(rouge_scores['rougeLsum'])\n",
    "\n",
    "    # Append BERTScore\n",
    "    bert_precision.append(bert_scores['precision'])\n",
    "    bert_recall.append(bert_scores['recall'])\n",
    "    bert_f1.append(bert_scores['f1'])\n",
    "\n",
    "# Add scores to DataFrame\n",
    "df['ROUGE-1'] = rouge1_scores\n",
    "df['ROUGE-2'] = rouge2_scores\n",
    "df['ROUGE-L'] = rougeL_scores\n",
    "df['ROUGE-Lsum'] = rougeLsum_scores\n",
    "df['BERT Precision'] = bert_precision\n",
    "df['BERT Recall'] = bert_recall\n",
    "df['BERT F1'] = bert_f1\n",
    "\n",
    "# Calculate average scores\n",
    "average_scores = {\n",
    "    'Average ROUGE-1': df['ROUGE-1'].mean(),\n",
    "    'Average ROUGE-2': df['ROUGE-2'].mean(),\n",
    "    'Average ROUGE-L': df['ROUGE-L'].mean(),\n",
    "    'Average ROUGE-Lsum': df['ROUGE-Lsum'].mean(),\n",
    "    'Average BERT Precision': df['BERT Precision'].mean(),\n",
    "    'Average BERT Recall': df['BERT Recall'].mean(),\n",
    "    'Average BERT F1': df['BERT F1'].mean()\n",
    "}\n",
    "\n",
    "# Display average scores\n",
    "print(\"Average Scores:\")\n",
    "for metric, score in average_scores.items():\n",
    "    print(f\"{metric}: {score:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "937772cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Email 1 mail:\n",
      "Microsoft Ignite may be over, but heres  continue learning  email? | View as a web page  for attending Microsoft Ignite 2022! Deliver efficiency by using automation and AI to streamline your business. Browse on-demand sessions Learn how Microsoft empowers organisations to do more with less by activating automation and AI in the technology stack you already have. Dive into the new innovations we announced at Microsoft Ignite that can help people of all skill levels benefit from leading automation and AI capabilities available across the Microsoft Cloud. Learn more using the resources below and on-demand sessions .  for attending and we look forward to seeing you at the next Microsoft event! Announcements at Ignite Explore the latest announcements for automation and AI innovations available across the Microsoft Cloud. Read the blog > Learning resources Deepen  hand-selected learning modules to help you deliver efficiency through automation and AI. Start learning >  Microsoft Virtual Training Days Get local guidance and ongoing training and support from Microsoft experts at Microsoft Virtual Training Days. Sign up now > Continue  skills that open doors. Learn how you can deliver efficiency through automation and AI with this collection of resources. Start now > The Microsoft Learn Cloud Skills Challenge Complete one challenge and earn a free Microsoft Certification exam Start now > Copyright 2022 Microsoft Corporation. Privacy Statement One Microsoft Way, Redmond, WA 98052 USA\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Email 2 mail:\n",
      "Webinar with demos of Windows 365 and vision for Windows in the commercial space  email? | View as web page Windows 365 for : stream the full Windows experience to any device Thursday, 17th November 2022, 14:00 - 15:00 GMT , Join us on the 17th November for our 'Windows 365 for : stream the full Windows experience to any device' event. This engaging 60-minute session will: * Introduce Windows 365 Cloud PCs and how easily t be deployed and managed using existing tools like Microsoft Endpoint Manager to provide simple new ways to deliver end user compute experiences anywhere * Pre-recorded address from Satya Nadella, Microsoft Chairman and CEO. He will share our vision for Windows in the commercial space and a roadmap of the experiences being built to meet customers hybrid work needs  from client to cloud. * Updates from Ignite, including the built-in Windows 365 app, Citrix HDX Plus integration for Windows 365, and new ways to deliver Cloud PCs to shift and part-time workers * Well show demos of Windows 365: How to provision a Cloud PC and how to get started using Windows 365. * Well conclude with a dedicated Q&A session so you can understand how Cloud PCs can support your organisations unique situation and computing requirements. Bring  challenges! Youll have the opportunity to see first-hand how Windows helps alleviate concerns and challenges organisations like  now face because of hybrid work, enhanced security needs, and remote operations. Learn more ab Windows 365. Dont miss the chance to learn how Windows 365 Cloud PCs help power the future of hybrid work. Date: Thursday, 17th November 2022 Time: 14:00-15:00 GMT Location: Teams Live Event  link to be provided on registration , Microsoft UK Team Note: This event may be recorded and shared publicly with others, including Microsofts global customers, partners, employees, and service providers. The recording may include  any questions you submit to Q&A. You may submit questions anonymously. Windows 365 for : stream the full Windows experience to any device Register for the Teams Live Event > Unsubscribe | Privacy Statement Microsoft Limited, registered in England and Wales Company number 01624297. Registered office: Microsoft Campus, Thames Valley Park Reading RG6 1WG, United Kingdom\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Email 3 mail:\n",
      "View Online Why AI and why now ? Why AI and why now ? We are witnessing and living through the rise of Artificial Intelligence (AI) at an unprecedented pace and scale. The AI market is forecast to grow to $500 billion in 20231. And over the next three years, the number of CIOs making investments in machine learning will nearly double to 64%.2 For AI workloads, a modernized data center is essential. But complexity, time to value and lack of expertise can prevent organizations from achieving this. The Dell Technologies end-to-end portfolio solves these common challenges, with AI-optimized solutions that can effect real business change  from creating new lines of revenue and profit, to achieving better efficiency. Explore the AI-ready infrastructure in the Demand Generation Center, or activate  with the Digital Marketing Tool. Visit the Partner Portal Activate with Digital Marketing Tool 1 IDC Forecasts Companies to Increase Spend on AI Solutions by 19.6% in 2022. IDC, Feb 2022. 2 The Global CIO Point of View: The New Agenda for Transformative Leadership: Reimagine Business for Machine Learning. Servicenow, 2019. <url> TechnologiesOne Dell WayRound RockTX78682 Manage Preferences Unsubscribe Privacy Statement You are subscribed as <email> For questions ab , contact us at <email> Dell Technologies Global Headquarters is located at One Dell Way, Round Rock, TX, 78682  2022 Dell Inc. or its subsidiaries. All Rights Reserved. Dell Technologies, Dell, EMC, Dell EMC and other trademarks are trademarks of Dell Inc. or its subsidiaries. Dell Corporation Limited. Registered in England. Reg. No. 02081369, Dell House, The Boulevard, Cain Road, Bracknell, Berkshire RG12 1LF. .\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Email 4 mail:\n",
      "Episode 1 airs December 13, 2022  email? | View as web page Register now for Microsoft Envision Season 3 You can learn how to drive success by doing more with less in transitional times. When and where? Find  by registering now for the Season 3 premiere of Microsoft Envision presented with Accenture and Avanade. This digital series resumes on December 13 and enables meaningful connections within a vital community and delivers impactful insights on business and technology. Join the conversation with leaders from Accenture, Avanade, and Microsoft for exclusive information from people who build the solutions that matter. Keynote speakers include Christie Smith, Global Lead of Talent and Organization/Human Potential at Accenture and a member of the Global Management Committee, and Alysa Taylor, Microsoft's Corporate Vice President of Industry, Apps, and Data Marketing. Visit the Microsoft Envision website for agenda details and a complete list of speakers. Register now for Microsoft Envision Season 3 Tuesday, December 13, 2022 08:00 AM Pacific Time Register for free > Christie Smith Global Lead, Talent and Organization/Human Potential, Accenture Alysa Taylor Corporate Vice President, Industry, Apps, and Data Marketing, Microsoft Corporation Unsubscribe | Privacy Statement One Microsoft Way, Redmond, WA 98052 USA\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Email 5 mail:\n",
      "Universal Analytics will no longer process new data in standard properties starting in 2023. Prepare now by setting up and switching over to a Google Analytics 4 property. Richard, here's  <url> RADDWebsite UA-121911062-1 | View: All Web Site Data 178135681 SEE FULL REPORT Top performance metrics Stats compared to previous month Users 71 2.7% Sessions 81 3.5% Bounce Rate 16.0% 3.7% Average Session Duration 00 98.9% User acquisition Breakdown of visitors acquired by channel Direct 51.8% Organic Search 39.5% All Other Sessions 8.6% Feature spotlight Our next-generation measurement solution, Google Analytics 4, is replacing Universal Analytics. Starting in 2023, Universal Analytics will no longer process data. In preparation, we encourage you to switch over to a Google Analytics 4 property as soon as possible. Switching to Google Analytics 4 allowed McDonald's Hong Kong to gather real-time ecommerce data from their app and use turnkey predictive audiences to gather customer insights. As a result, McDonald's was able to reach valuable audiences and increase in-app orders by 550%. GET STARTED Get insights on the go by downloading the free Google Analytics app .  2022 Google LLC 1600 Amphitheatre Parkway, Mountain View, CA 94043 This email was sent to <email> because you indicated that you'd like to receive updates and tips ab  account. If you don't want to receive such emails in the future, please unsubscribe here: <url> You can also change  your account's Google Analytics Email Communications page by logging into <url>\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Email 6 mail:\n",
      "This February 9 SAP Partner Live event is one you cant afford to miss! Align with SAPs strategic direction, explore our partner programs, and hear ab new opportunities to maximize growth in 2023 and beyond.  to view this email in . Please add <email> to  or safe senders list. 19 January 2023 Change the news I see Invite colleague to subscribe Top News | Business Trends | Events | Training & Enablement | Awards Top News Dont Miss : Global Partner Success Kickoff This February 9 SAP Partner Live event is one you cant afford to miss! Align with SAPs strategic direction, explore our partner programs, and hear ab new opportunities to maximize growth in 2023 and beyond. Business Trends Back to Top  for Lunch: SAP Build Portfolio The SAP Build portfolio is SAPs low-code offering, designed to empower business users to easily build business applications. Listen to this episode to hear ab this powerful portfolio of solutions from Partner Chief Technology Officer of Partner Applications, Jagdish Sahasrabudhe. Events Back to Top Fuel Growth in 2023: SAP Customer Success Success for SAP, our partners, and our customers will take focus on customer lifetime value, culture, and core. Listen to the global keynote presented by Scott Russell to the SAP Field teams and our partners for important updates that will help you succeed in 2023. Training & Enablement Back to Top SAP Build Process Automation Is Now Available for SAP S/4HANA Cloud, Public Edition The starter pack for SAP Build Process Automation is now part of SAP S/4HANA Cloud, public edition at no additional cost. Read this post for more details and discover whats included in the started pack. Awards Back to Top Theres Still Time Make sure you enter  the 2023 SAP Innovation Awards before February 1. Winners will be announced in April. See how to enter. Unsubscribe | Manage Your Profile | Get Help | Copyright/Trademark | Privacy | Impressum SAP Global Partner Communications - 10 Hudson Yards | New York, NY 10001, USA Pflichtangaben/Mandatory Disclosure Statements: <url> Diese E-Mail kann Betriebs- oder Geschftsgeheimnisse oder sonstige vertrauliche Informationen enthalten. Sollten Sie diese E-Mail irrtmlich erhalten haben, ist Ihnen eine Kenntnisnahme des Inhalts, eine Vervielfltigung oder Weitergabe der E-Mail ausdrcklich untersagt. Bitte benachrichtigen Sie uns und vernichten Sie die empfangene E-Mail. Vielen Dank. This e-mail may contain trade secrets or privileged, undisclosed, or otherwise confidential information. If you have received this e-mail in error, you are hereby notified that any review, copying, or distribution of it is strictly prohibited. Please inform us immediately and destroy the original transmittal.  for your cooperation\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Email 7 mail:\n",
      "Cyber Training Bulletin - December and January CSD Cyber Defense Education and Training (CDET) Offerings Highlights: What You Want to Know * A new category has been added to FedVTE under the Cybersecurity Courses called Non-Technical Cybersecurity. Some new courses that fall into this category include Cloud Monitoring, Critical Infrastructure Protection, and Cybersecurity Investigations. To see the full list of available courses in this category, visit <url> * CISA is offering the new Analysis of a Cyber Incident course on FedVTE. This three-module course teaches the beginner analyst how to develop the analytical skills and capabilities needed to handle a potential cyber incident  from analysis to reporting findings. This course is available to federal employees. For a full list of available courses on FedVTE for federal employees, please see the course catalog . For courses available to the public, please visit <url> * In December and January, U.S. Executive Branch employees and contractors can participate in numerous CDM Dashboard courses, including the new CDM and Federal Mandates-Featuring how to use the CDM Dashboard to enable automated BOD-22-01 Reporting course. This course presents information regarding current Federal cybersecurity directives, mandates, and policies, and how t be supported by the CDM Agency Dashboard. Featured prominently will be details on how to use the CDM Dashboard to enable automated BOD-22-01 Reporting.  Incident Response We offer no-cost, cybersecurity incident response training for government employees and contractors across federal, state, local, tribal, and territorial government, as well as educational and critical infrastructure partners. Course types include Awareness Webinars and Cyber Range Training. These courses provide valuable learning opportunities for everyone from cyber newbies to veteran cybersecurity engineers. IR Training Events through January 2023 Date Course Code Registration Begins Course Hours 2022-12-01 IR 106 2022-11-01 Preventing DNS Infrastructure Tampering 1 2022-12-08 IR 108 2022-11-08 Understanding Indicators of Compromise 1 2022-12-15 IR 204 2022-11-15 Defending Internet Accessible Systems 4 To learn more or register visit: Incident Response Training | CISA  Continuous Diagnostics and Mitigation We offer instructor-led, hands-on CDM Agency Dashboard training for U.S. Executive Branch employees and contractors in our cyber range virtual training environment. These courses are intended for those at agencies participating in the CDM program who monitor, manage and/or oversee controls on their information systems (e.g., ISSOs, CDM POCs, ISSMs and those who report metrics and measures). The two-day CDM 111 course offers 14 CPE credits. CDM Training Events through January 2023 Date Course Code Registration Begins Course Hours 2022-12-06 CDM 203 2022-11-06 Systems Analyst 4 2022-12-13 CDM 210 2022-11-13 Intro to CETH 4 2022-12-20 CDM 141 2022-11-20 Intro to CDM 4 2023-01-10 CDM220 2022-12-11 Federal Mandates and BOD 22-01 4 2023-01-19 CDM201 2022-12-20 Identity and Access Management 4 2023-01-25 CDM210 2022-12-26 Intro to CETH 4 2023-01-31 CDM142 2022-12-31 Asset Management 4 To learn more or register visit: <url>  Industrial Control Systems (ICS) We offer free, virtual ICS trainings geared toward Critical Infrastructure owners and operators. The trainings are designed to reduce cybersecurity risks to critical infrastructure and encourage cooperation between CISA and the private sector. Trainings vary in length and run from 08:00 a.m.  05:00 p.m. MST (10:00 a.m.  07:00 p.m. EST). All trainings are conducted through Online Training or CISA Virtual Learning Portal (VLP) , with the exception of the three-day, in-person pilot course at Idaho National Labs (INL) in Idaho Falls, ID. ICS Training Events through January 2023 Date Course Code Course 2022-12-05  2022-12-23 401v Industrial Control Systems Evaluation (401v) Scheduled Online Training 2022-12-05  2022-12-23 301v Industrial Control Systems Cybersecurity (301v) Scheduled Online Training 2023-01-09  2023-01-27 401v Industrial Control Systems Evaluation (401v) Scheduled Online Training 2023-01-09  2023-01-27 301v Industrial Control Systems Cybersecurity (301v) Scheduled Online Training 2023-01-16  2023-01-19 301L Industrial Control Systems Cybersecurity Training  In Person 4 Days On Demand 100W Operational Security (OPSEC) for Control Systems On Demand 210W-1 Differences in Deployments of ICS On Demand 210W-2 Influence of Common IT Components on ICS On Demand 210W-3 Common ICS Components On Demand 210W-4 Cybersecurity within IT & ICS Domains On Demand 210W-5 Cybersecurity Risk On Demand 210W-6 Current Trends (Threat) On Demand 210W-7 Current Trends (Vulnerabilities) On Demand 210W-8 Determining the Impacts of a Cybersecurity Incident On Demand 210W-9 Attack Methodologies in IT & ICS On Demand 210W-10 Mapping IT Defense-in-Depth Security Solutions to ICS - Part 1 On Demand 210W-11 Mapping IT Defense-in-Depth Security Solutions to ICS - Part 2 On Demand FRE2115 Industrial Control Systems Cybersecurity Landscape for Managers To learn more or sign up, visit: us-cert.cisa.gov/ics/calendar *The following virtual courses are prerequisites to attending in-person 301 and 401 trainings hosted by CISA at the Idaho National Laboratory: * ICS 301v: Focuses on understanding, protecting, and securing ICS from cyberattacks. * ICS 401v: Focuses on analyzing and evaluating an ICS network to determine its defense status and what changes need to be made.  CISAs K  12 Cybersecurity Education Training Assistance Program (CETAP) and CYBER.ORG : Through CISAs CETAP grantee, CYBER.ORG, we offer K-12 teachers with cybersecurity curricula and education tools. CYBER.ORG develops and distributes free cybersecurity, STEM, and computer science curricula to K-12 educators across the country. Below are upcoming training events through CYBER.ORG. CISA CETAP and CYBER.ORG Training Events through January 2023 Date Audience Course 2022-12-03  2022-12-06 Elementary, Middle School, and High School Teachers NICE K12 Cybersecurity: Learn ab using cyber range in your classroom, cyber ethics, and cybersecurity awareness for elementary and middle school students. Cybersecurity Education | Cyber.org 2022-12-08 High School/CTE Educators Simple Web Application Attacks: Learn the basics of web application attacks in the free virtual workshop for high school and CTE educators. Simple Web Application Attacks | Cyber.org 2022-12-13 K-12 Educators Advanced Web Application Attacks: This 90-minute virtual, fast-paced workshop will explore more complex web application attacks than can occur. Advanced Web Application Attacks | Cyber.org To learn more ab registration and requirements visit: Federal Cyber Defense Skilling Academy  CDET Mission CDET Vision Address todays cyber workforce challenges through innovative education and training opportunities Lead and influence national cyber training and education to promote and enable the cyber-ready workforce of tomorrow Contact Us: <email>  message? View it as a webpage . You are subscribed to updates from the Cybersecurity and Infrastructure Security Agency (CISA) Manage Subscriptions | Privacy Policy | Help Connect with CISA: Facebook | Twitter | Instagram | LinkedIn | Yube  This email was sent to <email> using GovDelivery Communications Cloud, on behalf of: Cybersecurity and Infrastructure Security Agency  707 17th St, Suite 4000  Denver, CO 80202\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Email 8 mail:\n",
      "Build Customer Breakthroughs With Multicloud View Online Partner News 22 September, 2022 | EMEA Discover the Endless Possibilities for Innovation Find  what innovations will lead to  at Dell Technologies Forum. Register Now  DEMAND GEN CENTER DEAL REGISTRATION DISCOVERY CENTER PARTNER ACADEMY EVENTS CALENDAR Introducing New Lifecycle Management Options for APEX Data Storage Services* The new customer-managed option for Dell APEX Data Storage Services lets you perform day-to-day management operations as a trusted advisor. Explore More  Multicloud by Design Check  the Partner Connection podcast to hear everything from Dell's own digital transformation to multicloud complexity as we introduce our APEX portfolio while discussing the importance of aaS (as-a-Service). Tune In  t Update on Operations in Russia and Belarus As Dell has ceased operations in Russia and Belarus, products and services may not be sold to Russia, Belarus or any of their territories. By placing new orders, Partner/Distributor acknowledges that Dell has no obligation to provide any Services if the request originates from Russia, Belarus or any of their territories. Read New Restrictions  Turn  Into a Competitive Advantage See how personalized marketing at scale can help you deliver value and turn customer experience into a competitive advantage. Dive In  r Program Updates Lead Statuses Within the Lead Management Tool Have Been Consolidated Products Solutions & Services APEX Flex on Demand Partner Event Kit Is Live** Deliver Multicloud by Design With APEX Cloud Services*** Drive Innovation With PowerEdge and OpenManage Start Conversations That Put People at the Center With Persona-Based Solutions Events & Webinars Join the Womens Partner Network Webcast for an Authentic Conversation on Leadership, Advocacy and Empowerment * Available in US, UK, France, Germany, Denmark, Finland, Ireland, Italy, Norway, Spain, Sweden, Australia, New Zealand, Singapore, S. Korea and Japan. ** Available in US, Canada, Brazil, Chile, Colombia, Mexico, France, UK, Germany, Austria, Belgium, Denmark, Finland, Ireland, Italy, Luxembourg, Netherlands, Norway, Portugal, S. Africa, Spain, Sweden, Switzerland, Australia, New Zealand, China, Singapore, S. Korea, Japan, Hong Kong, India, Poland and Taiwan. *** Available in US, UK, France, Germany, Australia and New Zealand. Connect with us: <url> TechnologiesOne Dell WayRound RockTX78682 Manage Preferences Unsubscribe Privacy Statement You are subscribed as <email> For questions ab , contact us at <email> Dell Technologies Global Headquarters is located at One Dell Way, Round Rock, TX, 78682  2022 Dell Inc. or its subsidiaries. All Rights Reserved. Dell Technologies, Dell, EMC, Dell EMC and other trademarks are trademarks of Dell Inc. or its subsidiaries. Dell Corporation Limited. Registered in England. Reg. No. 02081369, Dell House, The Boulevard, Cain Road, Bracknell, Berkshire RG12 1LF.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Email 9 mail:\n",
      "We have deals for you. Your Amazon.co.uk Today's Deals See All Departments Selection of Ring Home Security devices Ships from and sold by Amazon.co.uk Learn more Furniture by Sweetnight Ships and Sold by Amazon.co.uk Learn more Food Prep and Hot Beverage Makers by Tefal and Krups Shipped and sold by Amazon.co.uk Learn more Oral Care Appliances by Philips and Waterpik Dispatched and shipped by Amazon. Learn more Yankee Candle, Woodwick and Millefiori Milano Selection Shipped from and Sold by Amazon.co.uk Learn more Braun Male Grooming and Hair Removal Appliances Ships from and sold by Amazon.co.uk Learn more Housewares by: Thermos, Vileda, Philips, Tower and more Ships from and sold by Amazon.co.uk Learn more Oral-B Toothbrushes and Toothbrush Heads Shipped and Sold by Amazon.co.uk Learn more Makeup & Nailcare by Rimmel, OPI and more Ships from and sold by Amazon.co.uk Learn more Samsung Smartphones Ships from and sold by Amazon.co.uk Learn more Anker Power Banks, Chargers, Hubs, Cables and Power Stations Anker Power Banks, Chargers, Hubs, Cables and Power Stations Learn more Haircare, Shaving & Grooming from Philips, ghd and more Ships from and sold by Amazon.co.uk Learn more Marshall Speakers Sold and shipped by Amazon Learn more See all Deals of the Day Explore More Deals Samsung Earbuds and Smartwatches Baby Products from Tommee Tippee Save on TP-Link Deco S7 AC1900 Whole... Google Pixel 6 and Pixel 6 Pro Amazon eero Mesh Wi-Fi Rer Fitbit Wearables Shark Anti Hair Wrap Vacuums Musical Instruments and Accessories Find Great Deals on Millions of Items Storewide  AmazonBasics  Baby  Blu-ray  Books  Business and Industrial  Camera & Photo  Car & Motorbike  Clothing  Computing  DIY & Tools  DVD  Electronics  Grocery  Health & Beauty  Home & Garden  Jewellery  Kindle Store  Lighting  Digital Music  Music  Musical Instruments & DJ  Office Products  let  Pet Supplies  Shoes & Accessories  Software  Sports & doors  Toys & Games  Video Games  Watches Connect with us We hope you enjoyed receiving this message. However, if you'd rather not receive future e-mails of this sort from Amazon.co.uk please opt- here . Terms and conditions apply. Click on the offer for details of applicable products and terms and conditions. Offers are for a limited time only and subject to availability. Discounts and savings on offers on products sold by Amazon.co.uk (excluding MP3s) refer to savings against Recommended Retail Price (\"RRP\") or our previous selling price, as indicated. Discounts and savings on Amazon MP3s refer to savings against our previous selling price, or as otherwise indicated. Offers on products sold by a Marketplace seller are subject to that seller's terms and conditions of sale. For details see www.amazon.co.uk .  2023 Amazon.com, Inc. or its affiliates. Amazon and all related marks are trademarks of Amazon.com, Inc. or its affiliates. Please note that this promotional e-mail is being sent from an e-mail address that cannot receive e-mails. If you have any questions and wish to contact us,  . Reference: 774037371 Please note that this message was sent to the following e-mail address: <email>\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Email 10 mail:\n",
      "Read ab the top priorities of job seekers, how to manage Linux logs, and more. View in browser FEATURES Introduction to Linux Log Management Jeff Layton explains how to manage Linux logs and gather important system information. NEWS AND COMMUNITY Skill Development Opportunities Spur Job Changes Employees cite skill development and company culture as key considerations when changing jobs. Leap Seconds to be Discontinued in 2035 The International Bureau of Weights and Measures has resolved to discontinue use of leap seconds. Mastodon Security Vulnerabilities Emerge Researchers are looking more closely at security issues with the social media platform. How Splitting  Lower Stress and Improve Morale Tom Limoncelli explains the potential benefits of splitting teams into smaller functional units. LF Europe Launches Open Source Telco Project Project Sylva aims to create an open source telco cloud framework for European telcos and vendors. TypeScript 4.9 Introduces Error-Catching Feature Microsofts TypeScript 4.9 is now generally available; read ab new features. FEATURED JOBS FROM OPEN SOURCE JOBHUB * Product Marketing Manager (Remote)  Acquia * Senior C Developer (Remote)  CloudLinux * Linux Systems Administrator (Remote)  Collabora * Technical Journalist/Editor (Remote)  LWN.net * Thunderbird Android Developer (Remote)  Mozilla Looking for a job? Check  the latest listings on Open Source JobHub . Looking to add talent to your team? Post  jobs for free. UPCOMING EVENTS FOSDEM 2023 February 4-5, 2023  Brussels, Belgium DeveloperWeek February 15-17 & February 21-23, 2023  San Francisco, California + Virtual GeekBeacon Festival February 18, 2023 | Virtual Event Contact us to learn ab partnership and sponsorship opportunities. Stay Connected Also on Mastodon This message was sent by Linux New Media USA, LLC (FOSSlife) 4840 Bob Billings Parkway, Ste 104, Lawrence, KS 66049, USA You are receiving this email because you signed up to receive FOSSlife Weekly. Subscribe Forward to a Friend Manage Subscription One-Click Unsubscribe\n",
      "\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the summaries\n",
    "for i, summary in enumerate(df['Preprocessed_Body'].iloc[:10]):\n",
    "    print(f\"Email {i+1} mail:\")\n",
    "    print(summary)\n",
    "    print(\"\\n\" + \"-\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "baa4e984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Email 1 Summary:\n",
      "Dive into the new innovations we announced at Microsoft Ignite that can help people of all skill levels benefit from leading automation and AI capabilities available across the Microsoft Cloud. Learn how you can deliver efficiency through automation and AI with this collection of resources. Start now > The Microsoft Learn Cloud Skills Challenge Complete one challenge and earn a free Microsoft Certification exam Start now > Copyright 2022 Microsoft Corporation. Announcements at Ignite Explore the latest announcements for automation and AI innovations available across the Microsoft Cloud.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Email 2 Summary:\n",
      "* Updates from Ignite, including the built-in Windows 365 app, Citrix HDX Plus integration for Windows 365, and new ways to deliver Cloud PCs to shift and part-time workers * Well show demos of Windows 365: How to provision a Cloud PC and how to get started using Windows 365. | View as web page Windows 365 for : stream the full Windows experience to any device Thursday, 17th November 2022, 14:00 - 15:00 GMT , Join us on the 17th November for our 'Windows 365 for : stream the full Windows experience to any device' event. Learn more ab Windows 365. Youll have the opportunity to see first-hand how Windows helps alleviate concerns and challenges organisations like  now face because of hybrid work, enhanced security needs, and remote operations.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Email 3 Summary:\n",
      "No. . Servicenow, 2019. IDC, Feb 2022. Reg. Dell Corporation Limited.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Email 4 Summary:\n",
      "Register now for Microsoft Envision Season 3 Tuesday, December 13, 2022 08:00 AM Pacific Time Register for free > Christie Smith Global Lead, Talent and Organization/Human Potential, Accenture Alysa Taylor Corporate Vice President, Industry, Apps, and Data Marketing, Microsoft Corporation Unsubscribe | Privacy Statement One Microsoft Way, Redmond, WA 98052 USA Keynote speakers include Christie Smith, Global Lead of Talent and Organization/Human Potential at Accenture and a member of the Global Management Committee, and Alysa Taylor, Microsoft's Corporate Vice President of Industry, Apps, and Data Marketing.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Email 5 Summary:\n",
      "Starting in 2023, Universal Analytics will no longer process data. In preparation, we encourage you to switch over to a Google Analytics 4 property as soon as possible. Prepare now by setting up and switching over to a Google Analytics 4 property.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Email 6 Summary:\n",
      "Align with SAPs strategic direction, explore our partner programs, and hear ab new opportunities to maximize growth in 2023 and beyond. Align with SAPs strategic direction, explore our partner programs, and hear ab new opportunities to maximize growth in 2023 and beyond. Events Back to Top Fuel Growth in 2023: SAP Customer Success Success for SAP, our partners, and our customers will take focus on customer lifetime value, culture, and core. If you have received this e-mail in error, you are hereby notified that any review, copying, or distribution of it is strictly prohibited. This e-mail may contain trade secrets or privileged, undisclosed, or otherwise confidential information. See how to enter.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Email 7 Summary:\n",
      "This course presents information regarding current Federal cybersecurity directives, mandates, and policies, and how t be supported by the CDM Agency Dashboard. CISA CETAP and CYBER.ORG Training Events through January 2023 Date Audience Course 2022-12-03  2022-12-06 Elementary, Middle School, and High School Teachers NICE K12 Cybersecurity: Learn ab using cyber range in your classroom, cyber ethics, and cybersecurity awareness for elementary and middle school students. For courses available to the public, please visit <url> * In December and January, U.S. Executive Branch employees and contractors can participate in numerous CDM Dashboard courses, including the new CDM and Federal Mandates-Featuring how to use the CDM Dashboard to enable automated BOD-22-01 Reporting course. Incident Response We offer no-cost, cybersecurity incident response training for government employees and contractors across federal, state, local, tribal, and territorial government, as well as educational and critical infrastructure partners. Some new courses that fall into this category include Cloud Monitoring, Critical Infrastructure Protection, and Cybersecurity Investigations. CISAs K  12 Cybersecurity Education Training Assistance Program (CETAP) and CYBER.ORG : Through CISAs CETAP grantee, CYBER.ORG, we offer K-12 teachers with cybersecurity curricula and education tools. To see the full list of available courses in this category, visit <url> * CISA is offering the new Analysis of a Cyber Incident course on FedVTE. All trainings are conducted through Online Training or CISA Virtual Learning Portal (VLP) , with the exception of the three-day, in-person pilot course at Idaho National Labs (INL) in Idaho Falls, ID. These courses are intended for those at agencies participating in the CDM program who monitor, manage and/or oversee controls on their information systems (e.g., ISSOs, CDM POCs, ISSMs and those who report metrics and measures).\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Email 8 Summary:\n",
      "02081369, Dell House, The Boulevard, Cain Road, Bracknell, Berkshire RG12 1LF. Dell Technologies, Dell, EMC, Dell EMC and other trademarks are trademarks of Dell Inc. or its subsidiaries. Dive In  r Program Updates Lead Statuses Within the Lead Management Tool Have Been Consolidated Products Solutions & Services APEX Flex on Demand Partner Event Kit Is Live** Deliver Multicloud by Design With APEX Cloud Services*** Drive Innovation With PowerEdge and OpenManage Start Conversations That Put People at the Center With Persona-Based Solutions Events & Webinars Join the Womens Partner Network Webcast for an Authentic Conversation on Leadership, Advocacy and Empowerment * Available in US, UK, France, Germany, Denmark, Finland, Ireland, Italy, Norway, Spain, Sweden, Australia, New Zealand, Singapore, S. Korea and Japan. By placing new orders, Partner/Distributor acknowledges that Dell has no obligation to provide any Services if the request originates from Russia, Belarus or any of their territories. *** Available in US, UK, France, Germany, Australia and New Zealand.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Email 9 Summary:\n",
      "We have deals for you. Terms and conditions apply.  2023 Amazon.com, Inc. or its affiliates. If you have any questions and wish to contact us,  .\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Email 10 Summary:\n",
      "Read ab the top priorities of job seekers, how to manage Linux logs, and more. Post  jobs for free. Check  the latest listings on Open Source JobHub . Stay Connected Also on Mastodon This message was sent by Linux New Media USA, LLC (FOSSlife) 4840 Bob Billings Parkway, Ste 104, Lawrence, KS 66049, USA You are receiving this email because you signed up to receive FOSSlife Weekly.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the summaries\n",
    "for i, summary in enumerate(df['Summary_txt_grph_2'].iloc[:10]):\n",
    "    print(f\"Email {i+1} Summary:\")\n",
    "    print(summary)\n",
    "    print(\"\\n\" + \"-\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "831208a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>from</th>\n",
       "      <th>to</th>\n",
       "      <th>date</th>\n",
       "      <th>body</th>\n",
       "      <th>Body_Length</th>\n",
       "      <th>Subject_Length</th>\n",
       "      <th>Cleaned_Body</th>\n",
       "      <th>Cleaned_Subject</th>\n",
       "      <th>BERT_Embeddings</th>\n",
       "      <th>...</th>\n",
       "      <th>summary_TXTRNK_1</th>\n",
       "      <th>Summary</th>\n",
       "      <th>summary_BART</th>\n",
       "      <th>index_number</th>\n",
       "      <th>Tokenized_Email</th>\n",
       "      <th>Entities</th>\n",
       "      <th>Cluster_retrieved</th>\n",
       "      <th>Summary_human</th>\n",
       "      <th>Preprocessed_Body</th>\n",
       "      <th>Summary_txt_grph_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Thank you for attending Microsoft Ignite</td>\n",
       "      <td>Microsoft Team &lt;microsoft@email.microsoft.com&gt;</td>\n",
       "      <td>richard.potter@raddsolutions.co.uk</td>\n",
       "      <td>Wed, 19 Oct 2022 20:31:34 +0100</td>\n",
       "      <td>...</td>\n",
       "      <td>6232</td>\n",
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td>Thank you for attending Microsoft Ignite</td>\n",
       "      <td>[-0.059376951307058334, 0.17135855555534363, 0...</td>\n",
       "      <td>...</td>\n",
       "      <td>microsoft ignite may be over but heres your ch...</td>\n",
       "      <td>microsoft ignite may be over but heres your ch...</td>\n",
       "      <td>Learn how microsoft empowers organisations to ...</td>\n",
       "      <td>1543</td>\n",
       "      <td>{'input_ids': tensor([[  101,  7513, 16270,  4...</td>\n",
       "      <td>[('microsoft', 'ORG'), ('2022', 'DATE'), ('mic...</td>\n",
       "      <td>0</td>\n",
       "      <td>The email discusses post-Microsoft Ignite 2022...</td>\n",
       "      <td>Microsoft Ignite may be over, but heres  cont...</td>\n",
       "      <td>Dive into the new innovations we announced at ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Invitation to learn how Windows 365 Cloud PCs ...</td>\n",
       "      <td>Microsoft &lt;replyto@email.microsoft.com&gt;</td>\n",
       "      <td>richard.potter@raddsolutions.co.uk</td>\n",
       "      <td>Tue, 01 Nov 2022 11:01:50 +0000</td>\n",
       "      <td>Webinar with demos of Windows 365 and vision f...</td>\n",
       "      <td>2943</td>\n",
       "      <td>90</td>\n",
       "      <td>webinar with demos of windows 365 and vision f...</td>\n",
       "      <td>Invitation to learn how Windows 365 Cloud PCs ...</td>\n",
       "      <td>[-0.1439182013273239, 0.22149936854839325, 0.6...</td>\n",
       "      <td>...</td>\n",
       "      <td>No Summary</td>\n",
       "      <td></td>\n",
       "      <td>webinar with demos of windows 365 and vision f...</td>\n",
       "      <td>765</td>\n",
       "      <td>{'input_ids': tensor([[  101,  4773,  3981,  2...</td>\n",
       "      <td>[('thursday 17th', 'DATE'), ('2022 1400  1500'...</td>\n",
       "      <td>0</td>\n",
       "      <td>Webinar Announcement: \"Windows 365 for Your Hy...</td>\n",
       "      <td>Webinar with demos of Windows 365 and vision f...</td>\n",
       "      <td>* Updates from Ignite, including the built-in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dont fall behind  embrace AI with Dell Techn...</td>\n",
       "      <td>Dell Technologies Partner Program &lt;DellTechnol...</td>\n",
       "      <td>richard.potter@raddsolutions.co.uk</td>\n",
       "      <td>Tue, 15 Nov 2022 06:01:17 +0000</td>\n",
       "      <td>&lt;https://click.comm.delltechnologies.com/open...</td>\n",
       "      <td>4498</td>\n",
       "      <td>64</td>\n",
       "      <td>\\n   \\t\\r\\nview online \\n\\n  \\t\\r\\n \\t\\r\\nwhy...</td>\n",
       "      <td>Dont fall behind  embrace AI with Dell Technol...</td>\n",
       "      <td>[-0.15142026543617249, 0.11411778628826141, 0....</td>\n",
       "      <td>...</td>\n",
       "      <td>we are witnessing and living through the rise ...</td>\n",
       "      <td>why ai and why now\\nwe are witnessing and livi...</td>\n",
       "      <td>Artificial intelligence ai market is forecast ...</td>\n",
       "      <td>369</td>\n",
       "      <td>{'input_ids': tensor([[  101,  3193,  3784,  2...</td>\n",
       "      <td>[('500 billion', 'MONEY'), ('20231', 'DATE'), ...</td>\n",
       "      <td>0</td>\n",
       "      <td>The email discusses the rapid growth of artifi...</td>\n",
       "      <td>View Online Why AI and why now ? Why AI and wh...</td>\n",
       "      <td>No. . Servicenow, 2019. IDC, Feb 2022. Reg. De...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Microsoft Envision Season 3 begins December 13</td>\n",
       "      <td>Microsoft Team &lt;microsoft@email.microsoft.com&gt;</td>\n",
       "      <td>richard.potter@raddsolutions.co.uk</td>\n",
       "      <td>Wed, 09 Nov 2022 17:05:09 +0000</td>\n",
       "      <td>Episode 1 airs December 13, 2022 \\r\\nHaving tr...</td>\n",
       "      <td>3476</td>\n",
       "      <td>46</td>\n",
       "      <td>episode 1 airs december 13 2022 \\r\\nhaving tro...</td>\n",
       "      <td>Microsoft Envision Season 3 begins December 13</td>\n",
       "      <td>[-0.36103835701942444, 0.06514844298362732, 0....</td>\n",
       "      <td>...</td>\n",
       "      <td>episode 1 airs december 13 2022\\nregister now ...</td>\n",
       "      <td>episode 1 airs december 13 2022\\nregister now ...</td>\n",
       "      <td>register now for microsoft envision season 3. ...</td>\n",
       "      <td>895</td>\n",
       "      <td>{'input_ids': tensor([[  101,  2792,  1015, 14...</td>\n",
       "      <td>[('1', 'CARDINAL'), ('13 2022', 'DATE'), ('mic...</td>\n",
       "      <td>0</td>\n",
       "      <td>The email announces the premiere of Microsoft ...</td>\n",
       "      <td>Episode 1 airs December 13, 2022  email? | Vie...</td>\n",
       "      <td>Register now for Microsoft Envision Season 3 T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In September, you had 71 users visit your webs...</td>\n",
       "      <td>Google Analytics &lt;analytics-noreply@google.com&gt;</td>\n",
       "      <td>richard.potter@raddsolutions.co.uk</td>\n",
       "      <td>Tue, 11 Oct 2022 06:02:01 +0100</td>\n",
       "      <td>&lt;https://www.google.com/images/branding/googl...</td>\n",
       "      <td>5559</td>\n",
       "      <td>68</td>\n",
       "      <td>\\n \\r\\nuniversal analytics will no longer pr...</td>\n",
       "      <td>In September you had 71 users visit your websi...</td>\n",
       "      <td>[-0.10645909607410431, 0.17022578418254852, 0....</td>\n",
       "      <td>...</td>\n",
       "      <td>81 35 bounce rate\\nbreakdown of visitors acqui...</td>\n",
       "      <td>81 35 bounce rate\\nbreakdown of visitors acqui...</td>\n",
       "      <td>universal analytics will no longer process new...</td>\n",
       "      <td>740</td>\n",
       "      <td>{'input_ids': tensor([[  101,  5415, 25095,  2...</td>\n",
       "      <td>[('2023', 'DATE'), ('4', 'CARDINAL'), ('septem...</td>\n",
       "      <td>0</td>\n",
       "      <td>Starting in 2023, Universal Analytics will no ...</td>\n",
       "      <td>Universal Analytics will no longer process new...</td>\n",
       "      <td>Starting in 2023, Universal Analytics will no ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>949</th>\n",
       "      <td>Our Black Friday offers have landed!</td>\n",
       "      <td>IT Governance &lt;emailsupport@itgovernance.co.uk&gt;</td>\n",
       "      <td>richie.wynne@raddsolutions.co.uk</td>\n",
       "      <td>Mon, 21 Nov 2022 11:05:15 +0000</td>\n",
       "      <td>Youre not going to want to miss these savings...</td>\n",
       "      <td>3228</td>\n",
       "      <td>36</td>\n",
       "      <td>youre not going to want to miss these savings ...</td>\n",
       "      <td>Our Black Friday offers have landed</td>\n",
       "      <td>[0.09008561074733734, 0.16759826242923737, 0.6...</td>\n",
       "      <td>...</td>\n",
       "      <td>use promo code bf25\\nuse promo code bf25\\nunit...</td>\n",
       "      <td>were starting our black friday offers early wi...</td>\n",
       "      <td>were starting our black friday offers early wi...</td>\n",
       "      <td>1130</td>\n",
       "      <td>{'input_ids': tensor([[  101,  2115,  2063,  2...</td>\n",
       "      <td>[('friday', 'DATE'), ('25', 'CARDINAL'), ('25'...</td>\n",
       "      <td>7</td>\n",
       "      <td>The email advertises an early Black Friday off...</td>\n",
       "      <td>Youre not going to want to miss these savings...</td>\n",
       "      <td>Use promo code BF25 Shop now We accept Get in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>950</th>\n",
       "      <td>Jeff Fritz reviews IronPDF vs SyncFusion, iTex...</td>\n",
       "      <td>\"Jacob, Head of Engineering\" &lt;developers@irons...</td>\n",
       "      <td>Richard Potter &lt;richard.potter@raddsolutions.c...</td>\n",
       "      <td>Tue, 13 Dec 2022 15:31:28 +0000</td>\n",
       "      <td>&lt;https://ironsoftware.lt.acemlnb.com/Prod/lin...</td>\n",
       "      <td>8587</td>\n",
       "      <td>55</td>\n",
       "      <td>\\t \\r\\n \\t \\r\\nhi richard\\r\\nimagine spendin...</td>\n",
       "      <td>Jeff Fritz reviews IronPDF vs SyncFusion iText...</td>\n",
       "      <td>[-0.20470425486564636, 0.20281416177749634, 0....</td>\n",
       "      <td>...</td>\n",
       "      <td>would your project fail hopefully not but it n...</td>\n",
       "      <td>imagine spending a lot of money on software li...</td>\n",
       "      <td>iron software is a free open source solution t...</td>\n",
       "      <td>783</td>\n",
       "      <td>{'input_ids': tensor([[  101,  7632,  2957,  5...</td>\n",
       "      <td>[('jeff fritz', 'PERSON'), ('net conf', 'ORG')...</td>\n",
       "      <td>7</td>\n",
       "      <td>Jeff Fritz from .NET Conf reviewed IronPDF aga...</td>\n",
       "      <td>, Imagine spending a lot of money on software ...</td>\n",
       "      <td>Sent to: &lt;email&gt; Iron Software, 205 N. Michiga...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>951</th>\n",
       "      <td>Don't Miss Out: Help to Grow: Digital Ends in ...</td>\n",
       "      <td>Zym &lt;rebecca@zymplify.com&gt;</td>\n",
       "      <td>Richard Potter &lt;richard.potter@raddsolutions.c...</td>\n",
       "      <td>Tue, 17 Jan 2023 12:01:49 +0000</td>\n",
       "      <td>ZYM helps business owners understand their mar...</td>\n",
       "      <td>6966</td>\n",
       "      <td>56</td>\n",
       "      <td>zym helps business owners understand their mar...</td>\n",
       "      <td>Dont Miss Out Help to Grow Digital Ends in 16 ...</td>\n",
       "      <td>[0.0035861318465322256, 0.07786554843187332, 0...</td>\n",
       "      <td>...</td>\n",
       "      <td>you may qualify for the grant meaning you coul...</td>\n",
       "      <td>even better you can try zym for free for 14 da...</td>\n",
       "      <td>zym helps business owners understand their mar...</td>\n",
       "      <td>345</td>\n",
       "      <td>{'input_ids': tensor([[  101,  1062, 24335,  7...</td>\n",
       "      <td>[('uk', 'GPE'), ('up to 5000', 'CARDINAL'), ('...</td>\n",
       "      <td>7</td>\n",
       "      <td>Zym aids business owners in comprehending mark...</td>\n",
       "      <td>ZYM helps business owners understand their mar...</td>\n",
       "      <td>, Zym P.S. Copyright  2023 Zym AI, All rights...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>952</th>\n",
       "      <td>New videos coming in 2023</td>\n",
       "      <td>Clear Measure &lt;clearmeasure@clear-measure.com&gt;</td>\n",
       "      <td>richard.potter@raddsolutions.co.uk</td>\n",
       "      <td>Thu, 29 Dec 2022 10:00:02 +0000</td>\n",
       "      <td>New videos coming in 2023  made to empower yo...</td>\n",
       "      <td>3404</td>\n",
       "      <td>25</td>\n",
       "      <td>new videos coming in 2023  made to empower you...</td>\n",
       "      <td>New videos coming in 2023</td>\n",
       "      <td>[-0.08648061007261276, 0.11852650344371796, 0....</td>\n",
       "      <td>...</td>\n",
       "      <td>our new videos coming in 2023 are made to empo...</td>\n",
       "      <td>our new videos coming in 2023 are made to empo...</td>\n",
       "      <td>new videos coming in 2023  made to empower you...</td>\n",
       "      <td>958</td>\n",
       "      <td>{'input_ids': tensor([[  101,  2047,  6876,  2...</td>\n",
       "      <td>[('2023', 'DATE'), ('2023', 'DATE'), ('10815',...</td>\n",
       "      <td>7</td>\n",
       "      <td>Summary:\\n\\nClear Measure has announced an upc...</td>\n",
       "      <td>New videos coming in 2023  made to empower . ...</td>\n",
       "      <td>Our new videos coming in 2023 are made to empo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>953</th>\n",
       "      <td>Business PCs up to 40% off</td>\n",
       "      <td>Lenovo New beginnings! &lt;lenovo@ecomm.lenovo.com&gt;</td>\n",
       "      <td>richard.potter@raddsolutions.co.uk</td>\n",
       "      <td>Wed, 01 Feb 2023 09:04:42 +0000</td>\n",
       "      <td>&lt;https://trk.ecomm.lenovo.com/ss/o/k8leYzsS8M...</td>\n",
       "      <td>8108</td>\n",
       "      <td>26</td>\n",
       "      <td>\\n \\t\\r\\n\\tview it in browser instead \\n  fre...</td>\n",
       "      <td>Business PCs up to 40 off</td>\n",
       "      <td>[-0.09954569488763809, 0.13386352360248566, 0....</td>\n",
       "      <td>...</td>\n",
       "      <td>workspace refresh with up to 40 discount until...</td>\n",
       "      <td>thinkpad p1 gen 4\\nthinkpad z13 gen 1\\nideapad...</td>\n",
       "      <td>Free shipping on all orders with up to 40% dis...</td>\n",
       "      <td>132</td>\n",
       "      <td>{'input_ids': tensor([[  101,  3193,  2009,  1...</td>\n",
       "      <td>[('up to 40', 'CARDINAL'), ('up to 40', 'CARDI...</td>\n",
       "      <td>7</td>\n",
       "      <td>Email from Lenovo announces a workspace refres...</td>\n",
       "      <td>View it in browser instead Free shipping on al...</td>\n",
       "      <td>Lenovo and the Lenovo logo, ThinkPad and ideap...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>954 rows  23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               subject  \\\n",
       "0             Thank you for attending Microsoft Ignite   \n",
       "1    Invitation to learn how Windows 365 Cloud PCs ...   \n",
       "2    Dont fall behind  embrace AI with Dell Techn...   \n",
       "3       Microsoft Envision Season 3 begins December 13   \n",
       "4    In September, you had 71 users visit your webs...   \n",
       "..                                                 ...   \n",
       "949               Our Black Friday offers have landed!   \n",
       "950  Jeff Fritz reviews IronPDF vs SyncFusion, iTex...   \n",
       "951  Don't Miss Out: Help to Grow: Digital Ends in ...   \n",
       "952                          New videos coming in 2023   \n",
       "953                         Business PCs up to 40% off   \n",
       "\n",
       "                                                  from  \\\n",
       "0       Microsoft Team <microsoft@email.microsoft.com>   \n",
       "1              Microsoft <replyto@email.microsoft.com>   \n",
       "2    Dell Technologies Partner Program <DellTechnol...   \n",
       "3       Microsoft Team <microsoft@email.microsoft.com>   \n",
       "4      Google Analytics <analytics-noreply@google.com>   \n",
       "..                                                 ...   \n",
       "949    IT Governance <emailsupport@itgovernance.co.uk>   \n",
       "950  \"Jacob, Head of Engineering\" <developers@irons...   \n",
       "951                         Zym <rebecca@zymplify.com>   \n",
       "952     Clear Measure <clearmeasure@clear-measure.com>   \n",
       "953   Lenovo New beginnings! <lenovo@ecomm.lenovo.com>   \n",
       "\n",
       "                                                    to  \\\n",
       "0                   richard.potter@raddsolutions.co.uk   \n",
       "1                   richard.potter@raddsolutions.co.uk   \n",
       "2                   richard.potter@raddsolutions.co.uk   \n",
       "3                   richard.potter@raddsolutions.co.uk   \n",
       "4                   richard.potter@raddsolutions.co.uk   \n",
       "..                                                 ...   \n",
       "949                   richie.wynne@raddsolutions.co.uk   \n",
       "950  Richard Potter <richard.potter@raddsolutions.c...   \n",
       "951  Richard Potter <richard.potter@raddsolutions.c...   \n",
       "952                 richard.potter@raddsolutions.co.uk   \n",
       "953                 richard.potter@raddsolutions.co.uk   \n",
       "\n",
       "                                date  \\\n",
       "0    Wed, 19 Oct 2022 20:31:34 +0100   \n",
       "1    Tue, 01 Nov 2022 11:01:50 +0000   \n",
       "2    Tue, 15 Nov 2022 06:01:17 +0000   \n",
       "3    Wed, 09 Nov 2022 17:05:09 +0000   \n",
       "4    Tue, 11 Oct 2022 06:02:01 +0100   \n",
       "..                               ...   \n",
       "949  Mon, 21 Nov 2022 11:05:15 +0000   \n",
       "950  Tue, 13 Dec 2022 15:31:28 +0000   \n",
       "951  Tue, 17 Jan 2023 12:01:49 +0000   \n",
       "952  Thu, 29 Dec 2022 10:00:02 +0000   \n",
       "953  Wed, 01 Feb 2023 09:04:42 +0000   \n",
       "\n",
       "                                                  body  Body_Length  \\\n",
       "0                                                  ...         6232   \n",
       "1    Webinar with demos of Windows 365 and vision f...         2943   \n",
       "2     <https://click.comm.delltechnologies.com/open...         4498   \n",
       "3    Episode 1 airs December 13, 2022 \\r\\nHaving tr...         3476   \n",
       "4     <https://www.google.com/images/branding/googl...         5559   \n",
       "..                                                 ...          ...   \n",
       "949  Youre not going to want to miss these savings...         3228   \n",
       "950   <https://ironsoftware.lt.acemlnb.com/Prod/lin...         8587   \n",
       "951  ZYM helps business owners understand their mar...         6966   \n",
       "952  New videos coming in 2023  made to empower yo...         3404   \n",
       "953   <https://trk.ecomm.lenovo.com/ss/o/k8leYzsS8M...         8108   \n",
       "\n",
       "     Subject_Length                                       Cleaned_Body  \\\n",
       "0                40                                                ...   \n",
       "1                90  webinar with demos of windows 365 and vision f...   \n",
       "2                64   \\n   \\t\\r\\nview online \\n\\n  \\t\\r\\n \\t\\r\\nwhy...   \n",
       "3                46  episode 1 airs december 13 2022 \\r\\nhaving tro...   \n",
       "4                68    \\n \\r\\nuniversal analytics will no longer pr...   \n",
       "..              ...                                                ...   \n",
       "949              36  youre not going to want to miss these savings ...   \n",
       "950              55    \\t \\r\\n \\t \\r\\nhi richard\\r\\nimagine spendin...   \n",
       "951              56  zym helps business owners understand their mar...   \n",
       "952              25  new videos coming in 2023  made to empower you...   \n",
       "953              26   \\n \\t\\r\\n\\tview it in browser instead \\n  fre...   \n",
       "\n",
       "                                       Cleaned_Subject  \\\n",
       "0             Thank you for attending Microsoft Ignite   \n",
       "1    Invitation to learn how Windows 365 Cloud PCs ...   \n",
       "2    Dont fall behind  embrace AI with Dell Technol...   \n",
       "3       Microsoft Envision Season 3 begins December 13   \n",
       "4    In September you had 71 users visit your websi...   \n",
       "..                                                 ...   \n",
       "949                Our Black Friday offers have landed   \n",
       "950  Jeff Fritz reviews IronPDF vs SyncFusion iText...   \n",
       "951  Dont Miss Out Help to Grow Digital Ends in 16 ...   \n",
       "952                          New videos coming in 2023   \n",
       "953                          Business PCs up to 40 off   \n",
       "\n",
       "                                       BERT_Embeddings  ...  \\\n",
       "0    [-0.059376951307058334, 0.17135855555534363, 0...  ...   \n",
       "1    [-0.1439182013273239, 0.22149936854839325, 0.6...  ...   \n",
       "2    [-0.15142026543617249, 0.11411778628826141, 0....  ...   \n",
       "3    [-0.36103835701942444, 0.06514844298362732, 0....  ...   \n",
       "4    [-0.10645909607410431, 0.17022578418254852, 0....  ...   \n",
       "..                                                 ...  ...   \n",
       "949  [0.09008561074733734, 0.16759826242923737, 0.6...  ...   \n",
       "950  [-0.20470425486564636, 0.20281416177749634, 0....  ...   \n",
       "951  [0.0035861318465322256, 0.07786554843187332, 0...  ...   \n",
       "952  [-0.08648061007261276, 0.11852650344371796, 0....  ...   \n",
       "953  [-0.09954569488763809, 0.13386352360248566, 0....  ...   \n",
       "\n",
       "                                      summary_TXTRNK_1  \\\n",
       "0    microsoft ignite may be over but heres your ch...   \n",
       "1                                           No Summary   \n",
       "2    we are witnessing and living through the rise ...   \n",
       "3    episode 1 airs december 13 2022\\nregister now ...   \n",
       "4    81 35 bounce rate\\nbreakdown of visitors acqui...   \n",
       "..                                                 ...   \n",
       "949  use promo code bf25\\nuse promo code bf25\\nunit...   \n",
       "950  would your project fail hopefully not but it n...   \n",
       "951  you may qualify for the grant meaning you coul...   \n",
       "952  our new videos coming in 2023 are made to empo...   \n",
       "953  workspace refresh with up to 40 discount until...   \n",
       "\n",
       "                                               Summary  \\\n",
       "0    microsoft ignite may be over but heres your ch...   \n",
       "1                                                        \n",
       "2    why ai and why now\\nwe are witnessing and livi...   \n",
       "3    episode 1 airs december 13 2022\\nregister now ...   \n",
       "4    81 35 bounce rate\\nbreakdown of visitors acqui...   \n",
       "..                                                 ...   \n",
       "949  were starting our black friday offers early wi...   \n",
       "950  imagine spending a lot of money on software li...   \n",
       "951  even better you can try zym for free for 14 da...   \n",
       "952  our new videos coming in 2023 are made to empo...   \n",
       "953  thinkpad p1 gen 4\\nthinkpad z13 gen 1\\nideapad...   \n",
       "\n",
       "                                          summary_BART index_number  \\\n",
       "0    Learn how microsoft empowers organisations to ...         1543   \n",
       "1    webinar with demos of windows 365 and vision f...          765   \n",
       "2    Artificial intelligence ai market is forecast ...          369   \n",
       "3    register now for microsoft envision season 3. ...          895   \n",
       "4    universal analytics will no longer process new...          740   \n",
       "..                                                 ...          ...   \n",
       "949  were starting our black friday offers early wi...         1130   \n",
       "950  iron software is a free open source solution t...          783   \n",
       "951  zym helps business owners understand their mar...          345   \n",
       "952  new videos coming in 2023  made to empower you...          958   \n",
       "953  Free shipping on all orders with up to 40% dis...          132   \n",
       "\n",
       "                                       Tokenized_Email  \\\n",
       "0    {'input_ids': tensor([[  101,  7513, 16270,  4...   \n",
       "1    {'input_ids': tensor([[  101,  4773,  3981,  2...   \n",
       "2    {'input_ids': tensor([[  101,  3193,  3784,  2...   \n",
       "3    {'input_ids': tensor([[  101,  2792,  1015, 14...   \n",
       "4    {'input_ids': tensor([[  101,  5415, 25095,  2...   \n",
       "..                                                 ...   \n",
       "949  {'input_ids': tensor([[  101,  2115,  2063,  2...   \n",
       "950  {'input_ids': tensor([[  101,  7632,  2957,  5...   \n",
       "951  {'input_ids': tensor([[  101,  1062, 24335,  7...   \n",
       "952  {'input_ids': tensor([[  101,  2047,  6876,  2...   \n",
       "953  {'input_ids': tensor([[  101,  3193,  2009,  1...   \n",
       "\n",
       "                                              Entities  Cluster_retrieved  \\\n",
       "0    [('microsoft', 'ORG'), ('2022', 'DATE'), ('mic...                  0   \n",
       "1    [('thursday 17th', 'DATE'), ('2022 1400  1500'...                  0   \n",
       "2    [('500 billion', 'MONEY'), ('20231', 'DATE'), ...                  0   \n",
       "3    [('1', 'CARDINAL'), ('13 2022', 'DATE'), ('mic...                  0   \n",
       "4    [('2023', 'DATE'), ('4', 'CARDINAL'), ('septem...                  0   \n",
       "..                                                 ...                ...   \n",
       "949  [('friday', 'DATE'), ('25', 'CARDINAL'), ('25'...                  7   \n",
       "950  [('jeff fritz', 'PERSON'), ('net conf', 'ORG')...                  7   \n",
       "951  [('uk', 'GPE'), ('up to 5000', 'CARDINAL'), ('...                  7   \n",
       "952  [('2023', 'DATE'), ('2023', 'DATE'), ('10815',...                  7   \n",
       "953  [('up to 40', 'CARDINAL'), ('up to 40', 'CARDI...                  7   \n",
       "\n",
       "                                         Summary_human  \\\n",
       "0    The email discusses post-Microsoft Ignite 2022...   \n",
       "1    Webinar Announcement: \"Windows 365 for Your Hy...   \n",
       "2    The email discusses the rapid growth of artifi...   \n",
       "3    The email announces the premiere of Microsoft ...   \n",
       "4    Starting in 2023, Universal Analytics will no ...   \n",
       "..                                                 ...   \n",
       "949  The email advertises an early Black Friday off...   \n",
       "950  Jeff Fritz from .NET Conf reviewed IronPDF aga...   \n",
       "951  Zym aids business owners in comprehending mark...   \n",
       "952  Summary:\\n\\nClear Measure has announced an upc...   \n",
       "953  Email from Lenovo announces a workspace refres...   \n",
       "\n",
       "                                     Preprocessed_Body  \\\n",
       "0    Microsoft Ignite may be over, but heres  cont...   \n",
       "1    Webinar with demos of Windows 365 and vision f...   \n",
       "2    View Online Why AI and why now ? Why AI and wh...   \n",
       "3    Episode 1 airs December 13, 2022  email? | Vie...   \n",
       "4    Universal Analytics will no longer process new...   \n",
       "..                                                 ...   \n",
       "949  Youre not going to want to miss these savings...   \n",
       "950  , Imagine spending a lot of money on software ...   \n",
       "951  ZYM helps business owners understand their mar...   \n",
       "952  New videos coming in 2023  made to empower . ...   \n",
       "953  View it in browser instead Free shipping on al...   \n",
       "\n",
       "                                    Summary_txt_grph_2  \n",
       "0    Dive into the new innovations we announced at ...  \n",
       "1    * Updates from Ignite, including the built-in ...  \n",
       "2    No. . Servicenow, 2019. IDC, Feb 2022. Reg. De...  \n",
       "3    Register now for Microsoft Envision Season 3 T...  \n",
       "4    Starting in 2023, Universal Analytics will no ...  \n",
       "..                                                 ...  \n",
       "949  Use promo code BF25 Shop now We accept Get in ...  \n",
       "950  Sent to: <email> Iron Software, 205 N. Michiga...  \n",
       "951  , Zym P.S. Copyright  2023 Zym AI, All rights...  \n",
       "952  Our new videos coming in 2023 are made to empo...  \n",
       "953  Lenovo and the Lenovo logo, ThinkPad and ideap...  \n",
       "\n",
       "[954 rows x 23 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b2a909",
   "metadata": {},
   "source": [
    "## Preprocessing for target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ceed291d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\764883\\AppData\\Local\\Temp\\ipykernel_12060\\452633512.py:3: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(html, \"html.parser\")\n"
     ]
    }
   ],
   "source": [
    "def preprocess_summary_for_t5(summary):\n",
    "    summary = convert_html_to_text(summary)\n",
    "\n",
    "    # Remove extra whitespaces\n",
    "    summary = ' '.join(summary.split())\n",
    "\n",
    "    # Remove common greeting texts\n",
    "    greetings_pattern = (\n",
    "    r\"Hi\\s\\w+|\"\n",
    "    r\"Hello\\s\\w+|\"\n",
    "    r\"Dear\\s\\w+|\"\n",
    "    r\"Dear\\s[Mm]r\\.\\s\\w+|\"\n",
    "    r\"Dear\\s[Mm]rs\\.\\s\\w+|\"\n",
    "    r\"Dear\\s[Mm]s\\.\\s\\w+|\"\n",
    "    r\"Dear\\s[MD]r(s)?\\.\\s\\w+|\"\n",
    "    r\"Dear\\sProf\\.\\s\\w+|\"\n",
    "    r\"Dear\\sDoctor\\.\\s\\w+|\"\n",
    "    r\"Greetings|\"\n",
    "    r\"Good\\s[Mm]orning|\"\n",
    "    r\"Good\\s[Aa]fternoon|\"\n",
    "    r\"Good\\s[Ee]vening|\"\n",
    "    r\"Hey\\s\\w+|\"\n",
    "    r\"Hey\\sthere|\"\n",
    "    r\"Hello\\severyone|\"\n",
    "    r\"Hope\\syou\\sare\\sdoing\\swell|\"\n",
    "    r\"Hope\\syou\\sare\\sdoing\\swell|\"\n",
    "    r\"Hope\\sthis\\semail\\sfinds\\syou\\swell|\"\n",
    "    r\"Hope\\sthis\\semail\\sfinds\\syou\\swell|\"\n",
    "    r\"Hope\\sthis\\semail\\sfinds\\syou\\sin\\sgood\\shealth|\"\n",
    "    r\"How\\sare\\syou\\sdoing|\"\n",
    "    r\"How\\sis\\sit\\sgoing|\"\n",
    "    r\"To\\swhom\\sit\\smay\\sconcern|\"\n",
    "    r\"To\\swhom\\sit\\smay\\sconcern|\"\n",
    "    r\"I\\sam\\swriting\\sto\\syou\\sbecause|\"\n",
    "    r\"I\\sam\\swriting\\sto\\syou\\sto|\"\n",
    "    r\"I\\shope\\sthat\\syou|\"\n",
    "    r\"I\\swanted\\sto\\sreach\\sout\\sto|\"\n",
    "    r\"I\\swanted\\sto\\slet\\syou\\sknow|\"\n",
    "    r\"I\\swould\\slike\\sto\\sinform\\syou|\"\n",
    "    r\"It\\spleases\\sme\\sto\\scontact\\syou|\"\n",
    "    r\"It\\shas\\scome\\sto\\smy\\sattention|\"\n",
    "    r\"I\\swas\\sjust\\sthinking\\sabout\\syou\\sand\\s|\"\n",
    "    r\"Allow\\sme\\sto\\sintroduce\\smyself|\"\n",
    "    r\"I\\shope\\sthat\\severything\\sis\\sgoing\\swell|\"\n",
    "    r\"Thank\\syou\\sfor\\syour\\semail|\"\n",
    "    r\"Thank\\syou\\sfor\\sreaching\\sout\")\n",
    "\n",
    "    summary = re.sub(greetings_pattern, \"\", summary, flags=re.IGNORECASE)\n",
    "\n",
    "    # Remove common sign-offs\n",
    "    signoffs_pattern = (\n",
    "    r\"Best\\sregards|\"\n",
    "    r\"Best\\s\\w+|\"\n",
    "    r\"Sincerely|\"\n",
    "    r\"Yours\\sfaithfully|\"\n",
    "    r\"Yours\\ssincerely|\"\n",
    "    r\"Regards|\"\n",
    "    r\"Warm\\sregards|\"\n",
    "    r\"Kind\\sregards|\"\n",
    "    r\"Cheers|\"\n",
    "    r\"Thanks\\sand\\sregards|\"\n",
    "    r\"Thank\\syou|\"\n",
    "    r\"Take\\scare|\"\n",
    "    r\"Looking\\sforward|\"\n",
    "    r\"All\\sbest|\"\n",
    "    r\"Best\\swishes|\"\n",
    "    r\"Best|\"\n",
    "    r\"Yours\\struly|\"\n",
    "    r\"Cordially|\"\n",
    "    r\"With\\sappreciation|\"\n",
    "    r\"Respectfully|\"\n",
    "    r\"With\\sregards|\"\n",
    "    r\"Many\\sthanks|\"\n",
    "    r\"Hope\\sto\\shear\\sfrom\\syou\\ssoon|\"\n",
    "    r\"Until\\snext\\stime|\"\n",
    "    r\"Yours\\svery\\struly|\"\n",
    "    r\"Yours|\"\n",
    "    r\"In\\sgratitude|\"\n",
    "    r\"In\\ssympathy|\"\n",
    "    r\"Thoughtfully|\"\n",
    "    r\"With\\saffection|\"\n",
    "    r\"Fond\\sregards|\"\n",
    "    r\"With\\santicipation|\"\n",
    "    r\"Stay\\swell|\"\n",
    "    r\"Stay\\ssafe|\"\n",
    "    r\"Peace|\"\n",
    "    r\"God\\sbless|\"\n",
    "    r\"Love|\"\n",
    "    r\"Sent\\sfrom\\smy\\s\\w+|\"\n",
    "    r\"Sent\\sfrom\\smy\\s\\w+\\s\\w+|\"\n",
    "    r\"Talk\\sto\\syou\\ssoon|\"\n",
    "    r\"See\\syou\\ssoon|\"\n",
    "    r\"See\\sya|\"\n",
    "    r\"Ciao|\"\n",
    "    r\"Adieu|\"\n",
    "    r\"Farewell|\"\n",
    "    r\"Good\\sbye|\"\n",
    "    r\"Bye\\sfor\\snow|\"\n",
    "    r\"Signing\\soff|\"\n",
    "    r\"Out|\"\n",
    "    r\"Yours\\s[in]\\s\\w+|\"\n",
    "    r\"Your\\sfriend|\"\n",
    "    r\"Your\\s\\w+\\s\\w+|\"\n",
    "    r\"Keep\\sin\\stouch|\"\n",
    "    r\"Yours\\sfaithfully|\"\n",
    "    r\"Yours\\ssincerely|\"\n",
    "    r\"Yours\\sobediently|\"\n",
    "    r\"Yours\\saffectionately|\"\n",
    "    r\"Yours\\scordially|\"\n",
    "    r\"Yours\\srespectfully|\"\n",
    "    r\"Yours\\struly|\"\n",
    "    r\"Yours\\sever\")\n",
    "\n",
    "    summary = re.sub(signoffs_pattern, \"\", summary, flags=re.IGNORECASE)\n",
    "\n",
    "    # Pattern to remove repetitive characters like dashes or underscores\n",
    "    repetitive_pattern = r\"[-_]{3,}\"  \n",
    "    summary = re.sub(repetitive_pattern, \"\", summary)\n",
    "\n",
    "    # Standardize dates and times\n",
    "    summary = standardize_date_time(summary)\n",
    "\n",
    "    # Remove email signatures and disclaimers\n",
    "    summary = re.sub(r\"--\\s*[\\s\\S]*$\", \"\", summary)\n",
    "\n",
    "    # Standardize email addresses and URLs\n",
    "    summary = re.sub(r\"\\S+@\\S+\\.\\S+\", \"<email>\", summary)\n",
    "    summary = re.sub(r\"http\\S+\", \"<url>\", summary)\n",
    "\n",
    "    # Remove phrases indicating difficulty in viewing images or links\n",
    "    irrelevant_phrases_pattern = (\n",
    "    r\"difficulty in viewing this image|\"\n",
    "    r\"click here|\"\n",
    "    r\"having trouble viewing this|\"\n",
    "    r\"view this email in your browser|\"\n",
    "    r\"to ensure delivery to your inbox|\"\n",
    "    r\"if you cannot see this message|\"\n",
    "    r\"message not displaying correctly|\"\n",
    "    r\"trouble seeing this email|\"\n",
    "    r\"can't see the images below|\"\n",
    "    r\"email not looking quite right|\"\n",
    "    r\"viewing this email on a mobile device|\"\n",
    "    r\"can't read this email|\"\n",
    "    r\"images not showing up|\"\n",
    "    r\"to view the online version of this email|\"\n",
    "    r\"email doesn't display correctly|\"\n",
    "    r\"problems seeing this email|\"\n",
    "    r\"to unsubscribe or change preferences|\"\n",
    "    r\"this message was sent to <email>|\"\n",
    "    r\"not interested in these emails|\"\n",
    "    r\"you're receiving this email because|\"\n",
    "    r\"to stop receiving these emails|\"\n",
    "    r\"unsubscribe from this list|\"\n",
    "    r\"manage your email preferences\")\n",
    "    summary = re.sub(irrelevant_phrases_pattern, \"\", summary, flags=re.IGNORECASE)\n",
    "\n",
    "    # # Add task-specific prefix (if you are going to use this text directly for T5 summarization later)\n",
    "    # email_body = \"summarize: \" + email_body\n",
    "\n",
    "    return summary\n",
    "df['Preprocessed_Summary'] = df['Summary_human'].apply(lambda x: preprocess_summary_for_t5(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0978a5e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      The email discusses post-Microsoft Ignite 2022...\n",
       "1      Webinar Announcement: \"Windows 365 for \" event...\n",
       "2      The email discusses the rapid growth of artifi...\n",
       "3      The email announces the premiere of Microsoft ...\n",
       "4      Starting in 2023, Universal Analytics will no ...\n",
       "                             ...                        \n",
       "949    The email advertises an early Black Friday off...\n",
       "950    Jeff Fritz from .NET Conf reviewed IronPDF aga...\n",
       "951    Zym aids business owners in comprehending mark...\n",
       "952    Summary: Clear Measure has announced an upcomi...\n",
       "953    Email from Lenovo announces a workspace refres...\n",
       "Name: Preprocessed_Summary, Length: 954, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Preprocessed_Summary']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c1d844",
   "metadata": {},
   "source": [
    "## Preparing the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d086fc50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\764883\\AppData\\Local\\Temp\\ipykernel_12060\\2143533575.py:41: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_dataset = TensorDataset(torch.tensor(train_encodings['input_ids']), torch.tensor(train_encodings['attention_mask']), torch.tensor(train_encodings['labels']))\n",
      "C:\\Users\\764883\\AppData\\Local\\Temp\\ipykernel_12060\\2143533575.py:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  val_dataset = TensorDataset(torch.tensor(val_encodings['input_ids']), torch.tensor(val_encodings['attention_mask']), torch.tensor(val_encodings['labels']))\n",
      "C:\\Users\\764883\\AppData\\Local\\Temp\\ipykernel_12060\\2143533575.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_dataset = TensorDataset(torch.tensor(test_encodings['input_ids']), torch.tensor(test_encodings['attention_mask']), torch.tensor(test_encodings['labels']))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BartForConditionalGeneration(\n",
       "  (model): BartModel(\n",
       "    (shared): Embedding(50265, 1024, padding_idx=1)\n",
       "    (encoder): BartEncoder(\n",
       "      (embed_tokens): Embedding(50265, 1024, padding_idx=1)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): BartDecoder(\n",
       "      (embed_tokens): Embedding(50265, 1024, padding_idx=1)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=50265, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "## loading in the data\n",
    "data = df\n",
    "data = data[['Preprocessed_Body', 'Preprocessed_Summary']].dropna()\n",
    "data = data.sample(frac=1, random_state=0)  # Shuffle the data\n",
    "dataset = Dataset.from_pandas(data)\n",
    "dataset = dataset.train_test_split(test_size=0.3)  # Splitting the data\n",
    "dataset[\"test\"] = dataset[\"test\"].train_test_split(test_size=0.5)\n",
    "\n",
    "# Prepare the data\n",
    "train_source = dataset[\"train\"][\"Preprocessed_Body\"]\n",
    "train_target = dataset[\"train\"][\"Preprocessed_Summary\"]\n",
    "val_source = dataset[\"test\"][\"train\"][\"Preprocessed_Body\"]\n",
    "val_target = dataset[\"test\"][\"train\"][\"Preprocessed_Summary\"]\n",
    "test_source = dataset[\"test\"][\"test\"][\"Preprocessed_Body\"]\n",
    "test_target = dataset[\"test\"][\"test\"][\"Preprocessed_Summary\"]\n",
    "\n",
    "# Initialize tokenizer for BART\n",
    "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large\")\n",
    "\n",
    "# Tokenize the data\n",
    "def tokenize_data(source, target, max_length=1024, max_target_length=150):\n",
    "    encodings = tokenizer(source, truncation=True, max_length=max_length, padding=True)\n",
    "    decodings = tokenizer(target, truncation=True, max_length=max_target_length, padding=True)\n",
    "    return {\n",
    "        'input_ids': torch.tensor(encodings['input_ids']),\n",
    "        'attention_mask': torch.tensor(encodings['attention_mask']),\n",
    "        'labels': torch.tensor(decodings['input_ids'])\n",
    "    }\n",
    "\n",
    "train_encodings = tokenize_data(train_source, train_target)\n",
    "val_encodings = tokenize_data(val_source, val_target)\n",
    "test_encodings = tokenize_data(test_source, test_target)\n",
    "\n",
    "# Create TensorDatasets\n",
    "train_dataset = TensorDataset(torch.tensor(train_encodings['input_ids']), torch.tensor(train_encodings['attention_mask']), torch.tensor(train_encodings['labels']))\n",
    "val_dataset = TensorDataset(torch.tensor(val_encodings['input_ids']), torch.tensor(val_encodings['attention_mask']), torch.tensor(val_encodings['labels']))\n",
    "test_dataset = TensorDataset(torch.tensor(test_encodings['input_ids']), torch.tensor(test_encodings['attention_mask']), torch.tensor(test_encodings['labels']))\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True, num_workers=4)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=2,num_workers=4)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=2, num_workers=4)\n",
    "\n",
    "# Load BART model\n",
    "model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621793a4",
   "metadata": {},
   "source": [
    "## Creating metrics for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8dd7de97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\764883\\AppData\\Local\\Temp\\ipykernel_12060\\2820315665.py:2: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library  Evaluate: https://huggingface.co/docs/evaluate\n",
      "  rouge_metric = load_metric(\"rouge\")\n"
     ]
    }
   ],
   "source": [
    "# Load metrics\n",
    "rouge_metric = load_metric(\"rouge\")\n",
    "bert_metric = load_metric(\"bertscore\")\n",
    "\n",
    "def validate_and_calculate_metrics(model, dataloader, tokenizer, device):\n",
    "    model.eval()\n",
    "    rouge_scores = []\n",
    "    bert_scores = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Validating\"):\n",
    "            input_ids, attention_masks, labels = [t.to(device) for t in batch]\n",
    "            generated_ids = model.generate(input_ids, attention_mask=attention_masks, max_length=150)\n",
    "            preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
    "            refs = [tokenizer.decode(l, skip_special_tokens=True, clean_up_tokenization_spaces=True) for l in labels]\n",
    "\n",
    "            # Calculate ROUGE\n",
    "            rouge_scores.append(rouge_metric.compute(predictions=preds, references=refs))\n",
    "            \n",
    "            # Calculate BERTScore\n",
    "            P, R, F1 = bert_score(preds, refs, lang=\"en\", rescale_with_baseline=True)\n",
    "            bert_scores.append({\"precision\": P.mean().item(), \"recall\": R.mean().item(), \"f1\": F1.mean().item()})\n",
    "\n",
    "    # Aggregate scores\n",
    "    rouge_final = {key: sum(score[key].mid.fmeasure for score in rouge_scores) / len(rouge_scores) for key in rouge_scores[0]}\n",
    "    bert_final = {\n",
    "        \"precision\": sum(score[\"precision\"] for score in bert_scores) / len(bert_scores),\n",
    "        \"recall\": sum(score[\"recall\"] for score in bert_scores) / len(bert_scores),\n",
    "        \"f1\": sum(score[\"f1\"] for score in bert_scores) / len(bert_scores)\n",
    "    }\n",
    "\n",
    "    return rouge_final, bert_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808c5833",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e446a362",
   "metadata": {},
   "source": [
    "## Learning rate 1e-4, weight decay=0.01, epoch= 2, linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3719e088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ef99a896",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0a08086c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "806492f16e95426f95b0d378f86ed3d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 2.927431161889059\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcaaf4835c5243a39fbb015f88a8a172",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 1.3295451940761671\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31dce6bceaf04ad4a0f04f94c906d6d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Validation Scores: ROUGE: {'rouge1': 0.43327392718057545, 'rouge2': 0.21540926734910104, 'rougeL': 0.31794327911113235, 'rougeLsum': 0.31794327911113235}, BERTScore: {'precision': 0.2717319605872035, 'recall': 0.25954491602412116, 'f1': 0.265584509457565}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "epochs = 1\n",
    "\n",
    "\n",
    "# Define the optimizer with weight decay\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)\n",
    "\n",
    "# Total number of training steps\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Set up the learning rate scheduler\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps=200, \n",
    "                                            num_training_steps=total_steps)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch}\\n\")\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    with tqdm(train_dataloader, unit=\"batch\") as tepoch:    \n",
    "        for batch in tepoch:\n",
    "            tepoch.set_description(f\"Epoch {epoch}\")\n",
    "            input_ids, attention_masks, labels = tuple(t.to(device) for t in batch)\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_masks, labels=labels)\n",
    "\n",
    "            loss = outputs.loss\n",
    "            train_loss += loss.item()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()  # Update the learning rate\n",
    "            tepoch.set_postfix(loss=loss.item())\n",
    "        train_loss /= len(train_dataloader)\n",
    "        sleep(0.1)\n",
    "    print(f\"Training loss: {train_loss}\")\n",
    "\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        with tqdm(test_dataloader, unit=\"batch\") as tepoch:\n",
    "            for batch in tepoch:\n",
    "                tepoch.set_description(f\"Epoch {epoch}\")\n",
    "                input_ids, attention_masks, labels = tuple(t.to(device) for t in batch)\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_masks, labels=labels)\n",
    "                loss = outputs.loss\n",
    "                test_loss += loss.item()\n",
    "                tepoch.set_postfix(loss=loss.item())\n",
    "\n",
    "            test_loss /= len(test_dataloader)\n",
    "    print(f\"Testing loss: {test_loss}\")\n",
    "\n",
    "    # Validation and Metric Calculation\n",
    "    rouge_scores, bert_scores = validate_and_calculate_metrics(model, val_dataloader, tokenizer, device)\n",
    "    print(f\"Epoch {epoch} Validation Scores: ROUGE: {rouge_scores}, BERTScore: {bert_scores}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a33301",
   "metadata": {},
   "source": [
    "## BART with 5 epochs on the same hyprparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0bb0cfbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e8c8bff341a4b5db5069aebb0e30e9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 2.8091129871185667\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50ad68cd0bca433ba99db975be346ff4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 1.272396445274353\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22ebe3dc47824474a19ff88b92e4e894",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6808b9ac57164a74964bc041ba87754b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9518d42e30a0461983aafcc78a171c65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b12e96801cf4937bbafe4051a7ec82c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71486e8355ad44ebad22c2d28bd1518f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c10766c441c4893ad3bece349e2527c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Validation Scores: ROUGE: {'rouge1': 0.44409835951013554, 'rouge2': 0.22730734216977588, 'rougeL': 0.31648218563624236, 'rougeLsum': 0.31648218563624236}, BERTScore: {'precision': 0.23278838926408854, 'recall': 0.3053226168267429, 'f1': 0.2688204678706825}\n",
      "Epoch 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a18a531ec0ad43e9b73c56a4dbce8144",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 1.5711643076585438\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10d44874756c4cc99eaec6e113d88687",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 1.6292051101724307\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80776e694c0d4d31abc3e0bf3f178fc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Validation Scores: ROUGE: {'rouge1': 0.3686681432943903, 'rouge2': 0.18059792416538187, 'rougeL': 0.2711731934668927, 'rougeLsum': 0.2711731934668927}, BERTScore: {'precision': 0.24190812718330157, 'recall': 0.15013131892515552, 'f1': 0.19503008049084908}\n",
      "Epoch 2\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a623654ecc804267b8bd01338b6b576a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 1.0476717324314002\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c58cf5b92c44531a1856fe96f9aecf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 1.2189091791709263\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2249bbcb37a647439e26d045eefb561d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Validation Scores: ROUGE: {'rouge1': 0.4604125896978996, 'rouge2': 0.23879519689059933, 'rougeL': 0.3441206768032998, 'rougeLsum': 0.3441206768032998}, BERTScore: {'precision': 0.3646850304471122, 'recall': 0.2931119240561707, 'f1': 0.3285768216786285}\n",
      "Epoch 3\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bee7bc991ad6458cb79d12cefa066e66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.671960660752779\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "897d27a895884cb6884e98f48b6db202",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 1.196504696375794\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e67c846edcad4e4ebf8cb63746e37e78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Validation Scores: ROUGE: {'rouge1': 0.4687295892482586, 'rouge2': 0.24423883253431564, 'rougeL': 0.3518100657686962, 'rougeLsum': 0.3518100657686962}, BERTScore: {'precision': 0.3184850638111432, 'recall': 0.32375981498302686, 'f1': 0.32095723976898527}\n",
      "Epoch 4\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffb577822f9543ea9f96fc636cad9037",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.5060677409707429\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2483a7c239604b6a9b7b841d56101526",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 1.2521120599574513\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bd86ad5ca814f84a73a5d8dde14909a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Validation Scores: ROUGE: {'rouge1': 0.48353369712222705, 'rouge2': 0.25883781935541716, 'rougeL': 0.36041260979767364, 'rougeLsum': 0.36041260979767364}, BERTScore: {'precision': 0.3310216377592749, 'recall': 0.3433714819451173, 'f1': 0.33700355587320197}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "epochs = 5\n",
    "\n",
    "\n",
    "# Define the optimizer with weight decay\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)\n",
    "\n",
    "# Total number of training steps\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Set up the learning rate scheduler\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps=200, \n",
    "                                            num_training_steps=total_steps)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch}\\n\")\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    with tqdm(train_dataloader, unit=\"batch\") as tepoch:    \n",
    "        for batch in tepoch:\n",
    "            tepoch.set_description(f\"Epoch {epoch}\")\n",
    "            input_ids, attention_masks, labels = tuple(t.to(device) for t in batch)\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_masks, labels=labels)\n",
    "\n",
    "            loss = outputs.loss\n",
    "            train_loss += loss.item()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()  # Update the learning rate\n",
    "            tepoch.set_postfix(loss=loss.item())\n",
    "        train_loss /= len(train_dataloader)\n",
    "        sleep(0.1)\n",
    "    print(f\"Training loss: {train_loss}\")\n",
    "\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        with tqdm(test_dataloader, unit=\"batch\") as tepoch:\n",
    "            for batch in tepoch:\n",
    "                tepoch.set_description(f\"Epoch {epoch}\")\n",
    "                input_ids, attention_masks, labels = tuple(t.to(device) for t in batch)\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_masks, labels=labels)\n",
    "                loss = outputs.loss\n",
    "                test_loss += loss.item()\n",
    "                tepoch.set_postfix(loss=loss.item())\n",
    "\n",
    "            test_loss /= len(test_dataloader)\n",
    "    print(f\"Testing loss: {test_loss}\")\n",
    "\n",
    "    # Validation and Metric Calculation\n",
    "    rouge_scores, bert_scores = validate_and_calculate_metrics(model, val_dataloader, tokenizer, device)\n",
    "    print(f\"Epoch {epoch} Validation Scores: ROUGE: {rouge_scores}, BERTScore: {bert_scores}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "158c55f6",
   "metadata": {},
   "source": [
    "The model gave agood result in the rouge and bert score but still to address the overfitting the weight decay and the learning rates are adjusted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d0d5b4",
   "metadata": {},
   "source": [
    "## BART with 20 epochs  lr= 5e-4, warmup 500, and weight decay= 0.02,linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d7577b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# Suppress UserWarning from the Transformers library\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"Some weights of RobertaModel were not initialized from the model checkpoint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ca921099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9989390072bd425d8f7df0dec3114a72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 2.836790406239961\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e593ec36d3484037a72e681d370d680c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 4.831213623285294\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fcf73b6643f494f983e75745123b42a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Validation Scores: ROUGE: {'rouge1': 0.09544732657666358, 'rouge2': 0.002116496509172645, 'rougeL': 0.09327775400749234, 'rougeLsum': 0.09327775400749234}, BERTScore: {'precision': -0.2747952447583278, 'recall': -0.2476340795142783, 'f1': -0.25964157428178525}\n",
      "Epoch 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d58b7de770543dda57f729772661b24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 5.37050336420893\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b26e74ae17d5445090936dc10ef8b114",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 5.271217117706935\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0d4ee3fcf534a8a8c9d1e61ba6cc4c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Validation Scores: ROUGE: {'rouge1': 0.16056563886149233, 'rouge2': 0.0007839206181072132, 'rougeL': 0.10625334985168702, 'rougeLsum': 0.10625334985168702}, BERTScore: {'precision': -0.35757914144131875, 'recall': -0.35268449203835595, 'f1': -0.3535609183212121}\n",
      "Epoch 2\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "785349374ab045fdab5bacfd6284d41c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 4.952993058872794\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfbbaba766584e25b0a395eeb91fff30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 8.106919215785133\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfb1b3b21e834bb48d26415bfb81cc97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Validation Scores: ROUGE: {'rouge1': 0.157719738337757, 'rouge2': 0.005884029850851948, 'rougeL': 0.09541603892049401, 'rougeLsum': 0.09541603892049401}, BERTScore: {'precision': -0.41564667266276145, 'recall': -0.2374202302760548, 'f1': -0.32643311305178535}\n",
      "Epoch 3\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "750fb6216eb1478291c6d94a41be503d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 4.827484983170104\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebeb4f7a1c1247c49fbe8dd4a127ba53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 11.652085039350721\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d164dac965a84a73b7c20ccfd7a0ddb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Validation Scores: ROUGE: {'rouge1': 0.17951615475458463, 'rouge2': 0.0014948047005581095, 'rougeL': 0.10219684256337487, 'rougeLsum': 0.10219684256337487}, BERTScore: {'precision': -0.44472133699390626, 'recall': -0.2962072864174843, 'f1': -0.3697584341797564}\n",
      "Epoch 4\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f683dfee1134fd5ae0f1d543082e7e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 4.749902496794741\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "223a4556a52140ca878f08f344ed7c67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 10.937855429119534\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8bd6ca1d3b44b4586a13277cf9790df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Validation Scores: ROUGE: {'rouge1': 0.0008425457644207644, 'rouge2': 0.0, 'rougeL': 0.0008425457644207644, 'rougeLsum': 0.0008425457644207644}, BERTScore: {'precision': -0.7787823544608222, 'recall': -0.36854990261296433, 'f1': -0.5812809210684564}\n",
      "Epoch 5\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8d1c095c83f44098051c6f539761430",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 4.688252910168585\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5aeadf5032c6479ba52c37951bd879b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 11.698602040608725\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ae98148c1ec4e16b985855433d49665",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Validation Scores: ROUGE: {'rouge1': 0.10929331453427964, 'rouge2': 0.002709222793972359, 'rougeL': 0.07898264576605225, 'rougeLsum': 0.07898264576605225}, BERTScore: {'precision': -0.4774136423236794, 'recall': -0.2998206685814593, 'f1': -0.38842863134211963}\n",
      "Epoch 6\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2330b61c2c04a598e0ac32908d00495",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 4.681740412455119\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1e9dacf54d542828cccd40c1fa3032a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 12.207921995057\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84a39a230bb34e02b29eef824132df0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Validation Scores: ROUGE: {'rouge1': 0.1375924226951965, 'rouge2': 0.0, 'rougeL': 0.09310359763591486, 'rougeLsum': 0.09310359763591486}, BERTScore: {'precision': -0.42038285732269287, 'recall': -0.26720552725924385, 'f1': -0.34318630976809394}\n",
      "Epoch 7\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9fbffaa9bc84625b68fb4073aeaab14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 4.615901476608779\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a2fd9edcd714eba9e52f05076efd5e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 12.282319492763943\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f3509debeed47ab9ba218d27b70f836",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 Validation Scores: ROUGE: {'rouge1': 0.10715024780671109, 'rouge2': 0.0, 'rougeL': 0.0730935219525061, 'rougeLsum': 0.0730935219525061}, BERTScore: {'precision': -0.432013428873486, 'recall': -0.30245934964882004, 'f1': -0.36621519219544196}\n",
      "Epoch 8\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38027ad4926044d1bad54beae7f1d6d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 4.592114671261725\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab5aaf169b75444fbcb3fa3e68d5a252",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 14.773707588513693\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f487ba8802d84fdeb3adc3e7b976ea22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 Validation Scores: ROUGE: {'rouge1': 0.04831009232628938, 'rouge2': 0.00011574074074074075, 'rougeL': 0.044190106867480705, 'rougeLsum': 0.044190106867480705}, BERTScore: {'precision': -0.47835434186789727, 'recall': -0.3024789063880841, 'f1': -0.39014751257167923}\n",
      "Epoch 9\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67e34d2d1c1a469a9b049b664b57a497",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 4.55781073127678\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39ada798da734bbf81804eea59d400a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 15.282278882132637\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34d9a556e34c472c8f0f365d92641c18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 Validation Scores: ROUGE: {'rouge1': 0.019629842457913632, 'rouge2': 0.0, 'rougeL': 0.017649023535215646, 'rougeLsum': 0.017649023535215646}, BERTScore: {'precision': -0.5668989676568243, 'recall': -0.29563542434738743, 'f1': -0.4333456829190254}\n"
     ]
    }
   ],
   "source": [
    "# Suppress UserWarning from the Transformers library\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"Some weights of RobertaModel were not initialized from the model checkpoint\")\n",
    "\n",
    "model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "epochs = 10\n",
    "\n",
    "\n",
    "# Define the optimizer with weight decay, increasing the weight decay\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=0.02)\n",
    "\n",
    "# Total number of training steps\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Set up the learning rate scheduler\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps=500, \n",
    "                                            num_training_steps=total_steps)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch}\\n\")\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    with tqdm(train_dataloader, unit=\"batch\") as tepoch:    \n",
    "        for batch in tepoch:\n",
    "            tepoch.set_description(f\"Epoch {epoch}\")\n",
    "            input_ids, attention_masks, labels = tuple(t.to(device) for t in batch)\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_masks, labels=labels)\n",
    "\n",
    "            loss = outputs.loss\n",
    "            train_loss += loss.item()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # Update the learning rate\n",
    "            scheduler.step()  \n",
    "            tepoch.set_postfix(loss=loss.item())\n",
    "        train_loss /= len(train_dataloader)\n",
    "        sleep(0.1)\n",
    "    print(f\"Training loss: {train_loss}\")\n",
    "\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        with tqdm(test_dataloader, unit=\"batch\") as tepoch:\n",
    "            for batch in tepoch:\n",
    "                tepoch.set_description(f\"Epoch {epoch}\")\n",
    "                input_ids, attention_masks, labels = tuple(t.to(device) for t in batch)\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_masks, labels=labels)\n",
    "                loss = outputs.loss\n",
    "                test_loss += loss.item()\n",
    "                tepoch.set_postfix(loss=loss.item())\n",
    "\n",
    "            test_loss /= len(test_dataloader)\n",
    "    print(f\"Testing loss: {test_loss}\")\n",
    "\n",
    "    # Validation and Metric Calculation\n",
    "    rouge_scores, bert_scores = validate_and_calculate_metrics(model, val_dataloader, tokenizer, device)\n",
    "    print(f\"Epoch {epoch} Validation Scores: ROUGE: {rouge_scores}, BERTScore: {bert_scores}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f30aaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Increasing the weight decay resulted in losing the scores for the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4aae8f0",
   "metadata": {},
   "source": [
    "## BART with 20 epochs  lr= 5e-5, warmup 500, and weight decay= 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ab2aaa3d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cb84489133646a097824a6829f32c09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 4.198553119590897\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67eae31f4eee49d9a40aa20210f521f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 1.265609071486526\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3e0b5c38691456dbdbbe2aa6b3dc307",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Validation Scores: ROUGE: {'rouge1': 0.42992975108865955, 'rouge2': 0.21015567679324565, 'rougeL': 0.3064143133451763, 'rougeLsum': 0.3064143133451763}, BERTScore: {'precision': 0.24620901705283257, 'recall': 0.27527805873089367, 'f1': 0.2600406969173087}\n",
      "Epoch 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44f1f8c16db94cad8b5fb4ab4d95c4de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 1.2218948858762215\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7aee0a436f7840ca9d5a372e6b406546",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 1.1303776676456134\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e85d38eb4e0e4eefa9ad7986b28cafdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Validation Scores: ROUGE: {'rouge1': 0.47009352084384154, 'rouge2': 0.24933460574378385, 'rougeL': 0.34926481652378677, 'rougeLsum': 0.34926481652378677}, BERTScore: {'precision': 0.32189917171167004, 'recall': 0.31340148444804883, 'f1': 0.31745316000241375}\n",
      "Epoch 2\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50d1a5b63333401aa7741e2b50a3f200",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.9959551120054222\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b703aae2a53045a0b3134cecddde4f3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 1.1652871800793543\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4feb9ce89364746b2f88e27c5e9de65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Validation Scores: ROUGE: {'rouge1': 0.4777021650251794, 'rouge2': 0.25036627338143624, 'rougeL': 0.3506433287890394, 'rougeLsum': 0.3506433287890394}, BERTScore: {'precision': 0.29634263387156856, 'recall': 0.3395322486758232, 'f1': 0.3177118076321979}\n",
      "Epoch 3\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18d4991e140941d0b4dcf9627dfb8ee8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.7743233686792637\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b367182d9444158aa40de3e57de2ced",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 1.1336440237032042\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf2c227b237c46b388e600316f03bd06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Validation Scores: ROUGE: {'rouge1': 0.49332359710355855, 'rouge2': 0.2621617403569514, 'rougeL': 0.3726487687740378, 'rougeLsum': 0.3726487687740378}, BERTScore: {'precision': 0.35764329363074565, 'recall': 0.35380848142732346, 'f1': 0.35565182082872426}\n",
      "Epoch 4\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a2259fd3e7f41aabf0b4043063a364c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.6553838314142769\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dbc94e1c7914ca4ac475840e36ac651",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 1.21962411618895\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3971b9096ecc4645a52334074c8bd453",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Validation Scores: ROUGE: {'rouge1': 0.485209843770825, 'rouge2': 0.2548796486160744, 'rougeL': 0.3622992696801084, 'rougeLsum': 0.3622992696801084}, BERTScore: {'precision': 0.3565074626563324, 'recall': 0.33138163781000507, 'f1': 0.3438245496816105}\n",
      "Epoch 5\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d4d331c411b47bb9bab4bae6a91f318",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.6802360873647079\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27712e5297b6402d9e4dea31601dc276",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 1.353722592194875\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "371ddc5a25e44ebeafbdb882f6c554c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Validation Scores: ROUGE: {'rouge1': 0.4644729851663196, 'rouge2': 0.2358380882033971, 'rougeL': 0.33891212023627804, 'rougeLsum': 0.33891212023627804}, BERTScore: {'precision': 0.30581469301493297, 'recall': 0.3204384869378474, 'f1': 0.3131354064680636}\n",
      "Epoch 6\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7af1fcce5a7148f488eb8c878b700e3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.5583608803634872\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "204b2a9100a0466aa2236fe8bcd76110",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 1.3222903220189943\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4d3dc047ab04582a96bdff7f19530a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Validation Scores: ROUGE: {'rouge1': 0.4719723886067372, 'rouge2': 0.25261428165241595, 'rougeL': 0.35414901259449433, 'rougeLsum': 0.35414901259449433}, BERTScore: {'precision': 0.36224954906437135, 'recall': 0.31729619380914503, 'f1': 0.33966510991255444}\n",
      "Epoch 7\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e78c325f5c504d228057da70cb96d67f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.38727216342252174\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffc94a590d4d40ca81f44853cf7502f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 1.395846776664257\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74c4d524c22e4005badc3349851be0cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 Validation Scores: ROUGE: {'rouge1': 0.4687939467701723, 'rouge2': 0.2434051197668848, 'rougeL': 0.3550662603575806, 'rougeLsum': 0.3550662603575806}, BERTScore: {'precision': 0.3294706660219365, 'recall': 0.33321512086937827, 'f1': 0.3311923648127251}\n",
      "Epoch 8\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ee53f9e261b4508a827f8d8a7bfe932",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.28586142467792164\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40dbd96e8585470c889cdb7d8ca62bab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 1.4405482850140996\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6643091443fa407f921e6776b065d804",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 Validation Scores: ROUGE: {'rouge1': 0.48711078956978704, 'rouge2': 0.2531786079678581, 'rougeL': 0.37076356707881075, 'rougeLsum': 0.37076356707881075}, BERTScore: {'precision': 0.33135279019673664, 'recall': 0.35969658635763657, 'f1': 0.3454962295169632}\n",
      "Epoch 9\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d1efe806a3d45fd8ade1c46cf18e38c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.23565886818809423\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6097b9ebe138422bae107d6085e94b7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 1.5026456109351582\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "281b70b339454773b5837659267abf9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 Validation Scores: ROUGE: {'rouge1': 0.49210885621529804, 'rouge2': 0.25903629241355464, 'rougeL': 0.37177750687947553, 'rougeLsum': 0.37177750687947553}, BERTScore: {'precision': 0.34008643662350047, 'recall': 0.35876135486695504, 'f1': 0.3495029573225313}\n",
      "Epoch 10\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88d576d7f1744be2ba018dd4f6dba702",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.2665858169694147\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3be02f3f02aa482a821e0096e9eb00a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 1.5440117079350684\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf92bec4012d4d5bb5e02bf204f176f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 Validation Scores: ROUGE: {'rouge1': 0.48193462976464196, 'rouge2': 0.25027179407353406, 'rougeL': 0.36461678756524074, 'rougeLsum': 0.36461678756524074}, BERTScore: {'precision': 0.33254420545159113, 'recall': 0.3374915572090281, 'f1': 0.3349575722176168}\n",
      "Epoch 11\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca677a2c3e234a678c872e4b9d94a183",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.1671914685807542\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49726a648081471f863660568ca761d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 1.6087853891981974\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c573d4ed70a452596fed8950ddf769f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 Validation Scores: ROUGE: {'rouge1': 0.47690017624147985, 'rouge2': 0.243120591620792, 'rougeL': 0.3490229374639495, 'rougeLsum': 0.3490229374639495}, BERTScore: {'precision': 0.3103460380807519, 'recall': 0.3558525329248773, 'f1': 0.3329151870889796}\n",
      "Epoch 12\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2fef818e53e4613ae3d1dff0f5c0332",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.1412927472328176\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "637a219440bb4023a558905a43ac0024",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 1.6436093317137823\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0aeb0afaccba4f2190149708805795f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 Validation Scores: ROUGE: {'rouge1': 0.4804921388344531, 'rouge2': 0.24373958962668038, 'rougeL': 0.35970968326283265, 'rougeLsum': 0.35970968326283265}, BERTScore: {'precision': 0.3305728797697359, 'recall': 0.3391648790695601, 'f1': 0.3348854931278361}\n",
      "Epoch 13\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dc05d1d121a4eceb9423537a24789c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.1196634666994214\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7f83eedf1f2426ebe69457078e3ee36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 1.7178392468227281\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41d58667e5aa4af5b0b3118c02460677",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 Validation Scores: ROUGE: {'rouge1': 0.4874334422223161, 'rouge2': 0.25561861979481804, 'rougeL': 0.36937397754775403, 'rougeLsum': 0.36937397754775403}, BERTScore: {'precision': 0.3204414433696204, 'recall': 0.37356993680198985, 'f1': 0.34697465122573906}\n",
      "Epoch 14\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ee6396c006a4f6c91ded97aedb280de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.10432691169445386\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2c0657e92f9435586fd39891162b35f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 1.7331407889723778\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28f5486a98de422eab11b061c5f3d935",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 Validation Scores: ROUGE: {'rouge1': 0.47750363903488213, 'rouge2': 0.2449036089217114, 'rougeL': 0.35358731211005967, 'rougeLsum': 0.35358731211005967}, BERTScore: {'precision': 0.31514004178138244, 'recall': 0.355802019747595, 'f1': 0.3354447192202012}\n",
      "Epoch 15\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0888bb4d8e8f44bc8e6a1bcdf59d6e6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.09188711771206139\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cebdc80719f84ee093836209929cbe7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 1.743493754002783\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37170c891bce4900b5096b04a6443af9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 Validation Scores: ROUGE: {'rouge1': 0.48276371399328405, 'rouge2': 0.2467763973867233, 'rougeL': 0.36264186322894604, 'rougeLsum': 0.36264186322894604}, BERTScore: {'precision': 0.32445999928232694, 'recall': 0.3571980968117714, 'f1': 0.34080857204066384}\n",
      "Epoch 16\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba8d9e0d264647a39f42e42fee749b90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.07557963483414785\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6eef7691ab81481bac8e56a57831f372",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 1.788235240512424\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf3bf340ec82403da97d3eacd6e8009d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 Validation Scores: ROUGE: {'rouge1': 0.4728495232967556, 'rouge2': 0.24118841604248262, 'rougeL': 0.355081553852781, 'rougeLsum': 0.355081553852781}, BERTScore: {'precision': 0.32896998017612433, 'recall': 0.34860609585626257, 'f1': 0.33881258426441085}\n",
      "Epoch 17\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "194cc09953224f9b9a0ca01df12ce533",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.06668829102024525\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ed17e74023c486f827934b5e409c358",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 1.8193214957912762\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc03ae4e19a64ae69f04835144960c06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 Validation Scores: ROUGE: {'rouge1': 0.473606987439707, 'rouge2': 0.24462590629433045, 'rougeL': 0.3571662872691044, 'rougeLsum': 0.3571662872691044}, BERTScore: {'precision': 0.32706146221607924, 'recall': 0.34660061572988826, 'f1': 0.33675609942939544}\n",
      "Epoch 18\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1c9bee47a3940649e7df226c60cd168",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.06954340432201718\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0aee8559292f43bfb7658ef875a14763",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 1.8186356748143833\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f37eae5873ea423db9fd4a5443007e21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 Validation Scores: ROUGE: {'rouge1': 0.48170120588811965, 'rouge2': 0.24686418157456957, 'rougeL': 0.3613002553411576, 'rougeLsum': 0.3613002553411576}, BERTScore: {'precision': 0.3279254638279478, 'recall': 0.3464702634761731, 'f1': 0.3371775187551975}\n",
      "Epoch 19\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b704a1256ff41a39c2b1808abdbb17f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.05210775510870857\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e549157d7fc43fbb58a24d984d58210",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 1.8311698743038707\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f02db27aeb54329837ac405ea7f00f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 Validation Scores: ROUGE: {'rouge1': 0.4815021130346257, 'rouge2': 0.24858404400452475, 'rougeL': 0.363117657554722, 'rougeLsum': 0.363117657554722}, BERTScore: {'precision': 0.33130418084975743, 'recall': 0.3563834655409058, 'f1': 0.34381134357924265}\n"
     ]
    }
   ],
   "source": [
    "# Suppressing warnings from the transformers library\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
    "model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "epochs = 20\n",
    "\n",
    "\n",
    "# Define the optimizer with weight decay\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.01)\n",
    "\n",
    "# Total number of training steps\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Set up the learning rate scheduler\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps=500, \n",
    "                                            num_training_steps=total_steps)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch}\\n\")\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    with tqdm(train_dataloader, unit=\"batch\") as tepoch:    \n",
    "        for batch in tepoch:\n",
    "            tepoch.set_description(f\"Epoch {epoch}\")\n",
    "            input_ids, attention_masks, labels = tuple(t.to(device) for t in batch)\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_masks, labels=labels)\n",
    "\n",
    "            loss = outputs.loss\n",
    "            train_loss += loss.item()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()  # Update the learning rate\n",
    "            tepoch.set_postfix(loss=loss.item())\n",
    "        train_loss /= len(train_dataloader)\n",
    "        sleep(0.1)\n",
    "    print(f\"Training loss: {train_loss}\")\n",
    "\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        with tqdm(test_dataloader, unit=\"batch\") as tepoch:\n",
    "            for batch in tepoch:\n",
    "                tepoch.set_description(f\"Epoch {epoch}\")\n",
    "                input_ids, attention_masks, labels = tuple(t.to(device) for t in batch)\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_masks, labels=labels)\n",
    "                loss = outputs.loss\n",
    "                test_loss += loss.item()\n",
    "                tepoch.set_postfix(loss=loss.item())\n",
    "\n",
    "            test_loss /= len(test_dataloader)\n",
    "    print(f\"Testing loss: {test_loss}\")\n",
    "\n",
    "    # Validation and Metric Calculation\n",
    "    rouge_scores, bert_scores = validate_and_calculate_metrics(model, val_dataloader, tokenizer, device)\n",
    "    print(f\"Epoch {epoch} Validation Scores: ROUGE: {rouge_scores}, BERTScore: {bert_scores}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857f0707",
   "metadata": {},
   "source": [
    "## Despite the increase in the scores rouge and bert, the overfitting is an issue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd63279b",
   "metadata": {},
   "source": [
    "## Training BART model with a learning rate scheduler to address the overfitting"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c0ad82bf",
   "metadata": {},
   "source": [
    "## Using rthe ReduceLROnPlateau instead of get_linear_schedule_with_warmup, which was used to is used to first increase the learning rate linearly from a lower initial learning rate to the maximum learning rate specified in the optimizer over a number of warmup steps. After the warmup period, it then decreases the learning rate linearly to zero over the remaining steps of training.\n",
    "Usage: It is particularly useful in scenarios where you want to mitigate the risk of destabilizing the model's parameters at the start of the training due to high learning rates, or when you want to gently reduce the learning rate towards the end of training to fine-tune the model parameters.\n",
    "Parameters:\n",
    "num_warmup_steps: The number of steps for the warmup phase.\n",
    "num_training_steps: The total number of training steps.\n",
    "## The ReduceLROnPlateau dynamically adjusts the learning rate based on the performance of the model, typically on a validation metric. The learning rate is reduced by a factor if the performance metric does not improve for a set number of epochs (patience).\n",
    "Usage: It is used when you want to automatically reduce the learning rate in response to the stagnation or deterioration of model performance (like plateauing of validation loss). This helps in fine-tuning the model when further improvement seems difficult.\n",
    "Parameters:\n",
    "mode: 'min' for minimizing a metric, 'max' for maximizing.\n",
    "factor: The factor by which the learning rate will be reduced. new_lr = lr * factor.\n",
    "patience: Number of epochs with no improvement after which learning rate will be reduced.\n",
    "min_lr: A lower bound on the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "baae4dcf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acd4b97d33fb4c52ad2cc3ead6a7a46f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 2.2648295534406593\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85388b47b82f4d13ac34e5ab9f8499f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 1.2186410203576088\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12c3c60b9bd647f2a058d8e0b67afa21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Validation Scores: ROUGE: {'rouge1': 0.43921381702288503, 'rouge2': 0.21923885332859283, 'rougeL': 0.32013637920201243, 'rougeLsum': 0.32013637920201243}, BERTScore: {'precision': 0.2574951104405854, 'recall': 0.27993835700261926, 'f1': 0.2687686374410987}\n",
      "Epoch 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6a00d9652634fb5ae78b1e9c685b2ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 1.1010996257652066\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a08d0a50b0554327befd438eef95ce37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 1.1507796885238752\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e28b8589f8e44cfb273465847b5de82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Validation Scores: ROUGE: {'rouge1': 0.47433572479642144, 'rouge2': 0.25324167300472133, 'rougeL': 0.35662825784568425, 'rougeLsum': 0.35662825784568425}, BERTScore: {'precision': 0.32169255102053285, 'recall': 0.30765383287022513, 'f1': 0.3144998793852412}\n",
      "Epoch 2\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fc19bcdf3ea4d6096906f1e031fe9a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.9919501489507938\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "864b2b09a2594c9b95fd869da5ef9bca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 1.2017787852221065\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f25c92b6a1984d3093d0bf6f188a07c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Validation Scores: ROUGE: {'rouge1': 0.4579501379978427, 'rouge2': 0.23193337595419392, 'rougeL': 0.3448167339806004, 'rougeLsum': 0.3448167339806004}, BERTScore: {'precision': 0.3295570270986193, 'recall': 0.29702095764999586, 'f1': 0.31323363771662116}\n",
      "Epoch 3\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bb61d66aa3e4f068b5b01474626deba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 1.3071133151293515\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fac4d83bd774abd91548984d5723261",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 2.4361779905027814\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4357b9ba3f634cba8b108a4b07ead869",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Validation Scores: ROUGE: {'rouge1': 0.16521975892024449, 'rouge2': 0.022813354661923572, 'rougeL': 0.11632648189345196, 'rougeLsum': 0.11632648189345196}, BERTScore: {'precision': 0.020762782970753808, 'recall': -0.10938920098124072, 'f1': -0.043945640024806686}\n",
      "Epoch 4\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfc7f99da23b4acc95f6cba5820d072d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 1.9877912039885264\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f27092aedfff44fdb3d8e2e5321d315b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 2.1726371496915817\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "227d09cd09684d818f1d75558295c294",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Validation Scores: ROUGE: {'rouge1': 0.17318559587320959, 'rouge2': 0.017301600413507118, 'rougeL': 0.11473000489100287, 'rougeLsum': 0.11473000489100287}, BERTScore: {'precision': -0.03760805988632557, 'recall': -0.10282790360765325, 'f1': -0.06908675788953486}\n",
      "Epoch 5\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9279fe5d83cf4f949bb5e0dbff0dc786",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 1.559087811651344\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9c6e770f9bb4078b75c5ea2d0574989",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 2.171798061993387\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90de4a481ca1437daae586ae87c2d09a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Validation Scores: ROUGE: {'rouge1': 0.19549769027474817, 'rouge2': 0.02577427362867914, 'rougeL': 0.12237847030281819, 'rougeLsum': 0.12237847030281819}, BERTScore: {'precision': -0.08955275753719939, 'recall': -0.08342526749604279, 'f1': -0.08538637385289702}\n",
      "Epoch 6\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4fbb35234d84cf4a554b6746f0f9e3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 1.1056096360176624\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7799fe4ade0b497ebd04522660b866a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 2.2311366531583996\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8453bf0e97d4a3ab6431c01aa24d22c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Validation Scores: ROUGE: {'rouge1': 0.17908844528790674, 'rouge2': 0.019757313555578693, 'rougeL': 0.12774866099034282, 'rougeLsum': 0.12774866099034282}, BERTScore: {'precision': -0.054880595222736396, 'recall': -0.08019658951606187, 'f1': -0.06632535234611067}\n",
      "Epoch 7\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e45a22723fb40e0bb529cca9e0e7c30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 1.0184125937744528\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84b7cd983ae046e582941d603ca6bba2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 2.2634093496534557\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c62f04b8a9b94c6fa7c6cf3c6fdf6b81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 Validation Scores: ROUGE: {'rouge1': 0.18525591624126794, 'rouge2': 0.019534011803910074, 'rougeL': 0.12405130194736652, 'rougeLsum': 0.12405130194736652}, BERTScore: {'precision': -0.09009395301755932, 'recall': -0.07057123617010398, 'f1': -0.07900138271765576}\n",
      "Epoch 8\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da148d405fcd4e94b29db012df59083d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.9712481782464923\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39fa95e27c924116acb3435a6d5bfaea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 2.301465471585592\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fda83b9fc69c49d39130605cf3e37160",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 Validation Scores: ROUGE: {'rouge1': 0.17853137105118183, 'rouge2': 0.020962798593180437, 'rougeL': 0.12127299253383317, 'rougeLsum': 0.12127299253383317}, BERTScore: {'precision': -0.06101164246986931, 'recall': -0.07931181642278615, 'f1': -0.06909623227289154}\n",
      "Epoch 9\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d713fe0b816d4757a4088bbe11697e3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.9257773284069792\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ab779dbfeeb42edba5398ec015e0c39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 2.3360798209905624\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8381a751eb874e2d968b36788bca8d91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 Validation Scores: ROUGE: {'rouge1': 0.1778643498619585, 'rouge2': 0.017268249853761634, 'rougeL': 0.11902407197094084, 'rougeLsum': 0.11902407197094084}, BERTScore: {'precision': -0.060652027521023735, 'recall': -0.09924771908360223, 'f1': -0.07886918442737725}\n",
      "Epoch 10\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a1dc26334e84ca4b15a13956bfd21b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.8682104138557069\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77404de4547540a48f29d934021c7def",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 2.361746629079183\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbdf06e875254713a2d7a1d44f7cab99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 Validation Scores: ROUGE: {'rouge1': 0.1746059753972949, 'rouge2': 0.018771260391203696, 'rougeL': 0.11809212414191528, 'rougeLsum': 0.11809212414191528}, BERTScore: {'precision': -0.08015540385774027, 'recall': -0.09651629499987596, 'f1': -0.0872643243573192}\n",
      "Epoch 11\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "343eb8f8d4b94cb2907a8f13ef66de1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.8625873191627914\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e10b1e75540d438a80e2da915c25a907",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 2.362642993529638\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3c22f2391d54efb9ecfda5fdb2a5ef8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 Validation Scores: ROUGE: {'rouge1': 0.18266317649715452, 'rouge2': 0.02396696256340825, 'rougeL': 0.12620934621929897, 'rougeLsum': 0.12620934621929897}, BERTScore: {'precision': -0.0585006661583773, 'recall': -0.08768000119986634, 'f1': -0.0720821678923029}\n",
      "Epoch 12\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d947c9015e4e41d9a54a6a3dc3792814",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.8502187294338992\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79eea33c8e6e4482b42a9b9745211573",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 2.3793931239181094\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e4c21ed290f4e0cbf789b1d81886085",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 Validation Scores: ROUGE: {'rouge1': 0.1748145072894264, 'rouge2': 0.01991147493689657, 'rougeL': 0.12037008669437539, 'rougeLsum': 0.12037008669437539}, BERTScore: {'precision': -0.0714767378457408, 'recall': -0.09765873757553184, 'f1': -0.08356567760670765}\n",
      "Epoch 13\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dac0fa27ef0d419d8806df398530738b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.8537126859861933\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00a9b94a3e414575b45d53c49a13488e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 2.3798897365729013\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff122cb4311447f892e1756b641e7d52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 Validation Scores: ROUGE: {'rouge1': 0.17655614613176096, 'rouge2': 0.01923610430273959, 'rougeL': 0.11744086116879765, 'rougeLsum': 0.11744086116879765}, BERTScore: {'precision': -0.0973595007089898, 'recall': -0.0886475609537835, 'f1': -0.09186855636330114}\n",
      "Epoch 14\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a876fa1c7590427faa60cde2340a8bea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.8390117998787029\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ec88ecc997548308e29b1c94dbfc950",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 2.3908245315154395\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cd04ed0938043f7af138dfea3e602d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 Validation Scores: ROUGE: {'rouge1': 0.16848980530823277, 'rouge2': 0.01924341492640939, 'rougeL': 0.11728803897532922, 'rougeLsum': 0.11728803897532922}, BERTScore: {'precision': -0.048108311395885214, 'recall': -0.10110683328497948, 'f1': -0.07368107808805588}\n"
     ]
    }
   ],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torch\n",
    "from transformers import BartForConditionalGeneration\n",
    "from tqdm.auto import tqdm\n",
    "from time import sleep\n",
    "import logging\n",
    "\n",
    "# Suppressing warnings from the transformers library\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
    "model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "epochs = 15\n",
    "\n",
    "# Define the optimizer with weight decay\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.01)\n",
    "\n",
    "# Initialize the ReduceLROnPlateau scheduler\n",
    "lr_scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, min_lr=1e-6)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch}\\n\")\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    with tqdm(train_dataloader, unit=\"batch\") as tepoch:    \n",
    "        for batch in tepoch:\n",
    "            tepoch.set_description(f\"Epoch {epoch}\")\n",
    "            input_ids, attention_masks, labels = tuple(t.to(device) for t in batch)\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_masks, labels=labels)\n",
    "\n",
    "            loss = outputs.loss\n",
    "            train_loss += loss.item()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            tepoch.set_postfix(loss=loss.item())\n",
    "        train_loss /= len(train_dataloader)\n",
    "        sleep(0.1)\n",
    "    print(f\"Training loss: {train_loss}\")\n",
    "\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        with tqdm(test_dataloader, unit=\"batch\") as tepoch:\n",
    "            for batch in tepoch:\n",
    "                tepoch.set_description(f\"Epoch {epoch}\")\n",
    "                input_ids, attention_masks, labels = tuple(t.to(device) for t in batch)\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_masks, labels=labels)\n",
    "                loss = outputs.loss\n",
    "                test_loss += loss.item()\n",
    "                tepoch.set_postfix(loss=loss.item())\n",
    "\n",
    "            test_loss /= len(test_dataloader)\n",
    "    print(f\"Testing loss: {test_loss}\")\n",
    "\n",
    "    # Update the learning rate based on the validation loss\n",
    "    lr_scheduler.step(test_loss)  # Assuming 'test_loss' is your validation loss\n",
    "\n",
    "    # Validation and Metric Calculation\n",
    "    rouge_scores, bert_scores = validate_and_calculate_metrics(model, val_dataloader, tokenizer, device)\n",
    "    print(f\"Epoch {epoch} Validation Scores: ROUGE: {rouge_scores}, BERTScore: {bert_scores}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff196fce",
   "metadata": {},
   "source": [
    "## Using CyclicLR and training the bart model "
   ]
  },
  {
   "cell_type": "raw",
   "id": "075ee91c",
   "metadata": {},
   "source": [
    "Since the testing loss is still incresing with ReduceLROnPlateau, we now attempt a cyclical learning rate approach varies the learning rate cyclically within a certain range, which can help in both converging faster and escaping local minima. The concept was introduced by Leslie N. Smith in the paper \"Cyclical Learning Rates for Training Neural Networks.\""
   ]
  },
  {
   "cell_type": "raw",
   "id": "778dc1f0",
   "metadata": {},
   "source": [
    "Key Parameters of CLR:\n",
    "Base Learning Rate (base_lr): The minimum learning rate.\n",
    "Max Learning Rate (max_lr): The maximum learning rate.\n",
    "Mode: Common modes are 'triangular', 'triangular2', 'exp_range'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "afa841a3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e3c37823b054705a5a02e1af03d7929",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 3.263971385306227\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45ba40245f1a470080bc75772afc5c27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 1.1997249093320634\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21d4163158d44d45902a9cfbb88549f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Validation Scores: ROUGE: {'rouge1': 0.44887268129381536, 'rouge2': 0.2255464969042561, 'rougeL': 0.3171918698480852, 'rougeLsum': 0.3171918698480852}, BERTScore: {'precision': 0.2383776873919285, 'recall': 0.32160843938537353, 'f1': 0.2796457791700959}\n",
      "Epoch 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec8d829d6f58457dae60dff5ea572230",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 1.1705450246969382\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1181a9dd97c940faae38f43c1318d8d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 1.1619197121924825\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e30b2bc99fe45a79506814b131abb46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Validation Scores: ROUGE: {'rouge1': 0.4679470517182199, 'rouge2': 0.23571310650349414, 'rougeL': 0.3366144832878951, 'rougeLsum': 0.3366144832878951}, BERTScore: {'precision': 0.29311972114050555, 'recall': 0.333694194857445, 'f1': 0.3132455214444134}\n",
      "Epoch 2\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfa3a3b222ad49d7be54f6d11f155169",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 1.0121014206530805\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bdca1e0e61540b0b511fd0ce2a26cf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 1.1949389121598668\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c22a0bd761b94b9e832099ed70c809bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Validation Scores: ROUGE: {'rouge1': 0.4513084672722864, 'rouge2': 0.22064391424917998, 'rougeL': 0.32310563235888634, 'rougeLsum': 0.32310563235888634}, BERTScore: {'precision': 0.30222395967899096, 'recall': 0.31375791748157805, 'f1': 0.3080375137635403}\n",
      "Epoch 3\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acd79cffbcd747849cd7dabadab2a56a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.8859101083285794\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5c1bfa30cfc4c8d850b9d8a713fb877",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 1.253398921754625\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6226425ccd404439be2e3e67b93fac6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Validation Scores: ROUGE: {'rouge1': 0.4641724806501333, 'rouge2': 0.23447577128740255, 'rougeL': 0.33536781305599844, 'rougeLsum': 0.33536781305599844}, BERTScore: {'precision': 0.26963072080009926, 'recall': 0.32102594017568564, 'f1': 0.29510103662808734}\n",
      "Epoch 4\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22b858058831431da45e224fb8cf6328",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 1.9860697460031795\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "695d52d760944fbd8a97f92849e28356",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 5.397967192861769\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9f46460a81f417eaa8e86bf9d6ac2bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Validation Scores: ROUGE: {'rouge1': 0.10621467833207565, 'rouge2': 0.002199353750608824, 'rougeL': 0.08027617122019026, 'rougeLsum': 0.08027617122019026}, BERTScore: {'precision': -0.19338987146814665, 'recall': -0.28966056120892364, 'f1': -0.24070325141979587}\n",
      "Epoch 5\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28a9f91e34174adda37634c8cbccd2dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 5.097631241033177\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69892f2c08b94d91814a04b703f5b8f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 7.261530081431071\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5676af2e16e74510a9455ccb685bb73e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Validation Scores: ROUGE: {'rouge1': 0.2085294399283078, 'rouge2': 0.010566573900416591, 'rougeL': 0.12164347100691549, 'rougeLsum': 0.12164347100691549}, BERTScore: {'precision': -0.35565029746956295, 'recall': -0.22766901976946327, 'f1': -0.29079954740073943}\n",
      "Epoch 6\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdd1dd6784694cb8b436fcd1d660664b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 4.853356455614467\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab29f826c06c435e9b5d66c86ee1844f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 8.391318513287437\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a846dbd663964ee28ff80d7eeb0417aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Validation Scores: ROUGE: {'rouge1': 0.20738121164980705, 'rouge2': 0.006876433458889186, 'rougeL': 0.1106716144423654, 'rougeLsum': 0.1106716144423654}, BERTScore: {'precision': -0.3626541582246621, 'recall': -0.2853645454678271, 'f1': -0.3224853393104341}\n",
      "Epoch 7\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "078df14668544a5ba3dffd56f9abf3bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 4.78603363179875\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3c4104d279d4bbab694a9aa430876ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 7.635631375842625\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b556d86b75a46f0830ec2949ffc0ec7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 Validation Scores: ROUGE: {'rouge1': 0.17288255015294285, 'rouge2': 0.005510552710664713, 'rougeL': 0.09366217373317216, 'rougeLsum': 0.09366217373317216}, BERTScore: {'precision': -0.3622222567598025, 'recall': -0.2874247336553203, 'f1': -0.3233165993458695}\n",
      "Epoch 8\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90b92383bbbb42d898517e83ae9bb249",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 4.7152636229635\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42d0f06ecd1c43b19fd8e8e6450be94e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 8.543873270352682\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68f89c1d1a52474f8ff0ad47a0fe4c23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 Validation Scores: ROUGE: {'rouge1': 0.1848387488909431, 'rouge2': 0.0050700215143496125, 'rougeL': 0.10995737318650488, 'rougeLsum': 0.10995737318650488}, BERTScore: {'precision': -0.3943941249615616, 'recall': -0.2362530135239164, 'f1': -0.31486316956579685}\n",
      "Epoch 9\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "236f5ae1ef114e679536a8f6d58e60d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 4.679014928326636\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "651157e094c94022b253186f5c896a00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 8.559073792563545\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62fb3031d4dc43c4bc5ef4347b836fa9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 Validation Scores: ROUGE: {'rouge1': 0.18868983672231188, 'rouge2': 0.0009206758534327773, 'rougeL': 0.10608354332631297, 'rougeLsum': 0.10608354332631297}, BERTScore: {'precision': -0.36060033986965817, 'recall': -0.2469577467482951, 'f1': -0.3026919691926903}\n",
      "Epoch 10\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e36794ce85544b1b4dea59e425d99fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 4.66307925606916\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2124a080a3341969bb824c0d71308af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 9.027965466181437\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de7ab57f4fc0438bb722f959a5a370c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 Validation Scores: ROUGE: {'rouge1': 0.14496336350763617, 'rouge2': 0.0011411441464218423, 'rougeL': 0.08919632419976364, 'rougeLsum': 0.08919632419976364}, BERTScore: {'precision': -0.35555585473775864, 'recall': -0.2547434854010741, 'f1': -0.30393966866864097}\n",
      "Epoch 11\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "463cd16ff5514ffa93292290a2297004",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 4.65862126050595\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36df40ba5fee4c54a77e6f125d7b51ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 8.665956642892626\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "234a76d9dc544d45810c7d17b2b41e49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 Validation Scores: ROUGE: {'rouge1': 0.15798436277379052, 'rouge2': 0.000276355753029181, 'rougeL': 0.09323526621217898, 'rougeLsum': 0.09323526621217898}, BERTScore: {'precision': -0.46725598019030357, 'recall': -0.2922974100543393, 'f1': -0.37949930297003853}\n",
      "Epoch 12\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3a3a51dd2a748bd96d5536737aca34b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 4.7016727481773515\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4a5d87457744b0fb3994ed631f9c1c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 8.451668381690979\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e416f735c6f84c1b9f3f3739facf9cb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 Validation Scores: ROUGE: {'rouge1': 0.1901232463037809, 'rouge2': 0.0052045589085094515, 'rougeL': 0.10928894982177743, 'rougeLsum': 0.10928894982177743}, BERTScore: {'precision': -0.2664056051936414, 'recall': -0.2161588692623708, 'f1': -0.23973681032657623}\n",
      "Epoch 13\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae2f36289aad4cd08550b9b73ec27d7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 4.686826659533792\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42ad06c29a064b3dbc90cd020d9edd7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 9.582320849100748\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0847b03e1e6846f5bf72b340858c1f22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 Validation Scores: ROUGE: {'rouge1': 0.15648970367163617, 'rouge2': 0.0010675344791469595, 'rougeL': 0.09815210220538126, 'rougeLsum': 0.09815210220538126}, BERTScore: {'precision': -0.3560281458000342, 'recall': -0.24153522888405454, 'f1': -0.2977262215895785}\n",
      "Epoch 14\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67dcd4c9aae3464da49a3a1bd95b449f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Suppressing warnings from the transformers library\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
    "model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "epochs = 15\n",
    "\n",
    "# Define the optimizer with weight decay\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5, weight_decay=0.01)\n",
    "\n",
    "# Initialize the CyclicLR scheduler\n",
    "scheduler = CyclicLR(optimizer, base_lr=1e-5, max_lr=1e-4, \n",
    "                     step_size_up=5*len(train_dataloader), \n",
    "                     mode='triangular2', cycle_momentum=False)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch}\\n\")\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    with tqdm(train_dataloader, unit=\"batch\") as tepoch:    \n",
    "        for batch in tepoch:\n",
    "            tepoch.set_description(f\"Epoch {epoch}\")\n",
    "            input_ids, attention_masks, labels = tuple(t.to(device) for t in batch)\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_masks, labels=labels)\n",
    "\n",
    "            loss = outputs.loss\n",
    "            train_loss += loss.item()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()  # Update the learning rate at each batch step\n",
    "            tepoch.set_postfix(loss=loss.item())\n",
    "        train_loss /= len(train_dataloader)\n",
    "        sleep(0.1)\n",
    "    print(f\"Training loss: {train_loss}\")\n",
    "\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        with tqdm(test_dataloader, unit=\"batch\") as tepoch:\n",
    "            for batch in tepoch:\n",
    "                tepoch.set_description(f\"Epoch {epoch}\")\n",
    "                input_ids, attention_masks, labels = tuple(t.to(device) for t in batch)\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_masks, labels=labels)\n",
    "                loss = outputs.loss\n",
    "                test_loss += loss.item()\n",
    "                tepoch.set_postfix(loss=loss.item())\n",
    "\n",
    "            test_loss /= len(test_dataloader)\n",
    "    print(f\"Testing loss: {test_loss}\")\n",
    "\n",
    "    # Validation and Metric Calculation\n",
    "    rouge_scores, bert_scores = validate_and_calculate_metrics(model, val_dataloader, tokenizer, device)\n",
    "    print(f\"Epoch {epoch} Validation Scores: ROUGE: {rouge_scores}, BERTScore: {bert_scores}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b712d3be",
   "metadata": {},
   "source": [
    "## The kernal interrupted at the final epoch due to the reason that gpu became unresponsive due to overload, to clear the cache the kernel is interrupted and restarted. Regular monitoring of resources is neccessary for fine tuning large transformer models. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8de91aa",
   "metadata": {},
   "source": [
    "## Using One Cycle Learning Rate Policy\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c28a057b",
   "metadata": {},
   "source": [
    "First Phase: Increase the learning rate linearly from a lower rate to a higher rate.\n",
    "Second Phase: Decrease the learning rate linearly back to the lower rate.\n",
    "Final Phase: Further reduce the learning rate to a very small value to fine-tune the model.\n",
    "The policy allows the learning rate to briefly reach a maximum value, enabling the model to traverse several local minima and potentially find a better overall solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fa00c517",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcdb9f98d6cd436c866d8a05d7c972ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 7.110204751620036\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed0558e03eef4b18855147973ff2ef7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 5.3063979215092125\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "321823e0648e45c3870ab03f6524ab1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Validation Scores: ROUGE: {'rouge1': 0.38618009096459777, 'rouge2': 0.18827040785762908, 'rougeL': 0.2617862788619332, 'rougeLsum': 0.2617862788619332}, BERTScore: {'precision': 0.02605275429474811, 'recall': 0.23166611061121026, 'f1': 0.12682295606161156}\n",
      "Epoch 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5a925b6d1904cc885eae55e2c0a3f91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 4.117529133122838\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84353622962242e683629a02ed8850d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 2.690158893664678\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03a2e0efbdec458f8423fef879011dec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Validation Scores: ROUGE: {'rouge1': 0.40682566159326283, 'rouge2': 0.20115113111827052, 'rougeL': 0.2959144493221453, 'rougeLsum': 0.2959144493221453}, BERTScore: {'precision': 0.1949418629713667, 'recall': 0.2245189365154753, 'f1': 0.20941571278187135}\n",
      "Epoch 2\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac4e5497eab64901a5cba79f29b66ec6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 2.235314940264125\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4febd64b297741fbb1916021be470b9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 1.3300483541356192\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9920a2d9cdf8471eab4058515c58e819",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Validation Scores: ROUGE: {'rouge1': 0.4318013810633063, 'rouge2': 0.2134229024813722, 'rougeL': 0.3133325280440118, 'rougeLsum': 0.3133325280440118}, BERTScore: {'precision': 0.27384555908954805, 'recall': 0.2428416486363858, 'f1': 0.2580672462936491}\n",
      "Epoch 3\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63cfd0c857004efc8349976470ecab57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 1.304202904779754\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0753f671db274a54b3897bc8eb09a10e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 1.1515542268753052\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a6fa00adc3a42199a6c34dd65c0f04a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Validation Scores: ROUGE: {'rouge1': 0.44361336482708175, 'rouge2': 0.22461695698458012, 'rougeL': 0.3243729506004685, 'rougeLsum': 0.3243729506004685}, BERTScore: {'precision': 0.28560812396204305, 'recall': 0.27249451462800306, 'f1': 0.2786957595186929}\n",
      "Epoch 4\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "046091cc77cc4a049b8a00c4329b1341",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 1.1092827759995432\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dcc98ca4b00464ab5282656edd39676",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 1.1039647712475724\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39b7183873d5470c9ad4ef740605f97b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Validation Scores: ROUGE: {'rouge1': 0.45608161401603997, 'rouge2': 0.23373401511208758, 'rougeL': 0.336184430058316, 'rougeLsum': 0.336184430058316}, BERTScore: {'precision': 0.3228431266422073, 'recall': 0.2789389618879391, 'f1': 0.3005497185513377}\n",
      "Epoch 5\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8e8e011beb04153b9cd05109be66e8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 1.000259430287127\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25fe8c63f03c45a8bc9fc35780aa42e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 1.0796127973331346\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7411d9e6915c489882fe8f1087877487",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Validation Scores: ROUGE: {'rouge1': 0.46556069482098356, 'rouge2': 0.2454708363820147, 'rougeL': 0.35185869318160745, 'rougeLsum': 0.35185869318160745}, BERTScore: {'precision': 0.3203716297220025, 'recall': 0.30312468649612534, 'f1': 0.31143648570610416}\n",
      "Epoch 6\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67c0ef9808624be39040a2f4f487306d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.8893383610212874\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7937a8d33d5541d2bc4af262c719e6f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 1.084004891829358\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fc43fbf142241cab55ea60ca21940fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Validation Scores: ROUGE: {'rouge1': 0.4670921530907569, 'rouge2': 0.25130993458203066, 'rougeL': 0.3480101580544516, 'rougeLsum': 0.3480101580544516}, BERTScore: {'precision': 0.30445410947625834, 'recall': 0.30878547232391107, 'f1': 0.30635111851410735}\n",
      "Epoch 7\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e7ee5fefb994a92832e82ab488eefb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.811735956344062\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1643efbcc0594336bf42851d1e689295",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 1.0770765952765942\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf0a964e12ba44f8a5c83f3d5e9cf760",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 Validation Scores: ROUGE: {'rouge1': 0.47418359559503137, 'rouge2': 0.25728190029805964, 'rougeL': 0.3553954403992434, 'rougeLsum': 0.3553954403992434}, BERTScore: {'precision': 0.32432173161456984, 'recall': 0.3137370471118225, 'f1': 0.3187454381129808}\n",
      "Epoch 8\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad6581660d5f4cd9a3ab4ab9b32b1202",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.737485523887737\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dde7d5d2c364c9394af5d2f8d6cac6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 1.0816066385143333\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2d253c3bb27437ba56320a1882cb374",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 Validation Scores: ROUGE: {'rouge1': 0.47665192267123424, 'rouge2': 0.2558130517271631, 'rougeL': 0.3588989031861527, 'rougeLsum': 0.3588989031861527}, BERTScore: {'precision': 0.30981706325999564, 'recall': 0.33046404189533657, 'f1': 0.3198404431136118}\n",
      "Epoch 9\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "693d11b1e71044fda6000ba3698ac367",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.6852348639133448\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28b00d44b4034b758e0055dfbbb05ec3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 1.0947909346885152\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8da826660e14d4c92d1fdeda67e2363",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 Validation Scores: ROUGE: {'rouge1': 0.4591668914243978, 'rouge2': 0.24521866898372513, 'rougeL': 0.34204579424210485, 'rougeLsum': 0.34204579424210485}, BERTScore: {'precision': 0.3157110493630171, 'recall': 0.30396395621614325, 'f1': 0.3097225142021974}\n",
      "Epoch 10\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78d73bea686b4879b3d595307c81b0e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.6371916395996859\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33593219607b4e40bf65611b81d13990",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 1.1069328441388078\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2df60f37eef4ebe85f4a0815e20fa0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 Validation Scores: ROUGE: {'rouge1': 0.47663742430982153, 'rouge2': 0.2548306466852537, 'rougeL': 0.3560418546175229, 'rougeLsum': 0.3560418546175229}, BERTScore: {'precision': 0.33716157011480796, 'recall': 0.32016517666892874, 'f1': 0.328462772588763}\n",
      "Epoch 11\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b6c029d2f1f4e15a5e8ef2f485599c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.6027894877566549\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8167dcc6018945108610f9f565cdda76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 1.1216590185132291\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "213b273665814ca09c15f37267440bcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 Validation Scores: ROUGE: {'rouge1': 0.47668989344511686, 'rouge2': 0.2535637437751365, 'rougeL': 0.3529501523824881, 'rougeLsum': 0.3529501523824881}, BERTScore: {'precision': 0.3137656415088309, 'recall': 0.32716237756216693, 'f1': 0.3202472404162917}\n",
      "Epoch 12\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "321d3caa0de548e5a8a13cba1e04cf7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.5719121206484868\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8bccc72cd9242018a7a14955ddf29af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 1.122491639935308\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64c1c472ffe845e2b89744e21a2f8a09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 Validation Scores: ROUGE: {'rouge1': 0.48018074794679283, 'rouge2': 0.2594527828909457, 'rougeL': 0.36194159158697314, 'rougeLsum': 0.36194159158697314}, BERTScore: {'precision': 0.3218101299781766, 'recall': 0.337752733586563, 'f1': 0.32971136054644984}\n",
      "Epoch 13\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "596b8ea427504cc4989893e70d210128",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.5493757714054541\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd060d360667464c9940d33e11812854",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 1.136681071172158\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52132e8614094cab8184ca2776d9350a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 Validation Scores: ROUGE: {'rouge1': 0.47418689862381325, 'rouge2': 0.2532410868207272, 'rougeL': 0.35452488253954706, 'rougeLsum': 0.35452488253954706}, BERTScore: {'precision': 0.32225737037758034, 'recall': 0.3302794337893526, 'f1': 0.326232544456919}\n",
      "Epoch 14\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0108cf8b8c3646bd810731c067cc5bc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.5270172171517761\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "948cd378a90b4f0abd3be006e10c15c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 1.1477182366781764\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c05cc8eaed9456298b5a34e5598082d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 Validation Scores: ROUGE: {'rouge1': 0.4787213244165776, 'rouge2': 0.2555738457832121, 'rougeL': 0.35664403999558714, 'rougeLsum': 0.35664403999558714}, BERTScore: {'precision': 0.3296152494537334, 'recall': 0.3305083817491929, 'f1': 0.32992046755842036}\n"
     ]
    }
   ],
   "source": [
    "# Suppressing warnings from the transformers library\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
    "model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "epochs = 15\n",
    "\n",
    "# Define the optimizer with weight decay\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-6, weight_decay=0.01)\n",
    "\n",
    "# Total steps = number of epochs * number of batches per epoch\n",
    "total_steps = epochs * len(train_dataloader)\n",
    "\n",
    "# OneCycleLR Scheduler, learning starts from \n",
    "scheduler = OneCycleLR(optimizer, max_lr=1e-5, total_steps=total_steps, \n",
    "                       pct_start=0.3, anneal_strategy='linear')\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch}\\n\")\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    with tqdm(train_dataloader, unit=\"batch\") as tepoch:    \n",
    "        for batch in tepoch:\n",
    "            tepoch.set_description(f\"Epoch {epoch}\")\n",
    "            input_ids, attention_masks, labels = tuple(t.to(device) for t in batch)\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_masks, labels=labels)\n",
    "\n",
    "            loss = outputs.loss\n",
    "            train_loss += loss.item()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()  # Update the learning rate after each batch\n",
    "            tepoch.set_postfix(loss=loss.item())\n",
    "        train_loss /= len(train_dataloader)\n",
    "    print(f\"Training loss: {train_loss}\")\n",
    "\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        with tqdm(test_dataloader, unit=\"batch\") as tepoch:\n",
    "            for batch in tepoch:\n",
    "                tepoch.set_description(f\"Epoch {epoch}\")\n",
    "                input_ids, attention_masks, labels = tuple(t.to(device) for t in batch)\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_masks, labels=labels)\n",
    "                loss = outputs.loss\n",
    "                test_loss += loss.item()\n",
    "                tepoch.set_postfix(loss=loss.item())\n",
    "\n",
    "            test_loss /= len(test_dataloader)\n",
    "    print(f\"Testing loss: {test_loss}\")\n",
    "\n",
    "    # Validation and Metric Calculation\n",
    "    rouge_scores, bert_scores = validate_and_calculate_metrics(model, val_dataloader, tokenizer, device)\n",
    "    print(f\"Epoch {epoch} Validation Scores: ROUGE: {rouge_scores}, BERTScore: {bert_scores}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ebbc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "## The training is comparaitively good than using linear_schedule, the scores are comparitively good. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844645c9",
   "metadata": {},
   "source": [
    "## Experimenting linear_schedule_with_warmup for a very small learning rate 5e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cbd7cdcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fdb0a678ef84675877f172cb84ad683",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 7.296214528426439\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b6fa68602c74a2289f72f8949865d44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 5.2065467403994665\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55882d2fe6ee42369f9f0354f3e3142d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Validation Scores: ROUGE: {'rouge1': 0.37857587604097176, 'rouge2': 0.18325723851625714, 'rougeL': 0.259484907595833, 'rougeLsum': 0.259484907595833}, BERTScore: {'precision': 0.016517636987070244, 'recall': 0.21424116121811998, 'f1': 0.11338899745088485}\n",
      "Epoch 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d12e910f458943b2bb4cd78d71258d71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 3.8437148675233304\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a099e5a86b54471ca02fa1990b15142c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 2.310675556461016\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3745cac18a74764b505c7e90eaf3c0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Validation Scores: ROUGE: {'rouge1': 0.3980355713491541, 'rouge2': 0.195644036030355, 'rougeL': 0.2853491774850238, 'rougeLsum': 0.2853491774850238}, BERTScore: {'precision': 0.20032623018616708, 'recall': 0.20523991937645608, 'f1': 0.20223372141158003}\n",
      "Epoch 2\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa6a3704d08d49a3a0a4285ff12021ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 2.018593019354129\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a21c9953e3d4e78bbc21c3e19d1bc98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 1.3417542171147134\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "004b335cb93d40909bdedc784d3e6cf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Validation Scores: ROUGE: {'rouge1': 0.4240791840943802, 'rouge2': 0.20870863242886387, 'rougeL': 0.3054781572232724, 'rougeLsum': 0.3054781572232724}, BERTScore: {'precision': 0.25334714932574165, 'recall': 0.23410599854671293, 'f1': 0.24324575224373904}\n",
      "Epoch 3\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d3084f6377b42afbb209a253ed33ec9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 1.376252875535074\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e6927cf376840b9ab78da206aec30be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 1.1939505653248892\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db07040f6bd44a80a1384ef5c98b59ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Validation Scores: ROUGE: {'rouge1': 0.4387661027148908, 'rouge2': 0.21869292899942777, 'rougeL': 0.31881487692416344, 'rougeLsum': 0.31881487692416344}, BERTScore: {'precision': 0.27196333647912574, 'recall': 0.254105419644879, 'f1': 0.26256309630763197}\n",
      "Epoch 4\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "726056f9b65042309f20f191c746144f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 1.2146356275338612\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d3c4bddbc8c4237a864d3480473ba11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 1.1480199570457141\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e094c590f074bd89cc8118add0e8177",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Validation Scores: ROUGE: {'rouge1': 0.4394126635777003, 'rouge2': 0.22573941702682493, 'rougeL': 0.3196167818396243, 'rougeLsum': 0.3196167818396243}, BERTScore: {'precision': 0.266228362167668, 'recall': 0.28668950135923094, 'f1': 0.2761329648395379}\n",
      "Epoch 5\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a202fd895c8b464dbbed46082986de48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 1.1276533428780333\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c928a7b1cde94fd1bae49bcfc4fd09d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 1.1258296482264996\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7faa87be5354d33b2f04fe6f9b17fd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Validation Scores: ROUGE: {'rouge1': 0.44179455175834226, 'rouge2': 0.22193015468429342, 'rougeL': 0.3306825885269149, 'rougeLsum': 0.3306825885269149}, BERTScore: {'precision': 0.2900600294427325, 'recall': 0.2704586231460174, 'f1': 0.28002490455077755}\n",
      "Epoch 6\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3f14e96a2b8434aa4f091fcbdcae768",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 1.0698238332828363\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6529dd248c1b4efdb174502722174a6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 1.0968335428171687\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "157aaa26765a424c9dba80ca33eb8af6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Validation Scores: ROUGE: {'rouge1': 0.4447360529359002, 'rouge2': 0.22717763393639398, 'rougeL': 0.3328905736868518, 'rougeLsum': 0.3328905736868518}, BERTScore: {'precision': 0.2901587292758955, 'recall': 0.2744274431736105, 'f1': 0.28196557484463686}\n",
      "Epoch 7\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4831c899c7b749efa8be1c02e9562f88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 1.0207459186900876\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6433514fa58429e8184b2e3b0ddce89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 1.0941155412130885\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0653394183944026b125dd8ab82eaf4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 Validation Scores: ROUGE: {'rouge1': 0.4526833972286037, 'rouge2': 0.2305507017232624, 'rougeL': 0.33338110557869677, 'rougeLsum': 0.33338110557869677}, BERTScore: {'precision': 0.2887263561391996, 'recall': 0.2871934663400882, 'f1': 0.28781052953046227}\n",
      "Epoch 8\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56bae15d090f479b925491d147ee35a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.9712197168501552\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9b0ca0616fc452d8ac9dddcefe78953",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 1.0760136457780998\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d34cef037d38488fbe257b0b750c988a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 Validation Scores: ROUGE: {'rouge1': 0.449404935184047, 'rouge2': 0.22984024636648867, 'rougeL': 0.334559212644926, 'rougeLsum': 0.334559212644926}, BERTScore: {'precision': 0.3101767591304249, 'recall': 0.27417124559481937, 'f1': 0.2920619819520248}\n",
      "Epoch 9\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8ec9447af944aa5be5c8ed004f1bcf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.9348157265942968\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acb426052f5d4d908cc7e4f23de90cee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 1.074554959519042\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad3c53aee7bd4d918c30234bb68e518c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 Validation Scores: ROUGE: {'rouge1': 0.4550047322024974, 'rouge2': 0.23630070533677294, 'rougeL': 0.3404513744136889, 'rougeLsum': 0.3404513744136889}, BERTScore: {'precision': 0.3054625128085415, 'recall': 0.28497181160168517, 'f1': 0.29493820631048745}\n",
      "Epoch 10\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9de0b312cfcb4bc594c17ecfc1548015",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.8975515615440415\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa3b91ab41d2477f82e13410f94295e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 1.074798649383916\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6208d43b540a441bbac91d988e002697",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 Validation Scores: ROUGE: {'rouge1': 0.46348647914569224, 'rouge2': 0.244341358421171, 'rougeL': 0.3476072889798448, 'rougeLsum': 0.3476072889798448}, BERTScore: {'precision': 0.30114233198886114, 'recall': 0.2979778388722075, 'f1': 0.2994630928668711}\n",
      "Epoch 11\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "662cad5945b24491985ee12e3e474d9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.8647173801046646\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c820955a455343c8afebac7137465204",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 1.0753101205660238\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3165f74709146d4865d015e8714c939",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 Validation Scores: ROUGE: {'rouge1': 0.46252380391320586, 'rouge2': 0.2433693114046277, 'rougeL': 0.34641942893264954, 'rougeLsum': 0.34641942893264954}, BERTScore: {'precision': 0.31190310154731077, 'recall': 0.29537425205732387, 'f1': 0.30337833776138723}\n",
      "Epoch 12\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84397c19cffb4a428b1888b3cb1ffc07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.8411852156152269\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f64f023aa27148cb95189e203090bc3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 1.0732235602206655\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa397acc70e349e49dc53a8019bcf2cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 Validation Scores: ROUGE: {'rouge1': 0.4630087443454454, 'rouge2': 0.24387957963164286, 'rougeL': 0.3469821343379383, 'rougeLsum': 0.3469821343379383}, BERTScore: {'precision': 0.3077881620782945, 'recall': 0.29593179354237187, 'f1': 0.3016773057687614}\n",
      "Epoch 13\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a799687f410d49f094e60b47ebb72556",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.8157457856539481\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c15cab1f56d5422d8f932c06eed66966",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 1.0791341004272301\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba89eeccecb2424fb09ec0da10a9225e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 Validation Scores: ROUGE: {'rouge1': 0.46655534075713984, 'rouge2': 0.24726730404017555, 'rougeL': 0.3479506792075526, 'rougeLsum': 0.3479506792075526}, BERTScore: {'precision': 0.3025936704232461, 'recall': 0.32116538586301935, 'f1': 0.3117608877623247}\n",
      "Epoch 14\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a70030e243454c6d9e34f1ea609629a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.7983733337974834\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50a80c3073734dca94e004e49fe1b94c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 1.0747225259741147\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66c23c9512dd4ccf950a4c05db7e78c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 Validation Scores: ROUGE: {'rouge1': 0.4766616365751805, 'rouge2': 0.25127917276915784, 'rougeL': 0.3555354666001495, 'rougeLsum': 0.3555354666001495}, BERTScore: {'precision': 0.3183073912385023, 'recall': 0.32462129400422174, 'f1': 0.32133991711048615}\n",
      "Epoch 15\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a84a305c72b948459e909132c20329b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.7749737104077539\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "535da734ac1e45b0b0b4418cf8a07db2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 1.073685285117891\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6caec85aba040b4b18fd8cc52fbcd6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 Validation Scores: ROUGE: {'rouge1': 0.47324497857833236, 'rouge2': 0.25248304772324415, 'rougeL': 0.35227367832735035, 'rougeLsum': 0.35227367832735035}, BERTScore: {'precision': 0.323993564531621, 'recall': 0.3152050568411748, 'f1': 0.31960368792836863}\n",
      "Epoch 16\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60b23c591416402facb4e855b3c26e8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.767085330750414\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8f5cf71fc0946cb8d30ee029f5e48ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 1.0747589899433985\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c64243cb46384f59af9200a97813a80f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 Validation Scores: ROUGE: {'rouge1': 0.47426960268672247, 'rouge2': 0.25166828405991315, 'rougeL': 0.3532582113216971, 'rougeLsum': 0.3532582113216971}, BERTScore: {'precision': 0.3179735108278692, 'recall': 0.32293939197229016, 'f1': 0.320434392688589}\n",
      "Epoch 17\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2544c018db824da3be9575f61f95c066",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.7502140833767589\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42b8f30d24e64396adebe02008f6aad3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 1.081790233651797\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3086a74627d74cb8b6ba631ac665ff32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 Validation Scores: ROUGE: {'rouge1': 0.47079264597653075, 'rouge2': 0.24851624466635672, 'rougeL': 0.3484216301980105, 'rougeLsum': 0.3484216301980105}, BERTScore: {'precision': 0.3167384864451985, 'recall': 0.3183048781421449, 'f1': 0.3174557593754596}\n",
      "Epoch 18\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3324a40a108446379d0515b61ebbe3e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.7407321206288423\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4982e7e5cba9409e91fd96cc4d79ab9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 1.0844083606368966\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8796f4ec2d8b4672ada6724e3c994d68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 Validation Scores: ROUGE: {'rouge1': 0.4655740715374568, 'rouge2': 0.24137593819817266, 'rougeL': 0.3457965696147058, 'rougeLsum': 0.3457965696147058}, BERTScore: {'precision': 0.3108890140429139, 'recall': 0.31574110262509847, 'f1': 0.31331206045837867}\n",
      "Epoch 19\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a159bd7c0364ffaadbbd26f80d2d40d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.7311522871880474\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d086b07c9bd6418481a1a8d38def824a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 1.086009988354312\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "426303c9fdaf41f99137822bd9d340b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 Validation Scores: ROUGE: {'rouge1': 0.47436341010598515, 'rouge2': 0.24764616608394124, 'rougeL': 0.3510761738273753, 'rougeLsum': 0.3510761738273753}, BERTScore: {'precision': 0.3266270726712214, 'recall': 0.32241104832953876, 'f1': 0.32449808856472373}\n"
     ]
    }
   ],
   "source": [
    "# Suppressing warnings from the transformers library\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
    "model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "epochs = 20\n",
    "\n",
    "\n",
    "# Define the optimizer with weight decay\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-6, weight_decay=0.01)\n",
    "\n",
    "# Total number of training steps\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Set up the learning rate scheduler\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps=500, \n",
    "                                            num_training_steps=total_steps)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch}\\n\")\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    with tqdm(train_dataloader, unit=\"batch\") as tepoch:    \n",
    "        for batch in tepoch:\n",
    "            tepoch.set_description(f\"Epoch {epoch}\")\n",
    "            input_ids, attention_masks, labels = tuple(t.to(device) for t in batch)\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_masks, labels=labels)\n",
    "\n",
    "            loss = outputs.loss\n",
    "            train_loss += loss.item()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()  # Update the learning rate\n",
    "            tepoch.set_postfix(loss=loss.item())\n",
    "        train_loss /= len(train_dataloader)\n",
    "        sleep(0.1)\n",
    "    print(f\"Training loss: {train_loss}\")\n",
    "\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        with tqdm(test_dataloader, unit=\"batch\") as tepoch:\n",
    "            for batch in tepoch:\n",
    "                tepoch.set_description(f\"Epoch {epoch}\")\n",
    "                input_ids, attention_masks, labels = tuple(t.to(device) for t in batch)\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_masks, labels=labels)\n",
    "                loss = outputs.loss\n",
    "                test_loss += loss.item()\n",
    "                tepoch.set_postfix(loss=loss.item())\n",
    "\n",
    "            test_loss /= len(test_dataloader)\n",
    "    print(f\"Testing loss: {test_loss}\")\n",
    "\n",
    "    # Validation and Metric Calculation\n",
    "    rouge_scores, bert_scores = validate_and_calculate_metrics(model, val_dataloader, tokenizer, device)\n",
    "    print(f\"Epoch {epoch} Validation Scores: ROUGE: {rouge_scores}, BERTScore: {bert_scores}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab20616",
   "metadata": {},
   "source": [
    "## BART with data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf670383",
   "metadata": {},
   "source": [
    "## Using back translation for data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d2d6f308",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:197: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    }
   ],
   "source": [
    "def back_translate(text, source_lang='en', target_lang='fr', max_length=512):\n",
    "    # Ensure the text is not empty and within the max_length\n",
    "    if not text or len(text) > max_length:\n",
    "        return text  \n",
    "\n",
    "    # Load forward translation model and tokenizer\n",
    "    tokenizer_fw = MarianTokenizer.from_pretrained(f'Helsinki-NLP/opus-mt-{source_lang}-{target_lang}')\n",
    "    model_fw = MarianMTModel.from_pretrained(f'Helsinki-NLP/opus-mt-{source_lang}-{target_lang}')\n",
    "\n",
    "    # Translate to target language with truncation and padding\n",
    "    translated = model_fw.generate(**tokenizer_fw(text, return_tensors=\"pt\", truncation=True, padding='max_length', max_length=max_length))\n",
    "    text_translated = tokenizer_fw.batch_decode(translated, skip_special_tokens=True)[0]\n",
    "\n",
    "    # Load backward translation model and tokenizer\n",
    "    tokenizer_bw = MarianTokenizer.from_pretrained(f'Helsinki-NLP/opus-mt-{target_lang}-{source_lang}')\n",
    "    model_bw = MarianMTModel.from_pretrained(f'Helsinki-NLP/opus-mt-{target_lang}-{source_lang}')\n",
    "\n",
    "    # Translate back to source language with truncation and padding\n",
    "    back_translated = model_bw.generate(**tokenizer_bw(text_translated, return_tensors=\"pt\", truncation=True, padding='max_length', max_length=max_length))\n",
    "    text_back_translated = tokenizer_bw.batch_decode(back_translated, skip_special_tokens=True)[0]\n",
    "\n",
    "    return text_back_translated\n",
    "\n",
    "\n",
    "\n",
    "# Apply back translation \n",
    "df['augmented_input'] = df['Preprocessed_Body'].apply(lambda x: back_translate(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "644d292d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 954 entries, 0 to 953\n",
      "Data columns (total 24 columns):\n",
      " #   Column                Non-Null Count  Dtype \n",
      "---  ------                --------------  ----- \n",
      " 0   subject               954 non-null    object\n",
      " 1   from                  954 non-null    object\n",
      " 2   to                    954 non-null    object\n",
      " 3   date                  954 non-null    object\n",
      " 4   body                  954 non-null    object\n",
      " 5   Body_Length           954 non-null    int64 \n",
      " 6   Subject_Length        954 non-null    int64 \n",
      " 7   Cleaned_Body          954 non-null    object\n",
      " 8   Cleaned_Subject       954 non-null    object\n",
      " 9   BERT_Embeddings       954 non-null    object\n",
      " 10  Cluster_Label         954 non-null    int64 \n",
      " 11  Category              954 non-null    object\n",
      " 12  Cleaned_mails         954 non-null    object\n",
      " 13  summary_TXTRNK_1      954 non-null    object\n",
      " 14  Summary               954 non-null    object\n",
      " 15  summary_BART          954 non-null    object\n",
      " 16  index_number          954 non-null    int64 \n",
      " 17  Tokenized_Email       954 non-null    object\n",
      " 18  Entities              954 non-null    object\n",
      " 19  Cluster_retrieved     954 non-null    int64 \n",
      " 20  Summary_human         954 non-null    object\n",
      " 21  Preprocessed_Body     954 non-null    object\n",
      " 22  Preprocessed_Summary  954 non-null    object\n",
      " 23  augmented_input       954 non-null    object\n",
      "dtypes: int64(5), object(19)\n",
      "memory usage: 179.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1e56c5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving to a json file\n",
    "df.to_json('email_full_with_aug_inp.json', orient='records', lines=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "69e72bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_read = pd.read_json('email_full_with_aug_inp.json', lines=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5290717d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 954 entries, 0 to 953\n",
      "Data columns (total 24 columns):\n",
      " #   Column                Non-Null Count  Dtype \n",
      "---  ------                --------------  ----- \n",
      " 0   subject               954 non-null    object\n",
      " 1   from                  954 non-null    object\n",
      " 2   to                    954 non-null    object\n",
      " 3   date                  954 non-null    object\n",
      " 4   body                  954 non-null    object\n",
      " 5   Body_Length           954 non-null    int64 \n",
      " 6   Subject_Length        954 non-null    int64 \n",
      " 7   Cleaned_Body          954 non-null    object\n",
      " 8   Cleaned_Subject       954 non-null    object\n",
      " 9   BERT_Embeddings       954 non-null    object\n",
      " 10  Cluster_Label         954 non-null    int64 \n",
      " 11  Category              954 non-null    object\n",
      " 12  Cleaned_mails         954 non-null    object\n",
      " 13  summary_TXTRNK_1      954 non-null    object\n",
      " 14  Summary               954 non-null    object\n",
      " 15  summary_BART          954 non-null    object\n",
      " 16  index_number          954 non-null    int64 \n",
      " 17  Tokenized_Email       954 non-null    object\n",
      " 18  Entities              954 non-null    object\n",
      " 19  Cluster_retrieved     954 non-null    int64 \n",
      " 20  Summary_human         954 non-null    object\n",
      " 21  Preprocessed_Body     954 non-null    object\n",
      " 22  Preprocessed_Summary  954 non-null    object\n",
      " 23  augmented_input       954 non-null    object\n",
      "dtypes: int64(5), object(19)\n",
      "memory usage: 179.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1cee3f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index 20:\n",
      "Capitalize on the Appetite for Innovation View Online Partner News 12 January, 2023 | EMEA Embrace the AI Opportunity With Dell Technologies and NVIDIA Create real business change with the Dell Technologies AI-optimized portfolio, from new lines of revenue and profit to better efficiency. Check It   DEMAND GEN CENTER DEAL REGISTRATION DISCOVERY CENTER PARTNER ACADEMY EVENTS CALENDAR Why AI? Why Now? Learn how to help customers turn data into actionable insights, differentiate elf and where the 'sweet spots' are for partners with a NVIDIA solution. Listen Now  h Deploy Dell APEX on  Deliver a Streamlined Customer Experience Extend the value of Dell APEX by enhancing your value-added services with Partner-led Deployment of select Dell APEX offers.* Find  How  Program Updates Complete  the End of Year Tier Audit Deadline Products, Solutions & Services Nordic Partners: Transform  With a Concierge Service Engage in Strategic Conversations to Help Customers Innovate With Data Help Customers Streamline AI Adoption and Reduce Time to Value Start a Business-Critical Cybersecurity Conversation With  Professional Services Opportunities Fast With New Guide Address Demanding AI/ML Workloads With the Dell PowerEdge R750 Server See the Dell PowerEdge R7525 Server for Emerging AI/ML Workloads Events & Webinars Are You Using Marketing Development Funds? Hear 2023 Updates on January 24 Learn Ab the Edge, Trends and Industries to Solve Customer Problems Add Value and Extend  Modern Consumption Models Learn How to Foster Meaningful Relationships in a Remote Work Environment In the News IoT in Focus: Expanding to the Edge With Dell Technologies Take Risks, Find Balance and Get Inspired  Hear Shawn Trotter's Story Dell VxRail Named CRN's 2022 Tech Innovator Award Winner *Available in the United States, United Kingdom, France, Germany, Denmark, Finland, Ireland, Italy, Norway, Spain, Sweden, Australia, New Zealand, Singapore, Sh Korea and Japan. Connect with us: <url> TechnologiesOne Dell WayRound RockTX78682 Manage Preferences Unsubscribe Privacy Statement You are subscribed as <email> For questions ab , contact us at <email> Dell Technologies Global Headquarters is located at One Dell Way, Round Rock, TX, 78682  2023 Dell Inc. or its subsidiaries. All Rights Reserved. Dell Technologies, Dell and other trademarks are trademarks of Dell Inc. or its subsidiaries. Dell Corporation Limited. Registered in England. Reg. No. 02081369, Dell House, The Boulevard, Cain Road, Bracknell, Berkshire RG12 1LF. .\n",
      "\n",
      "Index 21:\n",
      "Introducing choice & flexibility Exciting Changes  More Benefits, Simplified & Flexible Criteria, Experience Upgrades You Can Now Earn AND Redeem Points We listened! In response to feedback from our partners, exciting changes are being made to simplify the Intel Partner Alliance program criteria and enhance the benefits that support . Points are one of the most popular benefits of Intel Partner Alliance. Among other benefit enhancements, Member-tier partners are now eligible to redeem points for valuable rewards! Partners now have more flexible options in the program criteria to qualify for a higher tier in Intel Partner Alliance. Member partners now have more options to achieve Gold  t meet one of three criteria: Revenue, Training, or Solutions. Learn more More Benefits and Value...with Points for All! Member Points Redemption: Member-tier partners can now earn AND redeem points for valuable products and rewards. Coming soon in the first half of 2023: * New points redemption options: Design Enablement Services vouchers, Cloud instance Distributor vouchers, and Cloud certification training vouchers. New Enhanced Digital Capabilities A reimagined Intel Partner University launched in October 2022. There is much to be excited ab with the new Intel Partner University experience, including: a complete upgrade to the digital experience, personalized training pathways, and a new content rating system to help partners find and consume the highest peer-rated content. Soon, we will launch a new and improved Intel Solutions Marketplace user experience. This will include a new look and feel, an improved experience for generating connections and leads between partners, easier site navigation, and improved overall search functionality. Intel Partner Alliance is constantly striving to provide more value, solutions, and benefits so you can thrive in today's environment. Find  more ab what Intel Partner Alliance provides to help you advance . Learn more View in browser. This email has been translated for  machine translation. Reasonable efforts have been made to provide an accurate translation; however, no automated translation is perfect. The official text is the English version of the content. Any discrepancies or differences created in the translation are not binding and have no legal effect for compliance or enforcement purposes. If you forward this email,  will appear in any auto-populated form connected to links in this email.  and is an important business communication related to  with Intel. To view and manage your marketing-related email preferences with Intel, please  .  2023 Intel Corporation Intel Corporation, 2200 Mission College Blvd., M/S RNB4-145, Santa Clara, CA 95054 USA. www.intel.com Privacy | Cookies | *Trademarks | Manage Preferences\n",
      "\n",
      "Index 22:\n",
      "Its the start of a brand-new year, which gives us a great opportunity to kick-off a brand-new Race for Q1 2023.                                     Introducing the EMEA North Q1 2023 Partner Race! , Its the start of a brand-new year, which gives us a great opportunity to kick-off a brand-new Race for Q1 2023. The SAP EMEA North Q1 Race for 2023 Pipeline Creation is open to you, our Sell Partners, and is based on SAP cloud solution deals registered on the SAP Partner Portal. To win, you will need to register at least 10 SAP Cloud solution deals with five different end-customers on the SAP Partner Portal during the incentive timeframe (January 1st to March 31st 2023). Each deal registered needs to have a minimum annual contract value (ACV) of 5,000. The winning partner in each Market Unit will enjoy a meal  at a restaurant of their choice, worth up to 150 per guest (with a maximum number of 20 guests per winning partner). The top three partners in each Market Unit will also receive a winners certificate. Heres a breakdown of all you need to know (a complete set of Terms & Conditions and calculations can be found here): The Q1 2023 Partner Incentive applies to deals registered on the SAP Partner Portal by Resell Partners on cloud solutions of the SAP Digital Enterprise Platform and the SAP applications solution portfolio, excluding SAP Business One and third party solutions The winning partner per Market Unit will receive an incentive reward for accumulating the highest number of Challenge Credits by the end of the validity period (31st March 2023) To win, partners need to register at least 10 (ten) deals and 5 (five) different end-customers on the SAP Partner Portal during the incentive validity period (January 1st to March 31st 2023) Each deal registered needs to have a minimum annual contract value (ACV) of 5,000 Deals registered need to have a closing date of before September 30th, 2023 Deals have to be registered from January 11st to March 31st, 2023 In the event of a tie on the number of opportunities registered by two partners in a Market Unit, the highest total registered revenue (ACV) will determine the winner. So, are you ready to Race again? Over to you! Veronique Hangard Head of Channel EMEA North This e-mail provides information on SAP's products and services that may be of interest to you as an SAP Partner. If you would prefer not to receive e-mails from this sender, please reply and we will remove you. Copyright | Trademark | Privacy | Impressum Pflichtangaben/Mandatory Disclosure Statements: <url> Diese E-Mail kann Betriebs- oder Geschftsgeheimnisse oder sonstige vertrauliche Informationen enthalten. Sollten Sie diese E-Mail irrtmlich erhalten haben, ist Ihnen eine Kenntnisnahme des Inhalts, eine Vervielfltigung oder Weitergabe der E-Mail ausdrcklich untersagt. Bitte benachrichtigen Sie uns und vernichten Sie die empfangene E-Mail. Vielen Dank. This e-mail may contain trade secrets or privileged, undisclosed, or otherwise confidential information. If you have received this e-mail in error, you are hereby notified that any review, copying, or distribution of it is strictly prohibited. Please inform us immediately and destroy the original transmittal.  for your cooperation\n",
      "\n",
      "Index 23:\n",
      "Join Us Wednesday, January 25 1:00pm ET (UTC-5) , Join Us Wednesday Jan 25 @ 1:00pm Eastern (UTC-5) / 12:00pm Central / 10:00am Pacific for our next State of .NET - State of AI and Machine Learning webinar (approximately 75 mins). Machine Learning and Artificial Intelligence is everywhere these days. While some developers are still wondering how their app could benefit from these technologies, it is more accurate to wonder which apps can't be made better with these types of features. This State of .NET event will take a look at what the latest and the greatest in Machine Learning is and how .NET developers can easily take advantage of it, with becoming mathematical geniuses or data scientists. Markus will take a look at the overall AI landscape and provide a high-level overview of the different types of AI (such as shallow learning, deep learning, and General Articicial Intelligence a.k.a. GAI) and how t apply to software capabilities and their impact. In addition to an introduction to these concepts, this event will focus a lot on concretely using various concepts in today's software development as it relates to all software developers (with the need to focus entirely on AI development, or being a data scientist).  to Register for FREE!  Upcoming CODE Developer Security Training Courses Course Title: Secure Coding For Developers (C# or Java Courses Available) Cyberthreats and data breaches are in the news almost daily. Come learn how to build robust and secure C# or Java applications from our CODE Security experts in our virtual, 3-day, hands-on developer security courses. Select a specific language and date from the list below for course details. C# Courses * March 7-9 * April 18-20 * June 6-8 Java Courses * March 21-23 * May 2-4 * June 20-22  Previous State of .NET Webinar Recordings and Slides Are Available Last Webinar: State of .NET - .NET 7 .NET 7 is the next major version of the .NET platform. This event will take a look at all the news that go with .NET 7.  to Access Our Previous Webinar Recordings Why we are sending you this notification You are getting this email notification, because our database shows you as having opted in for notifications of this kind. If you wish to change the notifications you receive from us, you can do so here: <url> or by emailing <email> . If you do not want to receive any further email communications from us, you can turn off all emails with a single  .\n",
      "\n",
      "Index 24:\n",
      "CODE Magazine 2023 - January/February Issue is Available Now! Don't miss  on our latest free webinars! See below for more information! , The 2023 - January/February issue of CODE Magazine is now available as part of your subscription. If  a digital subscription, you can download this issue from <url> in multiple digital formats. Print subscribers will receive their copy in the mail, if t't done so. (You can always see issues sent to you at <url> ). Active subscribers (including trial subscriptions) have access to all back issues of CODE Magazine via our mobile apps . Give friends a free, trial CODE Magazine subscription by sending them here . In This Issue: In the cover article ab smart contracts, Wei-Meng will help developers understand many terms in blockchains. Paul Sheriff demonstrates the most frequently used data annotations in .NET. Sahil talks ab some of the most secure technologies in modern encryption. In Bilal's article ab PHP Larvel, he discusses middleware and how to create res and group res. Meanwhile, Joydip will help you learn ab OpenTelemetry and how to use it. Upcoming CODE Developer Security Training Courses Course Title: Secure Coding For Developers (C# or Java Courses Available) Cyberthreats and data breaches are in the news almost daily. Come learn how to build robust and secure C# or Java applications from our CODE Security experts in our virtual, 3-day, hands-on developer security courses. Select a specific language and date from the list below for course details. Introduction to Cyber Security * Cyber attacks * Types of attacks * Cyber security 101 Introduction of Cryptography * Encryption * Hashing * Signatures * Public-Key infrastructures * SSL / TLS Introduction to Authentication and Authorization * Authorization concepts in general * Session management * Password handling and management * Multi-factor authentication Injection Attacks * SQL injection * Command injection * LDAP injection * Cross site scripting (XSS) Introduction to Secure Coding and Motivation * History and security incidents in the past * Common pitfalls * Software dependencies Secure Coding  * Security  * Proper usage of types * Encapsulation * Code signing * Input data sanitization * Logging * Concurrency / multithreading * Exception handling * Data serialization and deserialization * Security libraries and frameworks API Security Considerations * Security  * GraphQL Wrap Up * Code reviews * Static code analysis * Dynamic code analysis / testing * Secure software development process C# Courses * January 24-26 * March 7-9 * April 18-20 * June 6-8 Java Courses * January 31 - February 2 * March 21-23 * May 2-4 * June 20-22 CODE is Hiring REACT Developers! CODE Consulting is accepting resumes for REACT web developers ranging from junior to senior roles. We have multiple positions to fill and will consider candidates who seek full-time employment or contracting opportunities. Remote is ok. Apply today Why You Received This Notification You received this notification because you have an active subscription to CODE Magazine. If you wish to unsubscribe, you can do so here: <url> or by emailing <email> . If you want to continue  but do not want to receive further email notifications when new issues are available, you can unsubscribe from all email communications with a single  .\n",
      "\n",
      "Index 25:\n",
      "We have deals for you. Your Amazon.co.uk Today's Deals See All Departments Selection from The INKEY List Ships from and sold by Amazon.co.uk Learn more Etekcity Scales Sold and Shipped by Amazon Learn more Tefal Cookware, Irons and more Ships from and sold by Amazon.co.uk Learn more Steam Vacuum cleaners, Irons and accessories by Polti Ships from and sold by Amazon.co.uk Learn more Etekcity Bluetooth Scales Sold and Shipped by Amazon Learn more Sistema Containers Shipped from and sold by Amazon.co.uk Learn more tado - smart home solutions for comfortable heating shipped by amazon.co.uk Learn more Products from Clearblue Ships from and Sold by Amazon Learn more Tractive: Pet GPS Trackers Sold and shipped by Amazon Learn more Festive Chocolates Sold and shipped by Amazon Learn more Apple iPhones and Watches Ships from and sold by Amazon.co.uk Learn more Instant Pot Pressure Cookers and Air Fryers Shipped and sold by Amazon.co.uk Learn more Products from Ariel, Fairy, Flash & more Ships From and Sold By Amazon Learn more Selected Kindle Books Save at least 70% everyday on top-rated Kindle Books Learn more Products from Andrex and Kleenex Ships From and Sold By Amazon Learn more Products from Gillette and Gillette Venus Ships from and sold by Amazon.co.uk Learn more Products from Belkin Shipped and Sold by Amazon.co.uk Learn more Jewellery and Watches from Fossil, Michael Kors, Diesel and more Ships from and sold by Amazon.co.uk Learn more Discover Sharpie, Paper Mate, Parker and more Sold and Shipped by Amazon Learn more See all Deals of the Day Explore More Deals SGIN Windows 11 Laptop 15.6 Inch... Selection of Blink Cameras and Doorbells SIHOO Ergonomic Office Chair,... Monitors by Samsung Wi-Fi & Networking Devices from TP-Link Windows PCs and Chromebooks from ASUS Shark Corded and Cordless Vacuum... Neck Massager, Deep Tissue 3D... Find Great Deals on Millions of Items Storewide  AmazonBasics  Baby  Blu-ray  Books  Business and Industrial  Camera & Photo  Car & Motorbike  Clothing  Computing  DIY & Tools  DVD  Electronics  Grocery  Health & Beauty  Home & Garden  Jewellery  Kindle Store  Lighting  Digital Music  Music  Musical Instruments & DJ  Office Products  let  Pet Supplies  Shoes & Accessories  Software  Sports & doors  Toys & Games  Video Games  Watches Connect with us We hope you enjoyed receiving this message. However, if you'd rather not receive future e-mails of this sort from Amazon.co.uk please opt- here . Terms and conditions apply. Click on the offer for details of applicable products and terms and conditions. Offers are for a limited time only and subject to availability. Discounts and savings on offers on products sold by Amazon.co.uk (excluding MP3s) refer to savings against Recommended Retail Price (\"RRP\") or our previous selling price, as indicated. Discounts and savings on Amazon MP3s refer to savings against our previous selling price, or as otherwise indicated. Offers on products sold by a Marketplace seller are subject to that seller's terms and conditions of sale. For details see www.amazon.co.uk .  2023 Amazon.com, Inc. or its affiliates. Amazon and all related marks are trademarks of Amazon.com, Inc. or its affiliates. Please note that this promotional e-mail is being sent from an e-mail address that cannot receive e-mails. If you have any questions and wish to contact us,  . Reference: 766481981 Please note that this message was sent to the following e-mail address: <email>\n",
      "\n",
      "Index 26:\n",
      "Join this upcoming digital event ab open source on Azure.  email? | View as web page Azure Open Source Day Learn how to build intelligent, scalable apps faster and easier at this deep dive into open source and Azure. See the latest open-source technology in actionwhile connecting with the community of industry leaders, innovators, and open-source enthusiasts. Register now for this free digital event to: * See app-building demos using Azure and the latest in open-source technologies, cloud-native architectures, and microservices. * Get tips and  for open source from industry experts at companies like HashiCorp, GitHub, and Redis. * Learn to build cloud-native apps for relational and nonrelational data with Azure Cosmos DB, now supporting native PostgreSQL. * Discover new capabilities in IaaS, PaaS, containers, and serverless computing, including Azure Kubernetes Service (AKS). * Explore practical ways to optimize your open-source investments and gain more time for innovation. * Learn how to protect  business assets by building on a highly secure cloud platform designed to meet your open-source security and compliance needs. Plus, hear from open-source leaders like Brendan Burns and Donovan Brown of Microsoft and Stormy Peters of GitHuband ask  the live chat Q&A. Azure Open Source Day Tuesday, March 7, 2023 09:00 AM10:30 AM Pacific Time (UTC-8) NOTE: If this email invitation was forwarded to you, please register here instead . The registration link below is specific to the original recipient, and it wont work for you. Register instantly > Brendan Burns Corporate Vice President, Azure OSS Cloud Native, Microsoft Cassie Zimmerman Senior Director of Cloud Partnerships, Redis Stormy Peters Vice President of Communities, GitHub Unsubscribe | Privacy Statement Microsoft Corporation One Microsoft Way Redmond, WA 98052\n",
      "\n",
      "Index 27:\n",
      "With every passing year, cyberattacks continue to grow in scale and sophistication. Just one cyberattack holds the potential to cripple an entire enterprise and stop its IT operations in its tracks. Despite this growing threat, organizations can take an important step to drastically reduce the risk of attack: implementing hardware-based security solutions that go below the OS and protect PCs at the BIOS and system memory levels. In an independently conducted and Intel-sponsored study titled Security Innovation: Secure Systems Start with Foundational Hardware , the Ponemon Institute spoke with 1,406 IT decision makers around the globe to gather their insights on their cybersecurity priorities. The message was clear: Hardware-based security solutions are quickly becoming a foundational element of enterprise security strategies. * While 36% of those surveyed said theyve already adopted hardware-based security solutions, an additional 47% said that t to implement such technologies in either the next six months (24%) or 12 months (23%). * Perhaps more telling were the additional responses from the respondents who have already implemented hardware-based solutions, with 85% identifying this technology as a high or very high priority. * Furthermore, 64% said that hardware was a part of their organizations endpoint security strategy. In other words, those who were quick to implement hardware-based protective measures do not view this as a gimmick, but rather as an essential innovation that provides real value to their bottom line. But these survey insights are just scratching the surface of the Ponemon Institutes report. To gain even deeper insights into how top IT decision makers approach cybersecurity, and Intel Hardware Shield can factor into , click the link below to read the full report.  to download Security Innovation: Secure Systems Start with Foundational Hardware Find  how the hardware-based security of Intel vPro helps protect against cyberattacks. The built for business Intel vPro is designed to meet the needs of IT professionals: Comprehensive Multilayer Security With unique hardware-based security measures that are enabled right  of the box, along with protections below the OS and active monitoring for threats and attacks, Intel vPro provides multilayer security1 to help protect your businesss resources and data. Professional-grade Performance Equipping employees with PCs  from ultralight laptops to high-power workstations  built on Intel vPro amplifies their effectiveness with industry-leading performance tuned for the workloads and applications that business professionals use the most. Complete Management The office is everywhere. With PCs on Intel vPro, IT can be everywhere too. Intel vPro brings modern management options to remotely2 discover, repair, and help protect PCs in , which will help simplify support and improve user experience. Reliable Stability Demanding design requirements and rigorous testing ensure that all PCs built on Intel vPro deliver a reliable, stable foundation for smoother fleet management and allow you to scale with confidence. What is Intel vPro? Learn more here. 1 No product or component can be absolutely secure. 2 Requires a network connection. Must be a known network for Wi-Fi management. Ready to Get Started? Chat with us to find  more ab how the Intel vPro can work for your organization. Lets chat View in browser. Intel technologies may require enabled hardware, software, or service activation. No product or component can be absolutely secure.  vary. Performance varies by use, configuration, and other factors. Learn more at www.Intel.com/PerformanceIndex . Performance results are based on testing as of dates shown in configurations and may not reflect all publicly available updates. Learn more at intel.com/vPro . Intel Corporation If you forward this email,  will appear in any auto-populated form connected to links in this email. This was sent to <email> because you are subscribed to Newsletters. To view and manage your marketing-related email preferences with Intel, please .  2022 Intel Corporation Intel Corporation, 2200 Mission College Blvd., M/S RNB4-145, Santa Clara, CA 95054 USA. www.intel.com Privacy | Cookies | *Trademarks | Unsubscribe | Manage Preferences\n",
      "\n",
      "Index 28:\n",
      "We spent time under the hood to make GitKraken Client run even smoother for users... Not rendering correctly? View this email as a web page here . New Release: GitKraken Client v8.10 We spent time under the hood to make GitKraken Client run even smoother for users. With this release, we've introduced a dependency update: * GitKraken Clients version of Electron has been updated And a tuned-up engine goes perfect with these user-experience improvements from v8.10: * New Tab lay has been rearranged to make it easier to access repositories, Workspaces, and the Terminal Tab. * You can now double-click a section header in the left panel to maximize that section. This option is also available in a context menu. GitLens 13 Release Do you use VS Code? Our popular Git extension, GitLens, now includes the powerful GitKraken Commit Graph in . Check  the GitLens 13 release to learn how to access powerful GitLens+ features like the Commit Graph. Connect with us to get real-time updates! GitKraken 13835 N. Northsight Blvd. Suite 205 Scottsdale, AZ 85260 USA You received this email because you are subscribed to GKC - Product Updates from GitKraken Update  to choose the types of emails you receive. Unsubscribe from all future emails\n",
      "\n",
      "Index 29:\n",
      "Celebrate pay day with big savings on our laptops, gaming PCs and components View in browser LAPTOPS ASUS ROG Flow X16 16\" Gaming Laptop * AMD Ryzen 9 6900HS CPU * 32GB RAM * 1TB Solid State Drive * GeForce RTX 3070 Ti Graphics Card Now only 2,199.99 SAVE 616 Shop now >> Lenovo Legion 5 15.6\" Gaming Laptop * AMD Ryzen 7 6800H CPU * 16GB RAM * 512GB Solid State Drive * GeForce RTX 3070 Ti Graphics Card Now only 1,699.99 SAVE 100 Shop now >> GAMING PCs ASUS ROG Strix Gaming PC * AMD Ryzen 5 5600X CPU * 16GB RAM * GeForce RTX 3060 Graphics Card * 1TB Solid State Drive Now only 949.99 SAVE 349 Shop now >> Horizon Vectis Gaming PC * Intel Core i5 CPU * 16GB RAM * GeForce RTX 3060 Graphics Card * 500GB Samsung SSD Now only 1,052.99 SAVE 187 Shop now >> COMPONENTS Gigabyte GeForce RTX 4090 Graphics Card * 24GB Graphics Memory * PCI Express 4.0 x16 Interface * Cooler Type - Dynamic Fan * Nvidia GPU Boost Technology * Pre Overclocked Edition Now only 1,799.00 SAVE 196 Shop now >> AMD Ryzen 5 5600X AM4 CPU * Base: 3.7GHz, Turbo: 4.6GHz * 6 Cores, 12 Threads * Socket AM4 * No Integrated Graphics * Heatsink & Fan Included Now only 170.06 SAVE 124 Shop now >> All Deals   Gaming PCs  Laptops   Components  RATED EXCELLENT 4.8 rating on Trustpilot 3-YEAR ON SITE WARRANTY Includes parts & labour FREE UK DELIVERY When you spend over 50* CUSTOM PCS BUILT FAST 4-5 day delivery       CCL Computers Registered in England and Wales No: 3224671 | Inmoor Road, Birkenshaw, Bradford, BD11 2PS This email was sent to <email> we have sent you this email so that we can better understand  opinions on products and services from CCL to continue our improvement. This is in line with our Privacy Statement available here. If you wish to unsubscribe from our newsletter, \n",
      "\n",
      "Index 30:\n",
      "Register for upcoming events. Learn more ab the commercial marketplace and maximize  New Year, and welcome to this months Marketplace Office Hours for partners. Learn ab opportunities to grow  take advantage of Azure services and partner programs and resources. We offer a complete set of webinars every month to get you from A to Z with the marketplace. We also have a new special webinar ab understanding custom metered billing for . Marketplace Office Hours: Getting started in marketplace Learn how to reach new customers as t try, find, and buy solutions in online marketplaces. Youll hear ab programs, resources, and benefits designed to help  in the commercial marketplace. Monday, January 9, 2023, 08:30 AM Pacific Time Duration: 45 minutes Register here Monday, January 23, 2023, 08:30 AM Pacific Time Duration: 45 minutes Register here Marketplace Office Hours: Setting up  how to start with a new SaaS offer; set up the required fields in Partner Center and understand the options and tips to get you started faster. Monday, January 9, 2023, 09:00 AM Pacific Time Duration: 60 minutes Register here Marketplace Office Hours: Discover Custom Metering for Pricing and Billing Success on Marketplace Come and explore practical ways to create flexible multiyear, quarterly, and other scenarios that allow customers and their ISV partners to customize dealmaking on Azure Marketplace. Tuesday, January 10, 2023, 09:00 AM Pacific Time Duration: 60 minutes Register here Marketplace Office Hours: Developing  In this technical session, learn how to implement the components of a fully functional SaaS solution, including how to implement the SaaS landing page, a webhook to subscribe to change events, and the integration of  into the marketplace. Tuesday, January 17, 2023, 09:00 AM Pacific Time Duration: 60 minutes Register here Marketplace Office Hours:  to accelerate growth with Tackle.io Getting started with the Microsoft commercial marketplace is a big step forward in -to-market strategy. Learn how Tackle can get you listed in the Microsoft commercial marketplace, scale  over time, and accelerate revenue generation. Wednesday, January 11, 2023, 09:00 AM Pacific Time Duration: 30 minutes Register here Wednesday, January 18, 2023, 09:00 AM Pacific Time Duration: 30 minutes Register here Wednesday, January 25, 2023, 09:00 AM Pacific Time Duration: 30 minutes Register here Marketplace Office Hours: Activating  journey In this session, we cover the resources, tips, programs, and incentives to help you take advantage of  offers to increase sales. Wednesday, January 11, 2023, 10:00 AM Pacific Time Duration: 30 minutes Register here Wednesday, January 18, 2023, 10:00 AM Pacific Time Duration: 30 minutes Register here Wednesday, January 25, 2023, 10:00 AM Pacific Time Duration: 30 minutes Register here Marketplace Office Hours: Open Q&A Join our Commercial Marketplace Services team for a Q&A to help you publish  in the marketplace. Thursday, January 19, 2023, 08:30 AM Pacific Time Duration: 30 minutes Register here Marketplace Office Hours: Plans and pricing Learn ab the pays process life cycle for the Microsoft commercial marketplace, how to view and access pay reporting, and the payment processes that are supported within Partner Center. Thursday, January 12, 2023, 09:00 AM Pacific Time Duration: 60 minutes Register here Marketplace Office Hours: Billing and pays Learn ab the pays process life cycle for the Microsoft commercial marketplace, how to view and access pay reporting, and the payment processes that are supported within Partner Center. Tuesday, January 24, 2023, 09:00 AM Pacific Time Duration: 60 minutes Register here Marketplace Office Hours: Developing with the SaaS Accelerator This session will show how the SaaS accelerator project can help partners go to market quicker by accelerating the publishing of their transactable SaaS offers on Azure Marketplace. Monday, January 23, 2023, 09:00 AM Pacific Time Duration: 60 minutes Register here Marketplace Office Hours: Build  for the marketplace In this new webinar, learn how to set up and develop the new Azure container offer used to deploy containerized solutions as Kubernetes apps from the Azure marketplace. Thursday, January 19, 2023, 09:00 AM Pacific Time Duration: 60 minutes Register here Marketplace Office Hours: Developing Virtual Machine Offers in the Azure Marketplace In this session, well review the required technical configurations to make virtual machine apps and how to publish virtual machine offers to the Azure marketplace. Wednesday, January 25, 2023, 09:00 AM Pacific Time Duration: 60 minutes Register here Additional information * Explore the Mastering the Marketplace library to find more content, samples, and labs to help you build your solutions. * Register for any of our upcoming and previous Marketplace Office Hours sessions . * Grow  an independent software vendor (ISV) with the ISV Success Program . Unsubscribe | Privacy Statement Microsoft Corporation, One Microsoft Way, Redmond, WA 98052\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(20, 31):\n",
    "    print(f\"Index {i}:\\n{df['augmented_input'][i]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bc97a6",
   "metadata": {},
   "source": [
    "## Using the augmented data as input for the bart model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3046906",
   "metadata": {},
   "source": [
    "## Preprocessing the augmented input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1675e204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the preprocessing function to the augmented input column\n",
    "df['Preprocessed_aug_Body'] = df['augmented_input'].apply(lambda x: preprocess_email_for_t5(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f97686df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\764883\\AppData\\Local\\Temp\\ipykernel_12060\\2677226940.py:41: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_dataset = TensorDataset(torch.tensor(train_encodings['input_ids']), torch.tensor(train_encodings['attention_mask']), torch.tensor(train_encodings['labels']))\n",
      "C:\\Users\\764883\\AppData\\Local\\Temp\\ipykernel_12060\\2677226940.py:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  val_dataset = TensorDataset(torch.tensor(val_encodings['input_ids']), torch.tensor(val_encodings['attention_mask']), torch.tensor(val_encodings['labels']))\n",
      "C:\\Users\\764883\\AppData\\Local\\Temp\\ipykernel_12060\\2677226940.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_dataset = TensorDataset(torch.tensor(test_encodings['input_ids']), torch.tensor(test_encodings['attention_mask']), torch.tensor(test_encodings['labels']))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BartForConditionalGeneration(\n",
       "  (model): BartModel(\n",
       "    (shared): Embedding(50265, 1024, padding_idx=1)\n",
       "    (encoder): BartEncoder(\n",
       "      (embed_tokens): Embedding(50265, 1024, padding_idx=1)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): BartDecoder(\n",
       "      (embed_tokens): Embedding(50265, 1024, padding_idx=1)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=50265, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = df\n",
    "data = data[['Preprocessed_aug_Body', 'Preprocessed_Summary']].dropna()\n",
    "data = data.sample(frac=1, random_state=0)  # Shuffle the data\n",
    "dataset = Dataset.from_pandas(data)\n",
    "dataset = dataset.train_test_split(test_size=0.3)  # Splitting the data\n",
    "dataset[\"test\"] = dataset[\"test\"].train_test_split(test_size=0.5)\n",
    "\n",
    "# Prepare the data\n",
    "train_source = dataset[\"train\"][\"Preprocessed_aug_Body\"]\n",
    "train_target = dataset[\"train\"][\"Preprocessed_Summary\"]\n",
    "val_source = dataset[\"test\"][\"train\"][\"Preprocessed_aug_Body\"]\n",
    "val_target = dataset[\"test\"][\"train\"][\"Preprocessed_Summary\"]\n",
    "test_source = dataset[\"test\"][\"test\"][\"Preprocessed_aug_Body\"]\n",
    "test_target = dataset[\"test\"][\"test\"][\"Preprocessed_Summary\"]\n",
    "\n",
    "# Initialize tokenizer for BART\n",
    "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large\")\n",
    "\n",
    "# Tokenize the data\n",
    "def tokenize_data(source, target, max_length=1024, max_target_length=150):\n",
    "    encodings = tokenizer(source, truncation=True, max_length=max_length, padding=True)\n",
    "    decodings = tokenizer(target, truncation=True, max_length=max_target_length, padding=True)\n",
    "    return {\n",
    "        'input_ids': torch.tensor(encodings['input_ids']),\n",
    "        'attention_mask': torch.tensor(encodings['attention_mask']),\n",
    "        'labels': torch.tensor(decodings['input_ids'])\n",
    "    }\n",
    "\n",
    "train_encodings = tokenize_data(train_source, train_target)\n",
    "val_encodings = tokenize_data(val_source, val_target)\n",
    "test_encodings = tokenize_data(test_source, test_target)\n",
    "\n",
    "# Create TensorDatasets\n",
    "train_dataset = TensorDataset(torch.tensor(train_encodings['input_ids']), torch.tensor(train_encodings['attention_mask']), torch.tensor(train_encodings['labels']))\n",
    "val_dataset = TensorDataset(torch.tensor(val_encodings['input_ids']), torch.tensor(val_encodings['attention_mask']), torch.tensor(val_encodings['labels']))\n",
    "test_dataset = TensorDataset(torch.tensor(test_encodings['input_ids']), torch.tensor(test_encodings['attention_mask']), torch.tensor(test_encodings['labels']))\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True, num_workers=4)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=2,num_workers=4)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=2, num_workers=4)\n",
    "\n",
    "# Load BART model\n",
    "model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ae2724",
   "metadata": {},
   "source": [
    "## BART augmented input with 20 epochs  lr= 5e-5, warmup= 500 and linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "01f96e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebfd7999faa94fcfa05e0a93410f32cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 5.072212161061293\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b634f8acceda43a5b40014e21e936796",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 1.3563887576262157\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39c2e2b0ec174e579a5f14d25596e531",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Validation Scores: ROUGE: {'rouge1': 0.4254886000824933, 'rouge2': 0.2064922047600892, 'rougeL': 0.31036104380613777, 'rougeLsum': 0.31036104380613777}, BERTScore: {'precision': 0.2986354865651164, 'recall': 0.23293141838318357, 'f1': 0.2651307673400475}\n",
      "Epoch 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf8d39341a8440ff896714ae0d18b28b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 1.3395027545933238\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acc4bdcb967247de87cf7408ba7b8110",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 1.094107331501113\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e6f0fd8d157435e8d6376fcc36a896b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Validation Scores: ROUGE: {'rouge1': 0.45808455288767824, 'rouge2': 0.23095491802676316, 'rougeL': 0.33440301032669173, 'rougeLsum': 0.33440301032669173}, BERTScore: {'precision': 0.35382054189944434, 'recall': 0.2759935510241323, 'f1': 0.31440770434629584}\n",
      "Epoch 2\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3beb107624084203ae63699454d3e087",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 1.1056668100778215\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ca1269c1a9946db93292d6a2a8cfb59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 1.0618526087039046\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4665bcd4593449abcc9c824832ea379",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Validation Scores: ROUGE: {'rouge1': 0.47312369806100235, 'rouge2': 0.24144203869560574, 'rougeL': 0.3536234839611362, 'rougeLsum': 0.3536234839611362}, BERTScore: {'precision': 0.3178792273522251, 'recall': 0.32084022441671955, 'f1': 0.31944115252958405}\n",
      "Epoch 3\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f82d4f8a72094971a3758aadf36a9f93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.8842625806253114\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ece8cf23e1ac4c6aa99b95327d4883f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 1.083980642673042\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "968b392ebbea493099640fbb3592f549",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Validation Scores: ROUGE: {'rouge1': 0.4768785925352356, 'rouge2': 0.2500490589805259, 'rougeL': 0.36475311943729494, 'rougeLsum': 0.36475311943729494}, BERTScore: {'precision': 0.302889724365539, 'recall': 0.3383242361920161, 'f1': 0.3205900875748032}\n",
      "Epoch 4\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88155ef2e7994aae83fbe7cc43e76bd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.9143411104907533\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9d8f4d64a4f41398aa460fcb83169d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 1.0923768000470266\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c44b6bcf89d40f687d420fbaa29a69d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Validation Scores: ROUGE: {'rouge1': 0.49465461249752746, 'rouge2': 0.25758351534112495, 'rougeL': 0.3707264299590523, 'rougeLsum': 0.3707264299590523}, BERTScore: {'precision': 0.3385171646045314, 'recall': 0.3400636122872432, 'f1': 0.339177321539157}\n",
      "Epoch 5\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4eb21408866e4337bc7161b4e3fbc4dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.7815596321207321\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfedcb4b2bfe42feb1dc1c0f3bef841a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 1.115585546940565\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b871ba4e5c874f5faa46bb37dc0fe5f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Validation Scores: ROUGE: {'rouge1': 0.4800602653245908, 'rouge2': 0.2433527788587135, 'rougeL': 0.36374297005832484, 'rougeLsum': 0.36374297005832484}, BERTScore: {'precision': 0.34111118893552983, 'recall': 0.3355308158530129, 'f1': 0.33834842977941865}\n",
      "Epoch 6\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e1f489f145b436b9946964cf9705230",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.5998866802561069\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc83bb6f492442638987a79aaa0095d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 1.1725103275643454\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19c7b033a9eb4988af96e3b32cd64915",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Validation Scores: ROUGE: {'rouge1': 0.4875489350809089, 'rouge2': 0.24828036737917833, 'rougeL': 0.35839977836464115, 'rougeLsum': 0.35839977836464115}, BERTScore: {'precision': 0.30424423174311716, 'recall': 0.3507788013666868, 'f1': 0.3276122506293986}\n",
      "Epoch 7\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25ff1a11307e4bd7a17255e4ae3b7e9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.6813750130063045\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62b4af11913849dc892f4c5c6bab3bf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 1.1955889347526762\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "690f9dcec4a84eb3874875042cbbc7a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 Validation Scores: ROUGE: {'rouge1': 0.47018176243155935, 'rouge2': 0.23809221545479298, 'rougeL': 0.34826738053910594, 'rougeLsum': 0.34826738053910594}, BERTScore: {'precision': 0.3354997168191605, 'recall': 0.3187341242252539, 'f1': 0.3272058671961228}\n",
      "Epoch 8\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bd7650db3374810b728de5bd57ed6f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.5044558824090186\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "befe8411f0654a39856ba7dc7ad3277f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 1.2216756381094456\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "014b1bceae4d4639b3690c3f71341a9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 Validation Scores: ROUGE: {'rouge1': 0.47997643591678263, 'rouge2': 0.2375552040033225, 'rougeL': 0.3571455933114558, 'rougeLsum': 0.3571455933114558}, BERTScore: {'precision': 0.33085606060922146, 'recall': 0.3323038484280308, 'f1': 0.3315575044188235}\n",
      "Epoch 9\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "576bba9b272f4a6e8a4c9bb7d510c112",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.4355159432231309\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7f679f09370438a9cbda8f6557cc21e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 1.2690831451780267\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c1eb567462e4fe0ad4b1b2ff4deb158",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 Validation Scores: ROUGE: {'rouge1': 0.4863596103536827, 'rouge2': 0.24740015996392767, 'rougeL': 0.3619585396871985, 'rougeLsum': 0.3619585396871985}, BERTScore: {'precision': 0.3140662897688647, 'recall': 0.34607888540873927, 'f1': 0.3301492333929572}\n",
      "Epoch 10\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f8cfea9b2aa4acdb90536e32642bb58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.36410322165239356\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7c2671810c3488d84a9256a683862b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 1.2691203157107036\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4718ed2764d9434fb9e97c2178021edf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 Validation Scores: ROUGE: {'rouge1': 0.48237483855513325, 'rouge2': 0.24763777089754088, 'rougeL': 0.359703393541583, 'rougeLsum': 0.359703393541583}, BERTScore: {'precision': 0.33914674549467033, 'recall': 0.33018351242774063, 'f1': 0.3346995148393843}\n",
      "Epoch 11\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73ee1f9bdc4c4a2bb8d613d8f0d7e8bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.3487036726521161\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96924509626a48a6b3e738c80d44e4e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 1.339051845173041\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "add250d015f741baa6282e28b7378a3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 Validation Scores: ROUGE: {'rouge1': 0.4805700986178707, 'rouge2': 0.24059402782937067, 'rougeL': 0.3621877913566602, 'rougeLsum': 0.3621877913566602}, BERTScore: {'precision': 0.31544948814229834, 'recall': 0.34504652116447687, 'f1': 0.33033638416479033}\n",
      "Epoch 12\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daed8c0913a04e2285c123f39ede8d56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.295536148021678\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bcbf6bf11b34e92915f6a0fdc9b1fbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 1.3756015002727509\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ef1496d925a445ba86a53e415fcd33c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 Validation Scores: ROUGE: {'rouge1': 0.482768662278145, 'rouge2': 0.2432332746590658, 'rougeL': 0.3584247910043683, 'rougeLsum': 0.3584247910043683}, BERTScore: {'precision': 0.328774918957303, 'recall': 0.3461714844322867, 'f1': 0.33766036824737156}\n",
      "Epoch 13\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c618e6df8eb041e38dfbb5d2f9b671aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.2983821634270117\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cc790d049984309876687e34ae7d717",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 1.3568290389246411\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc86b57d14c24d33ab509f907003b856",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 Validation Scores: ROUGE: {'rouge1': 0.48556602545060495, 'rouge2': 0.24707474421298484, 'rougeL': 0.3708747059681898, 'rougeLsum': 0.3708747059681898}, BERTScore: {'precision': 0.3399461453470091, 'recall': 0.34905124372906154, 'f1': 0.3447076462002264}\n",
      "Epoch 14\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6178f580954d47f0a152a9b6fb722085",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.24627152225839163\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3279eb3d3094a8ebfc98cd03e83e749",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 1.4117086773945227\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e72042022f9543f79a5cb34753ee0b6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 Validation Scores: ROUGE: {'rouge1': 0.47677108208158003, 'rouge2': 0.23401735901680829, 'rougeL': 0.35382578433203066, 'rougeLsum': 0.35382578433203066}, BERTScore: {'precision': 0.3162636755344768, 'recall': 0.3368761487719085, 'f1': 0.32675268431194127}\n",
      "Epoch 15\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aeb94ca440d74138adcab171a65f703c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.2153580167113307\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c71635cf05548879241f456763b63d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 1.4111665383809142\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da8c424b30654a759dd51c8f98f35e45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 Validation Scores: ROUGE: {'rouge1': 0.4788729341796211, 'rouge2': 0.24388278148688858, 'rougeL': 0.3615861695684577, 'rougeLsum': 0.3615861695684577}, BERTScore: {'precision': 0.32976375333964825, 'recall': 0.3395702343744536, 'f1': 0.3347284624146091}\n",
      "Epoch 16\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "179673d8b52e476fabfb4d38bfe4832a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.20543495730725592\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f59aefb901a490b8a08d09abb29bf73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 1.4431363265547488\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8196102f63cc4963a70b87f00f2d6e7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 Validation Scores: ROUGE: {'rouge1': 0.4847565270122654, 'rouge2': 0.246550082744235, 'rougeL': 0.362365194621499, 'rougeLsum': 0.362365194621499}, BERTScore: {'precision': 0.3353529909315209, 'recall': 0.3474314936126272, 'f1': 0.34144707960594034}\n",
      "Epoch 17\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfe3536af97c4a6499c8beb37ac63444",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.19333209110769684\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5590b8cdf0524881bb32cda512fc36de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 1.4603503110508125\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b772ee677f8426b876e6d3c21ffb05a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 Validation Scores: ROUGE: {'rouge1': 0.48649934614700135, 'rouge2': 0.24791214216013616, 'rougeL': 0.3678268955497841, 'rougeLsum': 0.3678268955497841}, BERTScore: {'precision': 0.33721115393564105, 'recall': 0.3434674259689119, 'f1': 0.3404071719333943}\n",
      "Epoch 18\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d952d99baeff4de0a5454232fd28d070",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.17255460501029463\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15c960ea9e29436e94459f80b696b54e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 1.4701184936695628\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e14477be77645249d95a001ec0be869",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 Validation Scores: ROUGE: {'rouge1': 0.4840272988815856, 'rouge2': 0.2434987736825619, 'rougeL': 0.3583158729097758, 'rougeLsum': 0.3583158729097758}, BERTScore: {'precision': 0.3249350553378463, 'recall': 0.34587741813932854, 'f1': 0.33552021835930645}\n",
      "Epoch 19\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11972bed752e459687b45f063e24e36a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.15988686704773925\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f94191ade5e4aa3962b144bd1789dca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 1.481468781001038\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3a34ea63f8e4fd1b6c2414d01080552",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 Validation Scores: ROUGE: {'rouge1': 0.4837220493335938, 'rouge2': 0.24527579391209922, 'rougeL': 0.3648361270906705, 'rougeLsum': 0.3648361270906705}, BERTScore: {'precision': 0.329717842137648, 'recall': 0.34702809982829624, 'f1': 0.3385028752705289}\n"
     ]
    }
   ],
   "source": [
    "model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "epochs = 20\n",
    "\n",
    "\n",
    "# Define the optimizer with weight decay\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-5, weight_decay=0.01)\n",
    "\n",
    "# Total number of training steps\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Set up the learning rate scheduler\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps=500, \n",
    "                                            num_training_steps=total_steps)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch}\\n\")\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    with tqdm(train_dataloader, unit=\"batch\") as tepoch:    \n",
    "        for batch in tepoch:\n",
    "            tepoch.set_description(f\"Epoch {epoch}\")\n",
    "            input_ids, attention_masks, labels = tuple(t.to(device) for t in batch)\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_masks, labels=labels)\n",
    "\n",
    "            loss = outputs.loss\n",
    "            train_loss += loss.item()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()  # Update the learning rate\n",
    "            tepoch.set_postfix(loss=loss.item())\n",
    "        train_loss /= len(train_dataloader)\n",
    "        sleep(0.1)\n",
    "    print(f\"Training loss: {train_loss}\")\n",
    "\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        with tqdm(test_dataloader, unit=\"batch\") as tepoch:\n",
    "            for batch in tepoch:\n",
    "                tepoch.set_description(f\"Epoch {epoch}\")\n",
    "                input_ids, attention_masks, labels = tuple(t.to(device) for t in batch)\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_masks, labels=labels)\n",
    "                loss = outputs.loss\n",
    "                test_loss += loss.item()\n",
    "                tepoch.set_postfix(loss=loss.item())\n",
    "\n",
    "            test_loss /= len(test_dataloader)\n",
    "    print(f\"Testing loss: {test_loss}\")\n",
    "\n",
    "    # Validation and Metric Calculation\n",
    "    rouge_scores, bert_scores = validate_and_calculate_metrics(model, val_dataloader, tokenizer, device)\n",
    "    print(f\"Epoch {epoch} Validation Scores: ROUGE: {rouge_scores}, BERTScore: {bert_scores}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c1b862",
   "metadata": {},
   "source": [
    "## Training using One cycle learning rate policy with a learning rate starting 5e-6 to 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c7e8589e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec01d409853f4b898389f6e476edd90c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 7.375516775839343\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35c8ef397a09499a87f7979bf9683fbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 5.901495579216215\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d836d01f86548c99a874245775151b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Validation Scores: ROUGE: {'rouge1': 0.39623414919413197, 'rouge2': 0.1785703169135181, 'rougeL': 0.2755617980382468, 'rougeLsum': 0.2755617980382468}, BERTScore: {'precision': 0.05369401067340126, 'recall': 0.23344343916202584, 'f1': 0.14234592311549932}\n",
      "Epoch 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cb2794ed6564b1bb989534fc6a64140",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 4.5123587611192715\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edcc639a63e24a09a30fc87f3c7c995f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 2.791290377577146\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d121874fda7247de9c31eebb94f6ded1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Validation Scores: ROUGE: {'rouge1': 0.4158909505940553, 'rouge2': 0.19006736979921468, 'rougeL': 0.2883717434387565, 'rougeLsum': 0.2883717434387565}, BERTScore: {'precision': 0.19618093967437744, 'recall': 0.23031525179329845, 'f1': 0.21287031205267543}\n",
      "Epoch 2\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6993ec6ccc1d43e89e73c30a2164df6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 2.3400890630876234\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac602c315411417db1a29817df58a87d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 1.3189378297991223\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03de6c00582047fcb76b8750029bbcad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Validation Scores: ROUGE: {'rouge1': 0.45069723375567805, 'rouge2': 0.21803825630801002, 'rougeL': 0.31775261375836367, 'rougeLsum': 0.31775261375836367}, BERTScore: {'precision': 0.2591734119794435, 'recall': 0.2910665024796294, 'f1': 0.27481447098155815}\n",
      "Epoch 3\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac6025b69fbd4758ab6258ac6334f043",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 1.350737646311343\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae291fbdbc3e44b3aa9e8c5db693968e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 1.1245528898305364\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e3a7f8ba22b4d6aafbe7a578b4ac170",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Validation Scores: ROUGE: {'rouge1': 0.4559761280501849, 'rouge2': 0.22645346079089512, 'rougeL': 0.3353423357713696, 'rougeLsum': 0.3353423357713696}, BERTScore: {'precision': 0.29675405726043713, 'recall': 0.2930328513806065, 'f1': 0.29487672419701183}\n",
      "Epoch 4\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63b11c8803e644cc9cfe925598daea12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 1.1487150402840025\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b52b3047c1574018ac60430b3d37bfbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 1.0880754937728245\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc353942e01e44eb85f349f86bce894f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Validation Scores: ROUGE: {'rouge1': 0.4701017651266535, 'rouge2': 0.23555784564173118, 'rougeL': 0.34374022138495547, 'rougeLsum': 0.34374022138495547}, BERTScore: {'precision': 0.3187237731698487, 'recall': 0.29755407225133645, 'f1': 0.30812674745296437}\n",
      "Epoch 5\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce75004c44aa46f0b057698a79618368",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 1.0232516583389866\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0234f60ff744a3fa963c3daf8e02d2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 1.057898625317547\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f25f495576e0463d923d1841c4a16bc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Validation Scores: ROUGE: {'rouge1': 0.4790846634382664, 'rouge2': 0.25004612597677206, 'rougeL': 0.35673590425813273, 'rougeLsum': 0.35673590425813273}, BERTScore: {'precision': 0.3302769946555297, 'recall': 0.3220333681545324, 'f1': 0.3260301298772295}\n",
      "Epoch 6\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cfd448d411b40e1a6935033aee9bc0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.930163351004709\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96bb099466e54bc2903d8c5d7fd27552",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 1.0344065104921658\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef1eebd1b8f0413ca5bf2c77da5dfbb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Validation Scores: ROUGE: {'rouge1': 0.4829860889296207, 'rouge2': 0.24661567307439203, 'rougeL': 0.36331528527815876, 'rougeLsum': 0.36331528527815876}, BERTScore: {'precision': 0.34154709749337697, 'recall': 0.32058137903610867, 'f1': 0.33103675736735266}\n",
      "Epoch 7\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5d59a150deb426494d5c65b8f9b6240",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.8465079705336851\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e6390d124eb486aa357638bb1ca1648",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 1.0387474166022406\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b818a9d04ece404eb009e0ef8b46b23e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 Validation Scores: ROUGE: {'rouge1': 0.4872896015113137, 'rouge2': 0.25564862787386455, 'rougeL': 0.37100563248727486, 'rougeLsum': 0.37100563248727486}, BERTScore: {'precision': 0.35503091021544403, 'recall': 0.3200914736630188, 'f1': 0.33724708006613785}\n",
      "Epoch 8\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ad50d60d4d04de4b4eb7c84a23d197b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.7849343759927921\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7d87df3bc6a44948a176a807c1ea359",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 1.0434625045292907\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10f01fc632b143638d223aac1aafa175",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 Validation Scores: ROUGE: {'rouge1': 0.47423562022236987, 'rouge2': 0.24150554576226868, 'rougeL': 0.3592670701128011, 'rougeLsum': 0.3592670701128011}, BERTScore: {'precision': 0.3441704909006755, 'recall': 0.3061091916428672, 'f1': 0.32475722912285065}\n",
      "Epoch 9\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7daea98e11be4a68b9f46c28cd15740c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.7251220432941071\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fe88a7c2f224b49bb4015abf3630b6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 1.0555479692088232\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4447f07e9da14ca18cf8afdb3874d48f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 Validation Scores: ROUGE: {'rouge1': 0.48642653034124994, 'rouge2': 0.253495058892069, 'rougeL': 0.36456707906850216, 'rougeLsum': 0.36456707906850216}, BERTScore: {'precision': 0.3261305271751351, 'recall': 0.3301348871447974, 'f1': 0.3279869799088273}\n",
      "Epoch 10\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "291cbdda202949ddbd356174e955e449",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.6773404404074846\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "096b3a8acb904e63889479870b6ba35b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 1.0679133708278339\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e1418e099584778b67bba3b24b74a85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 Validation Scores: ROUGE: {'rouge1': 0.48070503242051976, 'rouge2': 0.24570527135922443, 'rougeL': 0.36547012029744663, 'rougeLsum': 0.36547012029744663}, BERTScore: {'precision': 0.3381066885776818, 'recall': 0.31884844166537124, 'f1': 0.32835140865710044}\n",
      "Epoch 11\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "036ab5aa31a74a3194ae7b840be15cb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.6393480607849396\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6417db7faa8b47078bb0a40fbc9bdd2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 1.0803837155302365\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faecb71579994363977e11bfc19b9351",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 Validation Scores: ROUGE: {'rouge1': 0.4856029972069449, 'rouge2': 0.2501189815061226, 'rougeL': 0.3698638032299978, 'rougeLsum': 0.3698638032299978}, BERTScore: {'precision': 0.34720879048109055, 'recall': 0.3301487360149622, 'f1': 0.33847686198229593}\n",
      "Epoch 12\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f98d7acec334aeb984bfb540134c27b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.6103755983853054\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8523d9158d474e6a8d1d4b5a2fa722b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 1.084916344533364\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83f131d5a8cf45089b63df32c36ef60f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 Validation Scores: ROUGE: {'rouge1': 0.4771634587237814, 'rouge2': 0.24312475336118242, 'rougeL': 0.35869300545281857, 'rougeLsum': 0.35869300545281857}, BERTScore: {'precision': 0.33568314080023104, 'recall': 0.3194233485394054, 'f1': 0.3274052360922926}\n",
      "Epoch 13\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "975fed7a257c4fe3b550aa49b50ab8f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.5781756697925265\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9423f7ae0f6499eafeded513f2bb8f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 1.0944337484737237\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce40dd9f6eea4312bd36280c4a809e00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 Validation Scores: ROUGE: {'rouge1': 0.484426252796968, 'rouge2': 0.24202263333928897, 'rougeL': 0.3630738329227631, 'rougeLsum': 0.3630738329227631}, BERTScore: {'precision': 0.3321341198558609, 'recall': 0.3259062723567088, 'f1': 0.32886217948463226}\n",
      "Epoch 14\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "575a0f8fce2e490f8892ed1dfe877010",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.582886495856111\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4399cb49885441f920bdb28257ed587",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 1.0975416356490717\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a07b8ed2ffb04aa685d3687bf85c9e44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 Validation Scores: ROUGE: {'rouge1': 0.4833623787667183, 'rouge2': 0.24748833552393018, 'rougeL': 0.3635321705060104, 'rougeLsum': 0.3635321705060104}, BERTScore: {'precision': 0.3255732232290838, 'recall': 0.33242724732392365, 'f1': 0.3289080125072764}\n"
     ]
    }
   ],
   "source": [
    "# Suppressing warnings from the transformers library\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
    "model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "epochs = 15\n",
    "\n",
    "# Define the optimizer with weight decay\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-6, weight_decay=0.01)\n",
    "\n",
    "# Total steps = number of epochs * number of batches per epoch\n",
    "total_steps = epochs * len(train_dataloader)\n",
    "\n",
    "# OneCycleLR Scheduler, learning starts from \n",
    "scheduler = OneCycleLR(optimizer, max_lr=1e-5, total_steps=total_steps, \n",
    "                       pct_start=0.3, anneal_strategy='linear')\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch}\\n\")\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    with tqdm(train_dataloader, unit=\"batch\") as tepoch:    \n",
    "        for batch in tepoch:\n",
    "            tepoch.set_description(f\"Epoch {epoch}\")\n",
    "            input_ids, attention_masks, labels = tuple(t.to(device) for t in batch)\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_masks, labels=labels)\n",
    "\n",
    "            loss = outputs.loss\n",
    "            train_loss += loss.item()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()  # Update the learning rate after each batch\n",
    "            tepoch.set_postfix(loss=loss.item())\n",
    "        train_loss /= len(train_dataloader)\n",
    "    print(f\"Training loss: {train_loss}\")\n",
    "\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        with tqdm(test_dataloader, unit=\"batch\") as tepoch:\n",
    "            for batch in tepoch:\n",
    "                tepoch.set_description(f\"Epoch {epoch}\")\n",
    "                input_ids, attention_masks, labels = tuple(t.to(device) for t in batch)\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_masks, labels=labels)\n",
    "                loss = outputs.loss\n",
    "                test_loss += loss.item()\n",
    "                tepoch.set_postfix(loss=loss.item())\n",
    "\n",
    "            test_loss /= len(test_dataloader)\n",
    "    print(f\"Testing loss: {test_loss}\")\n",
    "\n",
    "    # Validation and Metric Calculation\n",
    "    rouge_scores, bert_scores = validate_and_calculate_metrics(model, val_dataloader, tokenizer, device)\n",
    "    print(f\"Epoch {epoch} Validation Scores: ROUGE: {rouge_scores}, BERTScore: {bert_scores}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9a1a680b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>from</th>\n",
       "      <th>to</th>\n",
       "      <th>date</th>\n",
       "      <th>body</th>\n",
       "      <th>Body_Length</th>\n",
       "      <th>Subject_Length</th>\n",
       "      <th>Cleaned_Body</th>\n",
       "      <th>Cleaned_Subject</th>\n",
       "      <th>BERT_Embeddings</th>\n",
       "      <th>...</th>\n",
       "      <th>summary_BART</th>\n",
       "      <th>index_number</th>\n",
       "      <th>Tokenized_Email</th>\n",
       "      <th>Entities</th>\n",
       "      <th>Cluster_retrieved</th>\n",
       "      <th>Summary_human</th>\n",
       "      <th>Preprocessed_Body</th>\n",
       "      <th>Preprocessed_Summary</th>\n",
       "      <th>augmented_input</th>\n",
       "      <th>Preprocessed_aug_Body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Thank you for attending Microsoft Ignite</td>\n",
       "      <td>Microsoft Team &lt;microsoft@email.microsoft.com&gt;</td>\n",
       "      <td>richard.potter@raddsolutions.co.uk</td>\n",
       "      <td>Wed, 19 Oct 2022 20:31:34 +0100</td>\n",
       "      <td>...</td>\n",
       "      <td>6232</td>\n",
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td>Thank you for attending Microsoft Ignite</td>\n",
       "      <td>[-0.059376951307058334, 0.17135855555534363, 0...</td>\n",
       "      <td>...</td>\n",
       "      <td>Learn how microsoft empowers organisations to ...</td>\n",
       "      <td>1543</td>\n",
       "      <td>{'input_ids': tensor([[  101,  7513, 16270,  4...</td>\n",
       "      <td>[('microsoft', 'ORG'), ('2022', 'DATE'), ('mic...</td>\n",
       "      <td>0</td>\n",
       "      <td>The email discusses post-Microsoft Ignite 2022...</td>\n",
       "      <td>Microsoft Ignite may be over, but heres  cont...</td>\n",
       "      <td>The email discusses post-Microsoft Ignite 2022...</td>\n",
       "      <td>Microsoft Ignite may be over, but heres  cont...</td>\n",
       "      <td>Microsoft Ignite may be over, but heres conti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Invitation to learn how Windows 365 Cloud PCs ...</td>\n",
       "      <td>Microsoft &lt;replyto@email.microsoft.com&gt;</td>\n",
       "      <td>richard.potter@raddsolutions.co.uk</td>\n",
       "      <td>Tue, 01 Nov 2022 11:01:50 +0000</td>\n",
       "      <td>Webinar with demos of Windows 365 and vision f...</td>\n",
       "      <td>2943</td>\n",
       "      <td>90</td>\n",
       "      <td>webinar with demos of windows 365 and vision f...</td>\n",
       "      <td>Invitation to learn how Windows 365 Cloud PCs ...</td>\n",
       "      <td>[-0.1439182013273239, 0.22149936854839325, 0.6...</td>\n",
       "      <td>...</td>\n",
       "      <td>webinar with demos of windows 365 and vision f...</td>\n",
       "      <td>765</td>\n",
       "      <td>{'input_ids': tensor([[  101,  4773,  3981,  2...</td>\n",
       "      <td>[('thursday 17th', 'DATE'), ('2022 1400  1500'...</td>\n",
       "      <td>0</td>\n",
       "      <td>Webinar Announcement: \"Windows 365 for Your Hy...</td>\n",
       "      <td>Webinar with demos of Windows 365 and vision f...</td>\n",
       "      <td>Webinar Announcement: \"Windows 365 for \" event...</td>\n",
       "      <td>Webinar with demos of Windows 365 and vision f...</td>\n",
       "      <td>Webinar with demos of Windows 365 and vision f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dont fall behind  embrace AI with Dell Techn...</td>\n",
       "      <td>Dell Technologies Partner Program &lt;DellTechnol...</td>\n",
       "      <td>richard.potter@raddsolutions.co.uk</td>\n",
       "      <td>Tue, 15 Nov 2022 06:01:17 +0000</td>\n",
       "      <td>&lt;https://click.comm.delltechnologies.com/open...</td>\n",
       "      <td>4498</td>\n",
       "      <td>64</td>\n",
       "      <td>\\n   \\t\\r\\nview online \\n\\n  \\t\\r\\n \\t\\r\\nwhy...</td>\n",
       "      <td>Dont fall behind  embrace AI with Dell Technol...</td>\n",
       "      <td>[-0.15142026543617249, 0.11411778628826141, 0....</td>\n",
       "      <td>...</td>\n",
       "      <td>Artificial intelligence ai market is forecast ...</td>\n",
       "      <td>369</td>\n",
       "      <td>{'input_ids': tensor([[  101,  3193,  3784,  2...</td>\n",
       "      <td>[('500 billion', 'MONEY'), ('20231', 'DATE'), ...</td>\n",
       "      <td>0</td>\n",
       "      <td>The email discusses the rapid growth of artifi...</td>\n",
       "      <td>View Online Why AI and why now ? Why AI and wh...</td>\n",
       "      <td>The email discusses the rapid growth of artifi...</td>\n",
       "      <td>View Online Why AI and why now ? Why AI and wh...</td>\n",
       "      <td>View Online Why AI and why now ? Why AI and wh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Microsoft Envision Season 3 begins December 13</td>\n",
       "      <td>Microsoft Team &lt;microsoft@email.microsoft.com&gt;</td>\n",
       "      <td>richard.potter@raddsolutions.co.uk</td>\n",
       "      <td>Wed, 09 Nov 2022 17:05:09 +0000</td>\n",
       "      <td>Episode 1 airs December 13, 2022 \\r\\nHaving tr...</td>\n",
       "      <td>3476</td>\n",
       "      <td>46</td>\n",
       "      <td>episode 1 airs december 13 2022 \\r\\nhaving tro...</td>\n",
       "      <td>Microsoft Envision Season 3 begins December 13</td>\n",
       "      <td>[-0.36103835701942444, 0.06514844298362732, 0....</td>\n",
       "      <td>...</td>\n",
       "      <td>register now for microsoft envision season 3. ...</td>\n",
       "      <td>895</td>\n",
       "      <td>{'input_ids': tensor([[  101,  2792,  1015, 14...</td>\n",
       "      <td>[('1', 'CARDINAL'), ('13 2022', 'DATE'), ('mic...</td>\n",
       "      <td>0</td>\n",
       "      <td>The email announces the premiere of Microsoft ...</td>\n",
       "      <td>Episode 1 airs December 13, 2022  email? | Vie...</td>\n",
       "      <td>The email announces the premiere of Microsoft ...</td>\n",
       "      <td>Episode 1 airs December 13, 2022  email? | Vie...</td>\n",
       "      <td>Episode 1 airs December 13, 2022 email? | View...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In September, you had 71 users visit your webs...</td>\n",
       "      <td>Google Analytics &lt;analytics-noreply@google.com&gt;</td>\n",
       "      <td>richard.potter@raddsolutions.co.uk</td>\n",
       "      <td>Tue, 11 Oct 2022 06:02:01 +0100</td>\n",
       "      <td>&lt;https://www.google.com/images/branding/googl...</td>\n",
       "      <td>5559</td>\n",
       "      <td>68</td>\n",
       "      <td>\\n \\r\\nuniversal analytics will no longer pr...</td>\n",
       "      <td>In September you had 71 users visit your websi...</td>\n",
       "      <td>[-0.10645909607410431, 0.17022578418254852, 0....</td>\n",
       "      <td>...</td>\n",
       "      <td>universal analytics will no longer process new...</td>\n",
       "      <td>740</td>\n",
       "      <td>{'input_ids': tensor([[  101,  5415, 25095,  2...</td>\n",
       "      <td>[('2023', 'DATE'), ('4', 'CARDINAL'), ('septem...</td>\n",
       "      <td>0</td>\n",
       "      <td>Starting in 2023, Universal Analytics will no ...</td>\n",
       "      <td>Universal Analytics will no longer process new...</td>\n",
       "      <td>Starting in 2023, Universal Analytics will no ...</td>\n",
       "      <td>Universal Analytics will no longer process new...</td>\n",
       "      <td>Universal Analytics will no longer process new...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>949</th>\n",
       "      <td>Our Black Friday offers have landed!</td>\n",
       "      <td>IT Governance &lt;emailsupport@itgovernance.co.uk&gt;</td>\n",
       "      <td>richie.wynne@raddsolutions.co.uk</td>\n",
       "      <td>Mon, 21 Nov 2022 11:05:15 +0000</td>\n",
       "      <td>Youre not going to want to miss these savings...</td>\n",
       "      <td>3228</td>\n",
       "      <td>36</td>\n",
       "      <td>youre not going to want to miss these savings ...</td>\n",
       "      <td>Our Black Friday offers have landed</td>\n",
       "      <td>[0.09008561074733734, 0.16759826242923737, 0.6...</td>\n",
       "      <td>...</td>\n",
       "      <td>were starting our black friday offers early wi...</td>\n",
       "      <td>1130</td>\n",
       "      <td>{'input_ids': tensor([[  101,  2115,  2063,  2...</td>\n",
       "      <td>[('friday', 'DATE'), ('25', 'CARDINAL'), ('25'...</td>\n",
       "      <td>7</td>\n",
       "      <td>The email advertises an early Black Friday off...</td>\n",
       "      <td>Youre not going to want to miss these savings...</td>\n",
       "      <td>The email advertises an early Black Friday off...</td>\n",
       "      <td>Youre not going to want to miss these savings...</td>\n",
       "      <td>Youre not going to want to miss these savings...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>950</th>\n",
       "      <td>Jeff Fritz reviews IronPDF vs SyncFusion, iTex...</td>\n",
       "      <td>\"Jacob, Head of Engineering\" &lt;developers@irons...</td>\n",
       "      <td>Richard Potter &lt;richard.potter@raddsolutions.c...</td>\n",
       "      <td>Tue, 13 Dec 2022 15:31:28 +0000</td>\n",
       "      <td>&lt;https://ironsoftware.lt.acemlnb.com/Prod/lin...</td>\n",
       "      <td>8587</td>\n",
       "      <td>55</td>\n",
       "      <td>\\t \\r\\n \\t \\r\\nhi richard\\r\\nimagine spendin...</td>\n",
       "      <td>Jeff Fritz reviews IronPDF vs SyncFusion iText...</td>\n",
       "      <td>[-0.20470425486564636, 0.20281416177749634, 0....</td>\n",
       "      <td>...</td>\n",
       "      <td>iron software is a free open source solution t...</td>\n",
       "      <td>783</td>\n",
       "      <td>{'input_ids': tensor([[  101,  7632,  2957,  5...</td>\n",
       "      <td>[('jeff fritz', 'PERSON'), ('net conf', 'ORG')...</td>\n",
       "      <td>7</td>\n",
       "      <td>Jeff Fritz from .NET Conf reviewed IronPDF aga...</td>\n",
       "      <td>, Imagine spending a lot of money on software ...</td>\n",
       "      <td>Jeff Fritz from .NET Conf reviewed IronPDF aga...</td>\n",
       "      <td>, Imagine spending a lot of money on software ...</td>\n",
       "      <td>, Imagine spending a lot of money on software ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>951</th>\n",
       "      <td>Don't Miss Out: Help to Grow: Digital Ends in ...</td>\n",
       "      <td>Zym &lt;rebecca@zymplify.com&gt;</td>\n",
       "      <td>Richard Potter &lt;richard.potter@raddsolutions.c...</td>\n",
       "      <td>Tue, 17 Jan 2023 12:01:49 +0000</td>\n",
       "      <td>ZYM helps business owners understand their mar...</td>\n",
       "      <td>6966</td>\n",
       "      <td>56</td>\n",
       "      <td>zym helps business owners understand their mar...</td>\n",
       "      <td>Dont Miss Out Help to Grow Digital Ends in 16 ...</td>\n",
       "      <td>[0.0035861318465322256, 0.07786554843187332, 0...</td>\n",
       "      <td>...</td>\n",
       "      <td>zym helps business owners understand their mar...</td>\n",
       "      <td>345</td>\n",
       "      <td>{'input_ids': tensor([[  101,  1062, 24335,  7...</td>\n",
       "      <td>[('uk', 'GPE'), ('up to 5000', 'CARDINAL'), ('...</td>\n",
       "      <td>7</td>\n",
       "      <td>Zym aids business owners in comprehending mark...</td>\n",
       "      <td>ZYM helps business owners understand their mar...</td>\n",
       "      <td>Zym aids business owners in comprehending mark...</td>\n",
       "      <td>ZYM helps business owners understand their mar...</td>\n",
       "      <td>ZYM helps business owners understand their mar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>952</th>\n",
       "      <td>New videos coming in 2023</td>\n",
       "      <td>Clear Measure &lt;clearmeasure@clear-measure.com&gt;</td>\n",
       "      <td>richard.potter@raddsolutions.co.uk</td>\n",
       "      <td>Thu, 29 Dec 2022 10:00:02 +0000</td>\n",
       "      <td>New videos coming in 2023  made to empower yo...</td>\n",
       "      <td>3404</td>\n",
       "      <td>25</td>\n",
       "      <td>new videos coming in 2023  made to empower you...</td>\n",
       "      <td>New videos coming in 2023</td>\n",
       "      <td>[-0.08648061007261276, 0.11852650344371796, 0....</td>\n",
       "      <td>...</td>\n",
       "      <td>new videos coming in 2023  made to empower you...</td>\n",
       "      <td>958</td>\n",
       "      <td>{'input_ids': tensor([[  101,  2047,  6876,  2...</td>\n",
       "      <td>[('2023', 'DATE'), ('2023', 'DATE'), ('10815',...</td>\n",
       "      <td>7</td>\n",
       "      <td>Summary:\\n\\nClear Measure has announced an upc...</td>\n",
       "      <td>New videos coming in 2023  made to empower . ...</td>\n",
       "      <td>Summary: Clear Measure has announced an upcomi...</td>\n",
       "      <td>New videos coming in 2023  made to empower . ...</td>\n",
       "      <td>New videos coming in 2023  made to empower . ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>953</th>\n",
       "      <td>Business PCs up to 40% off</td>\n",
       "      <td>Lenovo New beginnings! &lt;lenovo@ecomm.lenovo.com&gt;</td>\n",
       "      <td>richard.potter@raddsolutions.co.uk</td>\n",
       "      <td>Wed, 01 Feb 2023 09:04:42 +0000</td>\n",
       "      <td>&lt;https://trk.ecomm.lenovo.com/ss/o/k8leYzsS8M...</td>\n",
       "      <td>8108</td>\n",
       "      <td>26</td>\n",
       "      <td>\\n \\t\\r\\n\\tview it in browser instead \\n  fre...</td>\n",
       "      <td>Business PCs up to 40 off</td>\n",
       "      <td>[-0.09954569488763809, 0.13386352360248566, 0....</td>\n",
       "      <td>...</td>\n",
       "      <td>Free shipping on all orders with up to 40% dis...</td>\n",
       "      <td>132</td>\n",
       "      <td>{'input_ids': tensor([[  101,  3193,  2009,  1...</td>\n",
       "      <td>[('up to 40', 'CARDINAL'), ('up to 40', 'CARDI...</td>\n",
       "      <td>7</td>\n",
       "      <td>Email from Lenovo announces a workspace refres...</td>\n",
       "      <td>View it in browser instead Free shipping on al...</td>\n",
       "      <td>Email from Lenovo announces a workspace refres...</td>\n",
       "      <td>View it in browser instead Free shipping on al...</td>\n",
       "      <td>View it in browser instead Free shipping on al...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>954 rows  25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               subject  \\\n",
       "0             Thank you for attending Microsoft Ignite   \n",
       "1    Invitation to learn how Windows 365 Cloud PCs ...   \n",
       "2    Dont fall behind  embrace AI with Dell Techn...   \n",
       "3       Microsoft Envision Season 3 begins December 13   \n",
       "4    In September, you had 71 users visit your webs...   \n",
       "..                                                 ...   \n",
       "949               Our Black Friday offers have landed!   \n",
       "950  Jeff Fritz reviews IronPDF vs SyncFusion, iTex...   \n",
       "951  Don't Miss Out: Help to Grow: Digital Ends in ...   \n",
       "952                          New videos coming in 2023   \n",
       "953                         Business PCs up to 40% off   \n",
       "\n",
       "                                                  from  \\\n",
       "0       Microsoft Team <microsoft@email.microsoft.com>   \n",
       "1              Microsoft <replyto@email.microsoft.com>   \n",
       "2    Dell Technologies Partner Program <DellTechnol...   \n",
       "3       Microsoft Team <microsoft@email.microsoft.com>   \n",
       "4      Google Analytics <analytics-noreply@google.com>   \n",
       "..                                                 ...   \n",
       "949    IT Governance <emailsupport@itgovernance.co.uk>   \n",
       "950  \"Jacob, Head of Engineering\" <developers@irons...   \n",
       "951                         Zym <rebecca@zymplify.com>   \n",
       "952     Clear Measure <clearmeasure@clear-measure.com>   \n",
       "953   Lenovo New beginnings! <lenovo@ecomm.lenovo.com>   \n",
       "\n",
       "                                                    to  \\\n",
       "0                   richard.potter@raddsolutions.co.uk   \n",
       "1                   richard.potter@raddsolutions.co.uk   \n",
       "2                   richard.potter@raddsolutions.co.uk   \n",
       "3                   richard.potter@raddsolutions.co.uk   \n",
       "4                   richard.potter@raddsolutions.co.uk   \n",
       "..                                                 ...   \n",
       "949                   richie.wynne@raddsolutions.co.uk   \n",
       "950  Richard Potter <richard.potter@raddsolutions.c...   \n",
       "951  Richard Potter <richard.potter@raddsolutions.c...   \n",
       "952                 richard.potter@raddsolutions.co.uk   \n",
       "953                 richard.potter@raddsolutions.co.uk   \n",
       "\n",
       "                                date  \\\n",
       "0    Wed, 19 Oct 2022 20:31:34 +0100   \n",
       "1    Tue, 01 Nov 2022 11:01:50 +0000   \n",
       "2    Tue, 15 Nov 2022 06:01:17 +0000   \n",
       "3    Wed, 09 Nov 2022 17:05:09 +0000   \n",
       "4    Tue, 11 Oct 2022 06:02:01 +0100   \n",
       "..                               ...   \n",
       "949  Mon, 21 Nov 2022 11:05:15 +0000   \n",
       "950  Tue, 13 Dec 2022 15:31:28 +0000   \n",
       "951  Tue, 17 Jan 2023 12:01:49 +0000   \n",
       "952  Thu, 29 Dec 2022 10:00:02 +0000   \n",
       "953  Wed, 01 Feb 2023 09:04:42 +0000   \n",
       "\n",
       "                                                  body  Body_Length  \\\n",
       "0                                                  ...         6232   \n",
       "1    Webinar with demos of Windows 365 and vision f...         2943   \n",
       "2     <https://click.comm.delltechnologies.com/open...         4498   \n",
       "3    Episode 1 airs December 13, 2022 \\r\\nHaving tr...         3476   \n",
       "4     <https://www.google.com/images/branding/googl...         5559   \n",
       "..                                                 ...          ...   \n",
       "949  Youre not going to want to miss these savings...         3228   \n",
       "950   <https://ironsoftware.lt.acemlnb.com/Prod/lin...         8587   \n",
       "951  ZYM helps business owners understand their mar...         6966   \n",
       "952  New videos coming in 2023  made to empower yo...         3404   \n",
       "953   <https://trk.ecomm.lenovo.com/ss/o/k8leYzsS8M...         8108   \n",
       "\n",
       "     Subject_Length                                       Cleaned_Body  \\\n",
       "0                40                                                ...   \n",
       "1                90  webinar with demos of windows 365 and vision f...   \n",
       "2                64   \\n   \\t\\r\\nview online \\n\\n  \\t\\r\\n \\t\\r\\nwhy...   \n",
       "3                46  episode 1 airs december 13 2022 \\r\\nhaving tro...   \n",
       "4                68    \\n \\r\\nuniversal analytics will no longer pr...   \n",
       "..              ...                                                ...   \n",
       "949              36  youre not going to want to miss these savings ...   \n",
       "950              55    \\t \\r\\n \\t \\r\\nhi richard\\r\\nimagine spendin...   \n",
       "951              56  zym helps business owners understand their mar...   \n",
       "952              25  new videos coming in 2023  made to empower you...   \n",
       "953              26   \\n \\t\\r\\n\\tview it in browser instead \\n  fre...   \n",
       "\n",
       "                                       Cleaned_Subject  \\\n",
       "0             Thank you for attending Microsoft Ignite   \n",
       "1    Invitation to learn how Windows 365 Cloud PCs ...   \n",
       "2    Dont fall behind  embrace AI with Dell Technol...   \n",
       "3       Microsoft Envision Season 3 begins December 13   \n",
       "4    In September you had 71 users visit your websi...   \n",
       "..                                                 ...   \n",
       "949                Our Black Friday offers have landed   \n",
       "950  Jeff Fritz reviews IronPDF vs SyncFusion iText...   \n",
       "951  Dont Miss Out Help to Grow Digital Ends in 16 ...   \n",
       "952                          New videos coming in 2023   \n",
       "953                          Business PCs up to 40 off   \n",
       "\n",
       "                                       BERT_Embeddings  ...  \\\n",
       "0    [-0.059376951307058334, 0.17135855555534363, 0...  ...   \n",
       "1    [-0.1439182013273239, 0.22149936854839325, 0.6...  ...   \n",
       "2    [-0.15142026543617249, 0.11411778628826141, 0....  ...   \n",
       "3    [-0.36103835701942444, 0.06514844298362732, 0....  ...   \n",
       "4    [-0.10645909607410431, 0.17022578418254852, 0....  ...   \n",
       "..                                                 ...  ...   \n",
       "949  [0.09008561074733734, 0.16759826242923737, 0.6...  ...   \n",
       "950  [-0.20470425486564636, 0.20281416177749634, 0....  ...   \n",
       "951  [0.0035861318465322256, 0.07786554843187332, 0...  ...   \n",
       "952  [-0.08648061007261276, 0.11852650344371796, 0....  ...   \n",
       "953  [-0.09954569488763809, 0.13386352360248566, 0....  ...   \n",
       "\n",
       "                                          summary_BART index_number  \\\n",
       "0    Learn how microsoft empowers organisations to ...         1543   \n",
       "1    webinar with demos of windows 365 and vision f...          765   \n",
       "2    Artificial intelligence ai market is forecast ...          369   \n",
       "3    register now for microsoft envision season 3. ...          895   \n",
       "4    universal analytics will no longer process new...          740   \n",
       "..                                                 ...          ...   \n",
       "949  were starting our black friday offers early wi...         1130   \n",
       "950  iron software is a free open source solution t...          783   \n",
       "951  zym helps business owners understand their mar...          345   \n",
       "952  new videos coming in 2023  made to empower you...          958   \n",
       "953  Free shipping on all orders with up to 40% dis...          132   \n",
       "\n",
       "                                       Tokenized_Email  \\\n",
       "0    {'input_ids': tensor([[  101,  7513, 16270,  4...   \n",
       "1    {'input_ids': tensor([[  101,  4773,  3981,  2...   \n",
       "2    {'input_ids': tensor([[  101,  3193,  3784,  2...   \n",
       "3    {'input_ids': tensor([[  101,  2792,  1015, 14...   \n",
       "4    {'input_ids': tensor([[  101,  5415, 25095,  2...   \n",
       "..                                                 ...   \n",
       "949  {'input_ids': tensor([[  101,  2115,  2063,  2...   \n",
       "950  {'input_ids': tensor([[  101,  7632,  2957,  5...   \n",
       "951  {'input_ids': tensor([[  101,  1062, 24335,  7...   \n",
       "952  {'input_ids': tensor([[  101,  2047,  6876,  2...   \n",
       "953  {'input_ids': tensor([[  101,  3193,  2009,  1...   \n",
       "\n",
       "                                              Entities Cluster_retrieved  \\\n",
       "0    [('microsoft', 'ORG'), ('2022', 'DATE'), ('mic...                 0   \n",
       "1    [('thursday 17th', 'DATE'), ('2022 1400  1500'...                 0   \n",
       "2    [('500 billion', 'MONEY'), ('20231', 'DATE'), ...                 0   \n",
       "3    [('1', 'CARDINAL'), ('13 2022', 'DATE'), ('mic...                 0   \n",
       "4    [('2023', 'DATE'), ('4', 'CARDINAL'), ('septem...                 0   \n",
       "..                                                 ...               ...   \n",
       "949  [('friday', 'DATE'), ('25', 'CARDINAL'), ('25'...                 7   \n",
       "950  [('jeff fritz', 'PERSON'), ('net conf', 'ORG')...                 7   \n",
       "951  [('uk', 'GPE'), ('up to 5000', 'CARDINAL'), ('...                 7   \n",
       "952  [('2023', 'DATE'), ('2023', 'DATE'), ('10815',...                 7   \n",
       "953  [('up to 40', 'CARDINAL'), ('up to 40', 'CARDI...                 7   \n",
       "\n",
       "                                         Summary_human  \\\n",
       "0    The email discusses post-Microsoft Ignite 2022...   \n",
       "1    Webinar Announcement: \"Windows 365 for Your Hy...   \n",
       "2    The email discusses the rapid growth of artifi...   \n",
       "3    The email announces the premiere of Microsoft ...   \n",
       "4    Starting in 2023, Universal Analytics will no ...   \n",
       "..                                                 ...   \n",
       "949  The email advertises an early Black Friday off...   \n",
       "950  Jeff Fritz from .NET Conf reviewed IronPDF aga...   \n",
       "951  Zym aids business owners in comprehending mark...   \n",
       "952  Summary:\\n\\nClear Measure has announced an upc...   \n",
       "953  Email from Lenovo announces a workspace refres...   \n",
       "\n",
       "                                     Preprocessed_Body  \\\n",
       "0    Microsoft Ignite may be over, but heres  cont...   \n",
       "1    Webinar with demos of Windows 365 and vision f...   \n",
       "2    View Online Why AI and why now ? Why AI and wh...   \n",
       "3    Episode 1 airs December 13, 2022  email? | Vie...   \n",
       "4    Universal Analytics will no longer process new...   \n",
       "..                                                 ...   \n",
       "949  Youre not going to want to miss these savings...   \n",
       "950  , Imagine spending a lot of money on software ...   \n",
       "951  ZYM helps business owners understand their mar...   \n",
       "952  New videos coming in 2023  made to empower . ...   \n",
       "953  View it in browser instead Free shipping on al...   \n",
       "\n",
       "                                  Preprocessed_Summary  \\\n",
       "0    The email discusses post-Microsoft Ignite 2022...   \n",
       "1    Webinar Announcement: \"Windows 365 for \" event...   \n",
       "2    The email discusses the rapid growth of artifi...   \n",
       "3    The email announces the premiere of Microsoft ...   \n",
       "4    Starting in 2023, Universal Analytics will no ...   \n",
       "..                                                 ...   \n",
       "949  The email advertises an early Black Friday off...   \n",
       "950  Jeff Fritz from .NET Conf reviewed IronPDF aga...   \n",
       "951  Zym aids business owners in comprehending mark...   \n",
       "952  Summary: Clear Measure has announced an upcomi...   \n",
       "953  Email from Lenovo announces a workspace refres...   \n",
       "\n",
       "                                       augmented_input  \\\n",
       "0    Microsoft Ignite may be over, but heres  cont...   \n",
       "1    Webinar with demos of Windows 365 and vision f...   \n",
       "2    View Online Why AI and why now ? Why AI and wh...   \n",
       "3    Episode 1 airs December 13, 2022  email? | Vie...   \n",
       "4    Universal Analytics will no longer process new...   \n",
       "..                                                 ...   \n",
       "949  Youre not going to want to miss these savings...   \n",
       "950  , Imagine spending a lot of money on software ...   \n",
       "951  ZYM helps business owners understand their mar...   \n",
       "952  New videos coming in 2023  made to empower . ...   \n",
       "953  View it in browser instead Free shipping on al...   \n",
       "\n",
       "                                 Preprocessed_aug_Body  \n",
       "0    Microsoft Ignite may be over, but heres conti...  \n",
       "1    Webinar with demos of Windows 365 and vision f...  \n",
       "2    View Online Why AI and why now ? Why AI and wh...  \n",
       "3    Episode 1 airs December 13, 2022 email? | View...  \n",
       "4    Universal Analytics will no longer process new...  \n",
       "..                                                 ...  \n",
       "949  Youre not going to want to miss these savings...  \n",
       "950  , Imagine spending a lot of money on software ...  \n",
       "951  ZYM helps business owners understand their mar...  \n",
       "952  New videos coming in 2023  made to empower . ...  \n",
       "953  View it in browser instead Free shipping on al...  \n",
       "\n",
       "[954 rows x 25 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929039be",
   "metadata": {},
   "source": [
    "## Fine tuning with the augmented input as training input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2fdfb8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the preprocessing function to the augmented input column\n",
    "df['Preprocessed_aug_Body'] = df['augmented_input'].apply(lambda x: preprocess_email_for_t5(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c21d7d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the preprocessing function to the augmented input column\n",
    "df['Preprocessed_Body'] = df['body'].apply(lambda x: preprocess_email_for_t5(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f45b7d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\764883\\AppData\\Local\\Temp\\ipykernel_12060\\3015448796.py:40: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_dataset = TensorDataset(torch.tensor(train_encodings['input_ids']), torch.tensor(train_encodings['attention_mask']), torch.tensor(train_encodings['labels']))\n",
      "C:\\Users\\764883\\AppData\\Local\\Temp\\ipykernel_12060\\3015448796.py:41: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  val_dataset = TensorDataset(torch.tensor(val_encodings['input_ids']), torch.tensor(val_encodings['attention_mask']), torch.tensor(val_encodings['labels']))\n",
      "C:\\Users\\764883\\AppData\\Local\\Temp\\ipykernel_12060\\3015448796.py:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_dataset = TensorDataset(torch.tensor(test_encodings['input_ids']), torch.tensor(test_encodings['attention_mask']), torch.tensor(test_encodings['labels']))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BartForConditionalGeneration(\n",
       "  (model): BartModel(\n",
       "    (shared): Embedding(50265, 1024, padding_idx=1)\n",
       "    (encoder): BartEncoder(\n",
       "      (embed_tokens): Embedding(50265, 1024, padding_idx=1)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): BartDecoder(\n",
       "      (embed_tokens): Embedding(50265, 1024, padding_idx=1)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=50265, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = df\n",
    "data = data[['Preprocessed_aug_Body', 'Preprocessed_Summary','Preprocessed_Body']].dropna()\n",
    "data = data.sample(frac=1, random_state=0)  # Shuffle the data\n",
    "dataset = Dataset.from_pandas(data)\n",
    "dataset = dataset.train_test_split(test_size=0.3)  # Splitting the data\n",
    "dataset[\"test\"] = dataset[\"test\"].train_test_split(test_size=0.5)\n",
    "\n",
    "# Prepare the data\n",
    "train_source = dataset[\"train\"][\"Preprocessed_aug_Body\"]\n",
    "train_target = dataset[\"train\"][\"Preprocessed_Summary\"]\n",
    "val_source = dataset[\"test\"][\"train\"][\"Preprocessed_Body\"]\n",
    "val_target = dataset[\"test\"][\"train\"][\"Preprocessed_Summary\"]\n",
    "test_source = dataset[\"test\"][\"test\"][\"Preprocessed_Body\"]\n",
    "test_target = dataset[\"test\"][\"test\"][\"Preprocessed_Summary\"]\n",
    "\n",
    "# Initialize tokenizer for BART\n",
    "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large\")\n",
    "\n",
    "# Tokenize the data\n",
    "def tokenize_data(source, target, max_length=1024, max_target_length=150):\n",
    "    encodings = tokenizer(source, truncation=True, max_length=max_length, padding=True)\n",
    "    decodings = tokenizer(target, truncation=True, max_length=max_target_length, padding=True)\n",
    "    return {\n",
    "        'input_ids': torch.tensor(encodings['input_ids']),\n",
    "        'attention_mask': torch.tensor(encodings['attention_mask']),\n",
    "        'labels': torch.tensor(decodings['input_ids'])\n",
    "    }\n",
    "\n",
    "train_encodings = tokenize_data(train_source, train_target)\n",
    "val_encodings = tokenize_data(val_source, val_target)\n",
    "test_encodings = tokenize_data(test_source, test_target)\n",
    "\n",
    "# Create TensorDatasets\n",
    "train_dataset = TensorDataset(torch.tensor(train_encodings['input_ids']), torch.tensor(train_encodings['attention_mask']), torch.tensor(train_encodings['labels']))\n",
    "val_dataset = TensorDataset(torch.tensor(val_encodings['input_ids']), torch.tensor(val_encodings['attention_mask']), torch.tensor(val_encodings['labels']))\n",
    "test_dataset = TensorDataset(torch.tensor(test_encodings['input_ids']), torch.tensor(test_encodings['attention_mask']), torch.tensor(test_encodings['labels']))\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True, num_workers=4)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=2,num_workers=4)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=2, num_workers=4)\n",
    "\n",
    "# Load BART model\n",
    "model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8a72a5be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b123016f88ff48deba80a3305de6d129",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 7.342431350382502\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fca7a883367e4bcd9a536219fbad6ff8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 5.759708437654707\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "480ae6a30dec4cccb34c62219647ab6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Validation Scores: ROUGE: {'rouge1': 0.38193880235908, 'rouge2': 0.17600840418131086, 'rougeL': 0.25904599680604673, 'rougeLsum': 0.25904599680604673}, BERTScore: {'precision': 0.019679095259764127, 'recall': 0.237516051882671, 'f1': 0.12616800677238238}\n",
      "Epoch 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5b8b039e25c48a5bfee92ab12340e0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 4.428986228868633\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe23bcccff264360af96735ecf7cb04b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 2.735091576973597\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "283830d9dbdd4a05928ab67f1b2974df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Validation Scores: ROUGE: {'rouge1': 0.415998539487397, 'rouge2': 0.20242755740833573, 'rougeL': 0.2981697368480092, 'rougeLsum': 0.2981697368480092}, BERTScore: {'precision': 0.20279130443102783, 'recall': 0.2454231157898903, 'f1': 0.22369850028513205}\n",
      "Epoch 2\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ade403731144485a0b4037ee36b31fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 2.330566484235718\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1f787dd06ce4096aaaf6a1f10d787bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 1.3216935843229294\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37e46bbfaa254a0f9081ccd506b33252",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Validation Scores: ROUGE: {'rouge1': 0.43966975050599755, 'rouge2': 0.22040873781726789, 'rougeL': 0.3223122290384883, 'rougeLsum': 0.3223122290384883}, BERTScore: {'precision': 0.26015478514859247, 'recall': 0.2728303558607068, 'f1': 0.2663806767409874}\n",
      "Epoch 3\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "815128b41be742cd871814f11af83fc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 1.3594643337997847\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6f5968329ba457db9323efe5bc1d8df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 1.1247988417744637\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c80ff4ed08348999245df0dac066585",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Validation Scores: ROUGE: {'rouge1': 0.4524241902052244, 'rouge2': 0.22628576367531142, 'rougeL': 0.33071281053859874, 'rougeLsum': 0.33071281053859874}, BERTScore: {'precision': 0.29209889626751345, 'recall': 0.28660403781880933, 'f1': 0.28921228523055714}\n",
      "Epoch 4\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e66277a30fd4208865822b33426a744",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 1.1585391472556634\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b5a4caad3a043d9ad7ead5a66311d8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 1.079310288445817\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb1ab266fc5e463ca3baddea746415b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Validation Scores: ROUGE: {'rouge1': 0.46394350732184353, 'rouge2': 0.23085677991136588, 'rougeL': 0.34049424687852414, 'rougeLsum': 0.34049424687852414}, BERTScore: {'precision': 0.32375632795608705, 'recall': 0.2904427405446768, 'f1': 0.3070227493945923}\n",
      "Epoch 5\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ca52c6c037e4c87ad6bebb5b4837304",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 1.032576815096918\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40ccc4490deb4370911e6ae5db3f2f93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 1.0622077095839713\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7a61d450851489b95aeae81422c7663",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Validation Scores: ROUGE: {'rouge1': 0.476817622093038, 'rouge2': 0.23895035183294366, 'rougeL': 0.34649248526579135, 'rougeLsum': 0.34649248526579135}, BERTScore: {'precision': 0.2817604139757653, 'recall': 0.3413585279033416, 'f1': 0.3113875075553854}\n",
      "Epoch 6\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "360502cd9a6a4ce295074b311707557b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.954693766054279\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16eaf11fdb6544088ddaae388f234f56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 1.104599637289842\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a98705800526449293eb50953185cfef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Validation Scores: ROUGE: {'rouge1': 0.48110197340454863, 'rouge2': 0.24165749008716775, 'rougeL': 0.34703293242005784, 'rougeLsum': 0.34703293242005784}, BERTScore: {'precision': 0.2825793316587806, 'recall': 0.34091135300695896, 'f1': 0.311657205534478}\n",
      "Epoch 7\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e45e41f7d1b74a2c9a5b12e53a16bc93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.8583606648855581\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b6f1f8707b449e483a51ca7364f3737",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 1.0613995177878275\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dce8b85767f421e8a2173e39663b0a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 Validation Scores: ROUGE: {'rouge1': 0.48187172057229133, 'rouge2': 0.2422200949028066, 'rougeL': 0.34522023640082605, 'rougeLsum': 0.34522023640082605}, BERTScore: {'precision': 0.2843668528108133, 'recall': 0.34729342359221643, 'f1': 0.31579984000159633}\n",
      "Epoch 8\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "938a072415b249c6bf0c615d26f3ec74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.7929101714830913\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cacb440eb8234e088443bd1faa768f9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 1.0583537539674177\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcdfc49c2cb74be199c440c39039e566",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 Validation Scores: ROUGE: {'rouge1': 0.4831971516222073, 'rouge2': 0.24494869331242386, 'rougeL': 0.35821571896540294, 'rougeLsum': 0.35821571896540294}, BERTScore: {'precision': 0.32291956897825, 'recall': 0.33043187939458424, 'f1': 0.3267650544229481}\n",
      "Epoch 9\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a71a8b09aef14e68a2db2e0dc184fa6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.7356408319608894\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24859abffa5b44fa88bb5d2ae11395df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 1.078984705524312\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "507cdc3b1ba04cb9b6191ab5ab8980eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 Validation Scores: ROUGE: {'rouge1': 0.48753727896568727, 'rouge2': 0.24667457469237902, 'rougeL': 0.3534320398560361, 'rougeLsum': 0.3534320398560361}, BERTScore: {'precision': 0.3228120876269208, 'recall': 0.33858744199905133, 'f1': 0.3305985036616524}\n",
      "Epoch 10\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13c4081f918248d18e88994fc80b74cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.6866015293105634\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70012134e92045a1805db59c3c5cf002",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 1.1041716043319967\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4d27abba5604595acb490c6b49064a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 Validation Scores: ROUGE: {'rouge1': 0.4807560411161349, 'rouge2': 0.2387103810764658, 'rougeL': 0.34866350871612695, 'rougeLsum': 0.34866350871612695}, BERTScore: {'precision': 0.3015142160778244, 'recall': 0.34263651931865347, 'f1': 0.32198408380564714}\n",
      "Epoch 11\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5e0b493ac16460e992b40650b419457",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.6490745849595099\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65c6c7635f9c4a42a5785e686fa07e54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 1.0975315156910155\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "034a185d2d584badac13cdce9bbf4a3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 Validation Scores: ROUGE: {'rouge1': 0.48676611100316247, 'rouge2': 0.2446337295007529, 'rougeL': 0.3547463667150754, 'rougeLsum': 0.3547463667150754}, BERTScore: {'precision': 0.3132800804451108, 'recall': 0.35226875233153504, 'f1': 0.3325641733697719}\n",
      "Epoch 12\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a8858270bef46a5905fd4a8b220f3bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.6141056521031671\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "268dd3ab10ce4c1b990eaccee4cd6735",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 1.1089567571050591\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "833af4182b984c209e4315904c940069",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 Validation Scores: ROUGE: {'rouge1': 0.48199983844282684, 'rouge2': 0.2430811358112393, 'rougeL': 0.34869172399849274, 'rougeLsum': 0.34869172399849274}, BERTScore: {'precision': 0.3199138718967636, 'recall': 0.33657793928351665, 'f1': 0.32819515290773577}\n",
      "Epoch 13\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43c09b29f8684bf7827f81dbf5264a6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.5886282941806102\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f239867c9f15417499c4b42a16f0b924",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 1.1127161524362035\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca2b53e23bd34d3bbdbf747a8f8c668a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 Validation Scores: ROUGE: {'rouge1': 0.48148317693559495, 'rouge2': 0.23995505414894033, 'rougeL': 0.3482084141470094, 'rougeLsum': 0.3482084141470094}, BERTScore: {'precision': 0.314207991067734, 'recall': 0.3434535761674245, 'f1': 0.3286821355836259}\n",
      "Epoch 14\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49f8b21abb354d738c3140bef2b01b98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.5718031021470795\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "168540ec53184eff95d9fbfa52304d8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 1.123224305609862\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78b62583189b4a0c83aba702a79a02ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 Validation Scores: ROUGE: {'rouge1': 0.4869630104638327, 'rouge2': 0.24265672590355214, 'rougeL': 0.3534967838429402, 'rougeLsum': 0.3534967838429402}, BERTScore: {'precision': 0.31468355904022854, 'recall': 0.3475769011096822, 'f1': 0.33092198489854735}\n"
     ]
    }
   ],
   "source": [
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "import torch\n",
    "from transformers import BartForConditionalGeneration\n",
    "from tqdm.auto import tqdm\n",
    "import logging\n",
    "\n",
    "# Suppressing warnings from the transformers library\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
    "model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "epochs = 15\n",
    "\n",
    "# Define the optimizer with weight decay\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-6, weight_decay=0.01)\n",
    "\n",
    "# Total steps = number of epochs * number of batches per epoch\n",
    "total_steps = epochs * len(train_dataloader)\n",
    "\n",
    "# OneCycleLR Scheduler, learning starts from \n",
    "scheduler = OneCycleLR(optimizer, max_lr=1e-5, total_steps=total_steps, \n",
    "                       pct_start=0.3, anneal_strategy='linear')\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch}\\n\")\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    with tqdm(train_dataloader, unit=\"batch\") as tepoch:    \n",
    "        for batch in tepoch:\n",
    "            tepoch.set_description(f\"Epoch {epoch}\")\n",
    "            input_ids, attention_masks, labels = tuple(t.to(device) for t in batch)\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_masks, labels=labels)\n",
    "\n",
    "            loss = outputs.loss\n",
    "            train_loss += loss.item()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()  # Update the learning rate after each batch\n",
    "            tepoch.set_postfix(loss=loss.item())\n",
    "        train_loss /= len(train_dataloader)\n",
    "    print(f\"Training loss: {train_loss}\")\n",
    "\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        with tqdm(test_dataloader, unit=\"batch\") as tepoch:\n",
    "            for batch in tepoch:\n",
    "                tepoch.set_description(f\"Epoch {epoch}\")\n",
    "                input_ids, attention_masks, labels = tuple(t.to(device) for t in batch)\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_masks, labels=labels)\n",
    "                loss = outputs.loss\n",
    "                test_loss += loss.item()\n",
    "                tepoch.set_postfix(loss=loss.item())\n",
    "\n",
    "            test_loss /= len(test_dataloader)\n",
    "    print(f\"Testing loss: {test_loss}\")\n",
    "\n",
    "    # Validation and Metric Calculation\n",
    "    rouge_scores, bert_scores = validate_and_calculate_metrics(model, val_dataloader, tokenizer, device)\n",
    "    print(f\"Epoch {epoch} Validation Scores: ROUGE: {rouge_scores}, BERTScore: {bert_scores}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7203358",
   "metadata": {},
   "source": [
    "## Visualising the results of different models loading from excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ec669cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.ticker import FuncFormatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "81fea541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: openpyxl in c:\\users\\764883\\appdata\\roaming\\python\\python311\\site-packages (3.1.2)\n",
      "Requirement already satisfied: et-xmlfile in c:\\users\\764883\\appdata\\roaming\\python\\python311\\site-packages (from openpyxl) (1.1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a1ad873e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model_Name</th>\n",
       "      <th>Experiment Description</th>\n",
       "      <th>BERTScore Precision</th>\n",
       "      <th>BERTScore Recall</th>\n",
       "      <th>BERTScore F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BART</td>\n",
       "      <td>1</td>\n",
       "      <td>0.318</td>\n",
       "      <td>0.324</td>\n",
       "      <td>0.321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BART</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.275</td>\n",
       "      <td>-0.248</td>\n",
       "      <td>-0.260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BART</td>\n",
       "      <td>3</td>\n",
       "      <td>0.358</td>\n",
       "      <td>0.354</td>\n",
       "      <td>0.356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BART</td>\n",
       "      <td>4</td>\n",
       "      <td>0.257</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BART</td>\n",
       "      <td>5</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.330</td>\n",
       "      <td>0.320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>BART</td>\n",
       "      <td>6</td>\n",
       "      <td>0.323</td>\n",
       "      <td>0.339</td>\n",
       "      <td>0.331</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Model_Name  Experiment Description  BERTScore Precision  BERTScore Recall  \\\n",
       "0       BART                       1                0.318             0.324   \n",
       "1       BART                       2               -0.275            -0.248   \n",
       "2       BART                       3                0.358             0.354   \n",
       "3       BART                       4                0.257             0.280   \n",
       "4       BART                       5                0.310             0.330   \n",
       "5       BART                       6                0.323             0.339   \n",
       "\n",
       "   BERTScore F1  \n",
       "0         0.321  \n",
       "1        -0.260  \n",
       "2         0.356  \n",
       "3         0.269  \n",
       "4         0.320  \n",
       "5         0.331  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data from Excel sheet into a pandas DataFrame.\n",
    "excel_file = 'BART_Results.xlsx'\n",
    "df_vis = pd.read_excel(excel_file, sheet_name=\"BART_with_BERT_Score\")\n",
    "df_vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "51cd8226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model_Name                 object\n",
      "Experiment Description      int64\n",
      "BERTScore Precision       float64\n",
      "BERTScore Recall          float64\n",
      "BERTScore F1              float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df_vis.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1981c1ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABKUAAAUJCAYAAACxFLBqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAADuZklEQVR4nOzdd1hT59sH8G9A9lKUIQ7AjVSLW9SKihMXDpxVsLW/Vm3dpe5d666jte6Bs04cIKKCe6DiwL1AUXALCAgInPcP3pwmkIQAgaB+P9eViyTnOc9zn5OQcecZEkEQBBARERERERERERUhHW0HQEREREREREREXx4mpYiIiIiIiIiIqMgxKUVEREREREREREWOSSkiIiIiIiIiIipyTEoREREREREREVGRY1KKiIiIiIiIiIiKHJNSRERERERERERU5JiUIiIiIiIiIiKiIsekFBERERERERERFTkmpYiKgQ8fPmD69On4+uuvYWJiAolEAolEgpEjR2o7tCLXokULSCQStGjRQmmZjIwMLFmyBA0bNoS5ubl4vjw9PeXKvXnzBmPHjoWTkxOMjIzEcosXLy7UYyAiItImBwcHSCQS+Pj4FEr9x48fF99Tjx8/XihtEBHRl+GLSUrJvnkqupiamqJatWoYMGAAQkJCVNYVFRWlsi5Fl+xflgFg2rRpCsvq6OjA3Nwc1atXx7fffovDhw8XuH1Fl/x4+vQppk2bhm+++QZWVlbQ09ODkZERypcvj+bNm2PEiBHYtWsX4uPj81X/l+jjx49o3bo1pk2bhuvXryM5OVnbIalN1fO3QoUKqFevHgYPHoxVq1bhzZs3Gmu3b9++GDlyJC5evIj3798rLBMfHw9XV1csXLgQd+7cQUpKisbaJ6JPT17fO6dNm6btkHMlTTwouhgYGKBs2bJo06YN/v7773y9t7Rs2VKsr23btmrvJ/1xIftFV1cXlpaWqFevHkaMGIGbN2/K7bdhw4YCf7ZR9YMGERERFT9fTFIqN0lJSbh//z42b94Md3d3eHt7IyMjQyuxCIKA9+/f4969e9iyZQvat2+PHj16IDU1VSvxSK1evRrVq1fH9OnTcfr0abx+/Rrp6elISUnBs2fPcOrUKSxduhReXl748ccftRrrp2Tnzp04e/YsAMDHxwehoaGIiIhAREQExo0bp+Xo8k76/H369CnCw8Oxdu1a/PjjjyhfvjwGDRqE169fF6j+s2fPYufOnQCAjh074siRI7h+/ToiIiKwdOlSsdzff/+N+/fvAwB8fX1x6tQp8bwOGDCgQDF8Dj6lL96kWXzsvwxpaWl4/vw5jh49ip9//hm1atXC3bt31d7/8ePHOHHihHj72LFjiImJKVBMmZmZePfuHcLDw7F06VJ8/fXXmDNnToHqJCIiok9bCW0HoA1DhgzB0KFDxduCIODt27c4d+4c/vzzT7x8+RJ+fn6oUKECZs2apbKurl275loGAMzNzVVuX7duHRo0aAAg60NbdHQ0zp49iz///BMfPnzAnj17MHr0aPz9998oV64cIiIilNZVq1YtAED9+vWxfv36XGNTx7Zt2/C///0PAGBoaIhBgwahXbt2KF++PARBQExMDC5duoSDBw/iypUrGmnzS3H06FEAgK2tLdasWQNdXV0tR5R32Z9rqampePfuHR48eIBTp05h7969+PDhAzZs2ICgoCDs3bsXjRs3VlhXbsMApOdLV1cXW7duVfq/JS1Xv359zJ07Nx9HRUSfK3Xeu62trYsomoKzs7PL0av6w4cPuHv3LlavXo2TJ0/i0aNH6NixI27evAkDA4Nc69y0aRMEQYCBgQEyMjKQnp6OzZs3w9fXN0+xyX5eSUtLw6NHj+Dv748tW7YgIyMD48ePR+XKleHl5QVPT0/Ur19fYT0XL17Ed999ByDn5zhZJiYmeYqPiIiItOuLTEpZW1vjq6++ynG/m5sbunTpgnr16iElJQVLly7FlClToK+vr7SukiVLKqwrrxwdHeXqqV27Njp27IiePXuiYcOGSE9Px6pVqzB58mTY2tqq1aaJiYlGYsvIyMDo0aMBAGZmZjh9+jRq166do1yXLl0wY8YM3L59W2XSjOQ9e/YMAFCpUqVPMiEFKH+utW7dGj/99BNev36NkSNHYsuWLXj+/Dm6dOmCsLAwODg45Lkt6fmysbFRmeyVlqtWrVqe2yCiz5um3ruLCz09PYXH06BBA/Tr1w/u7u44fvw4Hj58iD179qBv37651rlp0yYAQKdOnfDhwwcEBgZi06ZNeU5KZY+rbt266NmzJxo1aoThw4cDAKZPnw4vLy+ULFkSJUuWVFiPbC9bZZ/jiIiI6NPD4XvZ1KxZEx07dgQAvH//Hnfu3NFqPHXq1EGfPn0AAOnp6VqZTPLChQt4/vw5AODHH39UmJCS5eTkhF69ehVFaJ8F6bBMPT09LUdSeMqUKYPNmzfjp59+AgC8evUKI0aMyFdd6p6vL+G8EhHlRkdHB2PGjBFvX7x4Mdd9zp8/j3v37gEA+vfvj2+//RYAcOPGDYSHh2skrmHDhqFixYoAgJs3b4qfM4iIiOjLwqSUAo6OjuJ1bc/jBPw3HA8AoqOji7z9J0+eiNerVKmikTpfvXqFGTNmoGnTprC2toaenh5KlSqFRo0awdfXF9evX1e6b1RUFEaNGgVnZ2eYmZnB2NgYVatWxY8//phrD63sc6mEhITAy8sLFSpUgJ6ensKeO8+fP8fEiRNRv359WFpawsDAABUqVECvXr3EIWJ5JTvhrnTOjhMnTshN1qoolrS0NCxfvhwtW7aElZUV9PX1YWtrCw8PD2zevBmZmZlK2/Tx8ZGrNzY2Fr/99pt4HotiBZ3FixejQoUKAIADBw7kmOQWUL76nvS8bNy4EUDWfCfZJ7iVXdDg8ePHAICNGzeqNQmuv78/vLy8ULFiRRgaGqJkyZKoX78+pk+fjnfv3ik9pvyc14yMDGzcuBGdOnWCnZ0dDAwMULp0aTRr1gyLFi3Chw8flLaX/fw8e/YMo0ePRpUqVWBkZITSpUujXbt2OHTokML9pRMjS02fPj3Heczrak2ykxNHRUUhNTUVCxYsQN26dWFhYQFzc3M0atQIy5cvVzlXX2ZmJkJCQjB27Fg0bdoUZcqUgZ6eHkqWLAkXFxeMHTtW7vVInfNz//59/Pzzz6hatSqMjY3FGKViY2OxfPly9OzZE1WrVoWJiQkMDAxQrlw5dO3aFf/++6/K/6vsK1AJgoC1a9eiWbNmKF26NMzNzdGwYUOx14lUWloaVqxYgcaNG8PS0hJmZmZo2rQpduzYkfsJR/5elwry2D948ACjRo1CrVq1YGFhASMjI1SqVAk+Pj64dOmS2ucnMzMT69atQ8uWLWFjYwMdHZ0cbV6+fBnff/89qlWrBhMTExgaGooLJwwbNgz79++HIAhqnSdNy8zMxObNm+Hh4QFbW1vo6+vDysoKLVu2xPLly5GWlqZ0X9nFTYCsxRhmzpyJOnXqoGTJkpBIJNiwYUOhxJ3XzzV+fn4AgFKlSqFjx47w9PSEmZmZ3LaC0tHRgbOzs3hbG59vgJyfC0JDQ+Hp6Qk7OzsYGRnByckJM2fORFJSktx+gYGB8PDwEMvVrFkTf/zxh8rngFRBPsdIHTp0CB4eHrCysoKxsTGqVauG0aNHi72E1RUeHo6ffvoJ1atXh6mpKUxMTFC9enUMGTJETEwSEREVKuELERoaKgAQAAhTp05VWdbLy0ss+/z58xzbIyMjxe3e3t75jmnq1KliPaGhoUrL/fnnn2K5P//8M9d6pWXd3NzyHZus3bt3i3WOGDGiwPVt3rxZMDExEetUdLG3t1e478aNGwUDAwOl++nq6gqzZ89W2rbsc2DChAm5tqtOrN9//73w8ePHPJ0D2eeQuucgMjJSqFGjhsp9mjVrJrx580Zhm97e3mK9586dE8qUKZNjf1XPQ0Xy81ybPXu2uN/vv/+eY7ubm5vCOnM7X9L4cyuTvd63b98KrVq1UrmPtbW1cO7cOYXHk9fz+vjxY+Hrr79W2V6VKlWEu3fvKmxP9vycPn1aYXvSy/z583Psb29vn+s5yuvr2vr168V9w8PDhXr16imtu3nz5sL79+8V1iP7mqjsYmxsLOzZs0dpLLLnx9/fX+H/b2RkpCAIgpCeni7o6Ojk2mabNm2Uxiz7nAsODhY6d+6stJ7hw4cLgpD1nGvevLnScor+L2Tl93Upv4/9/PnzBT09PaX7SCQSYfLkybmen0OHDgmtW7dW2eaiRYvUekyUPR6qFPS9+82bN0LTpk1VxuXk5CRERUUp3F/2+X3v3j3BwcEhx/7r16/PU0zSx1TZe6ZUQECA2MacOXNUlk1NTRUsLS0FAML//vc/8X7pa521tXWu73nS/0NA9cfMrl27iuWuXLmismxePsflhWydf/zxhyCRSBQ+tk2aNBESExOFzMxMYfjw4UqfA+3btxfS09OVtlfQzzGCIAijRo1Sur+VlZVw8eJF8bmh7LmekZEhjBo1SunxAhBKlCghrFy5UuH+so9HXj87EBERyWJSKpvbt28LRkZGAgChcePGCssUdVLqf//7n1hu7969udYrLauppNSjR4/EOg0NDYVjx47luy4/Pz+5un755RchMDBQCA8PF06ePCn89ddfQtu2bQVHR8cc+x48eFD88GRqaipMnTpVOHXqlHDu3Dlh4cKFcl/Oly9frrB96fZatWqJf9etWyeEhYUJJ06cEJYsWSKW/ffff8X2KlWqJCxatEgICgoSLl++LOzevVvw8PAQ6xs1alSezkNaWpoQEREhRERECPXr1xcACPXr1xfvi4iIkEtKvH//XqhUqZLYnqenp7B//37h0qVLws6dO+W+ADRp0kThB2LpF4rSpUsLdnZ2gqmpqTBx4kTh+PHjQlhYmLB27Vrhzp07eTqO/DzXwsLC5D68Z6csKSU9L9IvMXZ2dnLnKyIiQkhMTBSv29nZCQCErl27ypV59OiRWGdKSopQt25d8YvAgAEDhG3btgnnz58XTp06Jfz+++9C6dKlBQBCqVKlFH7ZzMt5ff36tVChQgUBgGBgYCD8/PPPws6dO4WLFy8KoaGhwvjx4wVjY2PxORcXF6f0/FSrVk0oU6aMYG1tLcyZM0c4ffq0EBYWJixatEgoWbKk+IXixo0bcvvfvXtXiIiIEB+DIUOG5DiPT58+VfvxFAT5pFSDBg0EAELv3r2FwMBA4dKlS8LWrVvF+6XPX0UmTpwolC1bVhg6dKiwadMm4cyZM8Lly5cFf39/wdfXVzA1NRVfO27duqWwDun5cXR0FExNTQUrKythzpw5wpkzZ4Tz588Ly5YtE169eiUIgiB8/PhR0NHREVq1aiXMnz9f/P8+fvy4sG7dOsHV1VWMeeDAgQrbk31vadSokQBA6N+/vxAQECBcvnxZ2LZtm1C9enWxzJEjR4QuXboIJUqUEIYMGSIEBwcLly9fFtauXSs+Z3V1dXM8blIFeV3Kz2M/b948sXzt2rWFf/75Rzh69Khw6dIlYcuWLXLnSPb1U9H5qV27tgBA6NKli7Bnzx7h8uXLQmBgoLB9+3ZBEATh2rVrYkLK0dFRWLhwoXDs2DHhypUrwsmTJ4XVq1cL/fr1E0xMTIo8KZWeni53rG5ubsLOnTuFS5cuCfv37xc8PT3FbZUrV1YYn+x7fu3atQU9PT3hl19+EY4cOSJcunRJ2LZtm3D27Nk8xaVOUiozM1Nwd3cXgKwE4s2bN1XWKftD1MmTJ8X7jxw5It5/4MABlXWom5SqVq2aWO7t27cqyxZ2Uqphw4YCAMHV1VXYunWrcOnSJSEoKEjo0KGDWGbixInCwoULBQBChw4dhN27dwuXL18W9u3bJzRu3Fgs988//yhsSxOfY2R/qLSzsxOWLVsmXLhwQThx4oTg6+sr6OvrCw4ODoKVlZXK5/rQoUPFepo3by6sW7dOfN9avXq14OzsLG7ft29fjv2ZlCIiIk35IpNS2T+IX79+XTh58qQwd+5cwdbWVgAgWFhYKO0ZIfvBNvsXXmWXxMTEHPWok5R68uSJ+It4yZIl1fogrumklCAIQqdOncR6pV88p0yZIgQGBopf8HITExMjfuG2trYWIiIilJZ98uSJ3O20tDTxC5upqanCX1SjoqKEsmXLCkBWbwpFcckeg7u7u5CSkqKw/VevXgkWFhYCAOG7775T+quwtLeVjo5OnhM6UsqSMLLGjh0rxj1p0qQc2zMzM4X+/fur/DArTZ5Iz+HVq1fzFa+s/DzXUlNTxS+elSpVyrE9t/Mh2zNJldx+JRaE/x6/kiVLCpcuXVJYRvZ51a9fP6XxqHNe+/XrJ8YumxyTFR4eLv7PT5gwIcd22S979vb2ChNIp06dEr/4SHvnZKfJL3eySSkACn/l//jxo9CuXTuxTEBAQI4ykZGRQlpamtJ2oqOjhXLlygkAhG+//VZhGdnzY2dnJzx+/FhpfZmZmcL9+/dVHtuUKVPEL/P37t3LsT1777zFixfnKBMbGyuYmZkJQFYvBolEovAHBtmkjKLHTVOvS+o+9jdv3hR7SE2dOlXIzMzMUSYjI0P49ttvxed/9sRC9vOj6PVLavLkyQIAwcTERGEvZam4uDghIyNDZeyK5OW9O/v/1V9//SXuO3DgQIXnQrb3ra+vb47tsu/5Ojo6wuHDh/N8DNlJX+cUJekvXrwobNmyRWjZsqXY7m+//ZZrndLEv4ODg9xxZmRkiO/DXl5eKutQJyklm/xyd3fPNa7CTkoBEHr06JHjR5309HQx4WRmZiYYGhoKI0eOzFFPUlKS+HjUrl07x3ZNfI558eKF+DnK3t5eiI2NzVHHsWPHhBIlSojHpOg9MDg4WNy+Zs0aheflw4cPYi9ie3v7HK83TEoREZGmfJFJKVUXHR0d4aefflI6dEYQ1Bt6lf2i6A1bWVIqIyNDePz4sbBlyxahYsWKYplly5apdayFkZR69eqVXE+H7Jdq1aoJP//8s3D58mWldYwfP14s7+/vn6f2//33X3FfVUMPNm/eLJabN29eju2yj7N0CI8iM2bMEAAI5cqVU5q4EoSsL9rSL8mKEgjqyC0Jk5KSIvZ8cXZ2VjosID4+XuzVU7NmzRzbZZMnM2bMyFes2eX3uSb9Yl2qVKkc24oqKfX+/Xsxjtz+t5YvXy4AEPT09HIkmNU9r5GRkYKurq4A5N7LwNfXV/yimZ3sl739+/crrUP6JapOnToKtxdWUqp27doKv7ALQlZSSZrk6NixY77aWrx4sQBAMDc3V9iO7Pnx8/PLVxuy0tPTxd4LCxYsyLE9e08pZQYOHCiW6927t9Jy0mF9ih43Tb0uqfvYf/fddwKQ1YtT2WMqCILw7t07cTjSqlWr5LbJnp9q1aqpHNb0ww8/qHzOFlRe3ruzv244OTkJQFZSMSEhQWH9Hz9+FIdYlypVKsdjJPue/91332nkmNQZkglk9QJSNexV6vXr1+L/qKLnjvQHEkNDQ+Hdu3dK61GWlEpNTRVu374tzJ49W0yuGBsbC+fPn881tsJOShkbGysd/r5u3TqxXIUKFZQmz6VJbAA5erpq4nOMbM/FXbt2Ka1jyJAhSp/LgiCIyaYePXoorUMQBOHWrVtiPcHBwXLbmJQiIiJN4UTn2WRmZmL79u34559/inSS85YtW4qTberq6sLe3h79+/fHkydPYGdnhzVr1uDnn38usniyK1OmDM6cOYNVq1ahbt26Obbfu3cPf/31F+rVq4cBAwbkmBAUAA4ePAgAqFSpErp06ZKn9qUT90okEnz33XdKy3l5ecHCwkJuH0WaNm2qcCJxqf379wPIWgrbwMBAabkSJUrA1dUVAHDu3Dml5Qri8uXLiIuLA5A1qbaurq7Ccubm5uKqh7du3UJsbKzSOvv376/xOPPC1NQUQNYKl9py4sQJxMfHAwB69uypsmzz5s0BAB8/fsTly5eVllN1XgMCApCRkQFjY2N06NBBrfZiYmKUTuxdsmRJcaVQRerVqwcAePTokcq2NM3b21tuMm1Z5cuXR9u2bQFkTYCtatJzAEhISEBkZCRu3ryJGzdu4MaNGzA2Npbbpoy+vj68vLzyFHtmZiZiYmJw9+5dsb3bt2+jfPnyAIBr166p3F+6UqoiX3/9dZ7KKXrcivp16cCBAwCAHj16KH1MgaznonRBDlXt9e7dW+nrFwCULVsWQNbrV1hYWH5CLhQxMTG4ffs2AKBXr17ihN/ZlShRAoMGDQIAvHv3TuUqdUX9Gnzp0iUsX74810m0t23bho8fPwKAuOKeLOl9KSkp2Llzp1pty06kb2BgACcnJ0yYMAHJycmoW7cugoOD0ahRozwekea1adMGlpaWCrfJ/v92795d6aqusuWyvz5p4nOM9HapUqXQtWtXpXWoqj8hIUFcfCO39z4nJyeUKVMGQOF9xiEiIvoik1JTp06FkNVLTLwkJyfj+vXr+PXXX5GYmIjFixejdevWSE5OVlmXt7d3jroUXZSt+KWODh06aD2JAAB6enr44YcfcPnyZTx79gzbt2/H2LFj8c0338h9QNu8eTO6dOki94Xz48ePuHHjBgCgWbNmKr/gKCLd19HREVZWVkrL6evro06dOnL7KFK7dm2l2zIyMnD16lUAwMqVK3OsTpX9smvXLgAotOWsZY8jtw/ustuVHb+pqSkqVaqkmeDySZqMMjc311oMsiuGlS1bVuVj/NVXX4lllT3OuZ1XaXvJyckoUaKEyvY6deqUa3tVq1aFjo7yl3Dpl6uiTvw1aNBA5faGDRsCAJKSkhQmXh4/foxffvkFDg4OsLCwQKVKlfDVV1+hVq1aqFWrFv73v/+JZV+/fq20napVq8LQ0DDXeAVBwObNm9GyZUuYmpqiXLlyqFGjhtherVq1xNcDVe0BQLVq1ZRuK1myZJ7KZX/civp16fHjx3j16hUAYPz48bm2J31+q2pP1esuAPTt2xd6enpITU1F06ZN0blzZ6xYsQI3btzQ+Gp7ub13y66Cp+nXYCD3c5FX9vb2OY7h48ePePr0Kfz8/FCxYkUcPXoUzZo1w+nTp5XWI13dtG7dunBycsqx/euvvxZfDwu6Cp++vj6+//57NG3atED1aIom/3+BnP/DmvgcI00q1qlTByVKlFBah4uLC/T19RVuu3LliriiaN++fXP935a+7hXWZxwiIqIvMimliJGREWrVqoV58+Zh+fLlAIDTp09j9uzZRdL+unXrEBERgYiICISFhWHr1q1wc3MDAKxduxY9e/bU2hLYitjZ2aF3796YP38+Tp48iefPn2P8+PHil+SQkBBs27ZNLP/27Vsxfumv4Xnx9u1bAIC1tXWuZW1tbeX2UaRUqVIq20pPT89jhMg1gZlfsseR2/FLjz37frJkPzRrQ2pqqvhhXdmv0kXh5cuX+dpP2eOc23nVdHvSHkPKSP8XpV8+ikpuz1EbGxvxevbn6KFDh1CzZk389ddfePz4ca5tffjwQek2Vf/jUikpKejYsSMGDBiA48ePq6wvt/YA1Y+JbAJRnXLZH7eifl3S9PMVyP0xqVGjBrZt24ZSpUohPT0dBw8exJAhQ1CrVi1YW1tjwIABOHXqVL7iKghNvwYD6j0/C6pEiRIoV64cBgwYgLNnz8LS0hIJCQn49ttvFT6Xbt++LSYXFfWSkhowYAAA4MyZMyp7K0pJP9tERETg5MmT+Ouvv1C5cmWkpaVh2LBhmD9/fj6PULM0+f8LIEdPUE18jlG3jhIlSih9fy2M/20iIqKCUP4zyxfs+++/x7hx4/D27VusW7cOs2bNKvQ2HR0d5XpjNGjQAH369MH333+P9evXIyAgAIsXL8aoUaMKPZb8sLS0xOzZsyEIAubMmQMA2Llzp8oPtvmR1x5WyqgaQiL7QXLw4MEYMWKEWnUq+1VSkzRx/KqOvShcu3ZNTFBWr15da3HIPs7h4eFKh2NkJx3KlV1u51XaXpkyZRAaGqpmlFmvDZ+S/D5HX79+jX79+iE5ORmmpqYYO3Ys2rVrh8qVK8PCwkL8/woJCYG7uzsAqEzUq/M8//3333Ho0CEAgJubG4YNG4a6devC1tYWRkZG4hfM5s2b49SpU1r9YaCoX5dk25syZYraQyFNTEyUblPnMenRowdat26Nf//9F4cPH8apU6fw6tUrvH79Gps3b8bmzZvh7e2NdevWqewpWFiK4j2oMJQtWxYDBgzAkiVL8PjxY4SEhIhDaaVkez6NHj0ao0ePVlmnIAjw8/PD1KlTVZaT/WwDAN988w0GDhyIZs2a4fr165gwYQJatGiRay/Lz4UmnkMFqUP2f3vlypVo0qSJWvsVRSKViIi+TExKKaCjo4OqVaviwoULiI2NxZs3b1C6dOkij0MikeCvv/7CsWPH8OTJE0yfPh3e3t5a7V2Smx9++EFMSj148EC839LSEjo6OsjMzFQ515Ey0mN+8eJFrmWlXczze55k9xMEIccH6qImG8+LFy9UDh2Q7V5fXJ8nR44cEa83a9ZMa3HI/k9bWVkpTTZpur3379/DyclJ68nBwpLbc1T2f1j2Obpr1y5x7rS9e/eidevWCvdX1fskLwRBwJo1awBkfUkOCQlRmuTQVJsFUdSvS7L/H3p6ekX6OmhhYYH//e9/4lDN27dvY9++fVi2bBliYmKwceNG1KlTR+3EXEFlfw1WpTi/BteoUUO8HhERIZeUyszMxJYtW/Jc56ZNm3JNSiliZmYGPz8/1K1bF+np6RgzZgxOnjyZ53o+JZr4HFOqVCk8f/481zrS09OVvm7J/m8bGxtr/TMOERERh+8pIdu1PT9DJjTF2NgYU6ZMAQDEx8dj3rx5WotFHXZ2duJ12V/yZL/U5KfHgXTfyMhIcZ4TRT5+/IgrV67I7ZNX+vr6cHZ2BpA1PEHbZI/jwoULKsvKTg5cHD9opqSkYMWKFQCynh+qJmotbNI5O4CieZyl7aWmpsrNZ/W5uXjxolrbjY2N5ebgunnzJoCsL2HKElIANHbu3r59K37x8/LyUpqQSkxMxN27dzXSZkEU9etSpUqVxMmWtf066OTkhHHjxuH8+fNiT6wdO3YUWfufy2uwqs81oaGhiI6OBgD88ssv2LZtm8rLyJEjAQAPHz7M9/Pj66+/Rr9+/QBkfS4ICgrKVz2fCk18jpEuKHD16lWVn02vXbuGtLQ0hdtcXFzEz2fa/t8mIiICmJRSKDk5Gbdu3QKQNdeUdOURbRk4cCAqVqwIAFi+fHmR/2qflwSS7BfG7JM+d+7cGUDWB7J9+/blKQbpl1RBELB+/Xql5Xbt2iWuqKbqi21upKsD3rlzB4cPH853PZpQr149cb6ijRs3Kp0j6P379+IXtZo1a+Zr7q7CNmrUKDx9+hQA4OnpqXAi3aLSunVrcW6QpUuXFvrQrM6dO4tfBBYvXlyobalDOgm4plcZ3bRpk9Jz+ezZMwQHBwMAWrRoIddbTPoFKyUlRelzPDk5GZs2bdJInLJf6BStFiq1Zs0arf4wIUtTr0vqPPa6urrw8PAAAAQHB4urz2lThQoVxF54uU06r0l2dnbia9WOHTuQmJiosFxGRoY4QXqpUqUUrlSrTbLvzxUqVJDbJh26p6uri0mTJqFPnz4qLxMnThQn2i7IhOcTJ04UE8JFMVWCNmnic4z09tu3b8XVMRVZt26d0m1WVlZo3LgxAGDr1q0qE2RERERFgUkpBaZNmyZOaNuuXTutD7PR09ODr68vgKzEw5IlS4q0/UOHDqFXr17iL3fKvH37FsOHDxdvZ+8F8/PPP4u/cv/4448qVyaSJi6kPD09xV5Yv//+u8JlraOjozF27FgAWb0wpEtz58eIESNgamoKABg0aJDYi0OZgIAAXL9+Pd/tqWJgYIDBgwcDyFqJZ+bMmTnKCIKAn3/+Wfyi9vPPPxdKLPn1+vVrfPvtt2IvKRsbG60nZkqWLCmep7Nnz2LUqFEqJwV/8eKFONwrP6pXry7Oy7N9+3YsWrRIZfnIyEi5xQI0TZq0fPjwoUbrvXr1qsKJi9PT0/HDDz+Iv94PGTJEbnvVqlUBZCWeFPWCycjIwODBgxETE6OROK2srMRk77Zt2xQmaC5evIjJkydrpD1N0NTrkrqP/fjx46Grq4vMzEz07Nkzx+uyrIyMDGzZskVlmdz4+/uLQzgViY6Oxp07dwAU/Vxrw4YNAwC8evVK7n1O1vTp08UftH744QcYGBgUWXy5CQ8Px/bt2wFk9bqTTXYkJSVhz549ALKGsqozEXeZMmXExVh27NiR7+R2jRo10L17dwBZvXbyMt/ep0YTn2O8vb1hZGQEIGveL0XD+E6cOIFVq1apjGXSpEkAgISEBPTs2VPl/11qair+/vtvpKSkqKyTiIgov77IOaVevnyZIyGSkpKC+/fvw8/PT+xCbmhoiBkzZqisKy4uTmVyRUpXV7dAvUK+//57zJo1C8+fP8eyZcswduxYmJmZ5bu+vMjMzMTOnTuxc+dOfP311+jYsSMaNGiAsmXLQl9fHy9fvsTp06exatUqcVWXevXqwdvbW64eW1tb/PPPPxg4cCBevnyJhg0b4ocffkCHDh1ga2uLxMRE3LhxA/v378fdu3flvjDp6+tj1apV6Ny5MxISEtC0aVP8+uuvcHd3h66uLs6ePYs5c+aI7S9YsKBAPdxsbGywceNG9OzZE7Gxsahfvz58fHzQoUMHlC9fXlxqOywsDLt27cKjR49w4MABjS/zLTVlyhTs2bMHjx49wrRp0xAREYFBgwahbNmyiIyMxF9//YXjx48DAFxdXcW5WIpKUlKS3P9Bamoq4uLicP/+fZw+fRp79uwRE712dnbw9/cXe/9p04wZM3DixAlcuHABS5YswfHjx/HDDz/AxcUFJiYmePfuHW7evImjR4/i0KFDqFWrlpggzI9//vkHly5dwqNHjzBmzBjs27cPAwcOhLOzMwwMDPDmzRtcu3YNQUFBCAkJQbdu3dC3b18NHvF/mjRpgsjISOzfvx8rV65E06ZNxR405ubman0xVaR+/fr47bffcPXqVQwcOBDW1ta4f/8+Fi1aJA5t6ty5Mzp16iS3X69evTBhwgSkpqZi0KBBuHr1Ktq0aQMLCwvcvHkTy5Ytw+XLl9G0aVONDDnR0dFB//798ffff+P69eto1qwZRo8ejapVqyI+Ph6BgYFYvnw5TE1NYWdnh3v37hW4zYLS1OuSuo99rVq1sGDBAowaNQq3bt3CV199hf/9739o1aoVbGxskJKSgqioKJw7dw67du1CbGwsIiIi8j0/2+LFi9G/f3907NgRrVq1gpOTEywsLPDu3TtcunQJy5YtE19HfvrppwKcybz76aefsGXLFpw7dw7r16/H48ePMXToUDg6OiI2Nhbr1q0TEzuVK1cu8mTmx48fc3wWycjIwIsXL3D06FH8/fffYuLo119/lfv/3rNnj9j7q0ePHmq32aNHDxw7dgxxcXHYv3+/2pPhZzdhwgTs2rULQFZvqZYtW+arnuJOE59jbGxsMHPmTIwdOxZRUVGoV68exo8fj4YNGyIlJQWBgYH4888/Ua5cOSQnJyvtBeXh4YERI0ZgyZIlOHnyJJycnPDTTz+hWbNmKF26NJKSkvDgwQOcOnUKe/bswbt373J8piMiItIY4QsRGhoqAFD7YmVlJRw+fFhhXZGRkXmqC4BgYWGRo56pU6eK20NDQ3M9hnnz5onlZ8+erbSctIybm5uaZ0e106dPCyYmJmofa5s2bYTXr18rrW/Dhg2CkZGRyjrs7e2V7mtgYKB0P11dXbXOzdSpU9U69v379wuWlpa5HrOOjo4QEhKiVp3Zubm5qfV4RUZGCjVq1FAZR9OmTYU3b94o3N/b21vluc2PvPwPGBoaCt99953K54Yg5H4+1D0Oe3t7AYDg7e2tslxCQoLQvXt3tY6hZcuW+Y5HKjY2Vvjmm2/Uam/QoEE59lf3+SL7+qLIlStXlP4v5XbOslu/fr24b3h4uFCnTh2Vz9GEhASF9axbt07Q0dFRum/v3r2Fo0ePqnzdVPf8CIIgxMXFCS4uLkrbs7S0FE6cOKGyTtn3FlWv47LnKDIyUmm53B43QSj461JeH/tVq1YJxsbGubanr68v3L9/P1/nRxD+e+xyO6aZM2eqrEcZ2ffuvD7HBUEQ3rx5IzRt2lRlfE5OTkJUVJTC/dV5bPNK+jqnzkUikQgjRowQMjMz5epo3bq1uP3Zs2dqt/38+XPx/7VTp05y22QfS3V4eHiI5c+dO6ewjOxzSd33cHWoU6fsc2f9+vVKy6nzfC/o5xhBEIThw4cr3b9MmTJCWFhYru+BmZmZwvTp04USJUrk+twxMTERkpOT83ysRERE6uDwvf+nr68PW1tbuLu7Y+HChbh7926O5ZK1bciQIeJKLH/++SeSk5OLpN2mTZvi1atX2L9/P0aPHg03NzfY2dnBwMAAJUqUgKWlJerWrYsff/wRoaGhCA4OVrlaobe3Nx4+fIiJEyeK8yXp6uqiVKlSaNy4MSZMmKB0wlNvb2/cuXMHI0aMgJOTE0xMTGBkZITKlSvjhx9+wJUrVzB+/HiNHXvnzp0RGRmJBQsWiL0D9PT0YGRkBEdHR3Tq1AmLFi1CVFRUof+66+DggGvXruGvv/6Cm5sbSpcuDT09PdjY2KB9+/bYtGkTTp48WSxWfJL2LqlTpw6+//57rFq1Cs+ePcPatWu1spKlKmZmZti9ezdOnTqFwYMHo3r16jAzMxOf2w0aNMCwYcMQGBgot3Jgftna2uLkyZM4ePAg+vfvj0qVKsHY2Bh6enqwsrJCkyZNMGbMGJw4cULlvCAF5eLignPnzqFv376oWLGixoYalSpVCmfPnsUff/wBFxcXmJmZwdTUFA0aNMCyZctw4sQJpb08Bw0ahFOnTsHT0xNWVlbQ09ND2bJl0b59e/z777/Yvn27RodTW1hY4MyZM5g5cyZq1aoFQ0NDmJqawsnJCWPHjsW1a9fQvHlzjbWnKQV9XcrrY//DDz/g0aNHmD59Opo2bYoyZcqgRIkSMDExQbVq1dCjRw+sWLECz549Q5UqVfJ9XNu2bcOqVavQr18/uLi4wNbWFiVKlICpqSmcnZ0xZMgQXLlyRRx6VNQsLS1x8uRJ+Pn5oX379uJ5L126NFq0aIG//voLV69ehb29vVbiy05HRwcWFhaoU6cOfv75Z4SHh2Px4sVyi5A8e/YMISEhALJ62couVpIbGxsbNG3aFAAQFBRUoLmJJk6cKF5XNET9c6KJzzFLlixBQEAA2rVrB0tLSxgaGqJKlSoYPnw4rly5ggYNGuQah0QiwZQpU3Dv3j34+vqifv36sLS0hK6uLszMzFCzZk30798fGzduRGxsrDhskIiISNMkglDIs/sSEdFnbcOGDeLcJ5GRkXBwcNBuQERERERE9ElgTykiIiIiIiIiIipyTEoREREREREREVGRY1KKiIiIiIiIiIiKHJNSRERERERERERU5JiUIiIiIiIiIiKiIsfV94qRzMxMxMTEwMzMTG7JZiIiIiL6sgiCgPfv38POzg46OvwdmYiIPk8ltB0A/ScmJgYVKlTQdhhEREREVExER0ejfPny2g6DiIioUDApVYyYmZkByPrwYW5uruVoiIiIiEhbEhISUKFCBfHzIRER0eeISaliRDpkz9zcnEkpIiIiIuKUDkRE9FnjAHUiIiIiIiIiIipyTEoREREREREREVGRY1KKiIiIiIiIiIiKHJNSRERERERERERU5JiUIiIiIiIiIiKiIsekFBERERERERERFTkmpYiIiIiIiIiIqMgxKUVEREREREREREWOSSkiIiIiIiIiIipyTEoREREREREREVGRY1KKiIiIiIiIiIiKHJNSRERERERERERU5EpoOwAiIiIiIipcgiDg48ePyMzM1HYoRET0GdLR0YGenh4kEkme9mNSioiIiIjoM5WcnIz4+Hi8f/8eGRkZ2g6HiIg+Y7q6ujAzM4OFhQWMjY3V2odJKSKiIhYaGopffvkFycnJmDJlCnx8fLQdEhERfYbev3+Pp0+fQk9PDyVLloSJiQl0dHTy/Cs2ERGRKoIgIDMzE0lJSUhISEBcXBzKly8PMzOzXPdlUoqIqJDFx8fDwsJCvL18+XJcuHABANCwYUMmpYiISOOSk5Px9OlTmJubw87OjokoIiIqdCYmJrCyskJMTAyePn0Ke3v7XHtMcaJzIqJC1qVLF+zatUu8ra+vj/DwcISHh0NPT0+LkRER0ecqPj4eenp6TEgREVGRkkgksLOzg56eHuLj43Mtz6QUEVEhCw4OxtWrV+Hl5YWYmBjMnj0bq1evxpIlS7B69Wpth4fQ0FB89dVXqFSpEjZs2KDtcIiIqIAEQcD79+9hbm7OhBQRERU5iUQCc3NzvH//HoIgqCzL4XtERIXMwMAAs2bNQkREBLy9vdGtWzf4+flpLR4OJyQi+rx9/PgRGRkZMDEx0XYoRET0hTI2NsabN2/w8eNH6OvrKy3HnlJERIUsMzMTgYGBePHiBYKCgvDx40e0b98ed+7c0Uo8HE5IRPR5y8zMBJC1PDcREZE26OrqAvjvPUkZ9pQiIipkvXr1goWFBZKTk7Fnzx4sX74cnp6eGDlyJFxcXDB16tQijSc4OBgzZ87Ev//+iyVLlmD27NmYPHkykpOTi8VwQiIi0gwO3SMiIm1R9z2ISSkiokL24MEDXL16FQDg4uICALC3t8fevXuxbdu2fNcbGhqKX375BcnJyZgyZYraw+6K23BCIiIiIiL6MrFPLxFRIXN0dMTgwYPRr18/NGzYUG5b37591a4n++oV0rmgIiIiMH/+fLXrKW7DCYmIiIiI6MvEnlJERIVs586dOHz4MPT09NCmTZt819OlSxf88ssv6NmzJ4D/5oICkKe5oIrbcEIiIiIiIvoyMSlFRFTISpQogY4dO4q3X716hYiICDg5OaFs2bJq16OpuaAKazghERERERFRXjApRURUyAYOHIgFCxbA2toaISEh6N27NxwdHREVFYVVq1bB09NTrXo0NReUdDhhcnJygYYTEhERERERFQTnlCIiKmTXrl2DtbU1AGD69Ok4cuQIwsLCcOHCBUyfPl3tejQ1F9TOnTvRrVs3+Pj4YOXKlXnal4iIPk+SL/BCRJQbiUQCiUSCFi1aFEr9Pj4+YhtRUVGF0kZxx55SRESF7MOHD+L15ORkccico6MjMjIy1K5HU3NBaWo4IREREakvt+XRTUxMYG1tjdq1a8PT0xN9+/aFgYGB0vJRUVFwdHTMcxwWFhaIi4vLcb+DgwMeP36scB9DQ0OULFkSNWrUQPPmzeHj45OjbXWXf8+Nm5sbjh8/LndfWloa/P39sXfvXly+fBkvXrxAUlISjIyMYGNjg8qVK8PFxQVNmjRBy5YtYW5urpFYvmTHjx9Hy5YtFW6TSCQwMzODra0t6tWrBy8vL3Tp0gW6urpFHCV9DthTioiokLVr1w4jRoxAYmIiWrdujS1btkAQBBw6dAhlypRRu54HDx5g7dq12LZtG86ePQvgv7mgqlWrpnY9AwcOxMuXLwEAISEhqFmzJsaNG4evv/4a/v7+eTo2IiIi0oykpCRERkZi3759GDRoEJydncU5ILUtJSUFz58/x/HjxzFjxgzUqFEDc+bMKZK2w8LCULt2bfTu3Rvbt2/H/fv3kZCQgIyMDCQmJuLhw4cIDg7GvHnz4OnpiapVqxZJXF8yQRCQkJCAe/fuYdu2bejevTsaNWqEhw8fajs0+gSxpxQRUSFbuHAhfvvtN5QrVw6WlpZ4/PgxfHx84O7ujrVr16pdj6bmglI0nNDFxQWRkZHo3r272nNcERERUf7s3btX7rYgCIiLi8O1a9ewdetWvHr1Cg8fPoS7uztu3boFGxsblfVZWVlh1apVarWtzoq9K1euFD8rAFlJqYcPH2LHjh24fv060tLSMH78eJiZmWHYsGEKj0nWy5cv8eOPP6oVq+wPdpcvX4a7uzsSExMBAGXLlkWPHj1Qu3ZtlCpVCh8+fMDTp09x+fJlHDt2DHFxcXnqhU7qcXZ2xqxZs8TbgiDgzZs3OHnyJP7991+kpaXh8uXLaN26NcLDw1GqVCktRqtZgiAUav0bNmzAhg0bCrWN4k4iFPZZJrUlJCTAwsIC8fHx7HJK9BlKTk7Gw4cPkZ6ejvLly8PKyipP+6enp+Pw4cPQ09NDmzZt8t1Nvlq1arh37x4AoEGDBrh48aK4rXbt2rh+/Xq+6iUiIs0pyOfClJQUREZGwtHREYaGhmrt8yXOsVTUX4Jk37dVfQV7+/YtWrRogYiICACAr68v5s6dm6Oc7PA9e3v7As9HIzt8LzIyEg4ODjnKZGZmYtSoUVi6dCkAoFSpUnj69CmMjY1V1p3fWOvVq4fw8HAAgLe3N1asWKH0OZ2eno6jR49ix44dWLdunVr1k3Kyw/cUDamUunr1Klq1aoV3794BAMaNG4c//vijqMKkYkzd9yIO3yMiKmRXr16Fi4sLvvnmG+jo6GDSpElwcHBAxYoV85QAks4F1bZt2wLN26Cp4YRERESkeZaWlpgxY4Z4W1kyQBt0dHQwf/582NraAgDevXuHkydPFkpbt27dEhNSFSpUwOrVq1V+sS1RogTat2/PhFQRc3Fxwe+//y7e3rFjhxajoU8Rk1JERIVsxIgRmDZtGn755Rd4eHigT58+SEpKwtKlSzF27Fi169m5c6d4/fXr1+jYsSMsLCzQokULPHnyRO16Fi5cCB0dHZQrVw7bt2/HgAEDoK+vjyVLluRpOCEREREVjpo1a4rXExIStBhJTvr6+mjcuLF4W9r7WtNkVxd2dXVVa9ihulJTU7FmzRp0794dDg4OMDExgYGBASpUqAAPDw8sWLAAMTExKusICQmBj48PqlSpAlNTU5iYmKBKlSrw9vbGsWPHco0h+6pucXFxmDt3Lho3bgxra2vo6OgoXfHtwIEDGDhwIKpUqQIzMzMYGxvD0dER3377LY4ePZrX01FgXbp0Ea8/evQISUlJ4m1Fq8v5+/uje/fusLe3h4GBgdKV5x4+fIhx48ahQYMGsLKygr6+PmxsbNCqVSssWbIEycnJasd45swZDB06FLVq1YKlpSX09PRgaWmJRo0aYdSoUTh9+rTC/dRZfe/58+eYPn06mjZtijJlykBPTw8WFhaoXLkyXF1dMXToUAQGBiIzMzPHvnlZfe/+/fsYPXo0vv76a5QqVQqGhoYoX748OnfujA0bNuQ6dLVFixZiW1J79+5Fx44dUa5cORgYGMDOzg49evQotGSzQgIVG/Hx8QIAIT4+XtuhEJEGubi4iNcrVKggt+3rr79Wu546deqI1wcPHiz89ttvQmxsrLBw4ULB09Mzz3ElJSUJ169fF8LDw4WXL1/meX8iIio8Bflc+OHDB+HWrVvChw8f1N4HX+ClqCFrxKCgzlewM2fOiGXbtm2rsExkZKRYxt7evsDx2dvbi/VFRkaqLNunTx+x7Jw5c3KtOz+x/vvvv+I+bm5uau2jjpCQEKFcuXJyj4eii+znN1lJSUlC9+7dc92/e/fuQlJSktI4ZI8tPDxcqFixYo46sh/3kydPBFdX11zb7tGjh8q21REaGqr2+U9LS5Nr/9mzZ+I2b29v8f67d+8KPXr0UBiz7HMuIyNDGD9+vFCiRAmVx1m+fHnh0qVLKmN78+aN0KlTp1zPGQDh6tWrOfbP7RwEBgYKZmZmatX/6tWrHPvLnh9V/3czZ87M9Xw4OzsLDx48UFqHm5ubWPbDhw9Cz549VdY3b948lec2N+q+F3GicyKiQibIzBuRfWld2W15qScsLAzh4eHQ1dXF6NGjsXHjRrXruXr1Knx8fKCrqws/Pz9MmjQJx48fR+nSpXHw4EHUrl1b7bqIiIhI82QnAm/durUWI1Hs5s2b4vWKFSsWShtVqlQRr589exZhYWE5FnrJK39/f3h5eSE9PR1A1jybXl5eqF69OgwMDBAbG4uwsDAEBAQo/IyWkZEBDw8PnDhxAgBgamoKHx8fNGjQADo6OggLC8P69euRmJiIPXv24O3btzh69Ch0dXWVxvTmzRt07doV0dHRaNOmDTp37gwbGxs8f/5cXC0ZAKKjo9GoUSPExsYCAOrUqQNPT09UqVIFOjo6uHv3Lvz8/PDo0SPs3r0bSUlJCAwMLNCUD+qSjRMALCwsFJYbOXIkDh06BHt7ewwcOBA1atRASkoKwsLCYGBgIJbz9vbG5s2bAWQNZ+3duzfq1asHc3NzvHz5EgEBATh06BCePn2Kli1b4tKlSwpXon779i1cXV3F3nzGxsbo1asXXF1dUapUKbx//x43btxAUFAQbt++nedJzWNiYtCrVy9xIn43Nzd07NgRtra2MDAwwOvXr3Hjxg0cO3asQD0KJ0+eLE40L5FI0KNHD7Rt2xZmZma4e/cu1q9fj8ePH+PmzZto2rQpwsPDYWdnp7LO77//Hrt27cJXX32Fvn37onLlykhKSsKBAwfE1bh/++03uLq6olmzZvmOXS0FSn2RRrGnFNHnqW3btgr/r2NiYoSGDRuqXU+NGjWE69evC9euXcvRwyovPa6aN28u7N27V1i/fr1QsWJFwc/PTxAEQdi7d6/Qpk0bteshIqLCw55SX1ZPqczMTCEuLk44ceKE0KtXL7FczZo1lfZ40VZPqb1794rldHV1hUePHuVad35izczMFJycnMT9TE1NhV9//VU4d+6ckJaWpuZR/ScqKkquR8v06dOF9PR0hWU/fPggHDx4MMf98+bNE/d3cHBQeOyPHj2SO5dz585V2Ibs80FXV1fYvHmz0tgzMzPFHlK6urrCqlWrFJZLSUmR68W2evVqpXXmJi89pf755x+xrKOjo9w22Z5AAARPT0+Vr00rVqwQy3bu3Fl49+6dwnK7d+8Wew41bdpUYZnOnTuLdTVu3FiIiYlR2u6ZM2eE2NjYHPerOgfz588Xty9dulRp3YIgCOfPn1d43Ln1lDp//rygo6MjABAMDQ2FQ4cO5SiTmJgotG/fXqynQ4cOCmOQ7SkFQBg9erSQkZGRo9zMmTPlHoP8Uve9iEmpYoRJKaIvS1xcnPD48WO1y9vb2wsODg7iJTo6WqxHdmhfbjQ1nJCIiAoPk1Kfd1Iqt4udnZ0wfPhwlY+/bKInL5epU6cqrE9VUiolJUW4efOmMGnSJMHAwEAsN2DAALWOPb8JtHPnzgnGxsY5jkFfX1+oX7++8OOPPwobN24Unj59mmtdP/30k7j/0KFD1Y5BKi0tTbC1tRUACBKJRDh//rzKuCUSiQBAsLGxEVJTU3OUkT2eESNGqGx73759YtmZM2eqLJuamio4ODgIAIQaNWqodWyKqJuUun79ulC6dGmx7G+//Sa3XTbpUq5cOeH9+/dK60pJSRHKli0rABCcnJwUnjdZEyZMEOvO/nicP39e3Fa+fHnh7du3uR+0AqrOwY8//ihuz+9wydySUrJDRVUNp4uLixOfn4DioYiySSk3NzchMzNTYV3p6eniEFdDQ0Ph48eP+To2dd+LONE5EZGWWFhY5KnLe1RUFCIjI8VL+fLl8e7dO+jr62P37t1q1yMI/3VNLshwQiIiIiocenp6MDMz09r7sqOjozghskQigaGhIZydnTFr1iykpqYCANzd3bFixYpCjaNx48YICwvL8XklLS0Nly5dwsqVK+Ht7Y2KFSuiTZs2OHPmjMJ6MjIysHXrVgCAgYEBpk2bludYzp49i+fPnwPImjC6UaNGKuOWxvzixQulcUkNHz5c5XbpNA0GBga5ltXX10ffvn0BZE0Wn5fFcJR5/fo1/P39xcvevXuxdu1acejimzdvAAD29vbw9fVVWs93330HU1NTpduDg4PF4YkjR46Evr6+yri8vb3F64cPH5bbtmnTJvG6r68vSpUqpbKu/DAxMRGvX758WeP1p6amIiAgAEDWUNGhQ4cqLWthYSG3fc+ePSrrHjVqlNKhnbq6uuLzNyUlBQ8fPsxr6HnCOaWIiD4R165dg7e3tzgXlK+vL0JDQ1GmTBnxDUsdNjY2SEhIgLm5udxcVLGxsSqXWiYiIiLN2Lt3b477kpOTERUVhf379+PChQv4/fffsWXLFhw9ehSVK1dWWZ+VlZXcPFSq1KhRI18xS5mammLt2rXo2bMndHQKv4+Ds7MzQkJCcPPmTezevRunTp3CxYsXER8fL5bJzMzE0aNHcezYMcyYMQOTJk2Sq+P69eviKoZNmjSBlZVVnuO4cOGCeL1t27a5lm/Xrh1CQkIAAOfPn8+RWJOys7NDpUqVVNYlXQnNxsZGrFOVd+/eiddv3bpV4Hm/bt68iW7duqks4+Ligh07dsDS0lJpmW+++UZlHbIrvr1//16c20iZjx8/itdv3bolt+3UqVPi9a5du6qsJ7/atm2LRYsWAQC6d++O3377DV5eXrC3t9dI/deuXROTwE2bNpVLginSrl07TJkyBUDWc04VV1dXldvLly8vXpd9PhUGJqWIiD4Rw4cPx7Rp0xAXFwcPDw/MmjULAQEB8Pf3x5gxYxAcHKxWPdl/SZIyNjbGzp07NRkyERERKeDp6al024QJE7B06VKMGDECUVFR8PT0RHh4OPT09JTuY2xsrLLOvFq5ciWsra0BZH3xf/r0KYKCghAcHIzExETMmDEDTZo0kfviWticnZ3h7OwMIKtn96NHj3D+/HkEBgZi165dSEtLgyAImDx5MipVqoR+/fqJ+z59+lS8XrNmzXy1L+3BA0DhpNrZyZaR3Te73M5hUlISXr9+DQB48uRJrsmh7N6+fZun8uqQSCQwNTWFra0t6tati549e8LT0xMlSqhOL+R2rFFRUeL1sWPH5imm7McpfcxNTEwKbTL+du3aYeDAgfDz88Pr16/x66+/4tdff4WjoyNcXV3RvHlzeHh4oEKFCvmqv7CecwBQpkwZldtlJ55PSUnJte2C4PA9IqJPREJCAjw9PeHj4wNBEDBgwAAAWR9ss696kh95HU5IREREhWP48OFwc3MDANy4cQO7du0q0vbbtm0LT09PeHp6wsvLC6NGjcLhw4exa9cu6Ojo4ObNm2jXrh2Sk5OLNC4piUSCypUro3///tiyZQvu3Lkj94V86tSpcuWlvaQAqBw+psr79+/F67n1WMnejuy+2RkZGamsJy4uLvfgVEhLSyvQ/kDWqnKCzJRsmZmZSEhIwL1797B9+3b07Nkz14QUULjHmv04pY95fh9vdW3YsAEbNmyQW706MjISW7duxU8//QR7e3t07NgRd+/ezXPdhfWcA1AkvRzVVXwiISIilTgXFBER0Zejffv24vUjR45oMZL/9OjRA+PGjQOQNVxq/PjxWo4oi6OjIzZs2CDefvDggVyvG3Nzc/F6YmJivtowMzMTryclJeVaXrYd2X3zSjbRULdu3TzP6e/j45Pvtoua7LFev349T8d5/Phxubqkj3l+H291SSQSeHt749q1a4iKisLmzZsxbNgwuV59gYGBaNCgASIiIvJUt7aec0WNSSkiok+EdC4oAJwLioiI6DNXunRp8fqzZ8+0GIm8SZMmoWzZsgCA5cuX56sHSGFo3LixXFJDdviS7LCx7HMPqUt6zABw//79XMvfu3dPvG5nZ5evNoGsnuzS45Idhvg5kn2coqOjNVJXUlKSRiZ7V4e9vT369++Pv/76Czdu3MCtW7fEHo/v37/HhAkT8lSftp5zRY1JKSKiT8Thw4flfumT4lxQREREnx/pPEKAekN3ioqRkZHYQyo9PT3HpOLaIpFI5IaQySaoateuDQsLCwBZq+i9evUqz/XLrranzjyesnN4qlqpTx3SxMbLly8LZZW34kJ6nABw6NChAtXVvHlz8fq+ffsKVFd+OTk5Yffu3eJQOdnJ19Xh4uIizu10+vTpXIfLavI5V5SYlCIi+sRxLigiIqLPT2BgoHg9v5NzF5bBgwfDxsYGALB7925cv35d423ExcXlaT6kEydOiHMSGRkZya1YqKuri/79+wMAUlNTMW3atDzH06RJE7HnSmhoKMLCwpSWDQsLQ2hoKADA1tYWTZs2zXN7sry9vcXrkyZN+mynbejQoYO4MuK6devw4MGDfNclnXsVAObNm1foK8gpU7p0afFH5fT09Dztq6+vj06dOgHIGpq3fPlypWUTEhLwzz//iLd79OiRj2i1g0kpIiIiIvokpaam5jqZK9GnaNGiRTh9+jSArAmJ+/Tpo+WI5BkZGWH06NEAsubMyT6xuCacP38ejo6OmD9/fq4riV27dg0DBw4Ub/fo0QPGxsZyZX777TcxObB8+XLMmDEDGRkZCutLTU3N0VNHT09P7pj79OkjN2+VVFRUFPr06SMmjkaPHg19fX3VB5uLnj17ij1fgoKCMHDgQJVzJWVkZCAoKAizZs0qULtFzcTEREwYJicno127drhy5YrKfR48eIDRo0fnWPSnYcOG6Nq1K4CsYY8eHh4qn0fnz5/H8+fP8xTv9OnTcfjwYWRmZiots23bNjFZ6uLikqf6AeDXX38Ve1pNnjxZ4SraycnJ6Nevn3h8Hh4echOvF3e5T5FPRERERFTM7NixAzNmzEB6ejoGDhyY57k6iLTJ398/x30fPnxAVFQU9u3bhwsXLoj3jxkzBl999ZXK+pKTkxXWqUzLli3F4Wz5NXToUMydOxdv376Fv78/wsPDUbdu3QLVmV1MTAx8fX0xbtw4NGrUCK6urqhWrRosLS2Rnp6O6OhonDhxAocPHxYTTOXLl8e8efNy1FWxYkVs3LgRXl5eSE9Px9SpU7FlyxZ4eXmhRo0a0NfXx4sXL3Dp0iUcPHgQFSpUQIcOHeTqGDVqFA4ePIgTJ04gMjIStWrVwqBBg9CwYUNIJBKEhYVh/fr1YrK8RYsWYiKrICQSCXbv3g1XV1dER0dj8+bNCAgIgJeXF+rVqwdLS0ukpKQgJiYG165dw5EjR/Dq1Su4u7sXm+GV6ho6dCguX76MdevW4dGjR6hXrx7atWsHd3d3lC9fHhKJBG/fvsXt27dx6tQpXL16FQAUnud169ahcePGuH//Ps6fP48qVaqgd+/ecHV1RalSpfD+/Xvcvn0bQUFBiIiIwJUrV2Bra6t2rKGhoZg2bRqsra3Rrl07uLi4wNbWFjo6OoiNjcXhw4flFinIz/tUo0aNMGHCBMyaNQspKSno0KEDevbsibZt28LMzAz37t3DunXrxASpjY0NVq9ened2tEqgYiM+Pl4AIMTHx2s7FCIiIqJi5dGjR3K3e/bsKWRkZAjp6emCs7OzlqIqPAX5XPjhwwfh1q1bwocPH9TeJ89Len0Gl6IGIE8XPT09Ydq0aUJmZqbC+iIjI/Ncp/Ry5cqVHPXZ29uL2yMjI9U6punTp4v7dOzYUWk52Vjt7e3Vqvvy5cuCnZ1dno6rVatWwuPHj1XWGxwcLNja2uZaV506dRTun5iYKHTr1i3X/bt16yYkJSUpjUNazs3NTa3zIQiC8OLFC6FDhw5qnw9vb2+1684uNDQ0XzFm5+3tnefnVWZmpjB37lzB2NhYreMsU6aM8OrVK4V1vX79WmjXrp1a9Vy7di3H/qrOQYsWLdSq18TERFi3bl2Bzs+MGTOEEiVKqGynZs2awoMHD5TW4ebmJpbNzdSpU8WyoaGhuZZXRN33IvaUIiIiIqJib9iwYWjVqhVGjx4NHR0dmJmZYfPmzZBIJMVqEuhP1ec5Q82nxcDAACVLloSTkxPc3Nzg4+MDBwcHbYel0vDhw7Fw4UIkJCQgICAAYWFhaNiwoUbqrlu3Lp4+fYqLFy/i+PHjOH/+PO7evYtnz54hMTERenp6sLCwQNWqVVG/fn307NlTrbmb2rRpg0ePHmH9+vU4cOAArl+/jtevX0MikcDGxga1atVCmzZt0K9fP4X7m5iYYM+ePQgJCcHGjRtx+vRpcdiXjY0NmjVrBm9vb7i7u2vkPMiytrZGYGAgzp8/jy1btuD06dOIjo5GXFwcDA0NYWtrCycnJzRr1gydOnWCs7OzxmMoChKJBL6+vhg0aBDWrVuHo0eP4tatW3jz5g0AoGTJkqhSpQrq16+PNm3aoG3bttDT01NYV+nSpREUFISQkBDxnMXGxuLDhw+wsLBAlSpV0KxZM/Tq1SvPQ94OHDiAo0eP4sSJEwgPD8eDBw/w+vVrCIKAkiVLokaNGmjdujUGDx5c4NXwJk+ejN69e2PFihU4evQonjx5gg8fPqBMmTKoU6cOevbsiW+//VZusv9PhUQQPtNZ0j5BCQkJsLCwQHx8vMIVtoiIiIi+ZGvWrMH27duxYMECVKxYEYsXL0ZSUhKGDRuGSpUqaTs8jSrI58KUlBRERkbC0dERhoaGhRQhERGRcuq+F316aTQiIiIi+iINHjwYnTp1wqhRo1CxYkVMnz6dSRciIqJPGFffIyIiIqJi7/3791i+fDkOHjyI9evXo0mTJmjXrh1CQkK0HRoRERHlE5NSRESFQKKBCxER/adbt2548+YNHjx4AG9vb3Tt2hUHDhzA7t278f3332s7PCIiIsoHDt8jIiIiomLv5cuXmDx5MgRBEJedNzc3x99//42zZ89qOToiIiLKDyaliIiIiKjYa9KkCVq3bo3U1FR06tQpxzYiIiL69DApRUT0RdDEgEAu1kpE2rNixQrcvHkTenp6qFatmrbDISIiIg3gnFJERERE9ElwdnaWS0itXLlSi9EQERFRQbGnFBEREREVe/v3789x39SpU1G2bFkAQJcuXYo6JCIiIiogJqWIiIiIqNjz9PSEq6sr9PX1xfvi4+Px559/QiKRMClFRET0CWJSioioOFuogbmgxnAuKCL69K1duxZr1qzBokWLUKdOHQCAo6MjQkNDtRwZERER5RfnlCIiIiKiYm/QoEHYunUrfH19MWPGDGRkZEAiKXjiPjQ0FF999RUqVaqEDRs2FDxQIiIiUhuTUkRERET0SbC3t0dwcDBMTEzwzTffIDU1Nc91xMfHy91evnw5Lly4gIiICMyfP19ToRIREZEamJQiIiIi+n/sNVP8SSQSjBkzBqtXr8bPP/+c5/27dOmCXbt2ibf19fURHh6O8PBw6OnpaTJUIiIiygWTUkRERPTFYq+ZT8fVq1fh4uKCunXr4ubNm/D19cXMmTNRsWJFXL9+Xe16goODcfXqVXh5eSEmJgazZ8/G6tWrsWTJEqxevboQj4CIiIiy40TnRERE9MXq0qULfvnlF/Ts2RPAf71mALDXTDEzYsQITJs2DXFxcfDw8MCsWbMQEBAAf39/jB07FsHBwWrVY2BggFmzZiEiIgLe3t7o1q0b/Pz8Cjl6IiIiUoQ9pYiIiOiLxV4zn46EhAR4enrCx8cHgiBgwIABAABPT0+8fPlS7XoyMzMRGBiIFy9eICgoCB8/fkT79u1x586dwgqdiIiIlGBPKSIiIvpisdfMp0MQBPF6y5YtlW7LTa9evWBhYYHk5GTs2bMHy5cvh6enJ0aOHAkXFxdMnTpVYzETERGRauwpRURERF8s9pr5dNjY2CAhIQEAsHHjRvH+2NhYGBoaql3PgwcPsHbtWmzbtg1nz54FkLWq3969e1GtWjXNBk1EREQqsacUERERfbHYa+bTcfjwYYX3GxsbY+fOnWrX4+joiMGDByM5ORkNGzaU29a3b98CxUhERER5w6QUERERfbEePHiAq1evAgBcXFwA/NdrZtu2bdoLjNRmYWEBCwsLtcvv3LkThw8fhp6eHtq0aVOIkREREVFuOHyPiIiIvljSXjP9+vVjr5kvRIkSJdCxY0e0bdsWEokEAPDu3TstR0VERJ+i48ePQyKRQCKRYNq0aQrLtGjRQixDObGnFBWZ1NRUpKWlwczMTNuhEBERAWCvmS/R1atX4ePjAx0dHWzatAm+vr4IDQ1FmTJlcPDgQdSuXVvbIWrHwi/wy9IY9SfI14TcvpCamJjA2toatWvXhqenJ/r27QsDAwOl5aOiouDo6JjnOCwsLBAXF5fjfgcHBzx+/FjhPoaGhihZsiRq1KiB5s2bw8fHJ0fbmvrC7ebmhuPHj8vdl5aWBn9/f+zduxeXL1/GixcvkJSUBCMjI9jY2KBy5cpwcXFBkyZN0LJlS5ibm2skli/Z8ePHcywqISWRSGBmZgZra2t8/fXX6NKlC7y8vGBkZFTEUdLngD2lqEjs2LED9erVQ4MGDTB79mxth0NERASAvWa+RCNGjMC0adMwfPhweHh4oE+fPkhOTsbSpUsxduxYbYdHX7CkpCRERkZi3759GDRoEJydncXhxdqWkpKC58+f4/jx45gxYwZq1KiBOXPmFEnbYWFhqF27Nnr37o3t27fj/v37SEhIQEZGBhITE/Hw4UMEBwdj3rx58PT0RNWqVYskri+ZIAhISEjAgwcPsHv3bnh7e+Orr77CxYsXtR0afYLYU4oKRWRkpNyvJzt37sT169chCAK+/vprTJgwQYvRERERZWGvGe3QZu/phIQEeHp6AgCmTJmCAQMGAAA8PT2VDr0g0rS9e/fK3RYEAXFxcbh27Rq2bt2KV69e4eHDh3B3d8etW7dgY2Ojsj4rKyusWrVKrbb19PRyLbNy5UpYW1uLt1NSUvDw4UPs2LED169fR1paGsaPHw8zMzMMGzZM4THJevnyJX788Ue1Yi1Tpox4/fLly3B3d0diYiIAoGzZsujRowdq166NUqVK4cOHD3j69CkuX76MY8eOIS4uDhkZGbkeH+WNs7MzZs2aJd6WPl8vXbqELVu2ID4+Ho8ePUL79u1x9epVVKhQQYvR0qeGSSkqFMOGDUOrVq0wevRo6OjowMzMDJs3b4ZEIoGJiYm2wyMiIgLwX6+ZuLg4eHh4YNasWQgICIC/vz/Gjh2L4OBgbYf42dmxYwdmzJiB9PR0DBw4sMh/qBKE/4ZsZR+aIruNqDBJE6OKTJkyBS1atEBERATevn2LRYsWYe7cuSrrMzY2VllnXrVt2xYODg457h8/fjxGjRqFpUuXAgAmT56MQYMG5dp+VFRUvmL93//+JyakvL29sWLFChgaGiosm56ejqNHj2LHjh1q1U3qK1OmjMLHbNCgQZg4cSJatGiB+/fv4+3bt5g1axZWrlxZ9EHSJ4vD96hQBAYGomTJkmjbti2uXr2KBQsWiCsccTUjIiIqLqS9Znx8fCAIglyvmZcvX2o5us9DZGSk3G1p7+mbN29i69atRR6PjY0NEhISAAAbN24U74+NjVX6ZZeoKFlaWmLGjBni7ezzK2mTjo4O5s+fD1tbWwBZw51PnjxZKG3dunUL4eHhAIAKFSpg9erVKv9HS5Qogfbt22PdunWFEg8pZmdnhwULFoi39+3bp8Vo6FPEpBQVmsGDB2Pz5s2YO3cu5s6diwkTJmDhwoWoVKlSgepNTU3F+/fvNRQlERF9ydhrpvANGzYMCxYsQGZmJgCIvae3bt2qld7Thw8fzjEJ8rt372BsbIydO3cWeTxEitSsWVO8Lk2iFhf6+vpo3LixePvevXuF0s6dO3fE666urmoNO1RXamoq1qxZg+7du8PBwQEmJiYwMDBAhQoV4OHhgQULFiAmJkZlHSEhIfDx8UGVKlVgamoKExMTVKlSBd7e3jh27FiuMUhXY2vRogUAIC4uDnPnzkXjxo1hbW0NHR0dcVt2Bw4cwMCBA1GlShWYmZnB2NgYjo6O+Pbbb3H06NG8no4Cc3NzE6+/ePEi1+fsw4cPMW7cODRo0ABWVlbQ19eHjY0NWrVqhSVLliA5OVntts+cOYOhQ4eiVq1asLS0hJ6eHiwtLdGoUSOMGjUKp0+fVrhfeno6jhw5Al9fX7i5uaFs2bLQ19eHiYkJHBwc4OXlhV27donvXVR4OHyPCsX79++xadMm6OvrY/369Th8+DDatWuHqVOnolWrVvmuV9td/omI6PMi7TVjbm7OXjOFJDAwEGvWrEHbtm2xYMECLFiwAIsXL0ZSUpJWek9fu3YN3t7eCucRCwgIKPJ4iBR5/fq1eL1ixYpajEQx2dfHDx8+FEob6enp4vUXL15orN7Q0FAMGDAAz549y7Ht6dOnePr0KQ4dOoQtW7bgypUrOcokJydjwIAB2LNnT45tDx8+xMOHD+Hn54fu3btj06ZNMDY2zjWmK1euwNPTE0+ePFFZLjo6Gr1798a5c+dybIuKikJUVBS2bNmCHj16wM/PT622NSH7++WHDx8UroCYmZmJSZMmYf78+XKPL5A179jLly8RGhqKBQsWwN/fH/Xq1VPa5tu3b+Ht7Y2DBw/m2Pbu3TuEhYUhLCwMixcvxtWrV/H111/LlWnbti1CQ0Nz7Pvx40c8fvwYjx8/xq5du9CkSRPs2bMn13ndKP+YlKJC0a1bN7i5uSEpKQne3t74999/0bJlS4wfPx5btmzB2rVr1aqHE6YTEVFhOnz4cI77NNVrRpuTeRc3gwcPRqdOnTBq1ChUrFgR06dP11rSb/jw4UrnERszZgznEaNiQXYi8NatW2sxEsVu3rwpXi+spFmVKlXE62fPnkVYWBgaNmxYoDr9/f3h5eUlJkSqVasGLy8vVK9eHQYGBoiNjUVYWBgCAgIU9pbNyMiAh4cHTpw4AQAwNTWFj48PGjRoAB0dHYSFhWH9+vVITEzEnj178PbtWxw9ehS6urpKY3rz5g26du2K6OhotGnTBp07d4aNjQ2eP38uN4w8OjoajRo1QmxsLACgTp068PT0RJUqVaCjo4O7d+/Cz88Pjx49wu7du5GUlITAwEBxZdnCJPt8MDQ0lJskX5a3tzc2b94MIGuYau/evVGvXj2Ym5vj5cuXCAgIwKFDh/D06VO0bNkSly5dQrVq1XLU8/btW7i6uoq99IyNjdGrVy+4urqiVKlSeP/+PW7cuIGgoCDcvn1b4WOZnJwMExMTtGjRAvXq1YOjoyPMzMyQlJSE27dvY+fOnXj48CHOnj2Lbt264eTJkyhRgumTwsCzSoXi5cuXmDx5MgRBQN26dQEA5ubm+Pvvv3H27Fm16+GE6UREVJgKq9cMe/b+p7B6T+cXV9+j4kgQBCQkJODatWv4+++/xcm6a9asKa5uV1z4+/sjIiICAKCrqys3lE+T6tSpAycnJ9y+fRsfP36Eu7s7hgwZgu7du6NevXp5Hs73+PFjDBw4UExITZ8+HRMnTlSYMEpJSVE4BG/RokViQsrBwQEhISFyP6B/++23GDVqFFq2bInHjx/j+PHjWLhwIXx9fZXGdePGDejq6mLz5s3o37+/wjKCIKB3796IjY2Frq4u/vnnH/zwww85yo0bNw4+Pj7Yvn07goKCsHbtWgwePFj1iSkgQRAwc+ZM8XaTJk0UJsJWrlwpJqQ6d+4MPz8/lCxZUq7MsGHDsGfPHvTu3Rvv37/Hd999p3D4nY+Pj5iQaty4Mfbs2YOyZcvmKLdo0SKcPXtWnANN1qxZs9CkSROlvclmzpyJkSNH4u+//8a5c+ewfft2fPvtt8pPBOUb55SiQtGkSRO0bt0azZs3R6dOnXJsUxcnTCciosIk7TUzfPhweHh4oE+fPkhOTsbSpUsxZswYtespbpN5FyfdunXDmzdv8ODBA3h7e6Nr1644cOAAdu/eje+//77I4+E8YlQcSOcTkl50dHRQsmRJuLm5YceOHbCzs8Pw4cNx7tw5tYZgPX78OEedyi75Sb6mpqbi1q1bmDx5Mvr06SPe369fP7mkjCZJJBKsW7dOPP7ExETMnz8frq6uMDU1RYMGDfDTTz/Bz89P4VC87ObMmSPOSzt06FBMmTJFaQ8mQ0NDdOzYUe6+jx8/YtGiRWJs27dvV3jsjo6O2L59u5iYWbRoEdLS0lTG9vPPPytNSAFZc0hJh+xNmzZNYUIKAAwMDLBx40Zx5cSFCxeqbDe/BEFAXFwcjh49ig4dOsDf31/cpuhHmNTUVEyfPh0A4OTkhF27duVISEl1795dTOKdOXMGFy5ckNt+4cIFHDhwAABQvnx5BAYGKkxISTVp0kRhUqp169Yq/7dKlCiBxYsXi+dSdog/aRZ7SlGhWLFiBW7evAk9PT2FXS7zojh1+Scios+LpnrNsGevcprqPa0pnEeMPgV6enowMzPTWqJUnUSTu7s7VqxYUahxNG7cGGFhYfjll1/k5v9JS0vDpUuXcOnSJaxcuRI6Ojpo1aoVpk2bhqZNm+aoJyMjQ/yBwMDAIF+JubNnz+L58+cAgBYtWqBRo0Yq427ZsiVCQkLw4sULnDlzJkcSXNbw4cNVti19rTIwMMi1rL6+Pvr27Ys//vgDd+7cwZMnTwo8xPLEiRO5DgOUSCT4+++/4e7unmNbcHCwOOxw5MiR0NfXV1mXt7c3Zs+eDSBrmL3sud60aZN43dfXF6VKlVL7OPKqRIkSaNy4MaKiohAWFgZBEIpkOOSXhkkpylV+58RwdnYucNvFrcs/ERF9XjTVa6a4TeZdnEh7T6emphao97SmKJpHDABX36MitXfv3hz3JScnIyoqCvv378eFCxfw+++/Y8uWLTh69CgqV66ssj4rKyu5eahUqVGjRr5iljI1NcXatWvRs2dP6OgU/sAbZ2dnhISE4ObNm9i9ezdOnTqFixcvIj4+XiyTmZmJo0eP4tixY5gxYwYmTZokV8f169fFFeGaNGkCKyurPMch22Onbdu2uZZv164dQkJCAADnz59XmpSys7PLdXXykydPAshKqkvrVOXdu3fi9Vu3bhX6ZPlNmjTB2rVrlT63pPEDWd/vZHtWKfLx40fx+q1bt+S2nTp1SrzetWvXfET7n+TkZPz77784cOAAIiIi8OLFCyQmJip8/09ISEBCQgIsLCwK1CblxKQUqZTfOTEePnyIwYMH4/Hjx/D09MTs2bPFXx9dXV0VrhihiKYmTCciIlJEk71m2LNXMU32ni5MFhYW/LJBRUbaQ1ORCRMmYOnSpRgxYgSioqLg6emJ8PBwlXMoGRsbq6wzr1auXClOVv3x40c8ffoUQUFBCA4ORmJiImbMmIEmTZqgfPnyGmszN87OzuKP3oIg4NGjRzh//jwCAwOxa9cupKWlQRAETJ48GZUqVUK/fv3EfZ8+fSper1mzZr7al/b0AaDWa5lsGdl9s8vtHCYlJYmrMT558gTdunXLtW1Zb9++zVN5RZydnTFr1izxdkJCAh4+fIgNGzbgyZMnOHv2LFauXIlFixYp7EkUFRUlXh87dmye2s4ev/SxNDExKVCy7ezZs+jTpw+io6PV3odJqcLBpBTJ0dRqd0OHDkXPnj3RuHFjLFmyBO7u7ggKCoKZmRlSUlLUjqe4dfknIqLPi6Z6zbBnr2qa6D1N9CUZPnw49uzZgxMnTuDGjRvYtWsX+vbtW2Ttt23bVpxLR2rUqFHYvXs3evXqhZs3b6Jdu3a4ePGiWnNeaZpEIkHlypVRuXJl9O/fH7NmzUL79u3Fya+nTp0ql5SS9pICsnp65Yd0PioAag3Llm1Hdt/sjIyMVNYTFxeXe3Aq5DaflTrKlCmjMOk5adIk9O/fHzt37sTixYtRsmRJTJ06NUe5ghxD9vilj2V+H0cg6ztvu3btkJiYCCBrpcf27dujWrVqKFOmDAwNDcXk2tKlS8WhoxkZGfluk5RjUorkaGpOjJcvX4orhfj5+WH27Nlwd3fHkSNH8jQOt7h1+Scioi9DXnvNsGevcgXqPb2wgHN3jOHE5fTpat++vbjS25EjR4o0KaVMjx49MG7cOMyePRu3bt3C+PHjsWTJEm2HBUdHR2zYsEH8fvDgwQNERUWJiTVzc3OxrDQRkVeyU5kkJSXlWl62nbxOgyJLNvlSt25dXL58Od91aZqenh42btyIK1eu4MGDB5g5cyY6duyI+vXry5WTPYbr16+jVq1a+W7T3Nwcb9++zffjCACzZ88W9//tt9/wxx9/KP2OumXLlny3Q+rh6nskR1Or3X348EHu9oQJE9CrVy+4u7ur/KUguxUrVmDJkiVYu3at3FKjRERExYm0Z+8ff/wh/lIv7dmrjRXmihNp7+mdO3fi9evXcp8F8tJ7muhLU7p0afG6OqvLFZVJkyaJq50tX74cd+/e1XJEWRo3biyX/JAdMic7RC77HEXqkl3h7f79+7mWl74XAFnzRuWXhYWFeFyywxCLCyMjIyxYsABAVk+i0aNH5ygje/7zMlxOEWldSUlJePLkSb7qCA4OBgBYW1vj999/V9lpIvvquqR5TEpRDoMHD8bmzZsxd+5czJ07FxMmTMDChQtznYBPlpOTE4KCguTuGzt2LPr164eHDx/mKR5nZ+diPQcFERGRtGdv8+bN2bM3G2nv6Xr16sHPzw8dO3aEu7s74uPjuYoRkQrSeYQA9YaLFRUjIyOMHz8eAJCenp5jUnFtkUgkKFHiv4FAsgmq2rVri71fz549i1evXuW5ftkV4KRJDVVkh4erWqlPHW5ubgCyXk+LU08pqa5du4pTrZw6dQoBAQFy26XxA8ChQ4cK1Fbz5s3F6/v27ctXHdJVFB0dHaGrq6u0XGxsLK5du5avNkh9TEqRnPfv32P58uU4ePAg1q9fjyZNmsitHKGu7du355hD4927dxg9enSesuMPHz5Ey5YtUalSJYwePVruF1VXV9c8xURERFRY2LNXOU31nib60gQGBorX8zs5d2EZPHgwbGxsAAC7d+/G9evXNd5GXFxcnuZDOnHihDh3kZGRkdyKhbq6uujfvz+ArJXFp02blud4mjRpIvaWCg0NRVhYmNKyYWFh4jxEtra2aNq0aZ7bk+Xt7S1enzRpUp5Why0qEydOFK9nn1eqQ4cO4oqH69atw4MHD/LdzoABA8Tr8+bNk1tpUF3SJO/Dhw9Vnkvpgl9UuJiUIjndunXDmzdv8ODBA3h7e6Nr1644cOAAdu/enafhB3fu3EHDhg1Rt25d3Lx5Ex07dkS5cuVQsWLFPK0AwS7/RET0qWDPXsU02Xua6EuxaNEinD59GgCgo6ODPn36aDkieUZGRuIwLUEQFE5uXVDnz5+Ho6Mj5s+fr3L1OgC4du0aBg4cKN7u0aNHjgnYf/vtN3FuqeXLl2PGjBlKJ65OTU3N0aNHT09P7pj79Okjt6qcVFRUFPr06SMmO0aPHg19fX3VB5uLnj17ir2tgoKCMHDgQJVzKmVkZCAoKEhuxbzC1q1bN3FRi8uXL8v1YjIxMRETgcnJyWjXrh2uXLmisr4HDx5g9OjRePnypdz9DRs2RNeuXQFkDWf08PBQ+fw4f/682DNKqkGDBgCyeiMuXLhQ4X4LFy7EihUrVMZImiERimOa9QslXWIyPj5ebjK+olS7dm1xtb26devKvVicPXtW7SEIbm5uGDVqFOLi4jB16lTMmjULAwYMgL+/P5YvX65Wl1cAqFOnjlwMs2fPhr+/P44cOYKWLVsiPDw8bwdIVEQ0MSBFAAo+yS/w/xP9aiwiIlKgQJN5f+ZSU1MhkUjkvpS9e/cOpUqVwrNnz1CuXDnlO3/BE50X5HNhSkqKuKKy9HmYK02833xqivj5ITtcde/evTm2f/jwAVFRUdi3bx8uXLgg3v/rr79i3rx5OcpHRUWJq2ZbWVlh1apVasfSsmXLHIs5ODg44PHjxwCy5tHJvvpedomJibC3txd/cL58+bI4hEtVrPb29gqTOdkFBQWhQ4cOALISc40aNYKrqyuqVasGS0tLpKenIzo6GidOnMDhw4fFBFP58uURFhYmNweUlL+/P7y8vMTeL9WqVYOXlxdq1KgBfX19vHjxApcuXcLBgwdRoUIFXL16VW7/jIwMuLu7ixPQm5qaYtCgQWjYsCEkEgnCwsKwfv168Uf0Fi1a4OjRowqHiEmfD25ubjh+/Hiu5+PZs2dwdXUVR52UKlUKXl5eqFevHiwtLZGSkoKYmBhcu3YNR44cwatXr+Du7o6jR4/mWrcix48fR8uWLfMU49atW8UeaV9//TWuXLki97z//vvvsW7dOgBZx9+uXTu4u7ujfPnykEgkePv2LW7fvo1Tp06J5z46OlpuTioAePv2LRo3bizO7WVsbIzevXvD1dUVpUqVwvv373H79m0EBQUhIiICV65cgYuLi7j/oUOH4OHhId728PBA+/btYWNjgydPnmDHjh24ePEiypYti1q1aonfXRX9X8iep6lTpyrshdeiRQvxOfMlpV/UfS/i6nskR1Or3SUkJIjLhk6ZMkXsZunp6Zmn7rKKuvzr6+uzyz8RERUr0p69jRs3xpIlS+Du7o6goCCYmZl98T1779y5A29vb+jo6GDTpk3w9fVFaGgoypQpg4CAANVJKaLPVLdu3XIto6enh4kTJ2LKlCm5ln316pVadUpl/5KeH6amphgxYoTYS2rKlCk4ePBggeqUZW1tDTs7O8TExCAzMxPnzp3LNcHfqlUrrF+/XmFCCsj6LhIYGIiBAwfi+fPnuHfvHn7//XeFZe3t7XPcp6uri4CAAAwYMAB79+5FYmIili1bpnD/bt26YfPmzSrnLMqLcuXK4dKlS/Dx8cGhQ4fw7t27XBOR2ZM5ha13796YNm0a7t+/j2vXrmH37t3o2bOnuH3NmjWoXr06pk+fjuTkZAQFBeXoSSurTJkyCpMZlpaWOHfuHPr374/Dhw8jOTkZ69evx/r16xXWo6MjP0CsQ4cOmDZtmvi9NDAwUG64LJD1+O/ZswdLly5V9/Apn5iUIjkrVqzAzZs3oaenV6AhCLIZYGnmWNG23Ei7/Ldv3168b+zYsdDR0cHYsWPzHR8REZEmSSfzBgA/Pz/Mnj0b7u7uOHLkyBc/mffw4cMxbdo0xMXFwcPDA7NmzUJAQAD8/f0xZswYtXtPUyH7hHuVfS4MDAxQsmRJODk5wc3NDT4+Prn2VtK24cOHY+HChUhISEBAQADCwsLQsGFDjdRdt25dPH36FBcvXsTx48dx/vx53L17F8+ePUNiYiL09PRgYWGBqlWron79+ujZs6dacze1adMGjx49wvr163HgwAFcv34dr1+/hkQigY2NDWrVqoU2bdqgX79+Cvc3MTHBnj17EBISgo0bN+L06dPi8DAbGxs0a9YM3t7ecHd318h5kGVtbY3AwECcP38eW7ZswenTpxEdHY24uDgYGhrC1tYWTk5OaNasGTp16iQOpysqurq6GDdunDjty7Rp09C9e3cxKSSRSODr64tBgwZh3bp1OHr0KG7duoU3b94AAEqWLIkqVaqgfv36aNOmDdq2bQs9PT2FbZUuXRpBQUEICQkRz0VsbCw+fPgACwsLVKlSBc2aNUOvXr1Qu3btHPtPnToVzZs3x9KlS3Hu3Dm8ffsWJUuWRKVKldCtWzf8+OOPKFmyZOGcKJLD4XvFSHEYvqcp7dq1w86dO3McR2xsLDw9PeW6JKtSoC7/RFrE4XtEX5YaNWrgzp07cvctWLAA27dvR3x8vFrLh3+uZIfiV6xYUW4JbxcXlxzDY+Rw+F7RDd8jIiLSIHXfizjROcnR1Gp3hw8fVvgBytjYGDt37lS7Hk1NmE5ERFSYOJm3cprqPU1ERESfHyalSE5hr3ZnYWGBihUrql1e2uV/+PDh8PDwQJ8+fZCcnIylS5dizJgxBY6HiIhIE7Zv345WrVrJ3ffu3TuMHj1anJT2S2VjY4OEhAQAwMaNG8X7Y2Nj2YuHiIjoC8ekFMmRzolRr149+Pn5oWPHjnB3d0d8fLxW5sSQTpju4+MDQRDkJkzPvjwoERF9uUJDQ/HVV1+hUqVK2LBhQ5G3z569ymmq9zQRERF9fjjROckpbqvdscs/EREpEh8fL7ec+fLly8X5Chs2bAgfH58ijYeTeeedhYVFjiXpiYiI6MvCnlIkp7jNicEu/0REpEiXLl2wa9cu8ba+vj7Cw8MRHh6udKWewsSevcWdpIAXIiIiKgxMSpGc4jYnBrv8ExGRIsHBwbh69Sq8vLwQExOD2bNnY/Xq1ViyZAlWr15d5PGwZy8RERFR3nH4Hsm5c+cOvL29oaOjg02bNsHX1xehoaEoU6YMAgICUK5cOW2HCIBd/omIvnQGBgaYNWsWIiIi4O3tjW7dusHPz0+9nRdqoOfLGPlEk7Rnr7m5OXv2EhEREamJPaVIDle7IyKiT0FmZiYCAwPx4sULBAUF4ePHj2jfvj3u3LmjlXjYs5eIiIgo79hTiuRI58QAgClTpsjNiTFt2jTtBUZERCSjV69esLCwQHJyMvbs2YPly5fD09MTI0eOhIuLC6ZOnartEAGwZy8RERGRKkxKkRzOiUFERJ+CBw8e4OrVqwAAFxcXAIC9vT327t2Lbdu2aS8wIiIiIlIbk1Ikh3NiEBHRp8DR0RGDBw9GcnIyGjZsKLetb9++WoqKiIiIiPKCSSmSc/jwYYX3c04MIiIqTnbu3InDhw9DT08Pbdq00XY4RERERJQPTEqRWjgnBhERFSclSpRAx44dtR0GERERERUAV98jIiKiz0q1atW0HQIRERERqYE9pYiIiOiTc/36daXb3r9/X4SREBEREVF+MSlFREREnxwXFxc4ODgoXBn2zZs3WoiIiIiIiPKKSSkiIiL65Njb2+P06dOws7PLsa1ChQpaiIiIiIiI8opJKdKchZKC1zEm5y/eRERE2XXp0gWPHj1SmJTiBOhEREREnwYmpYiIiOiTs2TJkhz3vXv3DqVKlcKKFSu0EBHJKujPVPyJioiI6MvA1feIiIjok3Pt2jW4uLigbt26uHnzJjp27Ihy5cqhYsWKiIiI0HZ4RERERKQGJqWIiIjokzN8+HBMmzYNw4cPh4eHB/r06YPk5GQsXboUY8aM0XZ4RERERKQGJqWIiIjok5OQkABPT0/4+PhAEAQMGDAAAODp6YmXL19qOTqiT5HkC7wULYlEovJiamqKSpUqwdPTExs2bEBqaqrK+qKionKtU9GlZMmSCutzcHBQuo+RkRHKli2Lli1bYurUqYiMjMzz8al7adGiRY6609LSsGPHDvTt2xfVqlWDhYUFSpQoATMzM1SpUgXt2rXDb7/9hn379iEhISE/Dw9lc/z48Tw/dtOmTVNYV1RUFHbv3o3x48ejbdu2KF26tLiPg4NDkR4XFT+cU4qKIU18SOBsFEREnzNB+O91vmXLlkq3ERF9KpKSkhAZGYnIyEjs27cPs2bNwq5du+Di4qLt0JCSkoLnz5/j+fPnOH78OObMmYPp06dj3Lhxhd52WFgYBg4ciLt37+bYlpiYiMTERDx8+BDBwcEAAGtra7x48aLQ4yL1LFu2DMOHD9d2GFSMMSlFREREnxwbGxskJCTA3NwcGzduFO+PjY2FoaFhEUXBH1GIKH/27t0rd1sQBMTFxeHatWvYunUrXr16hYcPH8Ld3R23bt2CjY2NyvqsrKywatUqtdrW09PLtczKlSthbW0t3k5JScHDhw+xY8cOXL9+HWlpaRg/fjzMzMwwbNgwhcck6+XLl/jxxx/VirVMmTLi9cuXL8Pd3R2JiYkAgLJly6JHjx6oXbs2SpUqhQ8fPuDp06e4fPkyjh07hri4OGRkZOR6fJQ3zs7OmDVrVq7latSokeO+7I+HkZERqlatiuvXr2ssPvq0MSlFREREn5zDhw8rvN/Y2Bg7d+4s4miIiPLG09NT6bYpU6agRYsWiIiIwNu3b7Fo0SLMnTtXZX3GxsYq68yrtm3bKhxWNX78eIwaNQpLly4FAEyePBmDBg3Ktf2oqKh8xfq///1PTEh5e3tjxYoVSn94SE9Px9GjR7Fjxw616ib1lSlTJt/PLwcHBwwbNgz16tVDvXr14OzsjOjoaDg6Omo2SPpkcU4pIiIi+mxYWFigYsWK2g6DiCjfLC0tMWPGDPH28ePHtRdMNjo6Opg/fz5sbW0BAO/evcPJkycLpa1bt24hPDwcAFChQgWsXr1aZU/YEiVKoH379li3bl2hxEP54+npib/++guDBg1C7dq1oaurq+2QqJhhUoqIiIiIiKgYqVmzpni9uE3cra+vj8aNG4u37927Vyjt3LlzR7zu6uqq1rBDdaWmpmLNmjXo3r07HBwcYGJiAgMDA1SoUAEeHh5YsGABYmJiVNYREhICHx8fVKlSBaampjAxMUGVKlXg7e2NY8eO5RpD9ond4+LiMHfuXDRu3BjW1tbQ0dFROOk7ABw4cAADBw5ElSpVYGZmBmNjYzg6OuLbb7/F0aNH83o6iLSKw/eIM2IQERERERUjr1+/Fq8Xx96fsj2WPnz4UChtpKeni9c1OXF5aGgoBgwYgGfPnuXY9vTpUzx9+hSHDh3Cli1bcOXKlRxlkpOTMWDAAOzZsyfHtocPH+Lhw4fw8/ND9+7dsWnTJhgbG+ca05UrV+Dp6YknT56oLBcdHY3evXvj3LlzObZFRUUhKioKW7ZsQY8ePeDn56dW20TaxqQUERERERFRMSI7EXjr1q21GIliN2/eFK8XVtKsSpUq4vWzZ88iLCwMDRs2LFCd/v7+8PLyEhNe1apVg5eXF6pXrw4DAwPExsYiLCwMAQEBCldyzcjIgIeHB06cOAEAMDU1hY+PDxo0aAAdHR2EhYVh/fr1SExMxJ49e/D27VscPXpU5ZC1N2/eoGvXroiOjkabNm3QuXNn2NjY4Pnz53j58qVYLjo6Go0aNUJsbCwAoE6dOvD09ESVKlWgo6ODu3fvws/PD48ePcLu3buRlJSEwMBASCSa6IJAVHiYlCIiIiIiItIiQRCQkJCAa9eu4e+//xYn665Zs6a4ul1x4e/vj4iICACArq6u3FA+TapTpw6cnJxw+/ZtfPz4Ee7u7hgyZAi6d++OevXq5Xk43+PHjzFw4EAxITV9+nRMnDhRYcIoJSVF4RC8RYsWiQkpBwcHhISEyE3Y/e2332LUqFFo2bIlHj9+jOPHj2PhwoXw9fVVGteNGzegq6uLzZs3o3///grLCIKA3r17IzY2Frq6uvjnn3/www8/5Cg3btw4+Pj4YPv27QgKCsLatWsxePBg1SeGSMs4pxQREREREVERks4nJL3o6OigZMmScHNzw44dO2BnZ4fhw4fj3Llzag3Bevz4cY46lV2mTZuW53hTU1Nx69YtTJ48GX369BHv79evX6GtoiaRSLBu3Trx+BMTEzF//ny4urrC1NQUDRo0wE8//QQ/Pz+FQ/GymzNnDt6/fw8AGDp0KKZMmaK0B5OhoSE6duwod9/Hjx+xaNEiMbbt27crPHZHR0ds375d7KG0aNEipKWlqYzt559/VpqQArLmkJIO2Zs2bZrChBQAGBgYYOPGjeLKiQsXLlTZrrpOnDiR6/Nq5MiRGmmLvjxMShERERERERUjenp6MDMzUziErCg4OjrKJRwMDQ3h7OyMWbNmITU1FQDg7u6OFStWFGocjRs3RlhYGFq2bCl3f1paGi5duoSVK1fC29sbFStWRJs2bXDmzBmF9WRkZGDr1q0AshI3+UnMnT17Fs+fPwcAtGjRAo0aNVIZtzTmFy9eKI1Lavjw4Sq3b9y4EUBW7LmV1dfXR9++fQFkTRaf2zxVRNrG4XtERERERERFaO/evTnuS05ORlRUFPbv348LFy7g999/x5YtW3D06FFUrlxZZX1WVlZy81CpUqNGjXzFLGVqaoq1a9eiZ8+e0NEp/D4Ozs7OCAkJwc2bN7F7926cOnUKFy9eRHx8vFgmMzMTR48exbFjxzBjxgxMmjRJro7r16+Lqxg2adIEVlZWeY7jwoUL4vW2bdvmWr5du3YICQkBAJw/fz5HYk3Kzs4OlSpVUlnXyZMnAQA2NjZinaq8e/dOvH7r1q0Cz/slTUiqkttzlEgZJqWIiIiIiIiKkKenp9JtEyZMwNKlSzFixAhERUXB09MT4eHhKudQMjY2VllnXq1cuRLW1tYAsoatPX36FEFBQQgODkZiYiJmzJiBJk2aoHz58hprMzfOzs5wdnYGkDXH0qNHj3D+/HkEBgZi165dSEtLgyAImDx5MipVqoR+/fqJ+z59+lS8XrNmzXy1L51gHMiaID03smVk980ut3OYlJQkrsb45MkTdOvWLde2Zb19+zZP5RUpU6aMRp9fRLI4fI+IiIiIiKgYGT58ONzc3ABkTYS9a9euIm2/bdu28PT0hKenJ7y8vDBq1CgcPnwYu3btgo6ODm7evIl27dohOTm5SOOSkkgkqFy5Mvr3748tW7bgzp07ckmgqVOnypWX9pICsnp65Yd0PioAMDExybW8bDuy+2ZnZGSksp64uLjcg1Mht/msiLSNSSkiIiIiIqJipn379uL1I0eOaDGS//To0QPjxo0DkDUsbPz48VqOKIujoyM2bNgg3n7w4AGioqLE2+bm5uL1xMTEfLVhZmYmXk9KSsq1vGw7svvmlWxyq27duhAEIU8XHx+ffLdNVBSYlCIiIiIiIipmSpcuLV5XZ3W5ojJp0iSULVsWALB8+XLcvXtXyxFlady4sVwCR3bInOwQuVu3buWrfukxA8D9+/dzLX/v3j3xup2dXb7aBAALCwvxuGSHIRJ9LpiUIiIiIiIiKmak8wgB6g0XKypGRkZiD6n09PQck4pri0QiQYkS/02ZLJugql27NiwsLABkraL36tWrPNcvu9pecHBwruUPHz6scN/8kA7lfPnyJS5fvlyguoiKGyaliIiIiIiIipnAwEDxen4n5y4sgwcPho2NDQBg9+7duH79usbbiIuLy9N8SCdOnBDnXzIyMpJbDU5XVxf9+/cHAKSmpmLatGl5jqdJkyZib6nQ0FCEhYUpLRsWFobQ0FAAgK2tLZo2bZrn9mR5e3uL1ydNmgRBEApUH1FxwqQUERERERFRMbJo0SKcPn0aAKCjo4M+ffpoOSJ5RkZGGD16NICslfCyTyyuCefPn4ejoyPmz5+vcvU6ALh27RoGDhwo3u7RoweMjY3lyvz222/i3FLLly/HjBkzkJGRobC+1NRUHDp0SO4+PT09uWPu06eP3LxVUlFRUejTp4+YOBo9ejT09fVVH2wuevbsKfa2CgoKwsCBA1XOjZWRkYGgoCDMmjWrQO0SFYUSuRchIiIiIiIiTfH3989x34cPHxAVFYV9+/bhwoUL4v1jxozBV199pbK+5ORkhXUq07JlS3E4W34NHToUc+fOxdu3b+Hv74/w8HDUrVu3QHVmFxMTA19fX4wbNw6NGjWCq6srqlWrBktLS6SnpyM6OhonTpzA4cOHxQRT+fLlMW/evBx1VaxYERs3boSXlxfS09MxdepUbNmyBV5eXqhRowb09fXx4sULXLp0CQcPHkSFChXQoUMHuTpGjRqFgwcP4sSJE4iMjEStWrUwaNAgNGzYEBKJBGFhYVi/fr242l6LFi3ERFZBSCQS7N69G66uroiOjsbmzZsREBAALy8v1KtXD5aWlkhJSUFMTAyuXbuGI0eO4NWrV3B3dy8WwyuzxxAfHy9ej4uLy7Hd0dER33//fZHERtrHpBQRERF9EiQaqIMDHoioOOjWrVuuZfT09DBx4kRMmTIl17KvXr1Sq06pK1euwMXFRe3yipiammLEiBFiL6kpU6bg4MGDBapTlrW1Nezs7BATE4PMzEycO3cO586dU7lPq1atsH79erlJyWV5enoiMDAQAwcOxPPnz3Hv3j38/vvvCsva29vnuE9XVxcBAQEYMGAA9u7di8TERCxbtkzh/t26dcPmzZuhq6uby5Gqp1y5crh06RJ8fHxw6NAhvHv3DqtWrVK5j+wE79qk7BwDWQmq7Nvd3NyYlPqCMClFRERERPTFY8pW2wwMDFCyZEk4OTnBzc0NPj4+cHBw0HZYKg0fPhwLFy5EQkICAgICEBYWhoYNG2qk7rp16+Lp06e4ePEijh8/jvPnz+Pu3bt49uwZEhMToaenBwsLC1StWhX169dHz5491Zq7qU2bNnj06BHWr1+PAwcO4Pr163j9+jUkEglsbGxQq1YttGnTBv369VO4v4mJCfbs2YOQkBBs3LgRp0+fxvPnzwEANjY2aNasGby9veHu7q6R8yDL2toagYGBOH/+PLZs2YLTp08jOjoacXFxMDQ0hK2tLZycnNCsWTN06tQJzs7OGo+BSNMkAmdJKzYSEhJgYWGB+Ph4cbxzUdDYL88LNVDTGAH8LZw+B/y/ItI8/l99Ogp6ZsSzUtDHaoy0Jo1FVGQK8rkwJSUFkZGRcHR0hKGhYSFFSEREpJy670Wc6JyIiIiIiIiIiIock1JERERERERERFTkmJQiIiIiIiIiIqIix6QUEREREREREREVOSaliIiIiIiIiIioyDEpRURERERERERERY5JKSIiIiIiIiIiKnJMShERERERERERUZH75JNSKSkpGD16NJo3bw47OzsYGhrC1tYWTZs2xfr16/Hx48cc+yQkJGD06NGwt7eHgYEBHBwc8OuvvyIxMTFH2Xfv3mHQoEGwsbGBra0tvvvuO7x7905hLP369cNXX32lsE0iIiIiIiIiIvpPCW0HUFCJiYn4559/0LBhQ3Ts2BFWVlZ49+4dDh06hO+++w7bt2/HoUOHoKOTlX9LSkqCm5sbrl69irZt26Jv3764cuUKFixYgBMnTuDkyZMwNDQU6x8wYACCg4MxcOBACIIAPz8/vH79Gvv375eLIzAwEP/++y/OnDkDPT29Ij0HRERU/EVFRSEpKQnOzs7aDoWIiIiIqFj45JNSlpaWiI+Ph76+vtz96enpaNOmDYKDg3Ho0CF07NgRADBv3jxcvXoVv/32G+bMmSOWHzduHObOnYs///wT48ePBwDExsYiICAAs2bNwsSJEwEADg4OmDJlCp4/fw5bW1sAWYmxIUOGYNiwYWjcuHFRHDYREX1Cli1bhr1790IikaB69epYvny5tkMiIiIiItK6T374no6OTo6EFACUKFEC3bp1AwA8ePAAACAIAtasWQNTU1NMnjxZrvzkyZNhamqKNWvWiPdFR0cDAOrVqyfeV79+fQDAkydPxPsmTJgAQRAwe/ZsDR0VERF9ys6dOyd3+/Tp0wgJCcGxY8dw+vRpLUVFRF8aQRC0HQIREX2h1H0P+uSTUspkZmYiKCgIAPDVV18BAO7fv4+YmBg0bdoUJiYmcuVNTEzQtGlTPHr0SExGVahQAQBw5coVsVx4eDgAoGLFigCA8+fP4++//8Y///wDU1PTwj0oIiL6JKxatQo///wzkpKSAABWVlaYMWMGZs6cCUtLSy1HR0SfO+m0FZmZmVqOhIiIvlQZGRkA/ntPUuaTH74nlZaWhtmzZ0MQBLx58wbHjh3DnTt3MGjQILi7uwPISkoBQNWqVRXWUbVqVRw+fBj3799HhQoVULZsWXh4eGDq1Kl49OiROKdU586dYWtri48fP+KHH35Ar169xOGBeZGamorU1FTxdkJCQj6OnIiIipv169fjyJEj8PDwwNixY/Hnn39i06ZNSEpKwt69e7UdHhF95vT09KCrq4ukpKQcP8QSEREVheTkZOjq6uY65/ZnlZSaPn26eFsikWDs2LH4448/xPvi4+MBABYWFgrrMDc3lysHAJs2bcKoUaOwb98+SCQS9O/fH4sWLQIA/PHHH4iJicGxY8fw4sUL/PjjjwgKCoKRkRG+++47zJs3D7q6ukpj/uOPP+RiJiKiz0ebNm3QrFkzTJkyBdu2bcPixYthbW2t7bCI6AsgkUhgZmaGhIQEWFlZQSKRaDskIiL6ggiCgISEBJiZmeX6HvTZJKVMTU0hCAIyMzMRExODAwcOYMKECTh37hwCAwPFhFNeWVpaYuPGjTnuv337NmbPno0VK1bA2toa7dq1w507d7Bjxw7ExMRg5MiRsLGxga+vr9K6x48fj9GjR4u3ExISxCGDRET06YqOjsayZcugr68PX19fPHnyBH369EH//v3x/fffazs8IvoCWFhYIC4uDjExMbCzs2NiioiIioQgCIiJicHHjx+VdgiS9dkkpaR0dHRQvnx5DBkyBGXKlEGvXr3w+++/Y+7cueIJke0JJUs6fC63EycIAn744Qd888038PHxwZ07dxAcHIwtW7agS5cuAIDLly/jzz//VJmUMjAwgIGBQX4Ok4iIirHevXtjyJAhSEpKgre3NwIDA3HkyBEsXLgQHTt2REBAgLZDJKLPnLGxMcqXL4+nT5/iw4cPMDc3h7GxMXR1dZmgIiIijRIEARkZGUhOTkZCQgI+fvyI8uXLw9jYONd9P7uklKy2bdsCAI4fPw7gv7mkpHNLZZfbnFNS//zzD65cuYKIiAgAwN27dwEALi4uYpk6depgzZo1iI+PVys7SEREn4+EhAQMGDAAqampWLlyJQBAV1cXvr6+6Nmzp5ajI6IvhZmZGezt7REfH4+4uDi8efNG2yEREdFnTFdXF2ZmZrCwsFArIQV85kmpmJgYABAn1qpatSrs7Oxw5syZHBM/JiUl4cyZM3B0dFQ5hO7Zs2cYP348pk+fjkqVKsltk520XHqdv0QREX15evfujRo1aiAzM1NumDaAHO8dRESFydjYGMbGxuIiPVyRj4iICoOOjg709PTynAP55JNSt27dgoODQ44sXHJysvhFwMPDA0BWgmjw4MHistxz5swRy8+cOROJiYmYMGGCyvaGDh2KKlWqYNSoUeJ9NWrUAAAEBgaiTp064nU7O7t8z2VFRESfrsmTJ2PkyJHQ1dVV+1ciIqLCJJFIoK+vr+0wiIiI5HzySakdO3Zg0aJFaNasGRwcHGBubo5nz57h0KFDePPmDb755hu5BJKvry/27duHuXPn4sqVK6hbty7Cw8MRHByMBg0aYOTIkSrbCgwMRFhYmNyqetWrV0f79u0xbdo0PH78GLGxsTh69CgWLFhQmIdORETF2LNnz2BpaQljY2PcuXMHZ86cwVdffYVGjRppOzQiIiIiomJBR9sBFFSnTp3Qp08fPHnyBNu2bcPChQtx6NAh1K5dGytXrkRISAiMjIzE8iYmJjhx4gRGjhyJ27dvY+HChbhz5w7GjBmDY8eOyZWV9e7dOwwfPhyjR48We0PJ2rhxI7p06YItW7bg3Llz8PX1lUuGERHRl2P+/Plwc3ND/fr1sXnzZrRt2xaHDx9Gr169sGTJEm2HR0RERERULEgEQRC0HQRlSUhIgIWFBeLj44t02J8mZr0SAGChBmoaI0CDERFpDf+vvmzOzs44ffo0EhMTUaNGDdy4cQOOjo54/fo1WrRogRs3bmg7xE8S/68+HQU9M+JZKehjNUZak8YiKjLa+lxIRERUlD754XtERETFjYGBAUqVKoVSpUqhTJkycHR0BACUKVNGXHyDiIiIiOhL98kP3yMiIipuDAwMEBAQgM2bN0MikeDff/8FAISGhsrNSUhERERE9CVjTykiIiINW7JkCX788Ufo6Ohg3759mDNnDry9vWFqaiomqIiIiIiIvnRMShEREWlYw4YNceXKFfH2tm3b8ObNG5QqVQo6OuykTEREREQEcPgeERGRxj169AitWrVCpUqVMHr0aKSkpKB06dLQ0dGBq6urtsMjIiIiIioWmJQiIiLSsCFDhqBHjx7YuXMnXr9+DXd3d7x//x4AkJKSUqC6o6KicPPmTU2ESURERESkVRy+R0REpGEvX77EsGHDAAB+fn6YPXs23N3dceTIEUgk+V+aftmyZdi7dy8kEgmqV6+O5cuXaypkIiIiIqIix6QUERGRhn348EHu9oQJE6Cvry/XY0od586dkxvud/r0aYSEhAAAateurZlgiYiIiIi0hMP3iIiINMzJyQlBQUFy940dOxb9+vXDw4cP1a5n1apV+Pnnn5GUlAQAsLKywowZMzBz5kxYWlpqNGYiIiIioqLGpBQREZGGbd++HS1btsxx/+jRoxEdHa12PevXr0fXrl3h4eGBAwcO4M8//0T58uVRsmRJ7N27V5MhExEREREVOQ7fIyIi0jADAwOl28qVK5enutq0aYNmzZphypQp2LZtGxYvXgxra+uChkhEREREpHVMShERERVT0dHRWLZsGfT19eHr64snT56gT58+6N+/P77//ntth0dEREREVCAcvkdERFRM9e7dG7Vq1UL58uXxf+zdeZxXZcH///ewDcjmioQCipFi3rmA5pK3GrncbqW53JgLuQaaGpYFdyqkiZql3qW4lSsqbol+5RbTcgm1NDHNnxqCouaKGcMuyOf3BzJBgIozXcPMPJ+Pxzyczznnc851mAf26eV1rjniiCPSt2/f/OY3v8m7776bvfbaq6GHBwAAdWKmFACsompqanLYYYdl3rx5ueyyy5IkLVu2zKmnnpoDDjiggUcHAAB1I0oBwCrq4IMPziabbJKFCxdmyJAhS+3r1atXA40KAADqhygFAKuo0047LSeffHJatmyZ1VZbraGHAwAA9cqaUgCwivrLX/6Sjh07ClIAADRJohQArKK+8IUvZPPNN8///u//5u9//3tDDwcAAOqVKAUAq6jPf/7zOf3003PPPfekR48e+e///u/cd999DT0sAACoF6IUAKyiWrduna9//esZN25cnnvuuWy22WY57rjjssEGG+RHP/pRQw8PAADqRJQCgEage/fu+eEPf5jJkyfnl7/8ZV544YWGHhIAANSJ374HAKuoNm3aLHd7//79079//8KjAQCA+mWmFACsoh577LGGHgIAAPzbiFIAsIqaMmVKdtlll/Tq1StDhgzJ3Llza/dtt912DTgyAACoO1EKAFZRgwYNygEHHJBbbrkl06ZNS//+/TNjxowkWSpQAQBAY2RNKQCoDz+tqvs5Tqks9fLtt9/O8ccfnyS59tprc/bZZ6d///75zW9+k6qqergeAAA0IFEKAD7CvHnz8v7776djx47Frz1nzpylXg8bNixt2rRZasYUAAA0Vh7fA4AVuPnmm9O3b99svfXWOfvss4tfv0+fPrnnnnuW2vbd7343hxxySCZPnlx8PAAAUJ9EKQD40EsvvbTU61tuuSVPP/10nn322dxwww3Fx3PTTTdll112WWb7kCFD8uqrrxYfDwAA1CeP7wHAh44//vh8+ctfzpAhQ9KiRYt07Ngx119/faqqqtK+ffvi46murl7hvvXWW6/gSAAAoP6ZKQUAHxo3blxWX3317Lbbbnnqqady/vnn58UXX8xTTz2VG2+8saGHBwAATYqZUgCwhKOPPjp77713vvOd76RHjx4ZMWJE2rZt29DDAgCAJsdMKQD40IwZM3LJJZfk//2//5errroq22+/fXbffff89re/beihAQBAkyNKAcCH9ttvv7z77rt58cUXc8QRR+SrX/1q7rrrrtx222056qijGnp4AADQpHh8DwA+9Pbbb+e0005LpVLJVlttlSTp1KlTLr744jzyyCMNPDoAAGhaRCkA+ND222+fr3zlK5k3b1723nvvZfYBAAD1R5QCgA9deumlefbZZ9O6det87nOfa+jhAABAkyZKAcASPv/5z9d+/8wzz+Txxx/PF77whfTr168BRwUAAE2Phc4B4EP9+/fP22+/nSS5+eabs8cee+See+7JAQcckMsuu6yBRwcAAE2LmVIA8KF33nknXbp0SZJccMEFeeSRR9KzZ8/8/e9/z84775zjjjuugUcIAABNh5lSAPChefPm5YMPPkiSVCqV9OzZM0my5pprplKpNOTQAACgyRGlAOBDAwYMyMEHH5wXX3wxBxxwQH784x/n5ZdfzqhRo9KrV6+GHh4AADQpHt8DgA8NHz48F110UXbZZZe89dZbWbBgQc4777wMGDAgV111VUMPDwAAmhRRCgCWcNJJJ+Wkk07KjBkzsmDBgqyxxhoNPSQAAGiSPL4HAB+aPHlydtlll/Tq1StnnHFG2rVrV7tvu+22KzSKqnr4AgCAVZ8oBQAfGjx4cA444IDccsstmTZtWvr3758ZM2YkSebOndvAowMAgKZFlAKAD7399ts5/vjj07dv31x77bXZa6+90r9//0yfPj1VVWYgAQBAfbKmFAB8aM6cOUu9HjZsWNq0abPUjCkAAKB+mCkFAB/q06dP7rnnnqW2ffe7380hhxySyZMnN9CoAACgaTJTCgA+dNNNNy13+5AhQ3LwwQcXHg0AADRtohQAfKi6unqF+9Zbb72CIwEAgKbP43sAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFBcq4YeAAA0pKp6OEelHs4BAADNjZlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMU1+ij1t7/9LRdeeGF222239OjRI23atEnXrl3z9a9/PX/4wx+W+56ampoMGTIkPXv2THV1dTbYYIN873vfy8yZM5c59r333ss3v/nNrLvuuunatWuOPPLIvPfee8s97yGHHJLNNtss8+fPr9d7BAAAAGhqWjX0AOrq5z//ec4999xstNFG2W233bLOOutk0qRJueOOO3LHHXfkhhtuyMEHH1x7/KxZs7LTTjvlqaeeym677ZYBAwZk4sSJOf/88/Pggw/moYceStu2bWuPP+yww3Lvvffm8MMPT6VSybXXXptp06blzjvvXGoc48aNy5gxYzJhwoS0bt262P0DAAAANEaNPkpts802eeCBB7LTTjsttf3hhx9O//79M2jQoHzta19LdXV1kuS8887LU089le9///s555xzao//wQ9+kHPPPTcXXHBBhg4dmiR54403cvfdd+ess87K//zP/yRJNthgg5x++ul5880307Vr1yTJzJkzM2jQoBx//PHZdtttS9w2AAAAQKPW6B/f23///ZcJUkmy4447Zpdddsl7772XZ555JklSqVRy5ZVXpkOHDjnttNOWOv60005Lhw4dcuWVV9Zue/XVV5Mkffv2rd3Wr1+/JMkrr7xSu23YsGGpVCo5++yz6+/GAAAAAJqwRh+lPsrix+hatVo0IWzSpEl5/fXXs8MOO6R9+/ZLHdu+ffvssMMOmTJlSm2M6t69e5Jk4sSJtcc9+eSTSZIePXokSR577LFcfPHFGTVqVDp06LBS45s3b15qamqW+gIAAABoDhr943sr8sorr+S+++7LZz7zmfzHf/xHkkVRKkl69+693Pf07t0748ePz6RJk9K9e/d85jOfyZ577pkzzjgjU6ZMqV1Tap999knXrl0zf/78HHPMMTnooIOy1157rfQYR44cmREjRnz6mwQAAABopJrkTKn58+fnsMMOy7x583LuueemZcuWSZLp06cnSTp37rzc93Xq1Gmp45Lkuuuuy4ABAzJ27Njcdddd+cY3vpFrrrkmyaKo9Prrr+eiiy7KW2+9la997Wtp27Zt1lhjjZxyyin54IMPPnKcQ4cOzfTp02u/Fs/QAgAAAGjqmtxMqYULF2bgwIF56KGHcswxx+Swww6r0/nWXHPN2gi1pOeeey5nn312Lr300nTp0iW77757nn/++dx88815/fXXc/LJJ2fdddfNqaeeusJzV1dX1y7ADgAAANCcNKkotXDhwhx55JG54YYbcuihh+bSSy9dav/iGVJLzoRa0uI1nVY0k2qxSqWSY445JjvuuGMGDhyY559/Pvfee29Gjx6dfffdN0nypz/9KRdccMFHRikAAACA5qrJPL63cOHCfPOb38w111yTAQMG5Oqrr06LFkvf3uK1pBavLfWvPm7NqcVGjRqViRMn5rLLLkuSvPDCC0mSLbbYovaYLbfcMm+++eYKAxgAAABAc9YkotTiIHXttdfm4IMPznXXXVe7jtSSevfunW7dumXChAmZNWvWUvtmzZqVCRMmZMMNN6z9rXvL87e//S1Dhw7NiBEj0qtXr6X2zZs3b5nvq6qq6nJrAAAAAE1So49Six/Zu/baa3PggQfm+uuvX26QShYFoqOPPjozZ87MmWeeudS+M888MzNnzswxxxzzkdcbPHhwPvvZz+Y73/lO7bZNNtkkSTJu3LjabePGjUu3bt1qF08HAAAA4J8a/ZpSP/rRj3LNNdekQ4cO+dznPpezzjprmWO+9rWv1T5ad+qpp2bs2LE599xzM3HixGy11VZ58sknc++992brrbfOySefvMJr3XzzzRk3blz++Mc/LhW+Nt544+yxxx4ZPnx4pk6dmjfeeCP33Xdfzj///Pq+XQAAAIAmodFHqZdffjlJMnPmzPz4xz9e7jEbbLBBbZRq3759HnzwwQwfPjy33XZbfve73+Uzn/lMTjnllJxxxhlp167dcs/x3nvv5cQTT8yQIUOy5ZZbLrP/mmuuyaBBgzJ69Oi0a9cup5566lKzqQAAAAD4p6pKpVJp6EGwSE1NTTp37pzp06cXfeyvPla9qiTJT+vhTKdUUo8jggbj71Xj4WfVePhZNR51/ZOp/VOp68/qlMVnqrcRFdNQnwsBoKRGv6YUAAAAAI2PKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFBck4hS119/fY477rj069cv1dXVqaqqytVXX73C42tqajJkyJD07Nkz1dXV2WCDDfK9730vM2fOXObY9957L9/85jez7rrrpmvXrjnyyCPz3nvvLfe8hxxySDbbbLPMnz+/vm4NAAAAoElq1dADqA8//OEPM3Xq1Ky99tr5zGc+k6lTp67w2FmzZmWnnXbKU089ld122y0DBgzIxIkTc/755+fBBx/MQw89lLZt29Yef9hhh+Xee+/N4YcfnkqlkmuvvTbTpk3LnXfeudR5x40blzFjxmTChAlp3br1v+1eAQAAAJqCJjFT6sorr8zLL7+cd955J9/61rc+8tjzzjsvTz31VL7//e9n/PjxOeecczJ+/Ph8//vfz+OPP54LLrig9tg33ngjd999d84444xceeWV+eUvf5nTTz89d911V958883a42bOnJlBgwbl+OOPz7bbbvtvu08AAACApqJJRKmvfOUr6dmz58ceV6lUcuWVV6ZDhw457bTTltp32mmnpUOHDrnyyitrt7366qtJkr59+9Zu69evX5LklVdeqd02bNiwVCqVnH322XW6DwAAAIDmoklEqU9q0qRJef3117PDDjukffv2S+1r3759dthhh0yZMqU2RnXv3j1JMnHixNrjnnzyySRJjx49kiSPPfZYLr744owaNSodOnQocRsAAAAAjV6TWFPqk5o0aVKSpHfv3svd37t374wfPz6TJk1K9+7d85nPfCZ77rlnzjjjjEyZMqV2Tal99tknXbt2zfz583PMMcfkoIMOyl577bXS45k3b17mzZtX+7qmpubT3RgAAABAI9OsZkpNnz49SdK5c+fl7u/UqdNSxyXJddddlwEDBmTs2LG566678o1vfCPXXHNNkmTkyJF5/fXXc9FFF+Wtt97K1772tbRt2zZrrLFGTjnllHzwwQcfOZ6RI0emc+fOtV+LZ2YBAAAANHXNaqbUp7HmmmvWRqglPffcczn77LNz6aWXpkuXLtl9993z/PPP5+abb87rr7+ek08+Oeuuu25OPfXUFZ576NChGTJkSO3rmpoaYQoAAABoFppVlFo8Q2rJmVBLWvz43IpmUi1WqVRyzDHHZMcdd8zAgQPz/PPP5957783o0aOz7777Jkn+9Kc/5YILLvjIKFVdXZ3q6upPcysAAAAAjVqzenxv8VpSi9eW+lcft+bUYqNGjcrEiRNz2WWXJUleeOGFJMkWW2xRe8yWW26ZN998c4UBDAAAAKA5a3ZRqlu3bpkwYUJmzZq11L5Zs2ZlwoQJ2XDDDT/yEbq//e1vGTp0aEaMGJFevXottW/JRcsXf19VVVWPdwAAAADQNDSrKFVVVZWjjz46M2fOzJlnnrnUvjPPPDMzZ87MMccc85HnGDx4cD772c/mO9/5Tu22TTbZJEkybty42m3jxo1Lt27dahdPBwAAAOCfmsSaUldeeWV+//vfJ0meeeaZ2m0PPPBAkuRLX/pSjj766CTJqaeemrFjx+bcc8/NxIkTs9VWW+XJJ5/Mvffem6233jonn3zyCq9z8803Z9y4cfnjH/+Yli1b1m7feOONs8cee2T48OGZOnVq3njjjdx33305//zz/z03DAAAANDINYko9fvf/36Z35A3YcKETJgwofb14ijVvn37PPjggxk+fHhuu+22/O53v8tnPvOZnHLKKTnjjDPSrl275V7jvffey4knnpghQ4Zkyy23XGb/Nddck0GDBmX06NFp165dTj311KVmUwEAAADwT1WVSqXS0INgkZqamnTu3DnTp08v+thffax6VUmSn9bDmU6ppB5HBA3G36vGw8+q8fCzajzq+idT+6dS15/VKYvPVG8jKqahPhcCQEnNak0pAAAAAFYNohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxbWqz5PdeeedGT9+fKZOnZo5c+bk/vvvr903a9as/PnPf05VVVW22267+rwsAAAAAI1MvUSpV199Nfvvv3+efPLJJEmlUklVVdVSx7Rp0yYDBgzIa6+9lkceeSRf/OIX6+PSAAAAADRCdX58b9asWdltt93ypz/9Keutt16OP/74tG/ffpnjWrdunaOOOiqVSiW//vWv63pZAAAAABqxOkepiy++OC+88EK22mqrPPfcc/nf//3fdOjQYbnHfvWrX02STJgwoa6XBQAAAKARq3OUuu2221JVVZWf/exny50htaTNNtssLVu2zF//+te6XhYAAACARqzOUeqFF15Iy5Yts8MOO3zssS1btszqq6+ef/zjH3W9LAAAAACNWJ2j1Lx589KuXbu0bNnyEx0/e/bstG3btq6XBQAAAKARq3OUWnfddTNz5sxPNPvp2WefzZw5c9K9e/e6XhYAAACARqzOUepLX/pSkmTMmDEfe+x5552Xqqqq7LLLLnW9LAAAAACNWJ2j1ODBg1OpVDJ8+PD85S9/We4x77//foYOHZrrrrsuVVVVGTRoUF0vCwAAAEAj1qquJ9h+++3z7W9/Oz//+c+z7bbbZo899sjMmTOTJMOGDcvUqVNz3333Zdq0aUmSH/7wh9l0003relkAAAAAGrE6R6kkufDCC9OpU6ecc845uf3225MkVVVVOffcc5MklUolrVq1ymmnnZbTTjutPi4JAAAAQCNWL1GqqqoqZ555Zo4++uhcffXVmTBhQl5//fV88MEH6dq1a3bYYYcceeSR6dWrV31cDgAAAIBGrs5R6pVXXkmSdOnSJT179swZZ5xR50EBAAAA0LTVeaHzDTbYIL169crf//73+hgPAAAAAM1AnWdKdejQIa1bt063bt3qYzwAAAAANAP1MlNq9uzZ+eCDD+pjPAAAAAA0A3WOUl/72tfy/vvvZ9y4cfUxHgAAAACagTpHqe9///v57Gc/m29961t5+umn62NMAAAAADRxdV5T6rbbbstxxx2X4cOHp1+/ftljjz2yww47pEuXLmnZsuUK33f44YfX9dIAAAAANFJ1jlIDBw5MVVVVkqRSqeTuu+/O3Xff/ZHvqaqqEqUAAAAAmrE6R6kePXrURikAAAAA+CTqHKVefvnlehgGAAAAAM1JnRc6BwAAAICVJUoBAAAAUFydH9/7V88++2yeeOKJvP3220mSLl26ZOutt86mm25a35cCAAAAoJGqtyg1fvz4nHrqqfnLX/6y3P3/8R//kfPOOy+77bZbfV0SAAAAgEaqXh7f+8UvfpG99torf/nLX1KpVNKiRYt06dIlXbp0ScuWLVOpVPL000/nv/7rv3LxxRfXxyUBAAAAaMTqHKX+/Oc/5+STT87ChQuzzTbbZNy4cZk5c2beeOONvPHGG5kxY0bGjRuX7bbbLpVKJSeffHKefvrp+hg7AAAAAI1UnaPUz372syxcuDD77LNPfv/732ePPfZIdXV17f7q6ursscceeeihh7LPPvvkgw8+yAUXXFDXywIAAADQiNU5Sj344IOpqqrKRRddlJYtW67wuJYtW+bCCy9Mkvzud7+r62UBAAAAaMTqHKXeeuutdO7cORtssMHHHrvhhhtm9dVXz1tvvVXXywIAAADQiNU5SrVr1y6zZ8/OggULPvbYBQsWZPbs2WnXrl1dLwsAAABAI1bnKNWnT5/Mnz8/t95668cee8stt+T9999Pnz596npZAAAAABqxOkepAw88MJVKJYMHD87999+/wuPuu+++DB48OFVVVTnooIPqelkAAAAAGrFWdT3BoEGD8stf/jLPPvtsdtttt2y33Xb5yle+kvXWWy9J8tprr+X+++/Po48+mkqlks022yyDBg2q88ABAAAAaLzqHKWqq6szfvz47L///vnjH/+YRx55JI8++uhSx1QqlSTJF7/4xdx2221p06ZNXS8LAAAAQCNW58f3kqRbt2555JFHctNNN2W//fbL+uuvnzZt2qRNmzZZf/31s99++2XMmDGZMGFCunXrVh+XBAAAAKARq/NMqcVatGiRgw46yHpRAAAAAHysepkpBQAAAAAro16iVE1NTWbOnPmxx82cOTM1NTX1cUkAAAAAGrE6R6nbb789a6yxRo499tiPPfbQQw/NGmuskTvvvLOulwUAAACgEatzlLrllluSJEcdddTHHnvMMcekUqnk5ptvrutlAQAAAGjE6hylJk6cmBYtWmSHHXb42GO//OUvp0WLFnnyySfrelkAAAAAGrE6R6m//e1vWX311dO2bduPPbZdu3ZZffXV87e//a2ulwUAAACgEWtV1xNUVVVl9uzZn/j4OXPmpKqqqq6XBQAAAKARq/NMqe7du2fu3Ll55plnPvbYP//5z5kzZ07WW2+9ul4WAAAAgEaszlFq5513TqVSyRlnnPGxxw4fPjxVVVXZZZdd6npZAAAAABqxOkepb3/722nRokXGjh2bQw89NG+99dYyx7z11ls55JBDMnbs2LRo0SInnnhiXS8LAAAAQCNW5zWlNtlkk/z4xz/O0KFDc+ONN+bWW29N375907NnzyTJ1KlT88QTT2TBggVJkrPOOiubbrppXS8LAAAAQCNW5yiVJN///vfTqVOn/OAHP8iMGTPy6KOP5rHHHkuSVCqVJEmnTp1y3nnn5dhjj62PSwIAAADQiNVLlEqSQYMGZcCAAbn11lvzyCOP5M0330xVVVW6du2a7bffPgceeGA6depUX5cDAAAAoBGrtyiVJKuvvnqOPvroHH300fV5WgAAAACamHqNUkt6//33c8899+SFF15IdXV1ttpqq3zpS1/6d10OAAAAgEZkpaPUjBkz8utf/zpJcvDBB6e6unqZYx5//PEccMABee2115ba/sUvfjG33357unbt+imHCwAAAEBT0GJl33D//fdn4MCBufDCC5cbpN5+++3stddeee2111KpVJb6+sMf/pB99923XgYOAAAAQOO10lHq4YcfTpIccsghy91/7rnnZtq0aUmSI444IhMmTMif//znfOc730mlUsmf/vSn3HrrrXUYMgAAAACN3Uo/vvfHP/4xVVVV2WOPPZa7f/To0amqqso+++yTq666qnb7T3/60/z973/PNddck9tuuy0HHHDApx81AAAAAI3aSs+UeuONN9KqVatsuummy+x79tln8/bbbydJTjzxxGX2n3TSSUmSiRMnruxlAQAAAGhCVjpKvfXWW+nUqVNatFj2rX/84x+TJG3atFnub9rbbLPNUlVVlddff/1TDBUAAACApmKlo9QHH3yQmpqa5e7705/+lCTp06dP2rRps8z+Vq1aZY011sicOXNW9rIAAAAANCErHaW6dOmSBQsWZPLkycvse/TRR1NVVZWtt956he+fOXNm2rdvv7KXBQAAAKAJWekotdVWWyVJLr/88qW2T5o0KU899VSSZKeddlrue6dOnZr3338/66+//speFgAAAIAmZKWj1IABA1KpVHLBBRfkJz/5SV544YXcf//9OfDAA1OpVNK+ffvss88+y33vQw89lGTR2lIAAAAANF8rHaUOPPDA/Od//mcWLFiQH/zgB9l0002z22675ZlnnklVVVWGDBmSjh07Lve9Y8aMSVVV1XIXQQcAAACg+VjpKJUkY8eOzd57751KpVL7lSRHH310Tj/99OW+Z9KkSbnnnnuSJHvuueenHC4AAAAATUGrT/Omzp07584778yLL75Yu47U1ltvnZ49e67wPa1bt87YsWPTunXr9OrV61MNFgAAAICm4VNFqcU++9nP5rOf/ewnOnaDDTbIBhtsUJfLAQAAANBEfKrH9wAAAACgLkQpAAAAAIoTpQAAAAAoTpQCAAAAoDhRCgAAAIDiRCkAAAAAihOlAAAAAChOlAIAAACgOFEKAAAAgOJEKQAAAACKE6UAAAAAKE6UAgAAAKA4UQoAAACA4kQpAAAAAIoTpQAAAAAoTpQCAAAAoDhRCgAAAIDiRCkAAAAAihOlAAAAAChOlAIAAACgOFEKAAAAgOJEKQAAAACKE6UAAAAAKE6UAgAAAKA4UQoAAACA4kQpAAAAAIoTpQAAAAAoTpQCAAAAoLhmG6Uef/zx7Lnnnll99dXTvn37bLvttrn55puXOW7ChAnZbrvt0rFjx/Tp0ydXXnnlcs/31ltvZc0118zZZ5/97x46AAAAQKPXLKPU7373u+ywww75/e9/n4MOOijf+ta38uabb+bggw/OT3/609rjXnnlley222556623ctxxx2XNNdfMMccck9tvv32Zc377299O9+7dc+qpp5a8FQAAAIBGqVVDD6C0BQsW5JhjjkmLFi3y0EMPZYsttkiSnH766dlmm20ybNiwHHDAAenZs2dGjx6duXPn5oEHHkiPHj3ywQcfZNNNN83ll1+e/fffv/acd911V26//fY8+uijadWq2f2RAgAAAKy0ZjdT6re//W0mT56cQw45pDZIJUnnzp0zbNiwvP/++7nmmmuSJK+++mrWWWed9OjRI0nSsmXLbLHFFnnllVdq31dTU5PBgwfnxBNPzNZbb130XgAAAAAaq2Y3reeBBx5Ikuy2227L7Nt9992TJA8++GCSpHv37pk2bVpee+21rL/++lm4cGH+/Oc/Z4MNNqh9zw9+8IO0atUqZ5555kqPZd68eZk3b17t65qampU+BwAAAEBjVFWpVCoNPYiSDjzwwNx666154okn0rdv32X2d+zYMWussUZeeeWVTJ06NX369Em3bt2y33775dFHH82ECRNy2223Zf/998+ECRPyn//5nxk3blxt0FoZw4cPz4gRI5bZPn369HTq1OlT3R9NW1U9nKOSJD+thzOdUkk9jggAWEJNTU06d+7scyEATVqze3xv+vTpSRY9rrc8nTp1qj2mZ8+eGT9+fNZaa62MGjUq06ZNyxVXXJH9998/77//fo455pgccsgh2X333TNu3Lj06dMnrVq1yiabbJL/+7//+9ixDB06NNOnT6/9evXVV+vvRgEAAABWYc3u8b2VteOOO+YPf/jDMtvPOuusvPPOO7ngggsyderU7Lffftl///3zi1/8Ir/85S+z33775a9//WvtelTLU11dnerq6n/n8AEAAABWSc1uptTiGVKLZ0P9q8VTpT/Ks88+m3POOScXXHBB1l577YwaNSpt27bNr371q/Tv3z+//OUvU11dnVGjRtX7+AEAAACagmYXpXr37p0kmTRp0jL73nzzzcycObP2mOVZuHBhjj766PTv3z+HHnpokuSFF17IxhtvnHbt2iVJ2rVrl4033jjPP//8v+EOAAAAABq/ZheldtpppyTJvffeu8y+8ePHL3XM8vziF7/IM888s8wsqCV/i97i11VV9bEINAAAAEDT0+yiVP/+/dOrV6/ccMMNeeqpp2q3T58+PWeffXbatGmTww8/fLnvfeWVV/I///M/OfPMM7PBBhvUbu/Tp0+effbZTJ06NUkyderUPPvss+nTp8+/81YAAAAAGq2qSqXS7H4f++9+97vsvvvuadu2bf77v/87HTt2zG233ZapU6fm/PPPzymnnLLc9+21115555138thjj6VFi3/2vFdeeSWf+9znst5662XffffNnXfemddffz2TJk3K+uuv/4nH5Vf/8nHqY+5dJUl+Wg9nOqWSehwRALAEnwsBaA6a3UypJNlll13y+9//PjvssEPGjBmTUaNGZd11181NN920wiB1ww035N57782VV165VJBKkh49euSOO+5Iu3btcvHFF6ddu3YZO3bsSgUpAAAAgOakWc6UWlX5L2J8HDOlAKB58LkQgOagWc6UAgAAAKBhiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFNYkoddddd+Xb3/52dthhh7Rv3z5VVVUZPnz4R75n3rx5+dGPfpTevXunbdu26datW4499ti8/fbbyxw7d+7cDBkyJN27d89aa62V/fffP6+99tpyzzts2LB07do17733Xn3cGgAAAECT1KqhB1AffvrTn+bBBx9Mp06d0q1bt7z44osfefzChQvz1a9+NePHj8+2226br3/965k0aVKuvPLK3H///Xnssceyzjrr1B7/3e9+N5dcckkOOuigrL322rnqqquy77775vHHH0/Lli1rj3v66afzk5/8JKNHj84aa6zxb7tfAAAAgMauScyUOvPMM/PXv/41//jHP3LmmWd+7PHXXHNNxo8fnwEDBuSRRx7JOeeck9tuuy2XXHJJpkyZkh/+8Ie1xy5cuDC//OUvc9RRR+Wmm27KL37xi1x66aWZOHFiHn/88drjPvjggxx99NH5r//6rxx00EH/lvsEAAAAaCqaRJTacccd07t371RVVX2i46+44ookyciRI5d6z3HHHZdevXpl9OjRmTNnTpJk2rRpmTt3bvr27Vt7XL9+/ZIkr7zySu22iy66KM8//3wuueSSOt8PAAAAQFPXJKLUypg7d27+8Ic/ZOONN07Pnj2X2ldVVZVdd901s2bNyhNPPJEkWXvttdO2bdtMnDix9rgnn3wySdKjR48kycsvv5zTTz89I0eOzPrrr1/oTgAAAAAaryaxptTKmDx5chYuXJjevXsvd//i7ZMmTcqOO+6YFi1a5Mgjj8yoUaMyY8aMrLXWWrn66quz5ZZbZuutt06yaIbV5ptvnsGDB6/UWObNm5d58+bVvq6pqfmUdwUAAADQuDS7mVLTp09PknTu3Hm5+zt16rTUcUly/vnn58QTT8yDDz6Y0aNHZ9ddd82dd96Zli1b5tprr80DDzyQK664IrNmzcrAgQPToUOHdOjQIQMHDsysWbNWOJaRI0emc+fOtV/du3evxzsFAAAAWHU1iplSw4cPX2bbySefnNVXX73I9du1a5cLL7wwF1544VLb33nnnQwZMiRDhw7NpptumuOOOy5jx47N5ZdfnqqqqgwePDirrbbaCteZGjp0aIYMGVL7uqamRpgCAAAAmoVGEaVGjBixzLaBAwd+qii1eIbUkjOhlrT4EboVzaRa0kknnZR11103w4YNy4wZM/KrX/0qI0aMyCGHHJIkmTJlSkaMGJHzzjsvHTp0WOb91dXVqa6uXul7AAAAAGjsGkWUqlQq9XauXr16pUWLFpk0adJy9y/evqI1pxYbN25cxowZk4cffjht2rTJc889lwULFmSLLbaoPWbLLbfM/PnzM3ny5Gy++eb1dg8AAAAAjV2zW1OqXbt22WabbfLCCy9k6tSpS+2rVCr5zW9+k/bt26dfv34rPMfMmTMzaNCgDBo0KNtvv/1S+5ZcuHzx91VVVfV4BwAAAACNX7OLUkly7LHHJlm0ptOSs7Auu+yyTJkyJd/4xjfSrl27Fb5/2LBhWbhwYUaOHFm7baONNkrr1q0zbty42m3jxo1LmzZtstFGG/0b7gIAAACg8WoUj+99nDvuuCN33HFHkuSll16q3fbyyy8nSTbZZJP84Ac/qD3+iCOOyJgxY3LjjTfmpZdeyk477ZQXX3wxt99+ezbccMOcddZZK7zWY489losvvjh33HFHOnbsWLu9Q4cOOeqoo3LppZdm9uzZSZIbbrghJ5xwQtq3b1/PdwwAAADQuFVV6nPBpgYyfPjw5S6GvthOO+2UBx54YKlt8+bNyznnnJPrrrsur776atZcc83svffeOeuss7Luuusu9zzz58/PVlttlU033TRjxoxZZv+sWbNy4okn5tZbb02SHHjggfn5z3/+kbOullRTU5POnTtn+vTp6dSp0yd6D81LfTwIWkmSn9bDmU6ppB5HBAAswedCAJqDJhGlmgofPvg4ohQANA8+FwLQHDTLNaUAAAAAaFiiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFNfoo9e677+byyy/Pvvvum169eqW6ujprr712/uu//ivjx49f4fvmzZuXH/3oR+ndu3fatm2bbt265dhjj83bb7+9zLFz587NkCFD0r1796y11lrZf//989prry33vMOGDUvXrl3z3nvv1ds9AgAAADQ1rRp6AHV1yy23ZNCgQenWrVv69++f9dZbL6+99lpuu+223HPPPTnvvPPyve99b6n3LFy4MF/96lczfvz4bLvttvn617+eSZMm5corr8z999+fxx57LOuss07t8d/97ndzySWX5KCDDsraa6+dq666Kvvuu28ef/zxtGzZsva4p59+Oj/5yU8yevTorLHGGsX+DAAAAAAam6pKpVJp6EHUxW9/+9vMmjUre+21V1q0+OfErxdeeCFf/OIXM3v27Lz88svp1q1b7b6rrroqRx55ZAYMGJDRo0enqqoqSXLppZdm0KBBOfbYY3PZZZclWRSw2rdvn0MPPTRXXHFFkuS6667L4YcfnkcffTTbbrttkuSDDz7Idtttl65du+bOO+/8VPdSU1OTzp07Z/r06enUqdOnOgdNW1U9nKOSJD+thzOdUkk9jggAWILPhQA0B43+8b0vf/nL2WeffZYKUkmy8cYb5+CDD878+fPzyCOPLLVvcVwaOXJkbZBKkuOOOy69evXK6NGjM2fOnCTJtGnTMnfu3PTt27f2uH79+iVJXnnlldptF110UZ5//vlccskl9XuDAAAAAE1Qo49SH6V169ZJklat/vmU4ty5c/OHP/whG2+8cXr27LnU8VVVVdl1110za9asPPHEE0mStddeO23bts3EiRNrj3vyySeTJD169EiSvPzyyzn99NMzcuTIrL/++p94fPPmzUtNTc1SXwAAAADNQaNfU2pFampqcuutt6Zt27bZcccda7dPnjw5CxcuTO/evZf7vsXbJ02alB133DEtWrTIkUcemVGjRmXGjBlZa621cvXVV2fLLbfM1ltvnWTRDKvNN988gwcPXqkxjhw5MiNGjPiUdwgAAADQeDXZmVLf+ta38tZbb2XYsGFZa621ardPnz49SdK5c+flvm/xM/uLj0uS888/PyeeeGIefPDBjB49OrvuumvuvPPOtGzZMtdee20eeOCBXHHFFZk1a1YGDhyYDh06pEOHDhk4cGBmzZq1wjEOHTo006dPr/169dVX6+PWAQAAAFZ5jWKm1PDhw5fZdvLJJ2f11Vdf7vFDhw7NjTfemD322CPDhg2r8/XbtWuXCy+8MBdeeOFS2995550MGTIkQ4cOzaabbprjjjsuY8eOzeWXX56qqqoMHjw4q6222grXmaqurk51dXWdxwcAAADQ2DSKKLW8R9wGDhy43Ch12mmn5ZxzzsmXv/zl3H777WnZsuVS+xfPkFpyJtSSFq/rtKKZVEs66aSTsu6662bYsGGZMWNGfvWrX2XEiBE55JBDkiRTpkzJiBEjct5556VDhw4fez4AAACA5qJRRKlK5ZP9yvjTTjstZ511Vnbeeefcddddadeu3TLH9OrVKy1atMikSZOWe47F21e05tRi48aNy5gxY/Lwww+nTZs2ee6557JgwYJsscUWtcdsueWWmT9/fiZPnpzNN9/8E90DAAAAQHPQZNaUWhykdtppp9x9991ZbbXVlntcu3btss022+SFF17I1KlTl9pXqVTym9/8Ju3bt0+/fv1WeK2ZM2dm0KBBGTRoULbffvul9s2bN2+Z76uqqj7tbQEAAAA0SU0iSp1++uk566yzsuOOO35kkFrs2GOPTbJo7aklZ2FddtllmTJlSr7xjW8sd5bVYsOGDcvChQszcuTI2m0bbbRRWrdunXHjxtVuGzduXNq0aZONNtro094aAAAAQJPUKB7f+yhXX311zjzzzLRq1SrbbLNNfvKTnyxzzM4775ydd9659vURRxyRMWPG5MYbb8xLL72UnXbaKS+++GJuv/32bLjhhjnrrLNWeL3HHnssF198ce6444507NixdnuHDh1y1FFH5dJLL83s2bOTJDfccENOOOGEtG/fvv5uGAAAAKAJaPRR6uWXX06SLFiwID/96U9XeNySUapFixYZO3ZszjnnnFx33XW54IILsuaaa+aoo47KWWedlXXWWWe555g/f36OOeaYHHDAAdlnn32W2X/++efn/fffz6233pokOeqoo3Leeed9+psDAAAAaKKqKp90FXH+7WpqatK5c+dMnz49nTp1aujhsAqqj9XJKkny03o40ymV1OOIAIAl+FwIQHPQJNaUAgAAAKBxEaUAAAAAKE6UAgAAAKA4UQoAAACA4kQpAAAAAIoTpQAAAAAoTpQCAAAAoDhRCgAAAIDiRCkAAAAAihOlAAAAAChOlAIAAACgOFEKAAAAgOJEKQAAAACKE6UAAAAAKE6UAgAAAKA4UQoAAACA4kQpAAAAAIoTpQAAAAAoTpQCAAAAoDhRCgAAAIDiRCkAAAAAihOlAAAAAChOlAIAAACgOFEKAAAAgOJEKQAAAACKE6UAAAAAKE6UAgAAAKA4UQoAAACA4kQpAAAAAIoTpQAAAAAoTpQCljFvQTJjbkOPAgAAgKZMlAKWcvNTSd8Lkq0vSs6+v6FHAwAAQFMlSkEz99K7S7++5enk6VOSZ7+X3PBkw4wJAACApq9VQw8AaFjH/zr58meTIf+ZtGiRdKxOrn8yqapK2rdp6NEBAADQVJkpBc3cuKOT1dslu12RPPW35Px9khenLfr+xkMbenQAAAA0VWZKATn6i8nefZLv3Jn0WD0ZsXvStnVDjwoAAICmzEwpaOZmzE0umZD8v+eSqw5Ott8g2f2K5LeTGnpkAAAANGWiFDRz+12TvDt70SN7R9yUfHWz5K4jk9ueSY66uaFHBwAAQFPl8T1o5t6emZy2a1KpJFtdsGhbp7bJxfsnj7zcoEMDAACgCROloJnbvmfylcuSeQuSvTf9l30bNMiQAAAAaAZEKWjmLj0gefbNpHXL5HPrNPRoAAAAaC5EKSCf77r06/dmJ2us1jBjAQAAoHmw0Dk0cxc9/M/vX3o3+fxPkm4/Sjb8cfLMGw03LgAAAJo2UQqauWue+Of3w/4vGbx9Muec5Px9kiF3Nty4AAAAaNpEKaDW//dWcvwOi77/+heSd2Y17HgAAABouqwpBc3cP+Ykdz2bVJLM/2DpfZVKgwwJAACAZkCUgmaux+rJzx5a9P26HZO/TU/W65y8PSNp07JBhwYAAEATJkpBM/fA4OVvX6t98uAK9gEAAEBdWVMKWK6WLZLV2jT0KAAAAGiqRClo5iZPS3YZlfQ6O/nO2GTu/H/u2+7nDTcuAAAAmjZRCpq5wbcnB3whueXw5N3ZSf/LkhlzF+1bMlABAABAfRKloJl7e2Zy/A5J3/WTawcke/VZFKamz0mqqup27nnzkhkz6mecAAAANC2iFDRzc/5lNtSw/slBm384Y2repz/vzTcnffsmW2+dnH123cYIAABA0yNKQTPXZ93knueX3vbdnZNDtkwmv/vJz/PSS0u/vuWW5Omnk2efTW64oc7DBAAAoIlp1dADABrWTYcuu+2ax5MhOyUHb/HJz3P88cmXv5wMGZK0aJF07Jhcf/2iRwDbt6+34QIAANBEmCkFzVx1q0VfS7ro94v+uV7nT36eceOS1VdPdtsteeqp5PzzkxdfXPT9jTfW02ABAABoMsyUApZRqXy69x19dLL33sl3vpP06JGMGJG0bVu/YwMAAKBpMFMKWMaQnVb+PTNmJJdckvy//5dcdVWy/fbJ7rsnv/1t/Y8PAACAxk+UApZxWN+Vf89++yXvvrvokb0jjki++tXkrruS225Ljjqq/scIAABA4+bxPaBevP12ctppix7922qrRds6dUouvjh55JGGHRsAAACrHlEKqBfbb5985SvJvHmL1pX6130AAACwJFEKqBeXXpo8+2zSunXyuc819GgAAABY1YlSQL35/Of/+f0zzySPP5584QtJv34NNyYAAABWTRY6B+pF//6L1pVKkptvTvbYI7nnnuSAA5LLLmvYsQEAALDqMVMKqBfvvJN06bLo+wsuWLS4ec+eyd//nuy8c3LccQ06PAAAAFYxZkoB9WLevOSDDxZ9X6ksClJJsuaai14DAADAkkQpoF4MGJAcfHDy4ouLHtn78Y+Tl19ORo1KevVq6NEBAACwqvH4HlAvhg9PLroo2WWX5K23kgULkvPOWxSrrrqqoUcHAADAqkaUAurNSSct+poxY1GUWmONhh4RAAAAqypRCqh3HTs29AgAAABY1VlTCqgXkycvenSvV69kyJBk7tx/7ttuu4YbFwAAAKsmUQqoF4MHL1rg/JZbkmnTkv79Fz3GlywdqAAAACARpYB68vbbyfHHJ337Jtdem+y116IwNX16UlXV0KMDAABgVWNNKWiOTqnU+ynnzFn69bBhSZs2S8+YAgAAgMXMlALqRZ8+yT33LL3tu99NDjlk0XpTAAAAsCQzpYB6cdNNy2675ppFi54ffHD58QAAALBqM1MKmpKXX06efbZBLl1dvehrSRddtOif661XfjwAAACs2syUgqbi5z9Pfv3rRauKb7xxcsklDT2iVOp/6SoAAACaCFEKGqtHH0222+6fr3//++S3v130/Re+0DBj+hdDhjT0CAAAAFhViVLQWF1+eTJ6dHLuuUn79sk66yQ/+tGimVJrrtnQo0uSHHZYQ48AAACAVZU1paCxuuqq5KtfTfbcM7nrruSCC5L1109WX33RY3wAAACwCquqVKz6sqqoqalJ586dM3369HTq1Kmhh8MqqGp5G+fMSU4/Pfnb35ILL0y6dPnIc9TvX/jljmgl+VcQAPwrnwsBaA48vgeN1auvLlrcvE2b5NRTk1deSf77v5NvfCM56qiGHh0AAAB8JI/vQWN18MHJf/zHokf2jjgi6ds3+c1vknffTfbaq6FHBwAAAB/JTClorGpqFq0kPm9ectlli7a1bLlo1tQBBzTs2AAAAOBjiFLQWB18cLLJJsnChcmQIUvv69WrYcYEAAAAn5CFzlchFrTk4yyzrPiMGYtmR6222ic+h4XOAWDV53MhAM2BNaWgMfvb35KZMxd9//zzyS9/mfzhDw07JgAAAPgERClorH7yk2SnnZJ+/ZLrr0922y0ZPz456KDkoosaenQAAADwkawpBY3V1Vcvmh01c+aitaX+8pdkww2TadOSnXdOTjqpoUcIAAAAKyRKQWNVXZ2sscair7XXXhSkkkXft27dsGMDAACAj+HxPWisqquTu+9e9OheVVUyZsyi7b/73aLFzwEAAGAVZqYUNFb/+7/JsccmLVokY8cm55yTHHFE0qFDcvPNDT06AAAA+EhVlUrF72NfRfjVv3ycqo874N13Fz3O12LFkyDr9y/8x47oE/CvIAD4Vz4XAtAcmCkFTclaaxW+oKAEAADAp2NNKWisJk9Odtkl6dUrGTIkmTv3n/u2267hxgUAAACfgCgFjdXgwckBByS33JJMm5b075/MmLFo35KBCgAAAFZBohQ0Vm+/nRx/fNK3b3Lttcleey0KU9OnL/ptfAAAALAKs6YUNFZz5iz9etiwpE2bpWdMAQAAwCrKTClorPr0Se65Z+lt3/1ucsghi9abAgAAgFWYmVLQWN1007Lbrrlm0aLnBx9cfjwAAACwEsyUgsaqunrR15IuumjRP9dbr/x4AAAAYCWIUtCUVCoNPQIAAAD4REQpaEqGDGnoEQAAAMAnIkpBU3LYYQ09AgAAAPhERCkAAAAAihOlAAAAAChOlAIAAACgOFEKAAAAgOJEKQAAAACKE6UAAAAAKE6UAgAAAKA4UQoAAACA4kQpAAAAAIoTpQAAAAAoTpQCAAAAoDhRCgAAAIDiRCkAAAAAihOlAAAAAChOlAIAAACguFYNPQDgk6s09AAAAACgnpgpBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUFyTiFKnn356vvKVr6R79+5p165d1l577fTr1y8XXHBBZs+evdz3zJs3Lz/60Y/Su3fvtG3bNt26dcuxxx6bt99+e5lj586dmyFDhqR79+5Za621sv/+++e1115b7nmHDRuWrl275r333qvXewQAAABoSqoqlUqloQdRV2uuuWZ69+6dzTbbLF26dMmMGTPywAMP5Nlnn83mm2+eRx55JKuttlrt8QsXLsyee+6Z8ePHZ9ttt81OO+2USZMm5de//nU23HDDPPbYY1lnnXVqjz/hhBNyySWX5KCDDsraa6+dq666KhtvvHEef/zxtGzZsva4p59+On379s3o0aNz0EEHrfR91NTUpHPnzpk+fXo6depUtz8UAAAaLZ8LAWgOmkSUmjt3btq2bbvM9sMOOyzXX399fvGLX+T444+v3X7VVVflyCOPzIABAzJ69OhUVVUlSS699NIMGjQoxx57bC677LIkiwJW+/btc+ihh+aKK65Iklx33XU5/PDD8+ijj2bbbbdNknzwwQfZbrvt0rVr19x5552f6j58+AAAIPG5EIDmoUk8vre8IJUkBx54YJLkxRdfXGr74rg0cuTI2iCVJMcdd1x69eqV0aNHZ86cOUmSadOmZe7cuenbt2/tcf369UuSvPLKK7XbLrroojz//PO55JJL6uGOAAAAAJq2JhGlVuTuu+9Okmy22Wa12+bOnZs//OEP2XjjjdOzZ8+ljq+qqsquu+6aWbNm5YknnkiSrL322mnbtm0mTpxYe9yTTz6ZJOnRo0eS5OWXX87pp5+ekSNHZv311/+33hMAAABAU9CqoQdQn84777zMnj07//jHPzJhwoQ88cQT2W233XL44YfXHjN58uQsXLgwvXv3Xu45Fm+fNGlSdtxxx7Ro0SJHHnlkRo0alRkzZmSttdbK1VdfnS233DJbb711kkUzrDbffPMMHjx4pcY7b968zJs3r/Z1TU3Nyt4yAAAAQKPU5KLUu+++W/v60EMPzahRo9K6devabdOnT0+SdO7cebnnWPzM/uLjkuT8889P69atc8stt2TOnDnZdddd87//+79p2bJlrr322jzwwAOZOHFiZs2alRNOOCG33nprkuSAAw7IxRdfnPbt2y/3WiNHjsyIESPqdtMAAAAAjVCjiFLDhw9fZtvJJ5+c1Vdffalt06ZNS5K8+eab+e1vf5vvf//7+eIXv5jx48fX6bG6du3a5cILL8yFF1641PZ33nknQ4YMydChQ7PpppvmuOOOy9ixY3P55ZenqqoqgwcPzmqrrbbCdaaGDh2aIUOG1L6uqalJ9+7dP/U4AQAAABqLRhGlljebaODAgctEqcW6du2aQw45JL17984222yTU045JWPGjEnyzxlSS86EWtLiR+hWNJNqSSeddFLWXXfdDBs2LDNmzMivfvWrjBgxIoccckiSZMqUKRkxYkTOO++8dOjQYZn3V1dXp7q6+mOvAwAAANDUNIooValUPtX7tt5666yxxhp54IEHarf16tUrLVq0yKRJk5b7nsXbV7Tm1GLjxo3LmDFj8vDDD6dNmzZ57rnnsmDBgmyxxRa1x2y55ZaZP39+Jk+enM033/xT3QMAAABAU9Skf/vezJkzM3369KXWlGrXrl222WabvPDCC5k6depSx1cqlfzmN79J+/bt069fv48876BBgzJo0KBsv/32S+1bcuHyxd9XVVXVx+0AAAAANBmNPkpNmTIlf//735fZPn/+/Jx88slZuHBh9txzz6X2HXvssUkWrem05Cysyy67LFOmTMk3vvGNtGvXboXXHDZsWBYuXJiRI0fWbttoo43SunXrjBs3rnbbuHHj0qZNm2y00Uaf+v4AAAAAmqJG8fjeR3nooYfyrW99K1/60pfSq1evrLXWWnnzzTdz33335bXXXkufPn3y4x//eKn3HHHEERkzZkxuvPHGvPTSS9lpp53y4osv5vbbb8+GG26Ys846a4XXe+yxx3LxxRfnjjvuSMeOHWu3d+jQIUcddVQuvfTSzJ49O0lyww035IQTTljhb98DAAAAaK6qKp92waZVxKRJk/Lzn/88Dz/8cF599dX84x//SMeOHdOnT5/st99+Of7447Paaqst87558+blnHPOyXXXXZdXX301a665Zvbee++cddZZWXfddZd7rfnz52errbbKpptuWrtw+pJmzZqVE088MbfeemuS5MADD8zPf/7zj5x1taSampp07tw506dPT6dOnVbiTwEAgKbE50IAmoNGH6WaEh8+AABIfC4EoHlo9GtKAQAAAND4NPo1pZqSxZPWampqGngkAAA0pMWfBz3UAEBTJkqtQmbMmJEk6d69ewOPBACAVcGMGTPSuXPnhh4GAPxbWFNqFbJw4cK8/vrr6dixY6qqqhp6OPWmpqYm3bt3z6uvvmpNhFWcn1Xj4WfVePhZNR5+Vo1Hc/hZVSqVzJgxI926dUuLFlbcAKBpMlNqFdKiRYusv/76DT2Mf5tOnTo12Q+OTY2fVePhZ9V4+Fk1Hn5WjUdT/1mZIQVAU+c/uwAAAABQnCgFAAAAQHGiFP921dXVOeOMM1JdXd3QQ+Fj+Fk1Hn5WjYefVePhZ9V4+FkBQNNgoXMAAAAAijNTCgAAAIDiRCkAAAAAihOlAAAAAChOlAIAAACgOFEKAAAAgOJEKerVBx98kL/85S+5+uqr8+1vfzvbbbddVltttVRVVaWqqioDBw5s6CHyoRkzZuS2227LCSeckO233z7rrLNOWrdunU6dOmWTTTbJ4YcfnnvuuSd+QWfDe/zxx3PxxRdn4MCB2XrrrbPBBhukQ4cOqa6uzrrrrpudd945I0aMyNSpUxt6qHyEgQMH1v67sKqqKsOHD2/oITVrO++881I/j4/7evnllxt6yCSZOHFivve972XLLbfMOuusk+rq6qy33nrp169fTjjhhNx666354IMPGnqYAMAnVFXx/zipR1//+tdz++23r3D/EUcckauvvrrcgFiun/3sZ/mf//mfzJ0792OP3XHHHXP99denR48eBUbG8nTo0CGzZs362OOqq6tzxhlnZOjQoQVGxcr4v//7v+y5555LbTvjjDOEqQa0884758EHH/zEx7/00kvZYIMN/n0D4iPV1NTkpJNOyjXXXPOx/7Hkvffey+qrr15mYABAnbRq6AHQtPzrf51cc801s9Zaa2XSpEkNNCKW569//WttkFpvvfXyla98JX379k2XLl0yd+7cPPbYY7n++uszc+bMPPzww9l5553z2GOPpUuXLg088uarS5cu2WabbbL55ptnww03TOfOnTN//vy8/PLLufvuuzNhwoTMmzcvw4YNy/z583P66ac39JD5UE1NTY477rgkSfv27T9RYKSsX//61x97jH//NZy///3v2X333fPEE08kWfS/W/vvv38233zzdO7cOTNmzMikSZPym9/8Jn/6058aeLQAwMowU4p6dfbZZ2fGjBnp27dv+vbtmw033DBXX311vvnNbyYxU2pVMWjQoEyZMiXf/e53079//7RoseyTvFOnTs3uu++eF154IUnyzW9+M7/61a9KD5Ukf/nLX/L5z38+VVVVKzzm2muvzcCBA1OpVNKqVatMnTo13bp1KzhKVuS4447L5Zdfnu7du+fAAw/Mz372syRmSjW0JWdK+Si0attjjz0yfvz4JMkpp5ySs846K23btl3usa+//nq6dOmSVq38d1cAaAysKUW9GjZsWEaOHJkDDjggG264YUMPhxX48Y9/nPHjx2fXXXddbpBKkp49e2bMmDG1r8eMGZPZs2eXGiJL2GyzzT4ySCXJ4Ycfnr333jtJsmDBgtxzzz0lhsbH+O1vf5srrrgiSXLJJZekY8eODTwiaFyuvvrq2iA1aNCgnH/++SsMUknSrVs3QQoAGhFRCpqhNddc8xMdt/nmm2fjjTdOksyePTsvvvjiv3NY1NHnP//52u/ffPPNBhwJyaK/M8ccc0wqlUoOPvjg2mgIfHLnnntukkVr651zzjkNPBoAoL6JUsBH6tSpU+33c+bMacCR8HGWjIZdu3ZtwJGQJEOHDs2UKVOy5ppr5qKLLmro4UCjM2HChDz//PNJkq9+9atL/e8RANA0iFLACr3//vv561//Wvu6Z8+eDTgaPspdd91Vu1hz27Zts9deezXwiJq3Rx55JL/4xS+S/P/t3WlIVN8DxvHHJq2o8CeRmtqGrZRJFpgVRZaJLVRIvcg2hKBsI4misqRoMWgjogxNg6zeRUFZVDphC4rRuFFUSJmUVGa2QWk5/xf+vWjLtN+rzPcDwrl3zoUHBxeeufccaffu3fLz87M4EVyZNm2aAgMD5eXlJR8fHw0ZMkSLFy+W3W63Oppba747Ynh4uCTp9OnTmjJlivz9/dWhQwcFBARo6tSpyszM1KdPn6yKCgAAfhMP3QP4rpMnT+r169eSpLCwMO6+aQXy8vJUU1MjqbE0rKys1KVLl3Tp0iVJUvv27ZWamkoJYqEPHz4oPj5eDQ0NmjhxorHRA1qv8+fPG+Pa2lrV1tbqzp07Sk9PV2RkpLKystSjRw8LE7qnpt32JMnPz0+xsbE6ffp0izlVVVWqqqpSdna29u3bp7Nnz7KmJQAAbQilFIBvevHihdatW2ccJyUlWZgGTdauXauCgoKvznt4eGj8+PHasmWLxo0bZ0EyNNm8ebPu3bunTp066ciRI1bHgQs+Pj6KiorSyJEjFRgYKJvNpidPnignJ0cXLlyQ0+lUbm6uIiIilJ+fTzFvsqqqKmPc9HPl5eWlBQsWaOzYsfL09FRxcbHS09NVU1Oj0tJSTZgwQbdv3/7ptRMBAIC1KKUAfKWurk6xsbF6/vy5JGnmzJmaNWuWxangSmBgoKKiotS/f3+ro7i1wsJC7d27V5K0ZcsWBQcHW5wI37Nz506NGDFCXl5eX72WmJioW7duKTY2Vo8fP1ZFRYXi4+OVnZ1tQVL39erVK2N87949+fj4KCcnR8OHDzfOz507V6tXr9bEiRN1584dVVRUaMOGDUpNTbUiMgAA+EWsKQWghYaGBsXHx+vatWuSpODgYGVkZFicCk3y8/PldDrldDr17t07FRUVaevWrXr79q02btyokJAQXblyxeqYbqmurk7x8fH6/PmzwsLClJiYaHUkuBAREfHNQqrJyJEjdfHiRXXo0EGSdOHCBRUWFpoVD2r8e9Tc7t27WxRSTfz9/XXy5Enj+NixY3rz5s0/zwcAAP4cpRQAg9Pp1JIlS3TixAlJUq9evXTlyhX5+PhYnAzf0rlzZ4WGhmrTpk1yOBwKCAjQy5cvNXXqVJWWllodz+1s27ZNZWVlstlsSktLk81mszoS/tDgwYM1f/584/jcuXMWpnE/Xbt2NcadO3fWvHnzvjs3NDRUo0aNkiR9/PhRN27c+Of5AADAn6OUAiCpsZBKSEhQWlqaJCkoKEi5ubnq06ePtcHwU/r27auUlBRJjXfsbN++3eJE7qW4uNj4/icmJiosLMziRPhbJkyYYIzv3r1rYRL30/wDkZCQEJd3tkmNd7c1KS8v/2e5AADA38OaUgDkdDq1bNkyYw2OwMBA2e121sNpY2JiYozx1atXrQviho4dO6b6+nq1a9dOnp6e2rZt2zfn5eXltRg3zRs4cKBmz55tSlb8mu7duxvj2tpa64K4oUGDBiknJ0eS5O3t/cP5zefw+B4AAG0DpRTg5poKqcOHD0uSAgICZLfb1a9fP4uT4Vc1f9Sl+QLB+PecTqekxjVwduzY8VPX2O122e12SdKMGTMopVqp6upqY/zff/9ZF8QNhYaGGuPXr1//cH7zOT9TYgEAAOvx+B7gxr4spHr06CG73c4Obm3UgwcPjHHzuzsA/L6m4lBqvKMN5omJiZGHh4ckqbS0VHV1dS7n37p1yxjzXgEA0DZQSgFubPny5UYh5e/vL7vdrgEDBlicCr+r+RboY8aMsTCJ+9m/f7+xK6Krr+TkZOOa5ORk4/yZM2esC4/vun//vo4fP24cT5s2zcI07icoKEjjx4+XJL1//15ZWVnfnVtcXKz8/HxJjXeN8jsQAIC2gVIKcFMrVqzQoUOHJDUWUlevXuWT5VYoNTVVdrvdeDzsWz5//qyUlBTj/ZSkhIQEM+IBbdKBAwd08+ZNl3McDoeio6P14cMHSdLkyZMVHh5uRjw00/xx2DVr1sjhcHw159mzZ4qLizOOV65cqU6dOpmSDwAA/BnWlMJf9fDhQx09erTFuZKSEmPscDiUlJTU4vXIyEhFRkaakg+NkpKSdPDgQUmSh4eHVq1apbt37/5wZ6mwsDD16tXLjIj4v/z8fC1dulQ9e/ZUVFSUQkJC5OvrKy8vL9XW1qqsrExnz57Vo0ePjGvWr19v3F0A4Gu5ublatWqVgoODNWnSJA0dOlTdunWTzWbT06dPlZOTo+zsbDU0NEiSevfurczMTItTu6eIiAitW7dOu3bt0qtXrzRq1CgtXLhQY8eOlaenp4qKipSenq6amhpJjTvwffl/BgAAaL0opfBXVVRUuNyKvqSkpEVJJUnt27enlDLZ9evXjbHT6dT69et/6rrMzEwtWrToH6WCK5WVlcrIyHA5x9vbWzt37tTSpUtNSgW0beXl5SovL3c5Jzo6WhkZGQoICDApFb6UkpIim82mXbt2qa6uTmlpaUpLS/tqXnR0tE6dOqWOHTtakBIAAPwOSikAaMUOHDigGTNmKC8vTw6HQ+Xl5aqurlZ9fb26dOkiPz8/DRs2TNHR0Zo9ezY7TgE/Yc+ePZo+fboKCgpUXFys58+fq7q6Wh8/fpS3t7f69OmjiIgIxcXF8cheK7F9+3bNmTNHR48e1eXLl/XkyRPV19fL19dXo0eP1oIFCxQTE2N1TAAA8Is8nK4WKgEAAAAAAAD+ARY6BwAAAAAAgOkopQAAAAAAAGA6SikAAAAAAACYjlIKAAAAAAAApqOUAgAAAAAAgOkopQAAAAAAAGA6SikAAAAAAACYjlIKAAAAAAAApqOUAgAAAAAAgOkopQAAAAAAAGA6SikAAAAAAACYjlIKAAAAAAAApqOUAgAAAAAAgOkopQAAAAAAAGC6/wGQIASHaJ8FAwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1200x1300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Metrics to include in the bar plots.\n",
    "selected_metrics_vis = [\n",
    "    \"BERTScore Precision\",\n",
    "    \"BERTScore Recall\",\n",
    "    \"BERTScore F1\"    \n",
    "]\n",
    "\n",
    "# Filter data for \"With Transfer Learning\".\n",
    "BART_vis = df_vis[df_vis[\"Model_Name\"] == \"BART\"].set_index(\"Experiment Description\")[selected_metrics_vis]\n",
    "\n",
    "# Convert columns to numeric.\n",
    "numeric_cols_vis = BART_vis.columns\n",
    "BART_vis[numeric_cols_vis] = BART_vis[numeric_cols_vis].apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "# Define percentage formatter function.\n",
    "def percentage_formatter_vis(x, pos):\n",
    "    return f\"{x:.0%}\"\n",
    "\n",
    "# Plotting for \"BART\"\n",
    "fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(12, 13))\n",
    "\n",
    "num_models_vis = len(BART_vis.index)\n",
    "bar_width_vis = 0.15\n",
    "gap_vis = 0.5\n",
    "gap1_vis = 0.01\n",
    "index_vis = np.arange(num_models_vis) * (bar_width_vis * len(selected_metrics_vis) + gap_vis)\n",
    "\n",
    "# Define colors for each metric.\n",
    "colors_vis = {\n",
    "    \"BERTScore Precision\": 'cyan',\n",
    "    \"BERTScore Recall\": 'darkorange',\n",
    "    \"BERTScore F1\": 'yellow'\n",
    "\n",
    "}\n",
    "\n",
    "for i, metric_vis in enumerate(selected_metrics_vis):\n",
    "    values_vis = BART_vis[metric_vis].values\n",
    "    bars_vis = axes.bar(index_vis + i * (bar_width_vis + gap1_vis), values_vis, bar_width_vis, label=metric_vis, color=colors_vis[metric_vis])\n",
    "    \n",
    "    # Display percentage values above each bar.\n",
    "    for bar_vis in bars_vis:\n",
    "        yval_vis = bar_vis.get_height()\n",
    "        axes.text(bar_vis.get_x() + bar_vis.get_width()/2, yval_vis + 0.01, f\"{yval_vis:.0%}\", ha='center', va='bottom', rotation=90, fontsize=8)\n",
    "\n",
    "       \n",
    "axes.set_title(\"BERT Score for Different parameters For BART model\", fontsize=20)\n",
    "axes.set_ylabel(\"Score\", fontsize=18)\n",
    "\n",
    "axes.set_xticks(index_vis + (len(selected_metrics_vis) - 1) * bar_width_vis / 2)\n",
    "axes.set_xticklabels(BART_vis.index, ha=\"center\", fontsize=22)\n",
    "axes.legend(loc='best', bbox_to_anchor=(1, 1), fontsize=22)\n",
    "axes.yaxis.set_major_formatter(FuncFormatter(percentage_formatter_vis))\n",
    "axes.yaxis.set_tick_params(labelsize=14) \n",
    "\n",
    "# Display the plot.\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c7bd8a58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model_Name</th>\n",
       "      <th>Experiment Description</th>\n",
       "      <th>ROUGE-1</th>\n",
       "      <th>ROUGE-2</th>\n",
       "      <th>ROUGE-L</th>\n",
       "      <th>ROUGE-Lsum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BART</td>\n",
       "      <td>1</td>\n",
       "      <td>46.9</td>\n",
       "      <td>24.4</td>\n",
       "      <td>35.2</td>\n",
       "      <td>35.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BART</td>\n",
       "      <td>2</td>\n",
       "      <td>9.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.3</td>\n",
       "      <td>9.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BART</td>\n",
       "      <td>3</td>\n",
       "      <td>49.3</td>\n",
       "      <td>26.2</td>\n",
       "      <td>37.3</td>\n",
       "      <td>37.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BART</td>\n",
       "      <td>4</td>\n",
       "      <td>43.9</td>\n",
       "      <td>21.9</td>\n",
       "      <td>32.0</td>\n",
       "      <td>32.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BART</td>\n",
       "      <td>5</td>\n",
       "      <td>47.7</td>\n",
       "      <td>25.6</td>\n",
       "      <td>35.9</td>\n",
       "      <td>35.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>BART</td>\n",
       "      <td>6</td>\n",
       "      <td>48.8</td>\n",
       "      <td>24.7</td>\n",
       "      <td>35.3</td>\n",
       "      <td>35.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Model_Name  Experiment Description  ROUGE-1  ROUGE-2  ROUGE-L  ROUGE-Lsum\n",
       "0       BART                       1     46.9     24.4     35.2        35.2\n",
       "1       BART                       2      9.5      2.0      9.3         9.3\n",
       "2       BART                       3     49.3     26.2     37.3        37.3\n",
       "3       BART                       4     43.9     21.9     32.0        32.0\n",
       "4       BART                       5     47.7     25.6     35.9        35.9\n",
       "5       BART                       6     48.8     24.7     35.3        35.3"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data from Excel sheet into a pandas DataFrame.\n",
    "\n",
    "excel_file = 'BART_Results.xlsx'\n",
    "df_vis = pd.read_excel(excel_file, sheet_name=\"BART_with_ROUGE_Score\")\n",
    "df_vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5e9337b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model_Name                 object\n",
      "Experiment Description      int64\n",
      "ROUGE-1                   float64\n",
      "ROUGE-2                   float64\n",
      "ROUGE-L                   float64\n",
      "ROUGE-Lsum                float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df_vis.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b1aa78cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x1300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Metrics to include in the bar plots.\n",
    "selected_metrics_vis = [\n",
    "    \"ROUGE-1\",\n",
    "    \"ROUGE-2\",\n",
    "    \"ROUGE-L\",\n",
    "    \"ROUGE-Lsum\"    \n",
    "]\n",
    "\n",
    "# Filter data for \"With Transfer Learning\".\n",
    "BART_vis = df_vis[df_vis[\"Model_Name\"] == \"BART\"].set_index(\"Experiment Description\")[selected_metrics_vis]\n",
    "\n",
    "# Convert columns to numeric.\n",
    "numeric_cols_vis = BART_vis.columns\n",
    "BART_vis[numeric_cols_vis] = BART_vis[numeric_cols_vis].apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "# Define percentage formatter function.\n",
    "def percentage_formatter_vis(x, pos):\n",
    "    return f\"{x:.0%}\"\n",
    "\n",
    "# Plotting for \"BART\"\n",
    "fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(12, 13))\n",
    "\n",
    "num_models_vis = len(BART_vis.index)\n",
    "bar_width_vis = 0.15\n",
    "gap_vis = 0.5\n",
    "gap1_vis = 0.01\n",
    "index_vis = np.arange(num_models_vis) * (bar_width_vis * len(selected_metrics_vis) + gap_vis)\n",
    "\n",
    "# Define colors for each metric.\n",
    "colors_vis = {\n",
    "    \"ROUGE-1\": 'cyan',\n",
    "    \"ROUGE-2\": 'darkorange',\n",
    "    \"ROUGE-L\": 'yellow',\n",
    "    \"ROUGE-Lsum\": 'blue'\n",
    "\n",
    "}\n",
    "\n",
    "for i, metric_vis in enumerate(selected_metrics_vis):\n",
    "    values_vis = BART_vis[metric_vis].values\n",
    "    bars_vis = axes.bar(index_vis + i * (bar_width_vis + gap1_vis), values_vis, bar_width_vis, label=metric_vis, color=colors_vis[metric_vis])\n",
    "    \n",
    "    # Display percentage values above each bar.\n",
    "    for bar_vis in bars_vis:\n",
    "        yval_vis = bar_vis.get_height()\n",
    "        axes.text(bar_vis.get_x() + bar_vis.get_width()/2, yval_vis + 0.01, f\"{yval_vis:}\", ha='center', va='bottom', rotation=90, fontsize=12)\n",
    "\n",
    "       \n",
    "axes.set_title(\"ROUGE Scores for Different parameters for BART model\", fontsize=20)\n",
    "axes.set_ylabel(\"Score\", fontsize=18)\n",
    "\n",
    "axes.set_xticks(index_vis + (len(selected_metrics_vis) - 1) * bar_width_vis / 2)\n",
    "axes.set_xticklabels(BART_vis.index, ha=\"center\", fontsize=22)\n",
    "axes.legend(loc='best', bbox_to_anchor=(1, 1), fontsize=22)\n",
    "# axes.yaxis.set_major_formatter(FuncFormatter(percentage_formatter_vis))\n",
    "axes.yaxis.set_tick_params(labelsize=14) \n",
    "\n",
    "# Display the plot.\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f27f47f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model_Name</th>\n",
       "      <th>Experiment Description</th>\n",
       "      <th>ROUGE-1</th>\n",
       "      <th>ROUGE-2</th>\n",
       "      <th>ROUGE-L</th>\n",
       "      <th>ROUGE-Lsum</th>\n",
       "      <th>BERTScore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>T5</td>\n",
       "      <td>1</td>\n",
       "      <td>4.27</td>\n",
       "      <td>0</td>\n",
       "      <td>4.28</td>\n",
       "      <td>4.27</td>\n",
       "      <td>-0.806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>T5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>T5</td>\n",
       "      <td>3</td>\n",
       "      <td>14.33</td>\n",
       "      <td>0</td>\n",
       "      <td>14.33</td>\n",
       "      <td>14.32</td>\n",
       "      <td>-0.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>T5</td>\n",
       "      <td>4</td>\n",
       "      <td>14.31</td>\n",
       "      <td>0</td>\n",
       "      <td>14.30</td>\n",
       "      <td>14.30</td>\n",
       "      <td>-0.125</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Model_Name  Experiment Description  ROUGE-1  ROUGE-2  ROUGE-L  ROUGE-Lsum  \\\n",
       "0         T5                       1     4.27        0     4.28        4.27   \n",
       "1         T5                       2     0.00        0     0.00        0.00   \n",
       "2         T5                       3    14.33        0    14.33       14.32   \n",
       "3         T5                       4    14.31        0    14.30       14.30   \n",
       "\n",
       "   BERTScore  \n",
       "0     -0.806  \n",
       "1     -0.293  \n",
       "2     -0.125  \n",
       "3     -0.125  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data from Excel sheet into a pandas DataFrame.\n",
    "\n",
    "excel_file = 'BART_Results.xlsx'\n",
    "df_vis = pd.read_excel(excel_file, sheet_name=\"t5_non_human\")\n",
    "df_vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a73047bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x1300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Metrics to include in the bar plots.\n",
    "selected_metrics_vis = [\n",
    "   \"ROUGE-1\",\n",
    "    \"ROUGE-2\",\n",
    "    \"ROUGE-L\",\n",
    "    \"ROUGE-Lsum\",\n",
    "    \"BERTScore\"    \n",
    "]\n",
    "\n",
    "# Filter data for \"With Transfer Learning\".\n",
    "BART_vis = df_vis[df_vis[\"Model_Name\"] == \"T5\"].set_index(\"Experiment Description\")[selected_metrics_vis]\n",
    "\n",
    "# Convert columns to numeric.\n",
    "numeric_cols_vis = BART_vis.columns\n",
    "BART_vis[numeric_cols_vis] = BART_vis[numeric_cols_vis].apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "# Define percentage formatter function.\n",
    "def percentage_formatter_vis(x, pos):\n",
    "    return f\"{x:.0%}\"\n",
    "\n",
    "# Plotting for \"BART\"\n",
    "fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(12, 13))\n",
    "\n",
    "num_models_vis = len(BART_vis.index)\n",
    "bar_width_vis = 0.15\n",
    "gap_vis = 0.5\n",
    "gap1_vis = 0.01\n",
    "index_vis = np.arange(num_models_vis) * (bar_width_vis * len(selected_metrics_vis) + gap_vis)\n",
    "\n",
    "# Define colors for each metric.\n",
    "colors_vis = {\n",
    "    \"ROUGE-1\": 'cyan',\n",
    "    \"ROUGE-2\": 'darkorange',\n",
    "    \"ROUGE-L\": 'yellow',\n",
    "    \"ROUGE-Lsum\": 'blue',\n",
    "\"BERTScore\": 'lightgreen'\n",
    "}\n",
    "\n",
    "for i, metric_vis in enumerate(selected_metrics_vis):\n",
    "    values_vis = BART_vis[metric_vis].values\n",
    "    bars_vis = axes.bar(index_vis + i * (bar_width_vis + gap1_vis), values_vis, bar_width_vis, label=metric_vis, color=colors_vis[metric_vis])\n",
    "    \n",
    "    # Display percentage values above each bar.\n",
    "    for bar_vis in bars_vis:\n",
    "        yval_vis = bar_vis.get_height()\n",
    "        axes.text(bar_vis.get_x() + bar_vis.get_width()/2, yval_vis + 0.01, f\"{yval_vis:}\", ha='center', va='bottom', rotation=90, fontsize=12)\n",
    "\n",
    "       \n",
    "axes.set_title(\"ROUGE and BERT Score for Different Non-Human Generated target\", fontsize=20)\n",
    "axes.set_ylabel(\"Score\", fontsize=18)\n",
    "axes.set_xlabel(\"Different experiments on T5-small and T5-Base\", fontsize=18)\n",
    "axes.set_xticks(index_vis + (len(selected_metrics_vis) - 1) * bar_width_vis / 2)\n",
    "axes.set_xticklabels(BART_vis.index, ha=\"center\", fontsize=22)\n",
    "axes.legend(loc='best', bbox_to_anchor=(1, 1), fontsize=22)\n",
    "# axes.yaxis.set_major_formatter(FuncFormatter(percentage_formatter_vis))\n",
    "axes.yaxis.set_tick_params(labelsize=14) \n",
    "\n",
    "# Display the plot.\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af13983",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
