{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34fa719f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in c:\\users\\764883\\appdata\\roaming\\python\\python311\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\program files\\python311\\lib\\site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: joblib in c:\\program files\\python311\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\764883\\appdata\\roaming\\python\\python311\\site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\program files\\python311\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\program files\\python311\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in c:\\users\\764883\\appdata\\roaming\\python\\python311\\site-packages (4.35.2)\n",
      "Requirement already satisfied: filelock in c:\\program files\\python311\\lib\\site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\users\\764883\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (0.19.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\program files\\python311\\lib\\site-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\program files\\python311\\lib\\site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\program files\\python311\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\764883\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in c:\\program files\\python311\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\764883\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\764883\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\program files\\python311\\lib\\site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\764883\\appdata\\roaming\\python\\python311\\site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\program files\\python311\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
      "Requirement already satisfied: colorama in c:\\program files\\python311\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\program files\\python311\\lib\\site-packages (from requests->transformers) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\program files\\python311\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\program files\\python311\\lib\\site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\program files\\python311\\lib\\site-packages (from requests->transformers) (2023.5.7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: rouge in c:\\users\\764883\\appdata\\roaming\\python\\python311\\site-packages (1.0.1)\n",
      "Requirement already satisfied: six in c:\\program files\\python311\\lib\\site-packages (from rouge) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: bert_score in c:\\users\\764883\\appdata\\roaming\\python\\python311\\site-packages (0.3.13)\n",
      "Requirement already satisfied: torch>=1.0.0 in c:\\program files\\python311\\lib\\site-packages (from bert_score) (2.0.1+cu118)\n",
      "Requirement already satisfied: pandas>=1.0.1 in c:\\program files\\python311\\lib\\site-packages (from bert_score) (2.0.2)\n",
      "Requirement already satisfied: transformers>=3.0.0 in c:\\users\\764883\\appdata\\roaming\\python\\python311\\site-packages (from bert_score) (4.35.2)\n",
      "Requirement already satisfied: numpy in c:\\program files\\python311\\lib\\site-packages (from bert_score) (1.23.5)\n",
      "Requirement already satisfied: requests in c:\\program files\\python311\\lib\\site-packages (from bert_score) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.31.1 in c:\\program files\\python311\\lib\\site-packages (from bert_score) (4.65.0)\n",
      "Requirement already satisfied: matplotlib in c:\\program files\\python311\\lib\\site-packages (from bert_score) (3.7.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\program files\\python311\\lib\\site-packages (from bert_score) (23.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\program files\\python311\\lib\\site-packages (from pandas>=1.0.1->bert_score) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\program files\\python311\\lib\\site-packages (from pandas>=1.0.1->bert_score) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\program files\\python311\\lib\\site-packages (from pandas>=1.0.1->bert_score) (2023.3)\n",
      "Requirement already satisfied: filelock in c:\\program files\\python311\\lib\\site-packages (from torch>=1.0.0->bert_score) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\program files\\python311\\lib\\site-packages (from torch>=1.0.0->bert_score) (4.5.0)\n",
      "Requirement already satisfied: sympy in c:\\program files\\python311\\lib\\site-packages (from torch>=1.0.0->bert_score) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\program files\\python311\\lib\\site-packages (from torch>=1.0.0->bert_score) (3.0)\n",
      "Requirement already satisfied: jinja2 in c:\\program files\\python311\\lib\\site-packages (from torch>=1.0.0->bert_score) (3.1.2)\n",
      "Requirement already satisfied: colorama in c:\\program files\\python311\\lib\\site-packages (from tqdm>=4.31.1->bert_score) (0.4.6)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\users\\764883\\appdata\\roaming\\python\\python311\\site-packages (from transformers>=3.0.0->bert_score) (0.19.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\program files\\python311\\lib\\site-packages (from transformers>=3.0.0->bert_score) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\764883\\appdata\\roaming\\python\\python311\\site-packages (from transformers>=3.0.0->bert_score) (2023.10.3)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\764883\\appdata\\roaming\\python\\python311\\site-packages (from transformers>=3.0.0->bert_score) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\764883\\appdata\\roaming\\python\\python311\\site-packages (from transformers>=3.0.0->bert_score) (0.4.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\program files\\python311\\lib\\site-packages (from matplotlib->bert_score) (1.0.7)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\program files\\python311\\lib\\site-packages (from matplotlib->bert_score) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\program files\\python311\\lib\\site-packages (from matplotlib->bert_score) (4.39.4)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\program files\\python311\\lib\\site-packages (from matplotlib->bert_score) (1.4.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\program files\\python311\\lib\\site-packages (from matplotlib->bert_score) (9.5.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\764883\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib->bert_score) (2.4.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\program files\\python311\\lib\\site-packages (from requests->bert_score) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\program files\\python311\\lib\\site-packages (from requests->bert_score) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\program files\\python311\\lib\\site-packages (from requests->bert_score) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\program files\\python311\\lib\\site-packages (from requests->bert_score) (2023.5.7)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\764883\\appdata\\roaming\\python\\python311\\site-packages (from huggingface-hub<1.0,>=0.16.4->transformers>=3.0.0->bert_score) (2023.10.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\program files\\python311\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->bert_score) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\program files\\python311\\lib\\site-packages (from jinja2->torch>=1.0.0->bert_score) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\program files\\python311\\lib\\site-packages (from sympy->torch>=1.0.0->bert_score) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\764883\\appdata\\roaming\\python\\python311\\site-packages (0.1.99)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in c:\\program files\\python311\\lib\\site-packages (2.0.1+cu118)\n",
      "Requirement already satisfied: filelock in c:\\program files\\python311\\lib\\site-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\program files\\python311\\lib\\site-packages (from torch) (4.5.0)\n",
      "Requirement already satisfied: sympy in c:\\program files\\python311\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\program files\\python311\\lib\\site-packages (from torch) (3.0)\n",
      "Requirement already satisfied: jinja2 in c:\\program files\\python311\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\program files\\python311\\lib\\site-packages (from jinja2->torch) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\program files\\python311\\lib\\site-packages (from sympy->torch) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in c:\\users\\764883\\appdata\\roaming\\python\\python311\\site-packages (4.35.2)\n",
      "Requirement already satisfied: filelock in c:\\program files\\python311\\lib\\site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\users\\764883\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (0.19.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\program files\\python311\\lib\\site-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\program files\\python311\\lib\\site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\program files\\python311\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\764883\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in c:\\program files\\python311\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\764883\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\764883\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\program files\\python311\\lib\\site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\764883\\appdata\\roaming\\python\\python311\\site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\program files\\python311\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
      "Requirement already satisfied: colorama in c:\\program files\\python311\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\program files\\python311\\lib\\site-packages (from requests->transformers) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\program files\\python311\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\program files\\python311\\lib\\site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\program files\\python311\\lib\\site-packages (from requests->transformers) (2023.5.7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\program files\\python311\\lib\\site-packages (4.12.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\program files\\python311\\lib\\site-packages (from beautifulsoup4) (2.4.1)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: extract-msg in c:\\users\\764883\\appdata\\roaming\\python\\python311\\site-packages (0.46.2)\n",
      "Requirement already satisfied: olefile==0.46 in c:\\users\\764883\\appdata\\roaming\\python\\python311\\site-packages (from extract-msg) (0.46)\n",
      "Requirement already satisfied: tzlocal<6,>=4.2 in c:\\users\\764883\\appdata\\roaming\\python\\python311\\site-packages (from extract-msg) (5.2)\n",
      "Requirement already satisfied: compressed-rtf<2,>=1.0.6 in c:\\users\\764883\\appdata\\roaming\\python\\python311\\site-packages (from extract-msg) (1.0.6)\n",
      "Requirement already satisfied: ebcdic<2,>=1.1.1 in c:\\users\\764883\\appdata\\roaming\\python\\python311\\site-packages (from extract-msg) (1.1.1)\n",
      "Requirement already satisfied: beautifulsoup4<4.13,>=4.11.1 in c:\\program files\\python311\\lib\\site-packages (from extract-msg) (4.12.2)\n",
      "Requirement already satisfied: RTFDE<0.2,>=0.1.0 in c:\\users\\764883\\appdata\\roaming\\python\\python311\\site-packages (from extract-msg) (0.1.0)\n",
      "Requirement already satisfied: red-black-tree-mod==1.20 in c:\\users\\764883\\appdata\\roaming\\python\\python311\\site-packages (from extract-msg) (1.20)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\program files\\python311\\lib\\site-packages (from beautifulsoup4<4.13,>=4.11.1->extract-msg) (2.4.1)\n",
      "Requirement already satisfied: lark==1.1.5 in c:\\users\\764883\\appdata\\roaming\\python\\python311\\site-packages (from RTFDE<0.2,>=0.1.0->extract-msg) (1.1.5)\n",
      "Requirement already satisfied: oletools>=0.56 in c:\\users\\764883\\appdata\\roaming\\python\\python311\\site-packages (from RTFDE<0.2,>=0.1.0->extract-msg) (0.60.1)\n",
      "Requirement already satisfied: tzdata in c:\\program files\\python311\\lib\\site-packages (from tzlocal<6,>=4.2->extract-msg) (2023.3)\n",
      "Requirement already satisfied: pyparsing<3,>=2.1.0 in c:\\users\\764883\\appdata\\roaming\\python\\python311\\site-packages (from oletools>=0.56->RTFDE<0.2,>=0.1.0->extract-msg) (2.4.7)\n",
      "Requirement already satisfied: easygui in c:\\users\\764883\\appdata\\roaming\\python\\python311\\site-packages (from oletools>=0.56->RTFDE<0.2,>=0.1.0->extract-msg) (0.98.3)\n",
      "Requirement already satisfied: colorclass in c:\\users\\764883\\appdata\\roaming\\python\\python311\\site-packages (from oletools>=0.56->RTFDE<0.2,>=0.1.0->extract-msg) (2.2.2)\n",
      "Requirement already satisfied: pcodedmp>=1.2.5 in c:\\users\\764883\\appdata\\roaming\\python\\python311\\site-packages (from oletools>=0.56->RTFDE<0.2,>=0.1.0->extract-msg) (1.2.6)\n",
      "Requirement already satisfied: msoffcrypto-tool in c:\\users\\764883\\appdata\\roaming\\python\\python311\\site-packages (from oletools>=0.56->RTFDE<0.2,>=0.1.0->extract-msg) (5.1.1)\n",
      "Requirement already satisfied: win-unicode-console in c:\\users\\764883\\appdata\\roaming\\python\\python311\\site-packages (from pcodedmp>=1.2.5->oletools>=0.56->RTFDE<0.2,>=0.1.0->extract-msg) (0.5)\n",
      "Requirement already satisfied: cryptography>=35.0 in c:\\program files\\python311\\lib\\site-packages (from msoffcrypto-tool->oletools>=0.56->RTFDE<0.2,>=0.1.0->extract-msg) (40.0.2)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\program files\\python311\\lib\\site-packages (from cryptography>=35.0->msoffcrypto-tool->oletools>=0.56->RTFDE<0.2,>=0.1.0->extract-msg) (1.15.1)\n",
      "Requirement already satisfied: pycparser in c:\\program files\\python311\\lib\\site-packages (from cffi>=1.12->cryptography>=35.0->msoffcrypto-tool->oletools>=0.56->RTFDE<0.2,>=0.1.0->extract-msg) (2.21)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: py7zr in c:\\users\\764883\\appdata\\roaming\\python\\python311\\site-packages (0.20.8)\n",
      "Requirement already satisfied: texttable in c:\\users\\764883\\appdata\\roaming\\python\\python311\\site-packages (from py7zr) (1.7.0)\n",
      "Requirement already satisfied: pycryptodomex>=3.16.0 in c:\\users\\764883\\appdata\\roaming\\python\\python311\\site-packages (from py7zr) (3.19.0)\n",
      "Requirement already satisfied: pyzstd>=0.15.9 in c:\\users\\764883\\appdata\\roaming\\python\\python311\\site-packages (from py7zr) (0.15.9)\n",
      "Requirement already satisfied: pyppmd<1.2.0,>=1.1.0 in c:\\users\\764883\\appdata\\roaming\\python\\python311\\site-packages (from py7zr) (1.1.0)\n",
      "Requirement already satisfied: pybcj<1.1.0,>=1.0.0 in c:\\users\\764883\\appdata\\roaming\\python\\python311\\site-packages (from py7zr) (1.0.2)\n",
      "Requirement already satisfied: multivolumefile>=0.2.3 in c:\\users\\764883\\appdata\\roaming\\python\\python311\\site-packages (from py7zr) (0.2.3)\n",
      "Requirement already satisfied: inflate64<1.1.0,>=1.0.0 in c:\\users\\764883\\appdata\\roaming\\python\\python311\\site-packages (from py7zr) (1.0.0)\n",
      "Requirement already satisfied: brotli>=1.1.0 in c:\\users\\764883\\appdata\\roaming\\python\\python311\\site-packages (from py7zr) (1.1.0)\n",
      "Requirement already satisfied: psutil in c:\\program files\\python311\\lib\\site-packages (from py7zr) (5.9.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in c:\\program files\\python311\\lib\\site-packages (2.0.2)\n",
      "Requirement already satisfied: matplotlib in c:\\program files\\python311\\lib\\site-packages (3.7.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\program files\\python311\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\program files\\python311\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\program files\\python311\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\program files\\python311\\lib\\site-packages (from pandas) (1.23.5)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\program files\\python311\\lib\\site-packages (from matplotlib) (1.0.7)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\program files\\python311\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\program files\\python311\\lib\\site-packages (from matplotlib) (4.39.4)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\program files\\python311\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\program files\\python311\\lib\\site-packages (from matplotlib) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\program files\\python311\\lib\\site-packages (from matplotlib) (9.5.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\764883\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: six>=1.5 in c:\\program files\\python311\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\program files\\python311\\lib\\site-packages (4.12.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\program files\\python311\\lib\\site-packages (from beautifulsoup4) (2.4.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers[torch] in c:\\users\\764883\\appdata\\roaming\\python\\python311\\site-packages (4.35.2)\n",
      "Requirement already satisfied: filelock in c:\\program files\\python311\\lib\\site-packages (from transformers[torch]) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\users\\764883\\appdata\\roaming\\python\\python311\\site-packages (from transformers[torch]) (0.19.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\program files\\python311\\lib\\site-packages (from transformers[torch]) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\program files\\python311\\lib\\site-packages (from transformers[torch]) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\program files\\python311\\lib\\site-packages (from transformers[torch]) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\764883\\appdata\\roaming\\python\\python311\\site-packages (from transformers[torch]) (2023.10.3)\n",
      "Requirement already satisfied: requests in c:\\program files\\python311\\lib\\site-packages (from transformers[torch]) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\764883\\appdata\\roaming\\python\\python311\\site-packages (from transformers[torch]) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\764883\\appdata\\roaming\\python\\python311\\site-packages (from transformers[torch]) (0.4.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\program files\\python311\\lib\\site-packages (from transformers[torch]) (4.65.0)\n",
      "Requirement already satisfied: torch!=1.12.0,>=1.10 in c:\\program files\\python311\\lib\\site-packages (from transformers[torch]) (2.0.1+cu118)\n",
      "Requirement already satisfied: accelerate>=0.20.3 in c:\\users\\764883\\appdata\\roaming\\python\\python311\\site-packages (from transformers[torch]) (0.24.1)\n",
      "Requirement already satisfied: psutil in c:\\program files\\python311\\lib\\site-packages (from accelerate>=0.20.3->transformers[torch]) (5.9.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\764883\\appdata\\roaming\\python\\python311\\site-packages (from huggingface-hub<1.0,>=0.16.4->transformers[torch]) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\program files\\python311\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->transformers[torch]) (4.5.0)\n",
      "Requirement already satisfied: sympy in c:\\program files\\python311\\lib\\site-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\program files\\python311\\lib\\site-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (3.0)\n",
      "Requirement already satisfied: jinja2 in c:\\program files\\python311\\lib\\site-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (3.1.2)\n",
      "Requirement already satisfied: colorama in c:\\program files\\python311\\lib\\site-packages (from tqdm>=4.27->transformers[torch]) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\program files\\python311\\lib\\site-packages (from requests->transformers[torch]) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\program files\\python311\\lib\\site-packages (from requests->transformers[torch]) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\program files\\python311\\lib\\site-packages (from requests->transformers[torch]) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\program files\\python311\\lib\\site-packages (from requests->transformers[torch]) (2023.5.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\program files\\python311\\lib\\site-packages (from jinja2->torch!=1.12.0,>=1.10->transformers[torch]) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\program files\\python311\\lib\\site-packages (from sympy->torch!=1.12.0,>=1.10->transformers[torch]) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: textblob in c:\\users\\764883\\appdata\\roaming\\python\\python311\\site-packages (0.17.1)\n",
      "Requirement already satisfied: nltk>=3.1 in c:\\users\\764883\\appdata\\roaming\\python\\python311\\site-packages (from textblob) (3.8.1)\n",
      "Requirement already satisfied: click in c:\\program files\\python311\\lib\\site-packages (from nltk>=3.1->textblob) (8.1.3)\n",
      "Requirement already satisfied: joblib in c:\\program files\\python311\\lib\\site-packages (from nltk>=3.1->textblob) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\764883\\appdata\\roaming\\python\\python311\\site-packages (from nltk>=3.1->textblob) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\program files\\python311\\lib\\site-packages (from nltk>=3.1->textblob) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\program files\\python311\\lib\\site-packages (from click->nltk>=3.1->textblob) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: clean-text in c:\\users\\764883\\appdata\\roaming\\python\\python311\\site-packages (0.6.0)\n",
      "Requirement already satisfied: emoji<2.0.0,>=1.0.0 in c:\\users\\764883\\appdata\\roaming\\python\\python311\\site-packages (from clean-text) (1.7.0)\n",
      "Requirement already satisfied: ftfy<7.0,>=6.0 in c:\\users\\764883\\appdata\\roaming\\python\\python311\\site-packages (from clean-text) (6.1.1)\n",
      "Requirement already satisfied: wcwidth>=0.2.5 in c:\\program files\\python311\\lib\\site-packages (from ftfy<7.0,>=6.0->clean-text) (0.2.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "!pip install transformers\n",
    "!pip install rouge\n",
    "!pip install bert_score\n",
    "!pip install sentencepiece\n",
    "!pip install torch\n",
    "!pip install transformers\n",
    "!pip install beautifulsoup4\n",
    "!pip install extract-msg\n",
    "!pip install py7zr\n",
    "!pip install pandas matplotlib\n",
    "!pip install beautifulsoup4\n",
    "!pip install transformers[torch]\n",
    "!pip install textblob\n",
    "!pip install clean-text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68973cb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\764883\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\764883\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\764883\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\764883\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "Since the GPL-licensed package `unidecode` is not installed, using Python's `unicodedata` package which yields worse results.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\764883\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\764883\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\764883\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\764883\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\764883\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\764883\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\764883\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\764883\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import email\n",
    "import json\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "import torch\n",
    "from bs4 import BeautifulSoup\n",
    "from cleantext import clean  # pip install clean-text\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "from nltk import download\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from pathlib import Path\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from textblob import TextBlob\n",
    "from transformers import (AdamW, AutoTokenizer, BartTokenizer,\n",
    "                          BartForConditionalGeneration, BertForSequenceClassification,\n",
    "                          BertModel, BertTokenizer, GPT2LMHeadModel,\n",
    "                          GPT2Tokenizer, LongT5ForConditionalGeneration,\n",
    "                          MarianMTModel, MarianTokenizer, RobertaModel,\n",
    "                          T5ForConditionalGeneration, T5Tokenizer,\n",
    "                          TrainingArguments, Trainer)\n",
    "from torch.optim.lr_scheduler import OneCycleLR, CyclicLR\n",
    "import sentencepiece  # Required for LongT5\n",
    "from datasets import Dataset as HFDataset, load_metric\n",
    "from bert_score import score as bert_score\n",
    "from tqdm import tqdm\n",
    "from dateutil.parser import parse\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, TensorDataset\n",
    "\n",
    "download('stopwords')\n",
    "download('punkt')\n",
    "download('brown')\n",
    "download('averaged_perceptron_tagger')\n",
    "import sentencepiece\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict, load_metric\n",
    "from time import sleep\n",
    "from transformers import AutoTokenizer, T5ForConditionalGeneration\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, TensorDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b943c7bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\764883\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\764883\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\764883\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\764883\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\764883\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from transformers import LongT5ForConditionalGeneration\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "from transformers import BertForSequenceClassification, AdamW\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('omw-1.4')\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "import re\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from cleantext import clean  \n",
    "from textblob import TextBlob  \n",
    "from bert_score import score as bert_score \n",
    "import sentencepiece  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd230af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('email_data_all_summaries_everything.json',lines= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec5c50f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2280 entries, 0 to 2279\n",
      "Data columns (total 17 columns):\n",
      " #   Column                Non-Null Count  Dtype \n",
      "---  ------                --------------  ----- \n",
      " 0   subject               2280 non-null   object\n",
      " 1   from                  2280 non-null   object\n",
      " 2   to                    2280 non-null   object\n",
      " 3   date                  2280 non-null   object\n",
      " 4   body                  2280 non-null   object\n",
      " 5   Body_Length           2280 non-null   int64 \n",
      " 6   Subject_Length        2280 non-null   int64 \n",
      " 7   Cleaned_Body          2280 non-null   object\n",
      " 8   Cleaned_Subject       2280 non-null   object\n",
      " 9   BERT_Embeddings       2280 non-null   object\n",
      " 10  Cluster_Label         2280 non-null   int64 \n",
      " 11  Category              2280 non-null   object\n",
      " 12  Cleaned_mails         2280 non-null   object\n",
      " 13  summary_TXTRNK_1      2280 non-null   object\n",
      " 14  Summary               2280 non-null   object\n",
      " 15  summary_BART_from_TR  2280 non-null   object\n",
      " 16  summary_BART          2280 non-null   object\n",
      "dtypes: int64(3), object(14)\n",
      "memory usage: 302.9+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bea2f10",
   "metadata": {},
   "source": [
    "## T5 model without lemmatisation, target as summary and input as Cleaned_Body # epoch 10, batchsize 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09898b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model_name = 't5-small'\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "from transformers import T5ForConditionalGeneration\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d57ceda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataframe(df, tokenizer, lower=True):\n",
    "    email_content, eval_data = {}, {}\n",
    "    total = 0\n",
    "    input_lens, output_lens = [], []\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        total += 1\n",
    "\n",
    "        # Ensure Body and summary are strings\n",
    "        Body = str(row['Cleaned_mails']) if isinstance(row['Cleaned_mails'], str) else 'null'\n",
    "        summary = str(row['Summary']) if isinstance(row['Summary'], str) else 'null'\n",
    "\n",
    "        # Optional lowercase\n",
    "        if lower:\n",
    "            Body = Body.lower()\n",
    "            summary = summary.lower()\n",
    "\n",
    "        # Strip leading/trailing whitespace from article and summary\n",
    "        Body = Body.strip()\n",
    "        summary = summary.strip()\n",
    "\n",
    "        # Tokenize\n",
    "        input_tokens = tokenizer.tokenize(\"summarize: \" + Body)\n",
    "        output_tokens = tokenizer.tokenize(summary)\n",
    "\n",
    "        # Lengths\n",
    "        input_lens.append(len(input_tokens))\n",
    "        output_lens.append(len(output_tokens))\n",
    "\n",
    "        # Store input \n",
    "        email = {\"input\": input_tokens, \"output\": output_tokens, \"id\": total}\n",
    "        eval_data[str(total)] = (Body, summary)\n",
    "        email_content[str(total)] = email\n",
    "\n",
    "    # Shuffle email content\n",
    "    email_content = dict(np.random.permutation(list(email_content.items())))\n",
    "\n",
    "    print(f\"{len(email_content)} email contents in total\")\n",
    "    print(f\"max_input: {max(input_lens)}, max_output: {max(output_lens)}.\")\n",
    "    print(f\"avg_input: {np.mean(input_lens)}, avg_output: {np.mean(output_lens)}.\")\n",
    "\n",
    "    return email_content, eval_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "94f85cf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2177 email contents in total\n",
      "max_input: 77929, max_output: 2503.\n",
      "avg_input: 727.7615985300873, avg_output: 172.14010105649976.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "\n",
    "# Process your DataFrame\n",
    "email_content, eval_data = process_dataframe(df, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c684490b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows where 'summary_BART_from_TR' is NaN or contains only whitespace\n",
    "df = df[df['Summary'].notna() & df['Summary'].str.strip().astype(bool)]\n",
    "\n",
    "# Remove rows where 'summary_BART_from_TR' has less than 10 words\n",
    "df = df[df['Summary'].str.split().apply(len) >= 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "55ad7cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Extracting input and output texts\n",
    "input_texts = [email['input'] for email in email_content.values()]\n",
    "target_texts = [email['output'] for email in email_content.values()]\n",
    "\n",
    "# Split into training (80%) and validation (20%) sets\n",
    "train_inputs, val_inputs, train_targets, val_targets = train_test_split(\n",
    "    input_texts, target_texts, test_size=0.2, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cecc2152",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class EmailDataset(Dataset):\n",
    "    def __init__(self, tokenizer, inputs, targets, max_input_length=512, max_target_length=128):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "        self.max_input_length = max_input_length\n",
    "        self.max_target_length = max_target_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Tokenize input and target texts with appropriate padding\n",
    "        input_ids = self.tokenizer.encode(self.inputs[idx], max_length=self.max_input_length, truncation=True, padding=\"max_length\", return_tensors=\"pt\").squeeze()\n",
    "        target_ids = self.tokenizer.encode(self.targets[idx], max_length=self.max_target_length, truncation=True, padding=\"max_length\", return_tensors=\"pt\").squeeze()\n",
    "\n",
    "        # Prepare decoder_input_ids\n",
    "        decoder_input_ids = self._shift_right(target_ids)\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"labels\": target_ids,\n",
    "            \"decoder_input_ids\": decoder_input_ids\n",
    "        }\n",
    "\n",
    "    def _shift_right(self, target_ids):\n",
    "        # Shift the target ids to the right and add pad token at the beginning\n",
    "        shifted_target_ids = torch.cat([torch.tensor([self.tokenizer.pad_token_id]), target_ids[:-1]])\n",
    "        return shifted_target_ids\n",
    "\n",
    "# Prepare your data\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "# Create EmailDataset instances for each split\n",
    "train_dataset = EmailDataset(tokenizer, train_inputs, train_targets)\n",
    "val_dataset = EmailDataset(tokenizer, val_inputs, val_targets)\n",
    "# test_dataset = EmailDataset(tokenizer, test_inputs, test_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f49776e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback\n",
    "import numpy as np\n",
    "from rouge_score import rouge_scorer, scoring\n",
    "from bert_score import score\n",
    "import torch\n",
    "\n",
    "class CustomEvaluationCallback(TrainerCallback):\n",
    "    def __init__(self, eval_dataset, tokenizer, device):\n",
    "        self.eval_dataset = eval_dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "\n",
    "    def on_evaluate(self, args, state, control, **kwargs):\n",
    "        model = kwargs[\"model\"]\n",
    "        predictions, references = [], []\n",
    "\n",
    "        for batch in self.eval_dataset:\n",
    "            inputs = batch['input_ids'].to(self.device)\n",
    "#             attention_mask = batch.get('attention_mask', None).to(self.device) if 'attention_mask' in batch else None\n",
    "            if 'attention_mask' in batch:\n",
    "                attention_mask = batch['attention_mask'].to(self.device)\n",
    "            else:\n",
    "                attention_mask = None\n",
    "\n",
    "            if len(inputs.shape) == 1:\n",
    "                inputs = inputs.unsqueeze(0)\n",
    "            if attention_mask is not None and len(attention_mask.shape) == 1:\n",
    "                attention_mask = attention_mask.unsqueeze(0)\n",
    "\n",
    "            outputs = model.generate(inputs, attention_mask=attention_mask)\n",
    "            batch_predictions = [self.tokenizer.decode(o, skip_special_tokens=True) for o in outputs]\n",
    "\n",
    "            for i in range(len(batch_predictions)):\n",
    "                predictions.append(batch_predictions[i])\n",
    "                references.append(self.tokenizer.decode(batch['labels'][i], skip_special_tokens=True))\n",
    "\n",
    "        # Compute ROUGE and BERTScore\n",
    "        rouge_scores = self.compute_rouge(references, predictions)\n",
    "        bert_scores = self.compute_bertScore(references, predictions)\n",
    "        \n",
    "        \n",
    "        # Print formatted scores\n",
    "        self.print_formatted_scores(rouge_scores, bert_scores, state.epoch)\n",
    "\n",
    "        return control\n",
    "\n",
    "    def compute_rouge(self, targets, predictions, score_keys=None, use_stemmer=True):\n",
    "        if score_keys is None:\n",
    "            score_keys = [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]\n",
    "        scorer = rouge_scorer.RougeScorer(score_keys, use_stemmer=use_stemmer)\n",
    "        aggregator = scoring.BootstrapAggregator()\n",
    "\n",
    "        for target, prediction in zip(targets, predictions):\n",
    "            aggregator.add_scores(scorer.score(target=target, prediction=prediction))\n",
    "        result = aggregator.aggregate()\n",
    "\n",
    "        return {key: result[key].mid.fmeasure * 100 for key in score_keys}\n",
    "\n",
    "    def compute_bertScore(self, refs, cands, rescale_with_baseline=True):\n",
    "        P, R, F1 = score(cands, refs, lang=\"en\", rescale_with_baseline=rescale_with_baseline)\n",
    "        return {\"bertScore\": F1.mean().item() * 100}\n",
    "\n",
    "    def print_formatted_scores(self, rouge_scores, bert_scores, epoch):\n",
    "    # Headers for the scores\n",
    "        headers = [\"Epoch\"] + list(rouge_scores.keys()) + [\"bertScore\"]\n",
    "    \n",
    "    # Values for the scores\n",
    "        values = [f\"{epoch}\"] + [f\"{score:.2f}\" for score in rouge_scores.values()] + [f\"{bert_scores['bertScore']:.2f}\"]\n",
    "\n",
    "    # Calculate the max width for each column\n",
    "        column_widths = [max(len(header), len(value)) + 2 for header, value in zip(headers, values)]\n",
    "\n",
    "    # Print the header\n",
    "        header_row = \" | \".join(f\"{header:<{column_widths[idx]}}\" for idx, header in enumerate(headers))\n",
    "        print(header_row)\n",
    "        print(\"-\" * len(header_row))\n",
    "\n",
    "    # Print the scores\n",
    "        score_row = \" | \".join(f\"{value:<{column_widths[idx]}}\" for idx, value in enumerate(values))\n",
    "        print(score_row)\n",
    "        print(\"\\n\" + \"-\" * len(header_row))\n",
    "\n",
    "# Usage\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "custom_eval_callback = CustomEvaluationCallback(val_dataset, tokenizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9966a862",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4360' max='4360' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4360/4360 23:39, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.270300</td>\n",
       "      <td>0.509640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.516000</td>\n",
       "      <td>0.384039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.408000</td>\n",
       "      <td>0.344187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.368200</td>\n",
       "      <td>0.326658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.343000</td>\n",
       "      <td>0.314594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.325100</td>\n",
       "      <td>0.305587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.314200</td>\n",
       "      <td>0.299739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.302300</td>\n",
       "      <td>0.295789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.295500</td>\n",
       "      <td>0.292771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.287900</td>\n",
       "      <td>0.292145</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "1.0     | 6.32     | 0.00     | 6.34     | 6.34        | -77.50     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "2.0     | 6.98     | 0.00     | 7.00     | 6.96        | -78.65     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "3.0     | 6.31     | 0.00     | 6.32     | 6.33        | -78.81     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "4.0     | 6.74     | 0.00     | 6.74     | 6.76        | -78.71     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "5.0     | 6.65     | 0.00     | 6.65     | 6.62        | -77.47     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "6.0     | 6.76     | 0.00     | 6.76     | 6.77        | -77.40     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "7.0     | 6.65     | 0.00     | 6.64     | 6.68        | -77.43     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "8.0     | 6.80     | 0.00     | 6.79     | 6.81        | -77.44     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "9.0     | 6.77     | 0.00     | 6.76     | 6.75        | -77.53     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "10.0    | 6.81     | 0.00     | 6.81     | 6.80        | -77.45     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer, T5ForConditionalGeneration, AdamW\n",
    "\n",
    "# Load the T5 model\n",
    "model_name = 't5-small'  \n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "\n",
    "# Calculate the size of the training dataset\n",
    "training_dataset_size = len(train_dataset)\n",
    "\n",
    "N = training_dataset_size\n",
    "batch_size = 4  \n",
    "\n",
    "# Calculate steps per epoch\n",
    "steps_per_epoch = N // batch_size\n",
    "\n",
    "\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",                      # Directory for storing outputs\n",
    "    num_train_epochs=10,                         # Set number of epochs to 10\n",
    "    per_device_train_batch_size=4,               # Batch size for training\n",
    "    per_device_eval_batch_size=4,                # Batch size for evaluation\n",
    "    warmup_steps=500,                            # Number of warmup steps\n",
    "    weight_decay=0.01,                           # Weight decay for regularization\n",
    "    logging_dir=\"./logs\",                        # Directory for storing logs\n",
    "    logging_steps=steps_per_epoch,                            # Log every steps in epoch\n",
    "    do_train=True,                               # Enable training\n",
    "    do_eval=True,                                # Enable evaluation\n",
    "    evaluation_strategy=\"epoch\",                 # Evaluate at the end of each epoch\n",
    "    save_strategy=\"no\",                    # Disable saving the model\n",
    "    save_total_limit=0,                    # Limit on the total amount of checkpoints. Setting to 0 disables it\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,  # Use the dataset instance for training data\n",
    "    eval_dataset=val_dataset,     # Use the dataset instance for validation data\n",
    "    callbacks=[custom_eval_callback],  # Custom callback function for metrics\n",
    "    optimizers=(AdamW(model.parameters(), lr=5e-5), None)  # AdamW optimizer with a specified learning rate\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n",
    "# Plotting\n",
    "training_logs = trainer.state.log_history\n",
    "\n",
    "training_loss = [log['loss'] for log in training_logs if 'loss' in log]\n",
    "validation_loss = [log['eval_loss'] for log in training_logs if 'eval_loss' in log]\n",
    "epochs = range(1, len(training_loss) + 1)\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(epochs, training_loss, 'bo-', label='Training loss')\n",
    "plt.plot(epochs, validation_loss, 'ro-', label='Validation loss')\n",
    "plt.title('Training and Validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6864b16b",
   "metadata": {},
   "source": [
    "## T5 with batch size 4 lr = 5e-5, Cleaned_mails ans Summary, epoch= 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3468ce8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8720' max='8720' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8720/8720 48:13, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.270300</td>\n",
       "      <td>0.509640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.515700</td>\n",
       "      <td>0.383007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.405200</td>\n",
       "      <td>0.342684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.363500</td>\n",
       "      <td>0.321571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.334800</td>\n",
       "      <td>0.307889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.312700</td>\n",
       "      <td>0.296303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.297800</td>\n",
       "      <td>0.285261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.280000</td>\n",
       "      <td>0.280208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.266500</td>\n",
       "      <td>0.272381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.253000</td>\n",
       "      <td>0.266934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.245800</td>\n",
       "      <td>0.267929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.238600</td>\n",
       "      <td>0.261554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.232700</td>\n",
       "      <td>0.259892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.224000</td>\n",
       "      <td>0.257268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.227000</td>\n",
       "      <td>0.255918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.215600</td>\n",
       "      <td>0.256049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.211300</td>\n",
       "      <td>0.252472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.211300</td>\n",
       "      <td>0.253734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.210300</td>\n",
       "      <td>0.253989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.207700</td>\n",
       "      <td>0.253399</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "1.0     | 6.32     | 0.00     | 6.34     | 6.34        | -77.50     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "2.0     | 6.80     | 0.00     | 6.81     | 6.78        | -77.41     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "3.0     | 6.26     | 0.00     | 6.29     | 6.30        | -78.82     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "4.0     | 6.77     | 0.00     | 6.78     | 6.78        | -78.52     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "5.0     | 6.70     | 0.00     | 6.70     | 6.66        | -77.61     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "6.0     | 6.80     | 0.00     | 6.79     | 6.81        | -77.58     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "7.0     | 6.56     | 0.00     | 6.56     | 6.58        | -77.63     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "8.0     | 6.74     | 0.00     | 6.72     | 6.74        | -77.61     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "9.0     | 6.79     | 0.00     | 6.78     | 6.77        | -77.92     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "10.0    | 6.54     | 0.00     | 6.54     | 6.54        | -77.80     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "11.0    | 6.85     | 0.00     | 6.85     | 6.84        | -77.85     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "12.0    | 6.87     | 0.00     | 6.87     | 6.87        | -78.04     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "13.0    | 7.09     | 0.00     | 7.08     | 7.08        | -77.86     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "14.0    | 6.99     | 0.00     | 7.00     | 7.00        | -77.74     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "15.0    | 7.02     | 0.00     | 7.03     | 7.02        | -77.75     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "16.0    | 7.03     | 0.00     | 7.01     | 7.03        | -77.70     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "17.0    | 6.91     | 0.00     | 6.94     | 6.97        | -77.78     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "18.0    | 7.03     | 0.00     | 7.03     | 7.04        | -77.54     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "19.0    | 7.01     | 0.00     | 7.01     | 7.04        | -77.58     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "20.0    | 7.00     | 0.00     | 7.05     | 7.04        | -77.54     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer, T5ForConditionalGeneration, AdamW\n",
    "\n",
    "# Load the T5 model\n",
    "model_name = 't5-small' \n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "\n",
    "# Calculate the size of the training dataset\n",
    "training_dataset_size = len(train_dataset)\n",
    "\n",
    "N = training_dataset_size\n",
    "batch_size = 4  \n",
    "\n",
    "# Calculate steps per epoch\n",
    "steps_per_epoch = N // batch_size\n",
    "\n",
    "\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",                      # Directory for storing outputs\n",
    "    num_train_epochs=20,                         # Set number of epochs to 10\n",
    "    per_device_train_batch_size=4,               # Batch size for training\n",
    "    per_device_eval_batch_size=4,                # Batch size for evaluation\n",
    "    warmup_steps=500,                            # Number of warmup steps\n",
    "    weight_decay=0.01,                           # Weight decay for regularization\n",
    "    logging_dir=\"./logs\",                        # Directory for storing logs\n",
    "    logging_steps=steps_per_epoch,                            # Log every steps in epoch\n",
    "    do_train=True,                               # Enable training\n",
    "    do_eval=True,                                # Enable evaluation\n",
    "    evaluation_strategy=\"epoch\",                 # Evaluate at the end of each epoch\n",
    "    save_strategy=\"no\",                    # Disable saving the model\n",
    "    save_total_limit=0,                    # Limit on the total amount of checkpoints. Setting to 0 disables it\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,  # Use the dataset instance for training data\n",
    "    eval_dataset=val_dataset,     # Use the dataset instance for validation data\n",
    "    callbacks=[custom_eval_callback],  # Custom callback function for metrics\n",
    "    optimizers=(AdamW(model.parameters(), lr=5e-5), None)  # AdamW optimizer with a specified learning rate\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n",
    "# Plotting\n",
    "training_logs = trainer.state.log_history\n",
    "\n",
    "training_loss = [log['loss'] for log in training_logs if 'loss' in log]\n",
    "validation_loss = [log['eval_loss'] for log in training_logs if 'eval_loss' in log]\n",
    "epochs = range(1, len(training_loss) + 1)\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(epochs, training_loss, 'bo-', label='Training loss')\n",
    "plt.plot(epochs, validation_loss, 'ro-', label='Validation loss')\n",
    "plt.title('Training and Validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3595f205",
   "metadata": {},
   "source": [
    "## T5 with batch size 8 lr = 5e-5, Cleaned_mails ans Summary, epoch= 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "98c0f5e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4360' max='4360' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4360/4360 44:44, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.713200</td>\n",
       "      <td>0.693872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.703100</td>\n",
       "      <td>0.477080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.492700</td>\n",
       "      <td>0.383898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.419100</td>\n",
       "      <td>0.358215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.377800</td>\n",
       "      <td>0.336089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.353600</td>\n",
       "      <td>0.325603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.337700</td>\n",
       "      <td>0.314126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.324000</td>\n",
       "      <td>0.305975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.303900</td>\n",
       "      <td>0.296870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.290400</td>\n",
       "      <td>0.290606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.289000</td>\n",
       "      <td>0.286979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.278300</td>\n",
       "      <td>0.280394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.274100</td>\n",
       "      <td>0.278710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.266700</td>\n",
       "      <td>0.276235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.268100</td>\n",
       "      <td>0.275104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.252200</td>\n",
       "      <td>0.272938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.251500</td>\n",
       "      <td>0.271873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.252300</td>\n",
       "      <td>0.270923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.253900</td>\n",
       "      <td>0.270040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.242100</td>\n",
       "      <td>0.269919</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "1.0     | 4.30     | 0.00     | 4.28     | 4.26        | -78.11     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "2.0     | 6.62     | 0.00     | 6.64     | 6.61        | -78.64     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "3.0     | 6.91     | 0.00     | 6.94     | 6.94        | -81.98     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "4.0     | 6.85     | 0.00     | 6.87     | 6.88        | -80.87     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "5.0     | 6.65     | 0.00     | 6.64     | 6.63        | -77.66     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "6.0     | 6.64     | 0.00     | 6.65     | 6.68        | -77.64     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "7.0     | 6.01     | 0.00     | 6.00     | 6.05        | -77.58     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "8.0     | 6.53     | 0.00     | 6.51     | 6.53        | -77.54     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "9.0     | 6.31     | 0.00     | 6.29     | 6.29        | -77.69     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "10.0    | 6.69     | 0.00     | 6.70     | 6.67        | -77.56     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "11.0    | 6.96     | 0.00     | 6.96     | 6.94        | -77.62     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "12.0    | 6.80     | 0.00     | 6.78     | 6.77        | -77.85     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "13.0    | 6.68     | 0.00     | 6.68     | 6.67        | -77.98     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "14.0    | 6.81     | 0.00     | 6.81     | 6.82        | -77.84     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "15.0    | 6.65     | 0.00     | 6.68     | 6.67        | -77.61     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "16.0    | 6.66     | 0.00     | 6.66     | 6.67        | -77.71     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "17.0    | 6.73     | 0.00     | 6.76     | 6.78        | -77.63     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "18.0    | 6.78     | 0.00     | 6.79     | 6.81        | -77.63     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "19.0    | 6.85     | 0.00     | 6.86     | 6.88        | -77.67     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "20.0    | 6.78     | 0.00     | 6.79     | 6.77        | -77.78     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer, T5ForConditionalGeneration, AdamW\n",
    "\n",
    "# Load the T5 model\n",
    "model_name = 't5-small'  \n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "\n",
    "# Calculate the size of the training dataset\n",
    "training_dataset_size = len(train_dataset)\n",
    "\n",
    "N = training_dataset_size\n",
    "batch_size = 8  \n",
    "\n",
    "# Calculate steps per epoch\n",
    "steps_per_epoch = N // batch_size\n",
    "\n",
    "\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",                      # Directory for storing outputs\n",
    "    num_train_epochs=20,                         # Set number of epochs to 10\n",
    "    per_device_train_batch_size=8,               # Batch size for training\n",
    "    per_device_eval_batch_size=8,                # Batch size for evaluation\n",
    "    warmup_steps=500,                            # Number of warmup steps\n",
    "    weight_decay=0.01,                           # Weight decay for regularization\n",
    "    logging_dir=\"./logs\",                        # Directory for storing logs\n",
    "    logging_steps=steps_per_epoch,                            # Log every steps in epoch\n",
    "    do_train=True,                               # Enable training\n",
    "    do_eval=True,                                # Enable evaluation\n",
    "    evaluation_strategy=\"epoch\",                 # Evaluate at the end of each epoch\n",
    "    save_strategy=\"no\",                    # Disable saving the model\n",
    "    save_total_limit=0,                    # Limit on the total amount of checkpoints. Setting to 0 disables it\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,  # Use the dataset instance for training data\n",
    "    eval_dataset=val_dataset,     # Use the dataset instance for validation data\n",
    "    callbacks=[custom_eval_callback],  # Custom callback function for metrics\n",
    "    optimizers=(AdamW(model.parameters(), lr=5e-5), None)  # AdamW optimizer with a specified learning rate\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n",
    "# Plotting\n",
    "training_logs = trainer.state.log_history\n",
    "\n",
    "training_loss = [log['loss'] for log in training_logs if 'loss' in log]\n",
    "validation_loss = [log['eval_loss'] for log in training_logs if 'eval_loss' in log]\n",
    "epochs = range(1, len(training_loss) + 1)\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(epochs, training_loss, 'bo-', label='Training loss')\n",
    "plt.plot(epochs, validation_loss, 'ro-', label='Validation loss')\n",
    "plt.title('Training and Validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e359830",
   "metadata": {},
   "source": [
    "## T5 with batch size 8 lr = 5e-5, Cleaned_mails ans Summary, epoch= 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d9db8b50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4360' max='4360' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4360/4360 44:41, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.713200</td>\n",
       "      <td>0.693872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.703100</td>\n",
       "      <td>0.477080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.492700</td>\n",
       "      <td>0.383898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.419100</td>\n",
       "      <td>0.358215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.377800</td>\n",
       "      <td>0.336089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.353600</td>\n",
       "      <td>0.325603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.337700</td>\n",
       "      <td>0.314126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.324000</td>\n",
       "      <td>0.305975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.303900</td>\n",
       "      <td>0.296870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.290400</td>\n",
       "      <td>0.290606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.289000</td>\n",
       "      <td>0.286979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.278300</td>\n",
       "      <td>0.280394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.274100</td>\n",
       "      <td>0.278710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.266700</td>\n",
       "      <td>0.276235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.268100</td>\n",
       "      <td>0.275104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.252200</td>\n",
       "      <td>0.272938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.251500</td>\n",
       "      <td>0.271873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.252300</td>\n",
       "      <td>0.270923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.253900</td>\n",
       "      <td>0.270040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.242100</td>\n",
       "      <td>0.269919</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "1.0     | 4.30     | 0.00     | 4.28     | 4.26        | -78.11     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "2.0     | 6.62     | 0.00     | 6.64     | 6.61        | -78.64     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "3.0     | 6.91     | 0.00     | 6.94     | 6.94        | -81.98     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "4.0     | 6.85     | 0.00     | 6.87     | 6.88        | -80.87     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "5.0     | 6.65     | 0.00     | 6.64     | 6.63        | -77.66     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "6.0     | 6.64     | 0.00     | 6.65     | 6.68        | -77.64     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "7.0     | 6.01     | 0.00     | 6.00     | 6.05        | -77.58     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "8.0     | 6.53     | 0.00     | 6.51     | 6.53        | -77.54     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "9.0     | 6.31     | 0.00     | 6.29     | 6.29        | -77.69     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "10.0    | 6.69     | 0.00     | 6.70     | 6.67        | -77.56     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "11.0    | 6.96     | 0.00     | 6.96     | 6.94        | -77.62     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "12.0    | 6.80     | 0.00     | 6.78     | 6.77        | -77.85     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "13.0    | 6.68     | 0.00     | 6.68     | 6.67        | -77.98     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "14.0    | 6.81     | 0.00     | 6.81     | 6.82        | -77.84     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "15.0    | 6.65     | 0.00     | 6.68     | 6.67        | -77.61     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "16.0    | 6.66     | 0.00     | 6.66     | 6.67        | -77.71     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "17.0    | 6.73     | 0.00     | 6.76     | 6.78        | -77.63     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "18.0    | 6.78     | 0.00     | 6.79     | 6.81        | -77.63     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "19.0    | 6.85     | 0.00     | 6.86     | 6.88        | -77.67     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "20.0    | 6.78     | 0.00     | 6.79     | 6.77        | -77.78     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer, T5ForConditionalGeneration, AdamW\n",
    "\n",
    "# Load the T5 model\n",
    "model_name = 't5-small'  \n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "\n",
    "# Calculate the size of the training dataset\n",
    "training_dataset_size = len(train_dataset)\n",
    "\n",
    "N = training_dataset_size\n",
    "batch_size = 8  \n",
    "\n",
    "# Calculate steps per epoch\n",
    "steps_per_epoch = N // batch_size\n",
    "\n",
    "\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",                      # Directory for storing outputs\n",
    "    num_train_epochs=20,                         # Set number of epochs to 10\n",
    "    per_device_train_batch_size=8,               # Batch size for training\n",
    "    per_device_eval_batch_size=8,                # Batch size for evaluation\n",
    "    warmup_steps=500,                            # Number of warmup steps\n",
    "    weight_decay=0.01,                           # Weight decay for regularization\n",
    "    logging_dir=\"./logs\",                        # Directory for storing logs\n",
    "    logging_steps=steps_per_epoch,                            # Log every steps in epoch\n",
    "    do_train=True,                               # Enable training\n",
    "    do_eval=True,                                # Enable evaluation\n",
    "    evaluation_strategy=\"epoch\",                 # Evaluate at the end of each epoch\n",
    "    save_strategy=\"no\",                    # Disable saving the model\n",
    "    save_total_limit=0,                    # Limit on the total amount of checkpoints. Setting to 0 disables it\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,  # Use the dataset instance for training data\n",
    "    eval_dataset=val_dataset,     # Use the dataset instance for validation data\n",
    "    callbacks=[custom_eval_callback],  # Custom callback function for metrics\n",
    "    optimizers=(AdamW(model.parameters(), lr=5e-5), None)  # AdamW optimizer with a specified learning rate\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n",
    "# Plotting\n",
    "training_logs = trainer.state.log_history\n",
    "\n",
    "training_loss = [log['loss'] for log in training_logs if 'loss' in log]\n",
    "validation_loss = [log['eval_loss'] for log in training_logs if 'eval_loss' in log]\n",
    "epochs = range(1, len(training_loss) + 1)\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(epochs, training_loss, 'bo-', label='Training loss')\n",
    "plt.plot(epochs, validation_loss, 'ro-', label='Validation loss')\n",
    "plt.title('Training and Validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6812603e",
   "metadata": {},
   "source": [
    "## T5 with batch size 8 lr = 1e-4, Cleaned_mails ans Summary, epoch= 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ed66efae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4360' max='4360' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4360/4360 44:41, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.466100</td>\n",
       "      <td>0.576371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.566200</td>\n",
       "      <td>0.395052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.410700</td>\n",
       "      <td>0.337170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.352400</td>\n",
       "      <td>0.315482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.314600</td>\n",
       "      <td>0.292363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.286000</td>\n",
       "      <td>0.279840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.265800</td>\n",
       "      <td>0.266182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.248900</td>\n",
       "      <td>0.262395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.227100</td>\n",
       "      <td>0.255131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.214500</td>\n",
       "      <td>0.248743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.209200</td>\n",
       "      <td>0.253310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.198300</td>\n",
       "      <td>0.244121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.192100</td>\n",
       "      <td>0.244179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.186000</td>\n",
       "      <td>0.244652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.185800</td>\n",
       "      <td>0.246972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.172700</td>\n",
       "      <td>0.246909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.170500</td>\n",
       "      <td>0.244652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.171200</td>\n",
       "      <td>0.245642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.169900</td>\n",
       "      <td>0.244894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.160600</td>\n",
       "      <td>0.244681</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "1.0     | 5.93     | 0.00     | 5.92     | 5.93        | -77.49     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "2.0     | 6.80     | 0.00     | 6.82     | 6.79        | -77.26     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "3.0     | 6.24     | 0.00     | 6.26     | 6.27        | -78.85     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "4.0     | 6.69     | 0.00     | 6.73     | 6.70        | -77.31     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "5.0     | 6.43     | 0.00     | 6.45     | 6.42        | -77.79     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "6.0     | 6.39     | 0.00     | 6.40     | 6.42        | -77.82     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "7.0     | 6.50     | 0.00     | 6.51     | 6.51        | -77.89     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "8.0     | 6.86     | 0.00     | 6.85     | 6.88        | -77.69     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "9.0     | 6.43     | 0.00     | 6.43     | 6.40        | -78.18     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "10.0    | 6.62     | 0.00     | 6.60     | 6.59        | -77.82     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "11.0    | 6.97     | 0.00     | 6.96     | 6.93        | -77.77     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "12.0    | 6.58     | 0.00     | 6.56     | 6.56        | -77.72     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "13.0    | 6.77     | 0.00     | 6.78     | 6.77        | -77.88     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "14.0    | 6.56     | 0.00     | 6.58     | 6.57        | -77.74     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "15.0    | 7.03     | 0.00     | 7.05     | 7.04        | -77.59     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "16.0    | 6.62     | 0.00     | 6.58     | 6.62        | -77.93     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "17.0    | 6.85     | 0.00     | 6.88     | 6.90        | -77.93     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "18.0    | 6.74     | 0.00     | 6.74     | 6.75        | -77.83     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "19.0    | 6.92     | 0.00     | 6.92     | 6.94        | -77.72     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "20.0    | 6.97     | 0.00     | 7.00     | 6.96        | -77.78     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer, T5ForConditionalGeneration, AdamW\n",
    "\n",
    "# Load the T5 model\n",
    "model_name = 't5-small' \n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "\n",
    "# Calculate the size of the training dataset\n",
    "training_dataset_size = len(train_dataset)\n",
    "\n",
    "N = training_dataset_size\n",
    "batch_size = 8  \n",
    "\n",
    "# Calculate steps per epoch\n",
    "steps_per_epoch = N // batch_size\n",
    "\n",
    "\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",                      # Directory for storing outputs\n",
    "    num_train_epochs=20,                         # Set number of epochs to 10\n",
    "    per_device_train_batch_size=8,               # Batch size for training\n",
    "    per_device_eval_batch_size=8,                # Batch size for evaluation\n",
    "    warmup_steps=500,                            # Number of warmup steps\n",
    "    weight_decay=0.01,                           # Weight decay for regularization\n",
    "    logging_dir=\"./logs\",                        # Directory for storing logs\n",
    "    logging_steps=steps_per_epoch,                            # Log every steps in epoch\n",
    "    do_train=True,                               # Enable training\n",
    "    do_eval=True,                                # Enable evaluation\n",
    "    evaluation_strategy=\"epoch\",                 # Evaluate at the end of each epoch\n",
    "    save_strategy=\"no\",                    # Disable saving the model\n",
    "    save_total_limit=0,                    # Limit on the total amount of checkpoints. Setting to 0 disables it\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,  # Use the dataset instance for training data\n",
    "    eval_dataset=val_dataset,     # Use the dataset instance for validation data\n",
    "    callbacks=[custom_eval_callback],  # Custom callback function for metrics\n",
    "    optimizers=(AdamW(model.parameters(), lr=1e-4), None)  # AdamW optimizer with a specified learning rate\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n",
    "# Plotting\n",
    "training_logs = trainer.state.log_history\n",
    "\n",
    "training_loss = [log['loss'] for log in training_logs if 'loss' in log]\n",
    "validation_loss = [log['eval_loss'] for log in training_logs if 'eval_loss' in log]\n",
    "epochs = range(1, len(training_loss) + 1)\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(epochs, training_loss, 'bo-', label='Training loss')\n",
    "plt.plot(epochs, validation_loss, 'ro-', label='Validation loss')\n",
    "plt.title('Training and Validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb7f3c4",
   "metadata": {},
   "source": [
    "## T5 with batch size 8 lr = 5e-4, Cleaned_mails ans Summary, epoch= 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d0e15d10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4360' max='4360' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4360/4360 44:44, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.977600</td>\n",
       "      <td>0.385595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.383900</td>\n",
       "      <td>0.315192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.296100</td>\n",
       "      <td>0.266886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.231800</td>\n",
       "      <td>0.256503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.192700</td>\n",
       "      <td>0.252581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.161000</td>\n",
       "      <td>0.248863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.140000</td>\n",
       "      <td>0.245854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.122600</td>\n",
       "      <td>0.258627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.105600</td>\n",
       "      <td>0.262452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.092300</td>\n",
       "      <td>0.263197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.083100</td>\n",
       "      <td>0.271645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.074900</td>\n",
       "      <td>0.270884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.065600</td>\n",
       "      <td>0.273317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.060400</td>\n",
       "      <td>0.281381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.056700</td>\n",
       "      <td>0.285494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.049300</td>\n",
       "      <td>0.295403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.046900</td>\n",
       "      <td>0.292876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.043900</td>\n",
       "      <td>0.295944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.041800</td>\n",
       "      <td>0.301965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.037600</td>\n",
       "      <td>0.301588</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "1.0     | 7.09     | 0.00     | 7.07     | 7.08        | -78.56     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "2.0     | 6.84     | 0.00     | 6.84     | 6.80        | -77.32     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "3.0     | 5.76     | 0.00     | 5.78     | 5.77        | -77.76     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "4.0     | 6.66     | 0.00     | 6.68     | 6.69        | -77.42     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "5.0     | 7.53     | 0.00     | 7.53     | 7.54        | -78.29     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "6.0     | 7.33     | 0.00     | 7.33     | 7.35        | -78.23     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "7.0     | 6.89     | 0.00     | 6.90     | 6.91        | -78.13     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "8.0     | 7.19     | 0.00     | 7.18     | 7.19        | -77.81     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "9.0     | 7.17     | 0.00     | 7.16     | 7.17        | -77.54     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "10.0    | 6.81     | 0.00     | 6.82     | 6.82        | -77.87     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "11.0    | 7.21     | 0.00     | 7.21     | 7.20        | -77.91     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "12.0    | 7.29     | 0.00     | 7.29     | 7.28        | -77.76     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "13.0    | 7.35     | 0.00     | 7.34     | 7.34        | -78.06     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "14.0    | 7.33     | 0.00     | 7.33     | 7.34        | -78.13     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "15.0    | 7.38     | 0.00     | 7.39     | 7.39        | -78.07     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "16.0    | 7.04     | 0.00     | 7.02     | 7.05        | -78.03     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "17.0    | 7.12     | 0.00     | 7.17     | 7.19        | -78.10     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "18.0    | 7.49     | 0.00     | 7.48     | 7.50        | -77.85     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "19.0    | 7.40     | 0.00     | 7.41     | 7.43        | -77.93     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "20.0    | 7.20     | 0.00     | 7.20     | 7.19        | -77.93     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer, T5ForConditionalGeneration, AdamW\n",
    "\n",
    "# Load the T5 model\n",
    "model_name = 't5-small'  \n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "\n",
    "# Calculate the size of the training dataset\n",
    "training_dataset_size = len(train_dataset)\n",
    "\n",
    "N = training_dataset_size\n",
    "batch_size = 8  \n",
    "# Calculate steps per epoch\n",
    "steps_per_epoch = N // batch_size\n",
    "\n",
    "\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",                      # Directory for storing outputs\n",
    "    num_train_epochs=20,                         # Set number of epochs to 10\n",
    "    per_device_train_batch_size=8,               # Batch size for training\n",
    "    per_device_eval_batch_size=8,                # Batch size for evaluation\n",
    "    warmup_steps=500,                            # Number of warmup steps\n",
    "    weight_decay=0.01,                           # Weight decay for regularization\n",
    "    logging_dir=\"./logs\",                        # Directory for storing logs\n",
    "    logging_steps=steps_per_epoch,                            # Log every steps in epoch\n",
    "    do_train=True,                               # Enable training\n",
    "    do_eval=True,                                # Enable evaluation\n",
    "    evaluation_strategy=\"epoch\",                 # Evaluate at the end of each epoch\n",
    "    save_strategy=\"no\",                    # Disable saving the model\n",
    "    save_total_limit=0,                    # Limit on the total amount of checkpoints. Setting to 0 disables it\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,  # Use the dataset instance for training data\n",
    "    eval_dataset=val_dataset,     # Use the dataset instance for validation data\n",
    "    callbacks=[custom_eval_callback],  # Custom callback function for metrics\n",
    "    optimizers=(AdamW(model.parameters(), lr=5e-4), None)  # AdamW optimizer with a specified learning rate\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n",
    "# Plotting\n",
    "training_logs = trainer.state.log_history\n",
    "\n",
    "training_loss = [log['loss'] for log in training_logs if 'loss' in log]\n",
    "validation_loss = [log['eval_loss'] for log in training_logs if 'eval_loss' in log]\n",
    "epochs = range(1, len(training_loss) + 1)\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(epochs, training_loss, 'bo-', label='Training loss')\n",
    "plt.plot(epochs, validation_loss, 'ro-', label='Validation loss')\n",
    "plt.title('Training and Validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bd078a",
   "metadata": {},
   "source": [
    "## T5 with batch size 8 lr = 1e-3, Cleaned_mails ans Summary, epoch= 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6618d891",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4360' max='4360' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4360/4360 44:41, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.831100</td>\n",
       "      <td>0.352190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.356000</td>\n",
       "      <td>0.292674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.285400</td>\n",
       "      <td>0.269934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.217900</td>\n",
       "      <td>0.264489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.180800</td>\n",
       "      <td>0.267180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.145800</td>\n",
       "      <td>0.261638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.123000</td>\n",
       "      <td>0.259492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.103500</td>\n",
       "      <td>0.274438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.086000</td>\n",
       "      <td>0.282340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.073500</td>\n",
       "      <td>0.295080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.065800</td>\n",
       "      <td>0.295537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.056200</td>\n",
       "      <td>0.300210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.047100</td>\n",
       "      <td>0.306932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.041900</td>\n",
       "      <td>0.315672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.036600</td>\n",
       "      <td>0.323340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.032700</td>\n",
       "      <td>0.332507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.028300</td>\n",
       "      <td>0.329705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.345513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.023700</td>\n",
       "      <td>0.349805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.020900</td>\n",
       "      <td>0.348524</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "1.0     | 6.41     | 0.00     | 6.38     | 6.39        | -77.51     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "2.0     | 7.10     | 0.00     | 7.12     | 7.09        | -77.50     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "3.0     | 5.89     | 0.00     | 5.91     | 5.91        | -77.40     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "4.0     | 7.06     | 0.00     | 7.07     | 7.09        | -77.94     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "5.0     | 7.18     | 0.00     | 7.18     | 7.18        | -78.50     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "6.0     | 7.12     | 0.00     | 7.13     | 7.13        | -78.58     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "7.0     | 7.64     | 0.00     | 7.63     | 7.65        | -78.40     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "8.0     | 7.31     | 0.00     | 7.31     | 7.34        | -78.03     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "9.0     | 6.93     | 0.00     | 6.92     | 6.93        | -78.22     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "10.0    | 6.68     | 0.00     | 6.66     | 6.69        | -78.52     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "11.0    | 7.38     | 0.00     | 7.39     | 7.38        | -78.28     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "12.0    | 7.19     | 0.00     | 7.22     | 7.21        | -78.15     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "13.0    | 7.50     | 0.00     | 7.47     | 7.48        | -77.99     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "14.0    | 7.36     | 0.00     | 7.32     | 7.35        | -78.07     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "15.0    | 7.64     | 0.00     | 7.63     | 7.65        | -78.09     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "16.0    | 7.36     | 0.00     | 7.36     | 7.37        | -78.06     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "17.0    | 7.10     | 0.00     | 7.14     | 7.18        | -78.16     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "18.0    | 7.50     | 0.00     | 7.48     | 7.50        | -78.02     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "19.0    | 7.44     | 0.00     | 7.45     | 7.46        | -78.14     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "20.0    | 7.35     | 0.00     | 7.34     | 7.35        | -78.13     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer, T5ForConditionalGeneration, AdamW\n",
    "\n",
    "# Load the T5 model\n",
    "model_name = 't5-small'  \n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "\n",
    "# Calculate the size of the training dataset\n",
    "training_dataset_size = len(train_dataset)\n",
    "\n",
    "N = training_dataset_size\n",
    "batch_size = 8  \n",
    "\n",
    "# Calculate steps per epoch\n",
    "steps_per_epoch = N // batch_size\n",
    "\n",
    "\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",                      # Directory for storing outputs\n",
    "    num_train_epochs=20,                         # Set number of epochs to 10\n",
    "    per_device_train_batch_size=8,               # Batch size for training\n",
    "    per_device_eval_batch_size=8,                # Batch size for evaluation\n",
    "    warmup_steps=500,                            # Number of warmup steps\n",
    "    weight_decay=0.01,                           # Weight decay for regularization\n",
    "    logging_dir=\"./logs\",                        # Directory for storing logs\n",
    "    logging_steps=steps_per_epoch,                            # Log every steps in epoch\n",
    "    do_train=True,                               # Enable training\n",
    "    do_eval=True,                                # Enable evaluation\n",
    "    evaluation_strategy=\"epoch\",                 # Evaluate at the end of each epoch\n",
    "    save_strategy=\"no\",                    # Disable saving the model\n",
    "    save_total_limit=0,                    # Limit on the total amount of checkpoints. Setting to 0 disables it\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,  # Use the dataset instance for training data\n",
    "    eval_dataset=val_dataset,     # Use the dataset instance for validation data\n",
    "    callbacks=[custom_eval_callback],  # Custom callback function for metrics\n",
    "    optimizers=(AdamW(model.parameters(), lr=1e-3), None)  # AdamW optimizer with a specified learning rate\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n",
    "# Plotting\n",
    "training_logs = trainer.state.log_history\n",
    "\n",
    "training_loss = [log['loss'] for log in training_logs if 'loss' in log]\n",
    "validation_loss = [log['eval_loss'] for log in training_logs if 'eval_loss' in log]\n",
    "epochs = range(1, len(training_loss) + 1)\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(epochs, training_loss, 'bo-', label='Training loss')\n",
    "plt.plot(epochs, validation_loss, 'ro-', label='Validation loss')\n",
    "plt.title('Training and Validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1952cf50",
   "metadata": {},
   "source": [
    "## T5 with batch size 8 lr = 5e-3, Cleaned_mails ans Summary, epoch= 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6ecca99f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4360' max='4360' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4360/4360 44:41, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.638100</td>\n",
       "      <td>0.403960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.432000</td>\n",
       "      <td>0.383006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.426400</td>\n",
       "      <td>0.358248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.347200</td>\n",
       "      <td>0.329559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.273500</td>\n",
       "      <td>0.320276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.223100</td>\n",
       "      <td>0.298242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.185600</td>\n",
       "      <td>0.292150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.151200</td>\n",
       "      <td>0.310926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.122300</td>\n",
       "      <td>0.330250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.095400</td>\n",
       "      <td>0.324772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.084000</td>\n",
       "      <td>0.328159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.066300</td>\n",
       "      <td>0.342720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.053600</td>\n",
       "      <td>0.347217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.044300</td>\n",
       "      <td>0.365174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.036200</td>\n",
       "      <td>0.370206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.029100</td>\n",
       "      <td>0.389138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.024100</td>\n",
       "      <td>0.393348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.020100</td>\n",
       "      <td>0.408009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.017200</td>\n",
       "      <td>0.421306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.014000</td>\n",
       "      <td>0.421690</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "1.0     | 7.00     | 0.00     | 6.98     | 6.97        | -76.79     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "2.0     | 6.83     | 0.00     | 6.84     | 6.82        | -77.73     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "3.0     | 5.76     | 0.00     | 5.77     | 5.77        | -77.78     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "4.0     | 7.38     | 0.00     | 7.40     | 7.40        | -77.77     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "5.0     | 7.69     | 0.00     | 7.69     | 7.70        | -77.95     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "6.0     | 6.81     | 0.00     | 6.83     | 6.82        | -77.79     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "7.0     | 7.28     | 0.00     | 7.29     | 7.29        | -78.42     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "8.0     | 7.67     | 0.00     | 7.67     | 7.69        | -77.79     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "9.0     | 6.99     | 0.00     | 7.00     | 6.96        | -77.75     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "10.0    | 6.39     | 0.00     | 6.36     | 6.41        | -77.74     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "11.0    | 6.77     | 0.00     | 6.80     | 6.78        | -78.25     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "12.0    | 6.55     | 0.00     | 6.57     | 6.56        | -77.81     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "13.0    | 6.95     | 0.00     | 6.94     | 6.92        | -78.07     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "14.0    | 6.55     | 0.00     | 6.53     | 6.52        | -78.18     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "15.0    | 7.10     | 0.00     | 7.10     | 7.09        | -77.67     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "16.0    | 7.03     | 0.00     | 7.02     | 7.02        | -77.81     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "17.0    | 7.34     | 0.00     | 7.40     | 7.41        | -78.07     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "18.0    | 7.20     | 0.00     | 7.22     | 7.24        | -77.86     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "19.0    | 7.14     | 0.00     | 7.14     | 7.14        | -77.84     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "20.0    | 7.26     | 0.00     | 7.27     | 7.27        | -77.62     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer, T5ForConditionalGeneration, AdamW\n",
    "\n",
    "# Load the T5 model\n",
    "model_name = 't5-small'  \n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "\n",
    "# Calculate the size of the training dataset\n",
    "training_dataset_size = len(train_dataset)\n",
    "\n",
    "N = training_dataset_size\n",
    "batch_size = 8  \n",
    "\n",
    "# Calculate steps per epoch\n",
    "steps_per_epoch = N // batch_size\n",
    "\n",
    "\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",                      # Directory for storing outputs\n",
    "    num_train_epochs=20,                         # Set number of epochs to 10\n",
    "    per_device_train_batch_size=8,               # Batch size for training\n",
    "    per_device_eval_batch_size=8,                # Batch size for evaluation\n",
    "    warmup_steps=500,                            # Number of warmup steps\n",
    "    weight_decay=0.01,                           # Weight decay for regularization\n",
    "    logging_dir=\"./logs\",                        # Directory for storing logs\n",
    "    logging_steps=steps_per_epoch,                            # Log every steps in epoch\n",
    "    do_train=True,                               # Enable training\n",
    "    do_eval=True,                                # Enable evaluation\n",
    "    evaluation_strategy=\"epoch\",                 # Evaluate at the end of each epoch\n",
    "    save_strategy=\"no\",                    # Disable saving the model\n",
    "    save_total_limit=0,                    # Limit on the total amount of checkpoints. Setting to 0 disables it\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,  # Use the dataset instance for training data\n",
    "    eval_dataset=val_dataset,     # Use the dataset instance for validation data\n",
    "    callbacks=[custom_eval_callback],  # Custom callback function for metrics\n",
    "    optimizers=(AdamW(model.parameters(), lr=5e-3), None)  # AdamW optimizer with a specified learning rate\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n",
    "# Plotting\n",
    "training_logs = trainer.state.log_history\n",
    "\n",
    "training_loss = [log['loss'] for log in training_logs if 'loss' in log]\n",
    "validation_loss = [log['eval_loss'] for log in training_logs if 'eval_loss' in log]\n",
    "epochs = range(1, len(training_loss) + 1)\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(epochs, training_loss, 'bo-', label='Training loss')\n",
    "plt.plot(epochs, validation_loss, 'ro-', label='Validation loss')\n",
    "plt.title('Training and Validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1beee31",
   "metadata": {},
   "source": [
    "## Cleaned_mails as input and Summary_BART as target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b6c73e9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model_name = 't5-small'\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "from transformers import T5ForConditionalGeneration\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c68c7e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataframe(df, tokenizer, lower=True):\n",
    "    email_content, eval_data = {}, {}\n",
    "    total = 0\n",
    "    input_lens, output_lens = [], []\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        total += 1\n",
    "\n",
    "        # Ensure Body and summary are strings\n",
    "        Body = str(row['Cleaned_mails']) if isinstance(row['Cleaned_mails'], str) else 'null'\n",
    "        summary = str(row['summary_BART']) if isinstance(row['summary_BART'], str) else 'null'\n",
    "\n",
    "        # Optional lowercase\n",
    "        if lower:\n",
    "            Body = Body.lower()\n",
    "            summary = summary.lower()\n",
    "\n",
    "        # Strip leading/trailing whitespace from article and summary\n",
    "        Body = Body.strip()\n",
    "        summary = summary.strip()\n",
    "\n",
    "        # Tokenize\n",
    "        input_tokens = tokenizer.tokenize(\"summarize: \" + Body)\n",
    "        output_tokens = tokenizer.tokenize(summary)\n",
    "\n",
    "        # Lengths\n",
    "        input_lens.append(len(input_tokens))\n",
    "        output_lens.append(len(output_tokens))\n",
    "\n",
    "        # Store input \n",
    "        email = {\"input\": input_tokens, \"output\": output_tokens, \"id\": total}\n",
    "        eval_data[str(total)] = (Body, summary)\n",
    "        email_content[str(total)] = email\n",
    "\n",
    "    # Shuffle email content\n",
    "    email_content = dict(np.random.permutation(list(email_content.items())))\n",
    "\n",
    "    print(f\"{len(email_content)} email contents in total\")\n",
    "    print(f\"max_input: {max(input_lens)}, max_output: {max(output_lens)}.\")\n",
    "    print(f\"avg_input: {np.mean(input_lens)}, avg_output: {np.mean(output_lens)}.\")\n",
    "\n",
    "    return email_content, eval_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "538666e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Extracting input and output texts\n",
    "input_texts = [email['input'] for email in email_content.values()]\n",
    "target_texts = [email['output'] for email in email_content.values()]\n",
    "\n",
    "# Split into training (80%) and validation (20%) sets\n",
    "train_inputs, val_inputs, train_targets, val_targets = train_test_split(\n",
    "    input_texts, target_texts, test_size=0.2, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7eeacbfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'test_inputs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 40\u001b[0m\n\u001b[0;32m     38\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m EmailDataset(tokenizer, train_inputs, train_targets)\n\u001b[0;32m     39\u001b[0m val_dataset \u001b[38;5;241m=\u001b[39m EmailDataset(tokenizer, val_inputs, val_targets)\n\u001b[1;32m---> 40\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m EmailDataset(tokenizer, \u001b[43mtest_inputs\u001b[49m, test_targets)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test_inputs' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class EmailDataset(Dataset):\n",
    "    def __init__(self, tokenizer, inputs, targets, max_input_length=512, max_target_length=128):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "        self.max_input_length = max_input_length\n",
    "        self.max_target_length = max_target_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Tokenize input and target texts with appropriate padding\n",
    "        input_ids = self.tokenizer.encode(self.inputs[idx], max_length=self.max_input_length, truncation=True, padding=\"max_length\", return_tensors=\"pt\").squeeze()\n",
    "        target_ids = self.tokenizer.encode(self.targets[idx], max_length=self.max_target_length, truncation=True, padding=\"max_length\", return_tensors=\"pt\").squeeze()\n",
    "\n",
    "        # Prepare decoder_input_ids\n",
    "        decoder_input_ids = self._shift_right(target_ids)\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"labels\": target_ids,\n",
    "            \"decoder_input_ids\": decoder_input_ids\n",
    "        }\n",
    "\n",
    "    def _shift_right(self, target_ids):\n",
    "        # Shift the target ids to the right and add pad token at the beginning\n",
    "        shifted_target_ids = torch.cat([torch.tensor([self.tokenizer.pad_token_id]), target_ids[:-1]])\n",
    "        return shifted_target_ids\n",
    "\n",
    "# Prepare your data\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "# Create EmailDataset instances for each split\n",
    "train_dataset = EmailDataset(tokenizer, train_inputs, train_targets)\n",
    "val_dataset = EmailDataset(tokenizer, val_inputs, val_targets)\n",
    "test_dataset = EmailDataset(tokenizer, test_inputs, test_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0e932a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback\n",
    "import numpy as np\n",
    "from rouge_score import rouge_scorer, scoring\n",
    "from bert_score import score\n",
    "import torch\n",
    "\n",
    "class CustomEvaluationCallback(TrainerCallback):\n",
    "    def __init__(self, eval_dataset, tokenizer, device):\n",
    "        self.eval_dataset = eval_dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "\n",
    "    def on_evaluate(self, args, state, control, **kwargs):\n",
    "        model = kwargs[\"model\"]\n",
    "        predictions, references = [], []\n",
    "\n",
    "        for batch in self.eval_dataset:\n",
    "            inputs = batch['input_ids'].to(self.device)\n",
    "#             attention_mask = batch.get('attention_mask', None).to(self.device) if 'attention_mask' in batch else None\n",
    "            if 'attention_mask' in batch:\n",
    "                attention_mask = batch['attention_mask'].to(self.device)\n",
    "            else:\n",
    "                attention_mask = None\n",
    "\n",
    "            if len(inputs.shape) == 1:\n",
    "                inputs = inputs.unsqueeze(0)\n",
    "            if attention_mask is not None and len(attention_mask.shape) == 1:\n",
    "                attention_mask = attention_mask.unsqueeze(0)\n",
    "\n",
    "            outputs = model.generate(inputs, attention_mask=attention_mask)\n",
    "            batch_predictions = [self.tokenizer.decode(o, skip_special_tokens=True) for o in outputs]\n",
    "\n",
    "            for i in range(len(batch_predictions)):\n",
    "                predictions.append(batch_predictions[i])\n",
    "                references.append(self.tokenizer.decode(batch['labels'][i], skip_special_tokens=True))\n",
    "\n",
    "        # Compute ROUGE and BERTScore\n",
    "        rouge_scores = self.compute_rouge(references, predictions)\n",
    "        bert_scores = self.compute_bertScore(references, predictions)\n",
    "    \n",
    "        \n",
    "\n",
    "        # Print formatted scores\n",
    "        self.print_formatted_scores(rouge_scores, bert_scores, state.epoch)\n",
    "\n",
    "        return control\n",
    "\n",
    "    def compute_rouge(self, targets, predictions, score_keys=None, use_stemmer=True):\n",
    "        if score_keys is None:\n",
    "            score_keys = [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]\n",
    "        scorer = rouge_scorer.RougeScorer(score_keys, use_stemmer=use_stemmer)\n",
    "        aggregator = scoring.BootstrapAggregator()\n",
    "\n",
    "        for target, prediction in zip(targets, predictions):\n",
    "            aggregator.add_scores(scorer.score(target=target, prediction=prediction))\n",
    "        result = aggregator.aggregate()\n",
    "\n",
    "        return {key: result[key].mid.fmeasure * 100 for key in score_keys}\n",
    "\n",
    "    def compute_bertScore(self, refs, cands, rescale_with_baseline=True):\n",
    "        P, R, F1 = score(cands, refs, lang=\"en\", rescale_with_baseline=rescale_with_baseline)\n",
    "        return {\"bertScore\": F1.mean().item() * 100}\n",
    "\n",
    "    def print_formatted_scores(self, rouge_scores, bert_scores, epoch):\n",
    "    # Headers for the scores\n",
    "        headers = [\"Epoch\"] + list(rouge_scores.keys()) + [\"bertScore\"]\n",
    "    \n",
    "    # Values for the scores\n",
    "        values = [f\"{epoch}\"] + [f\"{score:.2f}\" for score in rouge_scores.values()] + [f\"{bert_scores['bertScore']:.2f}\"]\n",
    "\n",
    "    # Calculate the max width for each column\n",
    "        column_widths = [max(len(header), len(value)) + 2 for header, value in zip(headers, values)]\n",
    "\n",
    "    # Print the header\n",
    "        header_row = \" | \".join(f\"{header:<{column_widths[idx]}}\" for idx, header in enumerate(headers))\n",
    "        print(header_row)\n",
    "        print(\"-\" * len(header_row))\n",
    "\n",
    "    # Print the scores\n",
    "        score_row = \" | \".join(f\"{value:<{column_widths[idx]}}\" for idx, value in enumerate(values))\n",
    "        print(score_row)\n",
    "        print(\"\\n\" + \"-\" * len(header_row))\n",
    "\n",
    "# Usage\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "custom_eval_callback = CustomEvaluationCallback(val_dataset, tokenizer, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e08b4e9",
   "metadata": {},
   "source": [
    "## T5 model Cleaned_mails and Summary_Bart lr = 5e-5, batch size =8, epoch =20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "244e620a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4360' max='4360' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4360/4360 44:26, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.713200</td>\n",
       "      <td>0.693872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.703100</td>\n",
       "      <td>0.477080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.492700</td>\n",
       "      <td>0.383898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.419100</td>\n",
       "      <td>0.358215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.377800</td>\n",
       "      <td>0.336089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.353600</td>\n",
       "      <td>0.325603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.337700</td>\n",
       "      <td>0.314126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.324000</td>\n",
       "      <td>0.305975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.303900</td>\n",
       "      <td>0.296870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.290400</td>\n",
       "      <td>0.290606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.289000</td>\n",
       "      <td>0.286979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.278300</td>\n",
       "      <td>0.280394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.274100</td>\n",
       "      <td>0.278710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.266700</td>\n",
       "      <td>0.276235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.268100</td>\n",
       "      <td>0.275104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.252200</td>\n",
       "      <td>0.272938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.251500</td>\n",
       "      <td>0.271873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.252300</td>\n",
       "      <td>0.270923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.253900</td>\n",
       "      <td>0.270040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.242100</td>\n",
       "      <td>0.269919</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "1.0     | 4.30     | 0.00     | 4.28     | 4.26        | -78.11     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "2.0     | 6.62     | 0.00     | 6.64     | 6.61        | -78.64     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "3.0     | 6.91     | 0.00     | 6.94     | 6.94        | -81.98     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "4.0     | 6.85     | 0.00     | 6.87     | 6.88        | -80.87     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "5.0     | 6.65     | 0.00     | 6.64     | 6.63        | -77.66     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "6.0     | 6.64     | 0.00     | 6.65     | 6.68        | -77.64     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "7.0     | 6.01     | 0.00     | 6.00     | 6.05        | -77.58     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "8.0     | 6.53     | 0.00     | 6.51     | 6.53        | -77.54     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "9.0     | 6.31     | 0.00     | 6.29     | 6.29        | -77.69     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "10.0    | 6.69     | 0.00     | 6.70     | 6.67        | -77.56     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "11.0    | 6.96     | 0.00     | 6.96     | 6.94        | -77.62     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "12.0    | 6.80     | 0.00     | 6.78     | 6.77        | -77.85     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "13.0    | 6.68     | 0.00     | 6.68     | 6.67        | -77.98     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "14.0    | 6.81     | 0.00     | 6.81     | 6.82        | -77.84     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "15.0    | 6.65     | 0.00     | 6.68     | 6.67        | -77.61     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "16.0    | 6.66     | 0.00     | 6.66     | 6.67        | -77.71     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "17.0    | 6.73     | 0.00     | 6.76     | 6.78        | -77.63     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "18.0    | 6.78     | 0.00     | 6.79     | 6.81        | -77.63     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "19.0    | 6.85     | 0.00     | 6.86     | 6.88        | -77.67     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "20.0    | 6.78     | 0.00     | 6.79     | 6.77        | -77.78     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer, T5ForConditionalGeneration, AdamW\n",
    "\n",
    "# Load the T5 model\n",
    "model_name = 't5-small'  \n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "\n",
    "# Calculate the size of the training dataset\n",
    "training_dataset_size = len(train_dataset)\n",
    "\n",
    "N = training_dataset_size\n",
    "batch_size = 8 \n",
    "\n",
    "# Calculate steps per epoch\n",
    "steps_per_epoch = N // batch_size\n",
    "\n",
    "\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",                      # Directory for storing outputs\n",
    "    num_train_epochs=20,                         # Set number of epochs to 10\n",
    "    per_device_train_batch_size=8,               # Batch size for training\n",
    "    per_device_eval_batch_size=8,                # Batch size for evaluation\n",
    "    warmup_steps=500,                            # Number of warmup steps\n",
    "    weight_decay=0.01,                           # Weight decay for regularization\n",
    "    logging_dir=\"./logs\",                        # Directory for storing logs\n",
    "    logging_steps=steps_per_epoch,                            # Log every steps in epoch\n",
    "    do_train=True,                               # Enable training\n",
    "    do_eval=True,                                # Enable evaluation\n",
    "    evaluation_strategy=\"epoch\",                 # Evaluate at the end of each epoch\n",
    "    save_strategy=\"no\",                    # Disable saving the model\n",
    "    save_total_limit=0,                    # Limit on the total amount of checkpoints. Setting to 0 disables it\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,  # Use the dataset instance for training data\n",
    "    eval_dataset=val_dataset,     # Use the dataset instance for validation data\n",
    "    callbacks=[custom_eval_callback],  # Custom callback function for metrics\n",
    "    optimizers=(AdamW(model.parameters(), lr=5e-5), None)  # AdamW optimizer with a specified learning rate\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n",
    "# Plotting\n",
    "training_logs = trainer.state.log_history\n",
    "\n",
    "training_loss = [log['loss'] for log in training_logs if 'loss' in log]\n",
    "validation_loss = [log['eval_loss'] for log in training_logs if 'eval_loss' in log]\n",
    "epochs = range(1, len(training_loss) + 1)\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(epochs, training_loss, 'bo-', label='Training loss')\n",
    "plt.plot(epochs, validation_loss, 'ro-', label='Validation loss')\n",
    "plt.title('Training and Validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2945fca4",
   "metadata": {},
   "source": [
    "## T5 model Cleaned_mails and Summary_Bart lr = 1e-4, batch size =8, epoch =20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "41eaa5d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4360' max='4360' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4360/4360 44:17, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.466100</td>\n",
       "      <td>0.576371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.566200</td>\n",
       "      <td>0.395052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.410700</td>\n",
       "      <td>0.337170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.352400</td>\n",
       "      <td>0.315482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.314600</td>\n",
       "      <td>0.292363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.286000</td>\n",
       "      <td>0.279840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.265800</td>\n",
       "      <td>0.266182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.248900</td>\n",
       "      <td>0.262395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.227100</td>\n",
       "      <td>0.255131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.214500</td>\n",
       "      <td>0.248743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.209200</td>\n",
       "      <td>0.253310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.198300</td>\n",
       "      <td>0.244121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.192100</td>\n",
       "      <td>0.244179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.186000</td>\n",
       "      <td>0.244652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.185800</td>\n",
       "      <td>0.246972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.172700</td>\n",
       "      <td>0.246909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.170500</td>\n",
       "      <td>0.244652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.171200</td>\n",
       "      <td>0.245642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.169900</td>\n",
       "      <td>0.244894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.160600</td>\n",
       "      <td>0.244681</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "1.0     | 5.93     | 0.00     | 5.92     | 5.93        | -77.49     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "2.0     | 6.80     | 0.00     | 6.82     | 6.79        | -77.26     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "3.0     | 6.24     | 0.00     | 6.26     | 6.27        | -78.85     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "4.0     | 6.69     | 0.00     | 6.73     | 6.70        | -77.31     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "5.0     | 6.43     | 0.00     | 6.45     | 6.42        | -77.79     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "6.0     | 6.39     | 0.00     | 6.40     | 6.42        | -77.82     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "7.0     | 6.50     | 0.00     | 6.51     | 6.51        | -77.89     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "8.0     | 6.86     | 0.00     | 6.85     | 6.88        | -77.69     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "9.0     | 6.43     | 0.00     | 6.43     | 6.40        | -78.18     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "10.0    | 6.62     | 0.00     | 6.60     | 6.59        | -77.82     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "11.0    | 6.97     | 0.00     | 6.96     | 6.93        | -77.77     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "12.0    | 6.58     | 0.00     | 6.56     | 6.56        | -77.72     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "13.0    | 6.77     | 0.00     | 6.78     | 6.77        | -77.88     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "14.0    | 6.56     | 0.00     | 6.58     | 6.57        | -77.74     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "15.0    | 7.03     | 0.00     | 7.05     | 7.04        | -77.59     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "16.0    | 6.62     | 0.00     | 6.58     | 6.62        | -77.93     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "17.0    | 6.85     | 0.00     | 6.88     | 6.90        | -77.93     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "18.0    | 6.74     | 0.00     | 6.74     | 6.75        | -77.83     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "19.0    | 6.92     | 0.00     | 6.92     | 6.94        | -77.72     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "20.0    | 6.97     | 0.00     | 7.00     | 6.96        | -77.78     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer, T5ForConditionalGeneration, AdamW\n",
    "\n",
    "# Load the T5 model\n",
    "model_name = 't5-small'  \n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "\n",
    "# Calculate the size of the training dataset\n",
    "training_dataset_size = len(train_dataset)\n",
    "\n",
    "N = training_dataset_size\n",
    "batch_size = 8 \n",
    "\n",
    "# Calculate steps per epoch\n",
    "steps_per_epoch = N // batch_size\n",
    "\n",
    "\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",                      # Directory for storing outputs\n",
    "    num_train_epochs=20,                         # Set number of epochs to 10\n",
    "    per_device_train_batch_size=8,               # Batch size for training\n",
    "    per_device_eval_batch_size=8,                # Batch size for evaluation\n",
    "    warmup_steps=500,                            # Number of warmup steps\n",
    "    weight_decay=0.01,                           # Weight decay for regularization\n",
    "    logging_dir=\"./logs\",                        # Directory for storing logs\n",
    "    logging_steps=steps_per_epoch,                            # Log every steps in epoch\n",
    "    do_train=True,                               # Enable training\n",
    "    do_eval=True,                                # Enable evaluation\n",
    "    evaluation_strategy=\"epoch\",                 # Evaluate at the end of each epoch\n",
    "    save_strategy=\"no\",                    # Disable saving the model\n",
    "    save_total_limit=0,                    # Limit on the total amount of checkpoints. Setting to 0 disables it\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,  # Use the dataset instance for training data\n",
    "    eval_dataset=val_dataset,     # Use the dataset instance for validation data\n",
    "    callbacks=[custom_eval_callback],  # Custom callback function for metrics\n",
    "    optimizers=(AdamW(model.parameters(), lr=1e-4), None)  # AdamW optimizer with a specified learning rate\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n",
    "# Plotting\n",
    "training_logs = trainer.state.log_history\n",
    "\n",
    "training_loss = [log['loss'] for log in training_logs if 'loss' in log]\n",
    "validation_loss = [log['eval_loss'] for log in training_logs if 'eval_loss' in log]\n",
    "epochs = range(1, len(training_loss) + 1)\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(epochs, training_loss, 'bo-', label='Training loss')\n",
    "plt.plot(epochs, validation_loss, 'ro-', label='Validation loss')\n",
    "plt.title('Training and Validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e713b50",
   "metadata": {},
   "source": [
    "## T5 model Cleaned_mails and Summary_Bart lr = 5e-4, batch size =8, epoch =20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "09315ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4360' max='4360' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4360/4360 44:02, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.977600</td>\n",
       "      <td>0.385595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.383900</td>\n",
       "      <td>0.315192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.296100</td>\n",
       "      <td>0.266886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.231800</td>\n",
       "      <td>0.256503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.192700</td>\n",
       "      <td>0.252581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.161000</td>\n",
       "      <td>0.248863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.140000</td>\n",
       "      <td>0.245854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.122600</td>\n",
       "      <td>0.258627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.105600</td>\n",
       "      <td>0.262452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.092300</td>\n",
       "      <td>0.263197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.083100</td>\n",
       "      <td>0.271645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.074900</td>\n",
       "      <td>0.270884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.065600</td>\n",
       "      <td>0.273317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.060400</td>\n",
       "      <td>0.281381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.056700</td>\n",
       "      <td>0.285494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.049300</td>\n",
       "      <td>0.295403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.046900</td>\n",
       "      <td>0.292876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.043900</td>\n",
       "      <td>0.295944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.041800</td>\n",
       "      <td>0.301965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.037600</td>\n",
       "      <td>0.301588</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "1.0     | 7.09     | 0.00     | 7.07     | 7.08        | -78.56     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "2.0     | 6.84     | 0.00     | 6.84     | 6.80        | -77.32     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "3.0     | 5.76     | 0.00     | 5.78     | 5.77        | -77.76     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "4.0     | 6.66     | 0.00     | 6.68     | 6.69        | -77.42     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "5.0     | 7.53     | 0.00     | 7.53     | 7.54        | -78.29     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "6.0     | 7.33     | 0.00     | 7.33     | 7.35        | -78.23     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "7.0     | 6.89     | 0.00     | 6.90     | 6.91        | -78.13     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "8.0     | 7.19     | 0.00     | 7.18     | 7.19        | -77.81     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "9.0     | 7.17     | 0.00     | 7.16     | 7.17        | -77.54     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "10.0    | 6.81     | 0.00     | 6.82     | 6.82        | -77.87     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "11.0    | 7.21     | 0.00     | 7.21     | 7.20        | -77.91     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "12.0    | 7.29     | 0.00     | 7.29     | 7.28        | -77.76     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "13.0    | 7.35     | 0.00     | 7.34     | 7.34        | -78.06     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "14.0    | 7.33     | 0.00     | 7.33     | 7.34        | -78.13     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "15.0    | 7.38     | 0.00     | 7.39     | 7.39        | -78.07     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "16.0    | 7.04     | 0.00     | 7.02     | 7.05        | -78.03     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "17.0    | 7.12     | 0.00     | 7.17     | 7.19        | -78.10     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "18.0    | 7.49     | 0.00     | 7.48     | 7.50        | -77.85     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "19.0    | 7.40     | 0.00     | 7.41     | 7.43        | -77.93     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "20.0    | 7.20     | 0.00     | 7.20     | 7.19        | -77.93     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer, T5ForConditionalGeneration, AdamW\n",
    "\n",
    "# Load the T5 model\n",
    "model_name = 't5-small'  \n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "\n",
    "# Calculate the size of the training dataset\n",
    "training_dataset_size = len(train_dataset)\n",
    "\n",
    "N = training_dataset_size\n",
    "batch_size = 8 \n",
    "\n",
    "# Calculate steps per epoch\n",
    "steps_per_epoch = N // batch_size\n",
    "\n",
    "\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",                      # Directory for storing outputs\n",
    "    num_train_epochs=20,                         # Set number of epochs to 10\n",
    "    per_device_train_batch_size=8,               # Batch size for training\n",
    "    per_device_eval_batch_size=8,                # Batch size for evaluation\n",
    "    warmup_steps=500,                            # Number of warmup steps\n",
    "    weight_decay=0.01,                           # Weight decay for regularization\n",
    "    logging_dir=\"./logs\",                        # Directory for storing logs\n",
    "    logging_steps=steps_per_epoch,                            # Log every steps in epoch\n",
    "    do_train=True,                               # Enable training\n",
    "    do_eval=True,                                # Enable evaluation\n",
    "    evaluation_strategy=\"epoch\",                 # Evaluate at the end of each epoch\n",
    "    save_strategy=\"no\",                    # Disable saving the model\n",
    "    save_total_limit=0,                    # Limit on the total amount of checkpoints. Setting to 0 disables it\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,  # Use the dataset instance for training data\n",
    "    eval_dataset=val_dataset,     # Use the dataset instance for validation data\n",
    "    callbacks=[custom_eval_callback],  # Custom callback function for metrics\n",
    "    optimizers=(AdamW(model.parameters(), lr=5e-4), None)  # AdamW optimizer with a specified learning rate\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n",
    "# Plotting\n",
    "training_logs = trainer.state.log_history\n",
    "\n",
    "training_loss = [log['loss'] for log in training_logs if 'loss' in log]\n",
    "validation_loss = [log['eval_loss'] for log in training_logs if 'eval_loss' in log]\n",
    "epochs = range(1, len(training_loss) + 1)\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(epochs, training_loss, 'bo-', label='Training loss')\n",
    "plt.plot(epochs, validation_loss, 'ro-', label='Validation loss')\n",
    "plt.title('Training and Validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c1cde4",
   "metadata": {},
   "source": [
    "## T5 model Cleaned_mails and Summary_Bart lr = 1e-3, batch size =8, epoch =20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a9f1d457",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4360' max='4360' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4360/4360 44:34, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.831100</td>\n",
       "      <td>0.352190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.356000</td>\n",
       "      <td>0.292674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.285400</td>\n",
       "      <td>0.269934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.217900</td>\n",
       "      <td>0.264489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.180800</td>\n",
       "      <td>0.267180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.145800</td>\n",
       "      <td>0.261638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.123000</td>\n",
       "      <td>0.259492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.103500</td>\n",
       "      <td>0.274438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.086000</td>\n",
       "      <td>0.282340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.073500</td>\n",
       "      <td>0.295080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.065800</td>\n",
       "      <td>0.295537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.056200</td>\n",
       "      <td>0.300210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.047100</td>\n",
       "      <td>0.306932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.041900</td>\n",
       "      <td>0.315672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.036600</td>\n",
       "      <td>0.323340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.032700</td>\n",
       "      <td>0.332507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.028300</td>\n",
       "      <td>0.329705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.345513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.023700</td>\n",
       "      <td>0.349805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.020900</td>\n",
       "      <td>0.348524</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "1.0     | 6.41     | 0.00     | 6.38     | 6.39        | -77.51     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "2.0     | 7.10     | 0.00     | 7.12     | 7.09        | -77.50     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "3.0     | 5.89     | 0.00     | 5.91     | 5.91        | -77.40     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "4.0     | 7.06     | 0.00     | 7.07     | 7.09        | -77.94     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "5.0     | 7.18     | 0.00     | 7.18     | 7.18        | -78.50     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "6.0     | 7.12     | 0.00     | 7.13     | 7.13        | -78.58     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "7.0     | 7.64     | 0.00     | 7.63     | 7.65        | -78.40     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "8.0     | 7.31     | 0.00     | 7.31     | 7.34        | -78.03     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "9.0     | 6.93     | 0.00     | 6.92     | 6.93        | -78.22     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "10.0    | 6.68     | 0.00     | 6.66     | 6.69        | -78.52     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "11.0    | 7.38     | 0.00     | 7.39     | 7.38        | -78.28     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "12.0    | 7.19     | 0.00     | 7.22     | 7.21        | -78.15     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "13.0    | 7.50     | 0.00     | 7.47     | 7.48        | -77.99     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "14.0    | 7.36     | 0.00     | 7.32     | 7.35        | -78.07     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "15.0    | 7.64     | 0.00     | 7.63     | 7.65        | -78.09     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "16.0    | 7.36     | 0.00     | 7.36     | 7.37        | -78.06     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "17.0    | 7.10     | 0.00     | 7.14     | 7.18        | -78.16     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "18.0    | 7.50     | 0.00     | 7.48     | 7.50        | -78.02     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "19.0    | 7.44     | 0.00     | 7.45     | 7.46        | -78.14     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "20.0    | 7.35     | 0.00     | 7.34     | 7.35        | -78.13     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer, T5ForConditionalGeneration, AdamW\n",
    "\n",
    "# Load the T5 model\n",
    "model_name = 't5-small'  \n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "\n",
    "# Calculate the size of the training dataset\n",
    "training_dataset_size = len(train_dataset)\n",
    "\n",
    "N = training_dataset_size\n",
    "batch_size = 8  \n",
    "\n",
    "# Calculate steps per epoch\n",
    "steps_per_epoch = N // batch_size\n",
    "\n",
    "\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",                      # Directory for storing outputs\n",
    "    num_train_epochs=20,                         # Set number of epochs to 10\n",
    "    per_device_train_batch_size=8,               # Batch size for training\n",
    "    per_device_eval_batch_size=8,                # Batch size for evaluation\n",
    "    warmup_steps=500,                            # Number of warmup steps\n",
    "    weight_decay=0.01,                           # Weight decay for regularization\n",
    "    logging_dir=\"./logs\",                        # Directory for storing logs\n",
    "    logging_steps=steps_per_epoch,                            # Log every steps in epoch\n",
    "    do_train=True,                               # Enable training\n",
    "    do_eval=True,                                # Enable evaluation\n",
    "    evaluation_strategy=\"epoch\",                 # Evaluate at the end of each epoch\n",
    "    save_strategy=\"no\",                    # Disable saving the model\n",
    "    save_total_limit=0,                    # Limit on the total amount of checkpoints. Setting to 0 disables it\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,  # Use the dataset instance for training data\n",
    "    eval_dataset=val_dataset,     # Use the dataset instance for validation data\n",
    "    callbacks=[custom_eval_callback],  # Custom callback function for metrics\n",
    "    optimizers=(AdamW(model.parameters(), lr=1e-3), None)  # AdamW optimizer with a specified learning rate\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n",
    "# Plotting\n",
    "training_logs = trainer.state.log_history\n",
    "\n",
    "training_loss = [log['loss'] for log in training_logs if 'loss' in log]\n",
    "validation_loss = [log['eval_loss'] for log in training_logs if 'eval_loss' in log]\n",
    "epochs = range(1, len(training_loss) + 1)\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(epochs, training_loss, 'bo-', label='Training loss')\n",
    "plt.plot(epochs, validation_loss, 'ro-', label='Validation loss')\n",
    "plt.title('Training and Validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89afcd83",
   "metadata": {},
   "source": [
    "## T5 model Cleaned_mails and Summary_Bart lr = 5e-3, batch size = 8, epoch =20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d90e495f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4360' max='4360' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4360/4360 44:30, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.638100</td>\n",
       "      <td>0.403960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.432000</td>\n",
       "      <td>0.383006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.426400</td>\n",
       "      <td>0.358248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.347200</td>\n",
       "      <td>0.329559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.273500</td>\n",
       "      <td>0.320276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.223100</td>\n",
       "      <td>0.298242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.185600</td>\n",
       "      <td>0.292150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.151200</td>\n",
       "      <td>0.310926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.122300</td>\n",
       "      <td>0.330250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.095400</td>\n",
       "      <td>0.324772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.084000</td>\n",
       "      <td>0.328159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.066300</td>\n",
       "      <td>0.342720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.053600</td>\n",
       "      <td>0.347217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.044300</td>\n",
       "      <td>0.365174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.036200</td>\n",
       "      <td>0.370206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.029100</td>\n",
       "      <td>0.389138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.024100</td>\n",
       "      <td>0.393348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.020100</td>\n",
       "      <td>0.408009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.017200</td>\n",
       "      <td>0.421306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.014000</td>\n",
       "      <td>0.421690</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "1.0     | 7.00     | 0.00     | 6.98     | 6.97        | -76.79     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "2.0     | 6.83     | 0.00     | 6.84     | 6.82        | -77.73     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "3.0     | 5.76     | 0.00     | 5.77     | 5.77        | -77.78     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "4.0     | 7.38     | 0.00     | 7.40     | 7.40        | -77.77     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "5.0     | 7.69     | 0.00     | 7.69     | 7.70        | -77.95     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "6.0     | 6.81     | 0.00     | 6.83     | 6.82        | -77.79     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "7.0     | 7.28     | 0.00     | 7.29     | 7.29        | -78.42     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "8.0     | 7.67     | 0.00     | 7.67     | 7.69        | -77.79     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "9.0     | 6.99     | 0.00     | 7.00     | 6.96        | -77.75     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "10.0    | 6.39     | 0.00     | 6.36     | 6.41        | -77.74     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "11.0    | 6.77     | 0.00     | 6.80     | 6.78        | -78.25     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "12.0    | 6.55     | 0.00     | 6.57     | 6.56        | -77.81     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "13.0    | 6.95     | 0.00     | 6.94     | 6.92        | -78.07     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "14.0    | 6.55     | 0.00     | 6.53     | 6.52        | -78.18     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "15.0    | 7.10     | 0.00     | 7.10     | 7.09        | -77.67     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "16.0    | 7.03     | 0.00     | 7.02     | 7.02        | -77.81     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "17.0    | 7.34     | 0.00     | 7.40     | 7.41        | -78.07     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "18.0    | 7.20     | 0.00     | 7.22     | 7.24        | -77.86     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "19.0    | 7.14     | 0.00     | 7.14     | 7.14        | -77.84     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "20.0    | 7.26     | 0.00     | 7.27     | 7.27        | -77.62     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer, T5ForConditionalGeneration, AdamW\n",
    "\n",
    "# Load the T5 model\n",
    "model_name = 't5-small' \n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "\n",
    "# Calculate the size of the training dataset\n",
    "training_dataset_size = len(train_dataset)\n",
    "\n",
    "N = training_dataset_size\n",
    "batch_size = 8 \n",
    "\n",
    "# Calculate steps per epoch\n",
    "steps_per_epoch = N // batch_size\n",
    "\n",
    "\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",                      # Directory for storing outputs\n",
    "    num_train_epochs=20,                         # Set number of epochs to 10\n",
    "    per_device_train_batch_size=8,               # Batch size for training\n",
    "    per_device_eval_batch_size=8,                # Batch size for evaluation\n",
    "    warmup_steps=500,                            # Number of warmup steps\n",
    "    weight_decay=0.01,                           # Weight decay for regularization\n",
    "    logging_dir=\"./logs\",                        # Directory for storing logs\n",
    "    logging_steps=steps_per_epoch,                            # Log every steps in epoch\n",
    "    do_train=True,                               # Enable training\n",
    "    do_eval=True,                                # Enable evaluation\n",
    "    evaluation_strategy=\"epoch\",                 # Evaluate at the end of each epoch\n",
    "    save_strategy=\"no\",                    # Disable saving the model\n",
    "    save_total_limit=0,                    # Limit on the total amount of checkpoints. Setting to 0 disables it\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,  # Use the dataset instance for training data\n",
    "    eval_dataset=val_dataset,     # Use the dataset instance for validation data\n",
    "    callbacks=[custom_eval_callback],  # Custom callback function for metrics\n",
    "    optimizers=(AdamW(model.parameters(), lr=5e-3), None)  # AdamW optimizer with a specified learning rate\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n",
    "# Plotting\n",
    "training_logs = trainer.state.log_history\n",
    "\n",
    "training_loss = [log['loss'] for log in training_logs if 'loss' in log]\n",
    "validation_loss = [log['eval_loss'] for log in training_logs if 'eval_loss' in log]\n",
    "epochs = range(1, len(training_loss) + 1)\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(epochs, training_loss, 'bo-', label='Training loss')\n",
    "plt.plot(epochs, validation_loss, 'ro-', label='Validation loss')\n",
    "plt.title('Training and Validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "55c46010",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8720' max='8720' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8720/8720 1:27:28, Epoch 40/40]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.638100</td>\n",
       "      <td>0.403960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.432000</td>\n",
       "      <td>0.383006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.429700</td>\n",
       "      <td>0.362005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.358200</td>\n",
       "      <td>0.328155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.293800</td>\n",
       "      <td>0.322744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.246000</td>\n",
       "      <td>0.308323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.207200</td>\n",
       "      <td>0.300163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.177100</td>\n",
       "      <td>0.320702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.155300</td>\n",
       "      <td>0.339995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.136100</td>\n",
       "      <td>0.321147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.119200</td>\n",
       "      <td>0.318597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.104700</td>\n",
       "      <td>0.324743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.093300</td>\n",
       "      <td>0.351133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.080800</td>\n",
       "      <td>0.341566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.078900</td>\n",
       "      <td>0.346612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.064900</td>\n",
       "      <td>0.363476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.059600</td>\n",
       "      <td>0.384066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.054500</td>\n",
       "      <td>0.376994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.049500</td>\n",
       "      <td>0.369819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.044800</td>\n",
       "      <td>0.377903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.038100</td>\n",
       "      <td>0.371954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.036900</td>\n",
       "      <td>0.380440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.032100</td>\n",
       "      <td>0.413401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.028300</td>\n",
       "      <td>0.390935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.025900</td>\n",
       "      <td>0.415527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.022000</td>\n",
       "      <td>0.415154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.020600</td>\n",
       "      <td>0.423918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.018900</td>\n",
       "      <td>0.408972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.017300</td>\n",
       "      <td>0.431448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.015300</td>\n",
       "      <td>0.418211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.012900</td>\n",
       "      <td>0.430590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.011200</td>\n",
       "      <td>0.448448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.010700</td>\n",
       "      <td>0.451464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.009700</td>\n",
       "      <td>0.450552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.008300</td>\n",
       "      <td>0.438335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.007700</td>\n",
       "      <td>0.451276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.006400</td>\n",
       "      <td>0.460784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.005500</td>\n",
       "      <td>0.470288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.004900</td>\n",
       "      <td>0.475508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.004200</td>\n",
       "      <td>0.480050</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "1.0     | 7.00     | 0.00     | 6.98     | 6.97        | -76.79     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "2.0     | 6.83     | 0.00     | 6.84     | 6.82        | -77.73     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "3.0     | 6.05     | 0.00     | 6.08     | 6.09        | -77.86     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "4.0     | 6.63     | 0.00     | 6.66     | 6.65        | -77.65     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "5.0     | 6.99     | 0.00     | 7.00     | 7.00        | -77.34     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "6.0     | 7.54     | 0.00     | 7.55     | 7.53        | -77.49     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "7.0     | 6.94     | 0.00     | 6.95     | 6.95        | -78.80     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "8.0     | 7.03     | 0.00     | 7.01     | 7.03        | -78.56     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "9.0     | 7.18     | 0.00     | 7.18     | 7.18        | -78.05     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "10.0    | 7.12     | 0.00     | 7.11     | 7.13        | -78.38     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "11.0    | 6.08     | 0.00     | 6.09     | 6.05        | -78.64     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "12.0    | 6.65     | 0.00     | 6.66     | 6.65        | -78.18     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "13.0    | 7.46     | 0.00     | 7.45     | 7.46        | -77.96     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "14.0    | 7.15     | 0.00     | 7.13     | 7.15        | -78.42     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "15.0    | 7.03     | 0.00     | 7.04     | 7.05        | -78.05     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "16.0    | 7.42     | 0.00     | 7.41     | 7.41        | -78.52     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "17.0    | 6.44     | 0.00     | 6.46     | 6.49        | -78.32     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "18.0    | 7.49     | 0.00     | 7.49     | 7.48        | -77.94     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "19.0    | 7.16     | 0.00     | 7.16     | 7.18        | -77.43     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "20.0    | 7.14     | 0.00     | 7.15     | 7.14        | -77.79     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "21.0    | 7.19     | 0.00     | 7.20     | 7.16        | -77.76     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "22.0    | 7.14     | 0.00     | 7.13     | 7.12        | -77.46     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "23.0    | 7.47     | 0.00     | 7.47     | 7.48        | -77.66     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "24.0    | 6.83     | 0.00     | 6.83     | 6.85        | -77.66     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "25.0    | 7.02     | 0.00     | 7.06     | 7.05        | -77.58     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "26.0    | 6.76     | 0.00     | 6.77     | 6.78        | -77.85     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "27.0    | 7.24     | 0.00     | 7.23     | 7.22        | -77.86     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "28.0    | 6.97     | 0.00     | 6.95     | 7.00        | -77.72     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "29.0    | 7.38     | 0.00     | 7.37     | 7.37        | -77.99     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "30.0    | 7.22     | 0.00     | 7.21     | 7.25        | -78.12     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "31.0    | 7.35     | 0.00     | 7.38     | 7.35        | -77.98     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "32.0    | 7.19     | 0.00     | 7.22     | 7.20        | -77.85     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "33.0    | 7.45     | 0.00     | 7.44     | 7.43        | -77.96     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "34.0    | 7.17     | 0.00     | 7.20     | 7.16        | -78.01     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "35.0    | 7.36     | 0.00     | 7.38     | 7.36        | -77.50     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "36.0    | 7.67     | 0.00     | 7.67     | 7.67        | -77.67     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "37.0    | 7.55     | 0.00     | 7.53     | 7.55        | -77.76     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "38.0    | 7.33     | 0.00     | 7.32     | 7.34        | -77.75     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "39.0    | 7.43     | 0.00     | 7.45     | 7.45        | -77.83     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "40.0    | 7.50     | 0.00     | 7.49     | 7.48        | -77.84     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer, T5ForConditionalGeneration, AdamW\n",
    "\n",
    "# Load the T5 model\n",
    "model_name = 't5-small'  \n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "\n",
    "# Calculate the size of the training dataset\n",
    "training_dataset_size = len(train_dataset)\n",
    "\n",
    "N = training_dataset_size\n",
    "batch_size = 8  \n",
    "\n",
    "# Calculate steps per epoch\n",
    "steps_per_epoch = N // batch_size\n",
    "\n",
    "\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",                      # Directory for storing outputs\n",
    "    num_train_epochs=40,                         # Set number of epochs to 10\n",
    "    per_device_train_batch_size=8,               # Batch size for training\n",
    "    per_device_eval_batch_size=8,                # Batch size for evaluation\n",
    "    warmup_steps=500,                            # Number of warmup steps\n",
    "    weight_decay=0.01,                           # Weight decay for regularization\n",
    "    logging_dir=\"./logs\",                        # Directory for storing logs\n",
    "    logging_steps=steps_per_epoch,                            # Log every steps in epoch\n",
    "    do_train=True,                               # Enable training\n",
    "    do_eval=True,                                # Enable evaluation\n",
    "    evaluation_strategy=\"epoch\",                 # Evaluate at the end of each epoch\n",
    "    save_strategy=\"no\",                    # Disable saving the model\n",
    "    save_total_limit=0,                    # Limit on the total amount of checkpoints. Setting to 0 disables it\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,  # Use the dataset instance for training data\n",
    "    eval_dataset=val_dataset,     # Use the dataset instance for validation data\n",
    "    callbacks=[custom_eval_callback],  # Custom callback function for metrics\n",
    "    optimizers=(AdamW(model.parameters(), lr=5e-3), None)  # AdamW optimizer with a specified learning rate\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n",
    "# Plotting\n",
    "training_logs = trainer.state.log_history\n",
    "\n",
    "training_loss = [log['loss'] for log in training_logs if 'loss' in log]\n",
    "validation_loss = [log['eval_loss'] for log in training_logs if 'eval_loss' in log]\n",
    "epochs = range(1, len(training_loss) + 1)\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(epochs, training_loss, 'bo-', label='Training loss')\n",
    "plt.plot(epochs, validation_loss, 'ro-', label='Validation loss')\n",
    "plt.title('Training and Validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fcd39e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## The training after certain epochs are very unstable and is overfitiing to address this learning rate schedulers have to be implemented."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2914698",
   "metadata": {},
   "source": [
    "## Annotating the data manually for further training to do, since the human annoted targets are a must for training the models to get good results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9fb5bc9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>from</th>\n",
       "      <th>to</th>\n",
       "      <th>date</th>\n",
       "      <th>body</th>\n",
       "      <th>Body_Length</th>\n",
       "      <th>Subject_Length</th>\n",
       "      <th>Cleaned_Body</th>\n",
       "      <th>Cleaned_Subject</th>\n",
       "      <th>BERT_Embeddings</th>\n",
       "      <th>...</th>\n",
       "      <th>Cleaned_mails</th>\n",
       "      <th>summary_TXTRNK_1</th>\n",
       "      <th>Summary</th>\n",
       "      <th>summary_BART</th>\n",
       "      <th>index_number</th>\n",
       "      <th>Tokenized_Email</th>\n",
       "      <th>Entities</th>\n",
       "      <th>Cluster_retrieved</th>\n",
       "      <th>Summary_human</th>\n",
       "      <th>Preprocessed_Body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Thank you for attending Microsoft Ignite</td>\n",
       "      <td>Microsoft Team &lt;microsoft@email.microsoft.com&gt;</td>\n",
       "      <td>richard.potter@raddsolutions.co.uk</td>\n",
       "      <td>Wed, 19 Oct 2022 20:31:34 +0100</td>\n",
       "      <td>...</td>\n",
       "      <td>6232</td>\n",
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td>Thank you for attending Microsoft Ignite</td>\n",
       "      <td>[-0.059376951307058334, 0.17135855555534363, 0...</td>\n",
       "      <td>...</td>\n",
       "      <td>microsoft ignite may be over but heres your ch...</td>\n",
       "      <td>microsoft ignite may be over but heres your ch...</td>\n",
       "      <td>microsoft ignite may be over but heres your ch...</td>\n",
       "      <td>Learn how microsoft empowers organisations to ...</td>\n",
       "      <td>1543</td>\n",
       "      <td>{'input_ids': tensor([[  101,  7513, 16270,  4...</td>\n",
       "      <td>[('microsoft', 'ORG'), ('2022', 'DATE'), ('mic...</td>\n",
       "      <td>0</td>\n",
       "      <td>The email discusses post-Microsoft Ignite 2022...</td>\n",
       "      <td>summarize: Microsoft Ignite may be over, but h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Invitation to learn how Windows 365 Cloud PCs ...</td>\n",
       "      <td>Microsoft &lt;replyto@email.microsoft.com&gt;</td>\n",
       "      <td>richard.potter@raddsolutions.co.uk</td>\n",
       "      <td>Tue, 01 Nov 2022 11:01:50 +0000</td>\n",
       "      <td>Webinar with demos of Windows 365 and vision f...</td>\n",
       "      <td>2943</td>\n",
       "      <td>90</td>\n",
       "      <td>webinar with demos of windows 365 and vision f...</td>\n",
       "      <td>Invitation to learn how Windows 365 Cloud PCs ...</td>\n",
       "      <td>[-0.1439182013273239, 0.22149936854839325, 0.6...</td>\n",
       "      <td>...</td>\n",
       "      <td>webinar with demos of windows 365 and vision f...</td>\n",
       "      <td>No Summary</td>\n",
       "      <td></td>\n",
       "      <td>webinar with demos of windows 365 and vision f...</td>\n",
       "      <td>765</td>\n",
       "      <td>{'input_ids': tensor([[  101,  4773,  3981,  2...</td>\n",
       "      <td>[('thursday 17th', 'DATE'), ('2022 1400  1500'...</td>\n",
       "      <td>0</td>\n",
       "      <td>Webinar Announcement: \"Windows 365 for Your Hy...</td>\n",
       "      <td>summarize: Webinar with demos of Windows 365 a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Don’t fall behind – embrace AI with Dell Techn...</td>\n",
       "      <td>Dell Technologies Partner Program &lt;DellTechnol...</td>\n",
       "      <td>richard.potter@raddsolutions.co.uk</td>\n",
       "      <td>Tue, 15 Nov 2022 06:01:17 +0000</td>\n",
       "      <td>&lt;https://click.comm.delltechnologies.com/open...</td>\n",
       "      <td>4498</td>\n",
       "      <td>64</td>\n",
       "      <td>\\n   \\t\\r\\nview online \\n\\n  \\t\\r\\n \\t\\r\\nwhy...</td>\n",
       "      <td>Dont fall behind  embrace AI with Dell Technol...</td>\n",
       "      <td>[-0.15142026543617249, 0.11411778628826141, 0....</td>\n",
       "      <td>...</td>\n",
       "      <td>why ai and why now we are witnessing and livin...</td>\n",
       "      <td>we are witnessing and living through the rise ...</td>\n",
       "      <td>why ai and why now\\nwe are witnessing and livi...</td>\n",
       "      <td>Artificial intelligence ai market is forecast ...</td>\n",
       "      <td>369</td>\n",
       "      <td>{'input_ids': tensor([[  101,  3193,  3784,  2...</td>\n",
       "      <td>[('500 billion', 'MONEY'), ('20231', 'DATE'), ...</td>\n",
       "      <td>0</td>\n",
       "      <td>The email discusses the rapid growth of artifi...</td>\n",
       "      <td>summarize: View Online Why AI and why now? Why...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Microsoft Envision Season 3 begins December 13</td>\n",
       "      <td>Microsoft Team &lt;microsoft@email.microsoft.com&gt;</td>\n",
       "      <td>richard.potter@raddsolutions.co.uk</td>\n",
       "      <td>Wed, 09 Nov 2022 17:05:09 +0000</td>\n",
       "      <td>Episode 1 airs December 13, 2022 \\r\\nHaving tr...</td>\n",
       "      <td>3476</td>\n",
       "      <td>46</td>\n",
       "      <td>episode 1 airs december 13 2022 \\r\\nhaving tro...</td>\n",
       "      <td>Microsoft Envision Season 3 begins December 13</td>\n",
       "      <td>[-0.36103835701942444, 0.06514844298362732, 0....</td>\n",
       "      <td>...</td>\n",
       "      <td>episode 1 airs december 13 2022 having trouble...</td>\n",
       "      <td>episode 1 airs december 13 2022\\nregister now ...</td>\n",
       "      <td>episode 1 airs december 13 2022\\nregister now ...</td>\n",
       "      <td>register now for microsoft envision season 3. ...</td>\n",
       "      <td>895</td>\n",
       "      <td>{'input_ids': tensor([[  101,  2792,  1015, 14...</td>\n",
       "      <td>[('1', 'CARDINAL'), ('13 2022', 'DATE'), ('mic...</td>\n",
       "      <td>0</td>\n",
       "      <td>The email announces the premiere of Microsoft ...</td>\n",
       "      <td>summarize: Episode 1 airs December 13, 2022 Ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In September, you had 71 users visit your webs...</td>\n",
       "      <td>Google Analytics &lt;analytics-noreply@google.com&gt;</td>\n",
       "      <td>richard.potter@raddsolutions.co.uk</td>\n",
       "      <td>Tue, 11 Oct 2022 06:02:01 +0100</td>\n",
       "      <td>&lt;https://www.google.com/images/branding/googl...</td>\n",
       "      <td>5559</td>\n",
       "      <td>68</td>\n",
       "      <td>\\n \\r\\nuniversal analytics will no longer pr...</td>\n",
       "      <td>In September you had 71 users visit your websi...</td>\n",
       "      <td>[-0.10645909607410431, 0.17022578418254852, 0....</td>\n",
       "      <td>...</td>\n",
       "      <td>universal analytics will no longer process new...</td>\n",
       "      <td>81 35 bounce rate\\nbreakdown of visitors acqui...</td>\n",
       "      <td>81 35 bounce rate\\nbreakdown of visitors acqui...</td>\n",
       "      <td>universal analytics will no longer process new...</td>\n",
       "      <td>740</td>\n",
       "      <td>{'input_ids': tensor([[  101,  5415, 25095,  2...</td>\n",
       "      <td>[('2023', 'DATE'), ('4', 'CARDINAL'), ('septem...</td>\n",
       "      <td>0</td>\n",
       "      <td>Starting in 2023, Universal Analytics will no ...</td>\n",
       "      <td>summarize: Universal Analytics will no longer ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>949</th>\n",
       "      <td>Our Black Friday offers have landed!</td>\n",
       "      <td>IT Governance &lt;emailsupport@itgovernance.co.uk&gt;</td>\n",
       "      <td>richie.wynne@raddsolutions.co.uk</td>\n",
       "      <td>Mon, 21 Nov 2022 11:05:15 +0000</td>\n",
       "      <td>You’re not going to want to miss these savings...</td>\n",
       "      <td>3228</td>\n",
       "      <td>36</td>\n",
       "      <td>youre not going to want to miss these savings ...</td>\n",
       "      <td>Our Black Friday offers have landed</td>\n",
       "      <td>[0.09008561074733734, 0.16759826242923737, 0.6...</td>\n",
       "      <td>...</td>\n",
       "      <td>youre not going to want to miss these savings ...</td>\n",
       "      <td>use promo code bf25\\nuse promo code bf25\\nunit...</td>\n",
       "      <td>were starting our black friday offers early wi...</td>\n",
       "      <td>were starting our black friday offers early wi...</td>\n",
       "      <td>1130</td>\n",
       "      <td>{'input_ids': tensor([[  101,  2115,  2063,  2...</td>\n",
       "      <td>[('friday', 'DATE'), ('25', 'CARDINAL'), ('25'...</td>\n",
       "      <td>7</td>\n",
       "      <td>The email advertises an early Black Friday off...</td>\n",
       "      <td>summarize: You’re not going to want to miss th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>950</th>\n",
       "      <td>Jeff Fritz reviews IronPDF vs SyncFusion, iTex...</td>\n",
       "      <td>\"Jacob, Head of Engineering\" &lt;developers@irons...</td>\n",
       "      <td>Richard Potter &lt;richard.potter@raddsolutions.c...</td>\n",
       "      <td>Tue, 13 Dec 2022 15:31:28 +0000</td>\n",
       "      <td>&lt;https://ironsoftware.lt.acemlnb.com/Prod/lin...</td>\n",
       "      <td>8587</td>\n",
       "      <td>55</td>\n",
       "      <td>\\t \\r\\n \\t \\r\\nhi richard\\r\\nimagine spendin...</td>\n",
       "      <td>Jeff Fritz reviews IronPDF vs SyncFusion iText...</td>\n",
       "      <td>[-0.20470425486564636, 0.20281416177749634, 0....</td>\n",
       "      <td>...</td>\n",
       "      <td>imagine spending a lot of money on software li...</td>\n",
       "      <td>would your project fail hopefully not but it n...</td>\n",
       "      <td>imagine spending a lot of money on software li...</td>\n",
       "      <td>iron software is a free open source solution t...</td>\n",
       "      <td>783</td>\n",
       "      <td>{'input_ids': tensor([[  101,  7632,  2957,  5...</td>\n",
       "      <td>[('jeff fritz', 'PERSON'), ('net conf', 'ORG')...</td>\n",
       "      <td>7</td>\n",
       "      <td>Jeff Fritz from .NET Conf reviewed IronPDF aga...</td>\n",
       "      <td>summarize:, Imagine spending a lot of money on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>951</th>\n",
       "      <td>Don't Miss Out: Help to Grow: Digital Ends in ...</td>\n",
       "      <td>Zym &lt;rebecca@zymplify.com&gt;</td>\n",
       "      <td>Richard Potter &lt;richard.potter@raddsolutions.c...</td>\n",
       "      <td>Tue, 17 Jan 2023 12:01:49 +0000</td>\n",
       "      <td>ZYM helps business owners understand their mar...</td>\n",
       "      <td>6966</td>\n",
       "      <td>56</td>\n",
       "      <td>zym helps business owners understand their mar...</td>\n",
       "      <td>Dont Miss Out Help to Grow Digital Ends in 16 ...</td>\n",
       "      <td>[0.0035861318465322256, 0.07786554843187332, 0...</td>\n",
       "      <td>...</td>\n",
       "      <td>zym helps business owners understand their mar...</td>\n",
       "      <td>you may qualify for the grant meaning you coul...</td>\n",
       "      <td>even better you can try zym for free for 14 da...</td>\n",
       "      <td>zym helps business owners understand their mar...</td>\n",
       "      <td>345</td>\n",
       "      <td>{'input_ids': tensor([[  101,  1062, 24335,  7...</td>\n",
       "      <td>[('uk', 'GPE'), ('up to 5000', 'CARDINAL'), ('...</td>\n",
       "      <td>7</td>\n",
       "      <td>Zym aids business owners in comprehending mark...</td>\n",
       "      <td>summarize: ZYM helps business owners understan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>952</th>\n",
       "      <td>New videos coming in 2023</td>\n",
       "      <td>Clear Measure &lt;clearmeasure@clear-measure.com&gt;</td>\n",
       "      <td>richard.potter@raddsolutions.co.uk</td>\n",
       "      <td>Thu, 29 Dec 2022 10:00:02 +0000</td>\n",
       "      <td>New videos coming in 2023 … made to empower yo...</td>\n",
       "      <td>3404</td>\n",
       "      <td>25</td>\n",
       "      <td>new videos coming in 2023  made to empower you...</td>\n",
       "      <td>New videos coming in 2023</td>\n",
       "      <td>[-0.08648061007261276, 0.11852650344371796, 0....</td>\n",
       "      <td>...</td>\n",
       "      <td>new videos coming in 2023 made to empower your...</td>\n",
       "      <td>our new videos coming in 2023 are made to empo...</td>\n",
       "      <td>our new videos coming in 2023 are made to empo...</td>\n",
       "      <td>new videos coming in 2023  made to empower you...</td>\n",
       "      <td>958</td>\n",
       "      <td>{'input_ids': tensor([[  101,  2047,  6876,  2...</td>\n",
       "      <td>[('2023', 'DATE'), ('2023', 'DATE'), ('10815',...</td>\n",
       "      <td>7</td>\n",
       "      <td>Summary:\\n\\nClear Measure has announced an upc...</td>\n",
       "      <td>summarize: New videos coming in 2023... made t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>953</th>\n",
       "      <td>Business PCs up to 40% off</td>\n",
       "      <td>Lenovo New beginnings! &lt;lenovo@ecomm.lenovo.com&gt;</td>\n",
       "      <td>richard.potter@raddsolutions.co.uk</td>\n",
       "      <td>Wed, 01 Feb 2023 09:04:42 +0000</td>\n",
       "      <td>&lt;https://trk.ecomm.lenovo.com/ss/o/k8leYzsS8M...</td>\n",
       "      <td>8108</td>\n",
       "      <td>26</td>\n",
       "      <td>\\n \\t\\r\\n\\tview it in browser instead \\n  fre...</td>\n",
       "      <td>Business PCs up to 40 off</td>\n",
       "      <td>[-0.09954569488763809, 0.13386352360248566, 0....</td>\n",
       "      <td>...</td>\n",
       "      <td>view it in browser instead free shipping on al...</td>\n",
       "      <td>workspace refresh with up to 40 discount until...</td>\n",
       "      <td>thinkpad p1 gen 4\\nthinkpad z13 gen 1\\nideapad...</td>\n",
       "      <td>Free shipping on all orders with up to 40% dis...</td>\n",
       "      <td>132</td>\n",
       "      <td>{'input_ids': tensor([[  101,  3193,  2009,  1...</td>\n",
       "      <td>[('up to 40', 'CARDINAL'), ('up to 40', 'CARDI...</td>\n",
       "      <td>7</td>\n",
       "      <td>Email from Lenovo announces a workspace refres...</td>\n",
       "      <td>summarize: View it in browser instead Free shi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>954 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               subject  \\\n",
       "0             Thank you for attending Microsoft Ignite   \n",
       "1    Invitation to learn how Windows 365 Cloud PCs ...   \n",
       "2    Don’t fall behind – embrace AI with Dell Techn...   \n",
       "3       Microsoft Envision Season 3 begins December 13   \n",
       "4    In September, you had 71 users visit your webs...   \n",
       "..                                                 ...   \n",
       "949               Our Black Friday offers have landed!   \n",
       "950  Jeff Fritz reviews IronPDF vs SyncFusion, iTex...   \n",
       "951  Don't Miss Out: Help to Grow: Digital Ends in ...   \n",
       "952                          New videos coming in 2023   \n",
       "953                         Business PCs up to 40% off   \n",
       "\n",
       "                                                  from  \\\n",
       "0       Microsoft Team <microsoft@email.microsoft.com>   \n",
       "1              Microsoft <replyto@email.microsoft.com>   \n",
       "2    Dell Technologies Partner Program <DellTechnol...   \n",
       "3       Microsoft Team <microsoft@email.microsoft.com>   \n",
       "4      Google Analytics <analytics-noreply@google.com>   \n",
       "..                                                 ...   \n",
       "949    IT Governance <emailsupport@itgovernance.co.uk>   \n",
       "950  \"Jacob, Head of Engineering\" <developers@irons...   \n",
       "951                         Zym <rebecca@zymplify.com>   \n",
       "952     Clear Measure <clearmeasure@clear-measure.com>   \n",
       "953   Lenovo New beginnings! <lenovo@ecomm.lenovo.com>   \n",
       "\n",
       "                                                    to  \\\n",
       "0                   richard.potter@raddsolutions.co.uk   \n",
       "1                   richard.potter@raddsolutions.co.uk   \n",
       "2                   richard.potter@raddsolutions.co.uk   \n",
       "3                   richard.potter@raddsolutions.co.uk   \n",
       "4                   richard.potter@raddsolutions.co.uk   \n",
       "..                                                 ...   \n",
       "949                   richie.wynne@raddsolutions.co.uk   \n",
       "950  Richard Potter <richard.potter@raddsolutions.c...   \n",
       "951  Richard Potter <richard.potter@raddsolutions.c...   \n",
       "952                 richard.potter@raddsolutions.co.uk   \n",
       "953                 richard.potter@raddsolutions.co.uk   \n",
       "\n",
       "                                date  \\\n",
       "0    Wed, 19 Oct 2022 20:31:34 +0100   \n",
       "1    Tue, 01 Nov 2022 11:01:50 +0000   \n",
       "2    Tue, 15 Nov 2022 06:01:17 +0000   \n",
       "3    Wed, 09 Nov 2022 17:05:09 +0000   \n",
       "4    Tue, 11 Oct 2022 06:02:01 +0100   \n",
       "..                               ...   \n",
       "949  Mon, 21 Nov 2022 11:05:15 +0000   \n",
       "950  Tue, 13 Dec 2022 15:31:28 +0000   \n",
       "951  Tue, 17 Jan 2023 12:01:49 +0000   \n",
       "952  Thu, 29 Dec 2022 10:00:02 +0000   \n",
       "953  Wed, 01 Feb 2023 09:04:42 +0000   \n",
       "\n",
       "                                                  body  Body_Length  \\\n",
       "0                                                  ...         6232   \n",
       "1    Webinar with demos of Windows 365 and vision f...         2943   \n",
       "2     <https://click.comm.delltechnologies.com/open...         4498   \n",
       "3    Episode 1 airs December 13, 2022 \\r\\nHaving tr...         3476   \n",
       "4     <https://www.google.com/images/branding/googl...         5559   \n",
       "..                                                 ...          ...   \n",
       "949  You’re not going to want to miss these savings...         3228   \n",
       "950   <https://ironsoftware.lt.acemlnb.com/Prod/lin...         8587   \n",
       "951  ZYM helps business owners understand their mar...         6966   \n",
       "952  New videos coming in 2023 … made to empower yo...         3404   \n",
       "953   <https://trk.ecomm.lenovo.com/ss/o/k8leYzsS8M...         8108   \n",
       "\n",
       "     Subject_Length                                       Cleaned_Body  \\\n",
       "0                40                                                ...   \n",
       "1                90  webinar with demos of windows 365 and vision f...   \n",
       "2                64   \\n   \\t\\r\\nview online \\n\\n  \\t\\r\\n \\t\\r\\nwhy...   \n",
       "3                46  episode 1 airs december 13 2022 \\r\\nhaving tro...   \n",
       "4                68    \\n \\r\\nuniversal analytics will no longer pr...   \n",
       "..              ...                                                ...   \n",
       "949              36  youre not going to want to miss these savings ...   \n",
       "950              55    \\t \\r\\n \\t \\r\\nhi richard\\r\\nimagine spendin...   \n",
       "951              56  zym helps business owners understand their mar...   \n",
       "952              25  new videos coming in 2023  made to empower you...   \n",
       "953              26   \\n \\t\\r\\n\\tview it in browser instead \\n  fre...   \n",
       "\n",
       "                                       Cleaned_Subject  \\\n",
       "0             Thank you for attending Microsoft Ignite   \n",
       "1    Invitation to learn how Windows 365 Cloud PCs ...   \n",
       "2    Dont fall behind  embrace AI with Dell Technol...   \n",
       "3       Microsoft Envision Season 3 begins December 13   \n",
       "4    In September you had 71 users visit your websi...   \n",
       "..                                                 ...   \n",
       "949                Our Black Friday offers have landed   \n",
       "950  Jeff Fritz reviews IronPDF vs SyncFusion iText...   \n",
       "951  Dont Miss Out Help to Grow Digital Ends in 16 ...   \n",
       "952                          New videos coming in 2023   \n",
       "953                          Business PCs up to 40 off   \n",
       "\n",
       "                                       BERT_Embeddings  ...  \\\n",
       "0    [-0.059376951307058334, 0.17135855555534363, 0...  ...   \n",
       "1    [-0.1439182013273239, 0.22149936854839325, 0.6...  ...   \n",
       "2    [-0.15142026543617249, 0.11411778628826141, 0....  ...   \n",
       "3    [-0.36103835701942444, 0.06514844298362732, 0....  ...   \n",
       "4    [-0.10645909607410431, 0.17022578418254852, 0....  ...   \n",
       "..                                                 ...  ...   \n",
       "949  [0.09008561074733734, 0.16759826242923737, 0.6...  ...   \n",
       "950  [-0.20470425486564636, 0.20281416177749634, 0....  ...   \n",
       "951  [0.0035861318465322256, 0.07786554843187332, 0...  ...   \n",
       "952  [-0.08648061007261276, 0.11852650344371796, 0....  ...   \n",
       "953  [-0.09954569488763809, 0.13386352360248566, 0....  ...   \n",
       "\n",
       "                                         Cleaned_mails  \\\n",
       "0    microsoft ignite may be over but heres your ch...   \n",
       "1    webinar with demos of windows 365 and vision f...   \n",
       "2    why ai and why now we are witnessing and livin...   \n",
       "3    episode 1 airs december 13 2022 having trouble...   \n",
       "4    universal analytics will no longer process new...   \n",
       "..                                                 ...   \n",
       "949  youre not going to want to miss these savings ...   \n",
       "950  imagine spending a lot of money on software li...   \n",
       "951  zym helps business owners understand their mar...   \n",
       "952  new videos coming in 2023 made to empower your...   \n",
       "953  view it in browser instead free shipping on al...   \n",
       "\n",
       "                                      summary_TXTRNK_1  \\\n",
       "0    microsoft ignite may be over but heres your ch...   \n",
       "1                                           No Summary   \n",
       "2    we are witnessing and living through the rise ...   \n",
       "3    episode 1 airs december 13 2022\\nregister now ...   \n",
       "4    81 35 bounce rate\\nbreakdown of visitors acqui...   \n",
       "..                                                 ...   \n",
       "949  use promo code bf25\\nuse promo code bf25\\nunit...   \n",
       "950  would your project fail hopefully not but it n...   \n",
       "951  you may qualify for the grant meaning you coul...   \n",
       "952  our new videos coming in 2023 are made to empo...   \n",
       "953  workspace refresh with up to 40 discount until...   \n",
       "\n",
       "                                               Summary  \\\n",
       "0    microsoft ignite may be over but heres your ch...   \n",
       "1                                                        \n",
       "2    why ai and why now\\nwe are witnessing and livi...   \n",
       "3    episode 1 airs december 13 2022\\nregister now ...   \n",
       "4    81 35 bounce rate\\nbreakdown of visitors acqui...   \n",
       "..                                                 ...   \n",
       "949  were starting our black friday offers early wi...   \n",
       "950  imagine spending a lot of money on software li...   \n",
       "951  even better you can try zym for free for 14 da...   \n",
       "952  our new videos coming in 2023 are made to empo...   \n",
       "953  thinkpad p1 gen 4\\nthinkpad z13 gen 1\\nideapad...   \n",
       "\n",
       "                                          summary_BART index_number  \\\n",
       "0    Learn how microsoft empowers organisations to ...         1543   \n",
       "1    webinar with demos of windows 365 and vision f...          765   \n",
       "2    Artificial intelligence ai market is forecast ...          369   \n",
       "3    register now for microsoft envision season 3. ...          895   \n",
       "4    universal analytics will no longer process new...          740   \n",
       "..                                                 ...          ...   \n",
       "949  were starting our black friday offers early wi...         1130   \n",
       "950  iron software is a free open source solution t...          783   \n",
       "951  zym helps business owners understand their mar...          345   \n",
       "952  new videos coming in 2023  made to empower you...          958   \n",
       "953  Free shipping on all orders with up to 40% dis...          132   \n",
       "\n",
       "                                       Tokenized_Email  \\\n",
       "0    {'input_ids': tensor([[  101,  7513, 16270,  4...   \n",
       "1    {'input_ids': tensor([[  101,  4773,  3981,  2...   \n",
       "2    {'input_ids': tensor([[  101,  3193,  3784,  2...   \n",
       "3    {'input_ids': tensor([[  101,  2792,  1015, 14...   \n",
       "4    {'input_ids': tensor([[  101,  5415, 25095,  2...   \n",
       "..                                                 ...   \n",
       "949  {'input_ids': tensor([[  101,  2115,  2063,  2...   \n",
       "950  {'input_ids': tensor([[  101,  7632,  2957,  5...   \n",
       "951  {'input_ids': tensor([[  101,  1062, 24335,  7...   \n",
       "952  {'input_ids': tensor([[  101,  2047,  6876,  2...   \n",
       "953  {'input_ids': tensor([[  101,  3193,  2009,  1...   \n",
       "\n",
       "                                              Entities Cluster_retrieved  \\\n",
       "0    [('microsoft', 'ORG'), ('2022', 'DATE'), ('mic...                 0   \n",
       "1    [('thursday 17th', 'DATE'), ('2022 1400  1500'...                 0   \n",
       "2    [('500 billion', 'MONEY'), ('20231', 'DATE'), ...                 0   \n",
       "3    [('1', 'CARDINAL'), ('13 2022', 'DATE'), ('mic...                 0   \n",
       "4    [('2023', 'DATE'), ('4', 'CARDINAL'), ('septem...                 0   \n",
       "..                                                 ...               ...   \n",
       "949  [('friday', 'DATE'), ('25', 'CARDINAL'), ('25'...                 7   \n",
       "950  [('jeff fritz', 'PERSON'), ('net conf', 'ORG')...                 7   \n",
       "951  [('uk', 'GPE'), ('up to 5000', 'CARDINAL'), ('...                 7   \n",
       "952  [('2023', 'DATE'), ('2023', 'DATE'), ('10815',...                 7   \n",
       "953  [('up to 40', 'CARDINAL'), ('up to 40', 'CARDI...                 7   \n",
       "\n",
       "                                         Summary_human  \\\n",
       "0    The email discusses post-Microsoft Ignite 2022...   \n",
       "1    Webinar Announcement: \"Windows 365 for Your Hy...   \n",
       "2    The email discusses the rapid growth of artifi...   \n",
       "3    The email announces the premiere of Microsoft ...   \n",
       "4    Starting in 2023, Universal Analytics will no ...   \n",
       "..                                                 ...   \n",
       "949  The email advertises an early Black Friday off...   \n",
       "950  Jeff Fritz from .NET Conf reviewed IronPDF aga...   \n",
       "951  Zym aids business owners in comprehending mark...   \n",
       "952  Summary:\\n\\nClear Measure has announced an upc...   \n",
       "953  Email from Lenovo announces a workspace refres...   \n",
       "\n",
       "                                     Preprocessed_Body  \n",
       "0    summarize: Microsoft Ignite may be over, but h...  \n",
       "1    summarize: Webinar with demos of Windows 365 a...  \n",
       "2    summarize: View Online Why AI and why now? Why...  \n",
       "3    summarize: Episode 1 airs December 13, 2022 Ha...  \n",
       "4    summarize: Universal Analytics will no longer ...  \n",
       "..                                                 ...  \n",
       "949  summarize: You’re not going to want to miss th...  \n",
       "950  summarize:, Imagine spending a lot of money on...  \n",
       "951  summarize: ZYM helps business owners understan...  \n",
       "952  summarize: New videos coming in 2023... made t...  \n",
       "953  summarize: View it in browser instead Free shi...  \n",
       "\n",
       "[954 rows x 22 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11ef623",
   "metadata": {},
   "source": [
    "## The interface for summarsing and inputing the summary and saving into dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df90b4d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the iloc index (or 'exit' to stop): 50\n",
      "\n",
      "Cleaned Input:\n",
      " see this months technical resources training and local community events having trouble viewing this email view as web page microsoftsource newsletter issue 42 see the latest ideas and projects from the global developer community if someone forwarded you this newsletter and you want to receive future issues sign up inclusive tech benefits everyone learn how to develop4all with accessibility focused learning content for developers and inspiring stories about diverse communities learn new developer skills with stepbystep guidance start your journey today by exploring our learning paths and modules developer tools enable everyone to be productive microsoft experts share what microsoft is doing to ensure everyone can develop accessible products learn about developing accessible code gaming for everyone and inclusive storytelling why you cant always rely on ai when designing for accessibility in the absence of text descriptions provided by content authors screen readers are turning to machine learning lonie watson explains why thats not always enough developing inclusive windows apps learn to develop accessible windows apps that include keyboard navigation colour and contrast settings as well as support for assistive technologies xbox releases new accessibility updates microsoft is working to remove barriers to play ultimately making gaming more accessible around the globe events see all events the xtremepython december 27 online this coming xtremepython 2022 online conference includes short sessions with speakers from all over the world including kushal vijay is a software engineer at microsoft building a great developer experience january 16 online learn how to attract and retain developer talent by reducing friction supporting innovation and enabling them to do their best work with the tools and languages they love microsoft technology for social impact january 18 online as an isv focused on delivering innovative technology for the uk nfp sector join us to learn about exciting value creation and partnership opportunities with microsoft ten tuesdays to azure fundamentals january 24 online this series will provide a foundational level knowledge on cloud concepts core azure services security privacy compliance and trust and azure pricing and support using azure cognitive services to create more accessible experiences henk boelman joins scott hanselman to discuss how ai can help to create more accessible experiences with azure machine learning azure cognitive services and azure media services accessibility in technology challenge learn what accessibility means how to be respectful and inclusive of people of all abilities and why its important to know as a developer microsoft certification power automate rpa developer associate if youre a developer with a keen interest in providing automated solutions for your organization this certification could be a great fit for you with nearly 400 million players with disabilities in the world its important to create gaming experiences that are inclusive and accessible by design for as many players as possible visit azure community support to ask questions get answers and connect with azure experts microsoft limited registered in england and wales company number 01624297 registered office microsoft campus thames valley park reading rg6 1wg united kingdom\n",
      "\n",
      "Enter your summary for the above text: The MicrosoftSource Newsletter Issue 42 discusses the importance of inclusive technology and provides resources for developers to learn about and implement accessibility in their projects. The email highlights:  - Development4All: Accessibility-focused learning content and inspiring stories about diverse communities to teach developers about creating inclusive tech. - Developer Tools: Microsoft experts share insights on developing accessible products, including accessible code, gaming, and storytelling. - Inclusive Windows Apps: Advice on creating Windows apps that provide keyboard navigation, appropriate color and contrast, and support for assistive technologies. - Xbox Accessibility Updates: Efforts by Microsoft to make gaming more accessible globally\n",
      "\n",
      "Summary saved successfully.\n",
      "Enter the iloc index (or 'exit' to stop): exit\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def annotate_data(df):\n",
    "    while True:\n",
    "        # Ask for the iloc index\n",
    "        index = input(\"Enter the iloc index (or 'exit' to stop): \")\n",
    "        if index.lower() == 'exit':\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            index = int(index)\n",
    "            if index < 0 or index >= len(df):\n",
    "                print(\"Index out of range. Please try again.\")\n",
    "                continue\n",
    "\n",
    "            # Display the preprocessed input\n",
    "            print(\"\\nCleaned Input:\\n\", df.iloc[index]['Cleaned_mails'])\n",
    "\n",
    "            # Enter the human-generated summary\n",
    "            summary = input(\"\\nEnter your summary for the above text: \")\n",
    "\n",
    "            # Save the summary in a new column\n",
    "            if 'Summary_human' not in df.columns:\n",
    "                df['Summary_human'] = pd.NA  # Initialize column with missing values\n",
    "            df.at[index, 'Summary_human'] = summary\n",
    "\n",
    "            print(\"\\nSummary saved successfully.\")\n",
    "\n",
    "        except ValueError:\n",
    "            print(\"Invalid input. Please enter a numerical index.\")\n",
    "    return df\n",
    "# Call the function with your DataFrame\n",
    "df = annotate_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eaa2079f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index 50:\n",
      "The MicrosoftSource Newsletter Issue 42 discusses the importance of inclusive technology and provides resources for developers to learn about and implement accessibility in their projects. The email highlights:\n",
      "\n",
      "- Development4All: Accessibility-focused learning content and inspiring stories about diverse communities to teach developers about creating inclusive tech.\n",
      "- Developer Tools: Microsoft experts share insights on developing accessible products, including accessible code, gaming, and storytelling.\n",
      "- Inclusive Windows Apps: Advice on creating Windows apps that provide keyboard navigation, appropriate color and contrast, and support for assistive technologies.\n",
      "- Xbox Accessibility Updates: Efforts by Microsoft to make gaming more accessible globally\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(50, 51):\n",
    "    print(f\"Index {i}:\\n{df['Summary_human'][i]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81cb2003",
   "metadata": {},
   "source": [
    "# Fine tuning the t5 model based on the human annoted summary as targets"
   ]
  },
  {
   "cell_type": "raw",
   "id": "59592da7",
   "metadata": {},
   "source": [
    "## With the normal tokenisation and truncation , which seems to be a bit problematic at the moment, due to the truncation the long mails are loosing the context and relavant information since this model is only accepting 512 tokens for the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4a362d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9caa26e7",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1d083a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_json('sum_anno_human_proper.json', lines= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7158a025",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>from</th>\n",
       "      <th>to</th>\n",
       "      <th>date</th>\n",
       "      <th>body</th>\n",
       "      <th>Body_Length</th>\n",
       "      <th>Subject_Length</th>\n",
       "      <th>Cleaned_Body</th>\n",
       "      <th>Cleaned_Subject</th>\n",
       "      <th>BERT_Embeddings</th>\n",
       "      <th>...</th>\n",
       "      <th>Category</th>\n",
       "      <th>Cleaned_mails</th>\n",
       "      <th>summary_TXTRNK_1</th>\n",
       "      <th>Summary</th>\n",
       "      <th>summary_BART</th>\n",
       "      <th>index_number</th>\n",
       "      <th>Tokenized_Email</th>\n",
       "      <th>Entities</th>\n",
       "      <th>Cluster_retrieved</th>\n",
       "      <th>Summary_human</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Thank you for attending Microsoft Ignite</td>\n",
       "      <td>Microsoft Team &lt;microsoft@email.microsoft.com&gt;</td>\n",
       "      <td>richard.potter@raddsolutions.co.uk</td>\n",
       "      <td>Wed, 19 Oct 2022 20:31:34 +0100</td>\n",
       "      <td>...</td>\n",
       "      <td>6232</td>\n",
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td>Thank you for attending Microsoft Ignite</td>\n",
       "      <td>[-0.059376951307058334, 0.17135855555534363, 0...</td>\n",
       "      <td>...</td>\n",
       "      <td>Cyber Alerts</td>\n",
       "      <td>microsoft ignite may be over but heres your ch...</td>\n",
       "      <td>microsoft ignite may be over but heres your ch...</td>\n",
       "      <td>microsoft ignite may be over but heres your ch...</td>\n",
       "      <td>Learn how microsoft empowers organisations to ...</td>\n",
       "      <td>1543</td>\n",
       "      <td>{'input_ids': tensor([[  101,  7513, 16270,  4...</td>\n",
       "      <td>[('microsoft', 'ORG'), ('2022', 'DATE'), ('mic...</td>\n",
       "      <td>0</td>\n",
       "      <td>The email discusses post-Microsoft Ignite 2022...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Invitation to learn how Windows 365 Cloud PCs ...</td>\n",
       "      <td>Microsoft &lt;replyto@email.microsoft.com&gt;</td>\n",
       "      <td>richard.potter@raddsolutions.co.uk</td>\n",
       "      <td>Tue, 01 Nov 2022 11:01:50 +0000</td>\n",
       "      <td>Webinar with demos of Windows 365 and vision f...</td>\n",
       "      <td>2943</td>\n",
       "      <td>90</td>\n",
       "      <td>webinar with demos of windows 365 and vision f...</td>\n",
       "      <td>Invitation to learn how Windows 365 Cloud PCs ...</td>\n",
       "      <td>[-0.1439182013273239, 0.22149936854839325, 0.6...</td>\n",
       "      <td>...</td>\n",
       "      <td>Social Media Alerts</td>\n",
       "      <td>webinar with demos of windows 365 and vision f...</td>\n",
       "      <td>No Summary</td>\n",
       "      <td></td>\n",
       "      <td>webinar with demos of windows 365 and vision f...</td>\n",
       "      <td>765</td>\n",
       "      <td>{'input_ids': tensor([[  101,  4773,  3981,  2...</td>\n",
       "      <td>[('thursday 17th', 'DATE'), ('2022 1400  1500'...</td>\n",
       "      <td>0</td>\n",
       "      <td>Webinar Announcement: \"Windows 365 for Your Hy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Don’t fall behind – embrace AI with Dell Techn...</td>\n",
       "      <td>Dell Technologies Partner Program &lt;DellTechnol...</td>\n",
       "      <td>richard.potter@raddsolutions.co.uk</td>\n",
       "      <td>Tue, 15 Nov 2022 06:01:17 +0000</td>\n",
       "      <td>&lt;https://click.comm.delltechnologies.com/open...</td>\n",
       "      <td>4498</td>\n",
       "      <td>64</td>\n",
       "      <td>\\n   \\t\\r\\nview online \\n\\n  \\t\\r\\n \\t\\r\\nwhy...</td>\n",
       "      <td>Dont fall behind  embrace AI with Dell Technol...</td>\n",
       "      <td>[-0.15142026543617249, 0.11411778628826141, 0....</td>\n",
       "      <td>...</td>\n",
       "      <td>Social Media Alerts</td>\n",
       "      <td>why ai and why now we are witnessing and livin...</td>\n",
       "      <td>we are witnessing and living through the rise ...</td>\n",
       "      <td>why ai and why now\\nwe are witnessing and livi...</td>\n",
       "      <td>Artificial intelligence ai market is forecast ...</td>\n",
       "      <td>369</td>\n",
       "      <td>{'input_ids': tensor([[  101,  3193,  3784,  2...</td>\n",
       "      <td>[('500 billion', 'MONEY'), ('20231', 'DATE'), ...</td>\n",
       "      <td>0</td>\n",
       "      <td>The email discusses the rapid growth of artifi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Microsoft Envision Season 3 begins December 13</td>\n",
       "      <td>Microsoft Team &lt;microsoft@email.microsoft.com&gt;</td>\n",
       "      <td>richard.potter@raddsolutions.co.uk</td>\n",
       "      <td>Wed, 09 Nov 2022 17:05:09 +0000</td>\n",
       "      <td>Episode 1 airs December 13, 2022 \\r\\nHaving tr...</td>\n",
       "      <td>3476</td>\n",
       "      <td>46</td>\n",
       "      <td>episode 1 airs december 13 2022 \\r\\nhaving tro...</td>\n",
       "      <td>Microsoft Envision Season 3 begins December 13</td>\n",
       "      <td>[-0.36103835701942444, 0.06514844298362732, 0....</td>\n",
       "      <td>...</td>\n",
       "      <td>Social Media Alerts</td>\n",
       "      <td>episode 1 airs december 13 2022 having trouble...</td>\n",
       "      <td>episode 1 airs december 13 2022\\nregister now ...</td>\n",
       "      <td>episode 1 airs december 13 2022\\nregister now ...</td>\n",
       "      <td>register now for microsoft envision season 3. ...</td>\n",
       "      <td>895</td>\n",
       "      <td>{'input_ids': tensor([[  101,  2792,  1015, 14...</td>\n",
       "      <td>[('1', 'CARDINAL'), ('13 2022', 'DATE'), ('mic...</td>\n",
       "      <td>0</td>\n",
       "      <td>The email announces the premiere of Microsoft ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In September, you had 71 users visit your webs...</td>\n",
       "      <td>Google Analytics &lt;analytics-noreply@google.com&gt;</td>\n",
       "      <td>richard.potter@raddsolutions.co.uk</td>\n",
       "      <td>Tue, 11 Oct 2022 06:02:01 +0100</td>\n",
       "      <td>&lt;https://www.google.com/images/branding/googl...</td>\n",
       "      <td>5559</td>\n",
       "      <td>68</td>\n",
       "      <td>\\n \\r\\nuniversal analytics will no longer pr...</td>\n",
       "      <td>In September you had 71 users visit your websi...</td>\n",
       "      <td>[-0.10645909607410431, 0.17022578418254852, 0....</td>\n",
       "      <td>...</td>\n",
       "      <td>Social Media Alerts</td>\n",
       "      <td>universal analytics will no longer process new...</td>\n",
       "      <td>81 35 bounce rate\\nbreakdown of visitors acqui...</td>\n",
       "      <td>81 35 bounce rate\\nbreakdown of visitors acqui...</td>\n",
       "      <td>universal analytics will no longer process new...</td>\n",
       "      <td>740</td>\n",
       "      <td>{'input_ids': tensor([[  101,  5415, 25095,  2...</td>\n",
       "      <td>[('2023', 'DATE'), ('4', 'CARDINAL'), ('septem...</td>\n",
       "      <td>0</td>\n",
       "      <td>Starting in 2023, Universal Analytics will no ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>949</th>\n",
       "      <td>Our Black Friday offers have landed!</td>\n",
       "      <td>IT Governance &lt;emailsupport@itgovernance.co.uk&gt;</td>\n",
       "      <td>richie.wynne@raddsolutions.co.uk</td>\n",
       "      <td>Mon, 21 Nov 2022 11:05:15 +0000</td>\n",
       "      <td>You’re not going to want to miss these savings...</td>\n",
       "      <td>3228</td>\n",
       "      <td>36</td>\n",
       "      <td>youre not going to want to miss these savings ...</td>\n",
       "      <td>Our Black Friday offers have landed</td>\n",
       "      <td>[0.09008561074733734, 0.16759826242923737, 0.6...</td>\n",
       "      <td>...</td>\n",
       "      <td>Service Alerts</td>\n",
       "      <td>youre not going to want to miss these savings ...</td>\n",
       "      <td>use promo code bf25\\nuse promo code bf25\\nunit...</td>\n",
       "      <td>were starting our black friday offers early wi...</td>\n",
       "      <td>were starting our black friday offers early wi...</td>\n",
       "      <td>1130</td>\n",
       "      <td>{'input_ids': tensor([[  101,  2115,  2063,  2...</td>\n",
       "      <td>[('friday', 'DATE'), ('25', 'CARDINAL'), ('25'...</td>\n",
       "      <td>7</td>\n",
       "      <td>The email advertises an early Black Friday off...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>950</th>\n",
       "      <td>Jeff Fritz reviews IronPDF vs SyncFusion, iTex...</td>\n",
       "      <td>\"Jacob, Head of Engineering\" &lt;developers@irons...</td>\n",
       "      <td>Richard Potter &lt;richard.potter@raddsolutions.c...</td>\n",
       "      <td>Tue, 13 Dec 2022 15:31:28 +0000</td>\n",
       "      <td>&lt;https://ironsoftware.lt.acemlnb.com/Prod/lin...</td>\n",
       "      <td>8587</td>\n",
       "      <td>55</td>\n",
       "      <td>\\t \\r\\n \\t \\r\\nhi richard\\r\\nimagine spendin...</td>\n",
       "      <td>Jeff Fritz reviews IronPDF vs SyncFusion iText...</td>\n",
       "      <td>[-0.20470425486564636, 0.20281416177749634, 0....</td>\n",
       "      <td>...</td>\n",
       "      <td>Service Alerts</td>\n",
       "      <td>imagine spending a lot of money on software li...</td>\n",
       "      <td>would your project fail hopefully not but it n...</td>\n",
       "      <td>imagine spending a lot of money on software li...</td>\n",
       "      <td>iron software is a free open source solution t...</td>\n",
       "      <td>783</td>\n",
       "      <td>{'input_ids': tensor([[  101,  7632,  2957,  5...</td>\n",
       "      <td>[('jeff fritz', 'PERSON'), ('net conf', 'ORG')...</td>\n",
       "      <td>7</td>\n",
       "      <td>Jeff Fritz from .NET Conf reviewed IronPDF aga...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>951</th>\n",
       "      <td>Don't Miss Out: Help to Grow: Digital Ends in ...</td>\n",
       "      <td>Zym &lt;rebecca@zymplify.com&gt;</td>\n",
       "      <td>Richard Potter &lt;richard.potter@raddsolutions.c...</td>\n",
       "      <td>Tue, 17 Jan 2023 12:01:49 +0000</td>\n",
       "      <td>ZYM helps business owners understand their mar...</td>\n",
       "      <td>6966</td>\n",
       "      <td>56</td>\n",
       "      <td>zym helps business owners understand their mar...</td>\n",
       "      <td>Dont Miss Out Help to Grow Digital Ends in 16 ...</td>\n",
       "      <td>[0.0035861318465322256, 0.07786554843187332, 0...</td>\n",
       "      <td>...</td>\n",
       "      <td>Service Alerts</td>\n",
       "      <td>zym helps business owners understand their mar...</td>\n",
       "      <td>you may qualify for the grant meaning you coul...</td>\n",
       "      <td>even better you can try zym for free for 14 da...</td>\n",
       "      <td>zym helps business owners understand their mar...</td>\n",
       "      <td>345</td>\n",
       "      <td>{'input_ids': tensor([[  101,  1062, 24335,  7...</td>\n",
       "      <td>[('uk', 'GPE'), ('up to 5000', 'CARDINAL'), ('...</td>\n",
       "      <td>7</td>\n",
       "      <td>Zym aids business owners in comprehending mark...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>952</th>\n",
       "      <td>New videos coming in 2023</td>\n",
       "      <td>Clear Measure &lt;clearmeasure@clear-measure.com&gt;</td>\n",
       "      <td>richard.potter@raddsolutions.co.uk</td>\n",
       "      <td>Thu, 29 Dec 2022 10:00:02 +0000</td>\n",
       "      <td>New videos coming in 2023 … made to empower yo...</td>\n",
       "      <td>3404</td>\n",
       "      <td>25</td>\n",
       "      <td>new videos coming in 2023  made to empower you...</td>\n",
       "      <td>New videos coming in 2023</td>\n",
       "      <td>[-0.08648061007261276, 0.11852650344371796, 0....</td>\n",
       "      <td>...</td>\n",
       "      <td>Service Alerts</td>\n",
       "      <td>new videos coming in 2023 made to empower your...</td>\n",
       "      <td>our new videos coming in 2023 are made to empo...</td>\n",
       "      <td>our new videos coming in 2023 are made to empo...</td>\n",
       "      <td>new videos coming in 2023  made to empower you...</td>\n",
       "      <td>958</td>\n",
       "      <td>{'input_ids': tensor([[  101,  2047,  6876,  2...</td>\n",
       "      <td>[('2023', 'DATE'), ('2023', 'DATE'), ('10815',...</td>\n",
       "      <td>7</td>\n",
       "      <td>Summary:\\n\\nClear Measure has announced an upc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>953</th>\n",
       "      <td>Business PCs up to 40% off</td>\n",
       "      <td>Lenovo New beginnings! &lt;lenovo@ecomm.lenovo.com&gt;</td>\n",
       "      <td>richard.potter@raddsolutions.co.uk</td>\n",
       "      <td>Wed, 01 Feb 2023 09:04:42 +0000</td>\n",
       "      <td>&lt;https://trk.ecomm.lenovo.com/ss/o/k8leYzsS8M...</td>\n",
       "      <td>8108</td>\n",
       "      <td>26</td>\n",
       "      <td>\\n \\t\\r\\n\\tview it in browser instead \\n  fre...</td>\n",
       "      <td>Business PCs up to 40 off</td>\n",
       "      <td>[-0.09954569488763809, 0.13386352360248566, 0....</td>\n",
       "      <td>...</td>\n",
       "      <td>Service Alerts</td>\n",
       "      <td>view it in browser instead free shipping on al...</td>\n",
       "      <td>workspace refresh with up to 40 discount until...</td>\n",
       "      <td>thinkpad p1 gen 4\\nthinkpad z13 gen 1\\nideapad...</td>\n",
       "      <td>Free shipping on all orders with up to 40% dis...</td>\n",
       "      <td>132</td>\n",
       "      <td>{'input_ids': tensor([[  101,  3193,  2009,  1...</td>\n",
       "      <td>[('up to 40', 'CARDINAL'), ('up to 40', 'CARDI...</td>\n",
       "      <td>7</td>\n",
       "      <td>Email from Lenovo announces a workspace refres...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>954 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               subject  \\\n",
       "0             Thank you for attending Microsoft Ignite   \n",
       "1    Invitation to learn how Windows 365 Cloud PCs ...   \n",
       "2    Don’t fall behind – embrace AI with Dell Techn...   \n",
       "3       Microsoft Envision Season 3 begins December 13   \n",
       "4    In September, you had 71 users visit your webs...   \n",
       "..                                                 ...   \n",
       "949               Our Black Friday offers have landed!   \n",
       "950  Jeff Fritz reviews IronPDF vs SyncFusion, iTex...   \n",
       "951  Don't Miss Out: Help to Grow: Digital Ends in ...   \n",
       "952                          New videos coming in 2023   \n",
       "953                         Business PCs up to 40% off   \n",
       "\n",
       "                                                  from  \\\n",
       "0       Microsoft Team <microsoft@email.microsoft.com>   \n",
       "1              Microsoft <replyto@email.microsoft.com>   \n",
       "2    Dell Technologies Partner Program <DellTechnol...   \n",
       "3       Microsoft Team <microsoft@email.microsoft.com>   \n",
       "4      Google Analytics <analytics-noreply@google.com>   \n",
       "..                                                 ...   \n",
       "949    IT Governance <emailsupport@itgovernance.co.uk>   \n",
       "950  \"Jacob, Head of Engineering\" <developers@irons...   \n",
       "951                         Zym <rebecca@zymplify.com>   \n",
       "952     Clear Measure <clearmeasure@clear-measure.com>   \n",
       "953   Lenovo New beginnings! <lenovo@ecomm.lenovo.com>   \n",
       "\n",
       "                                                    to  \\\n",
       "0                   richard.potter@raddsolutions.co.uk   \n",
       "1                   richard.potter@raddsolutions.co.uk   \n",
       "2                   richard.potter@raddsolutions.co.uk   \n",
       "3                   richard.potter@raddsolutions.co.uk   \n",
       "4                   richard.potter@raddsolutions.co.uk   \n",
       "..                                                 ...   \n",
       "949                   richie.wynne@raddsolutions.co.uk   \n",
       "950  Richard Potter <richard.potter@raddsolutions.c...   \n",
       "951  Richard Potter <richard.potter@raddsolutions.c...   \n",
       "952                 richard.potter@raddsolutions.co.uk   \n",
       "953                 richard.potter@raddsolutions.co.uk   \n",
       "\n",
       "                                date  \\\n",
       "0    Wed, 19 Oct 2022 20:31:34 +0100   \n",
       "1    Tue, 01 Nov 2022 11:01:50 +0000   \n",
       "2    Tue, 15 Nov 2022 06:01:17 +0000   \n",
       "3    Wed, 09 Nov 2022 17:05:09 +0000   \n",
       "4    Tue, 11 Oct 2022 06:02:01 +0100   \n",
       "..                               ...   \n",
       "949  Mon, 21 Nov 2022 11:05:15 +0000   \n",
       "950  Tue, 13 Dec 2022 15:31:28 +0000   \n",
       "951  Tue, 17 Jan 2023 12:01:49 +0000   \n",
       "952  Thu, 29 Dec 2022 10:00:02 +0000   \n",
       "953  Wed, 01 Feb 2023 09:04:42 +0000   \n",
       "\n",
       "                                                  body  Body_Length  \\\n",
       "0                                                  ...         6232   \n",
       "1    Webinar with demos of Windows 365 and vision f...         2943   \n",
       "2     <https://click.comm.delltechnologies.com/open...         4498   \n",
       "3    Episode 1 airs December 13, 2022 \\r\\nHaving tr...         3476   \n",
       "4     <https://www.google.com/images/branding/googl...         5559   \n",
       "..                                                 ...          ...   \n",
       "949  You’re not going to want to miss these savings...         3228   \n",
       "950   <https://ironsoftware.lt.acemlnb.com/Prod/lin...         8587   \n",
       "951  ZYM helps business owners understand their mar...         6966   \n",
       "952  New videos coming in 2023 … made to empower yo...         3404   \n",
       "953   <https://trk.ecomm.lenovo.com/ss/o/k8leYzsS8M...         8108   \n",
       "\n",
       "     Subject_Length                                       Cleaned_Body  \\\n",
       "0                40                                                ...   \n",
       "1                90  webinar with demos of windows 365 and vision f...   \n",
       "2                64   \\n   \\t\\r\\nview online \\n\\n  \\t\\r\\n \\t\\r\\nwhy...   \n",
       "3                46  episode 1 airs december 13 2022 \\r\\nhaving tro...   \n",
       "4                68    \\n \\r\\nuniversal analytics will no longer pr...   \n",
       "..              ...                                                ...   \n",
       "949              36  youre not going to want to miss these savings ...   \n",
       "950              55    \\t \\r\\n \\t \\r\\nhi richard\\r\\nimagine spendin...   \n",
       "951              56  zym helps business owners understand their mar...   \n",
       "952              25  new videos coming in 2023  made to empower you...   \n",
       "953              26   \\n \\t\\r\\n\\tview it in browser instead \\n  fre...   \n",
       "\n",
       "                                       Cleaned_Subject  \\\n",
       "0             Thank you for attending Microsoft Ignite   \n",
       "1    Invitation to learn how Windows 365 Cloud PCs ...   \n",
       "2    Dont fall behind  embrace AI with Dell Technol...   \n",
       "3       Microsoft Envision Season 3 begins December 13   \n",
       "4    In September you had 71 users visit your websi...   \n",
       "..                                                 ...   \n",
       "949                Our Black Friday offers have landed   \n",
       "950  Jeff Fritz reviews IronPDF vs SyncFusion iText...   \n",
       "951  Dont Miss Out Help to Grow Digital Ends in 16 ...   \n",
       "952                          New videos coming in 2023   \n",
       "953                          Business PCs up to 40 off   \n",
       "\n",
       "                                       BERT_Embeddings  ...  \\\n",
       "0    [-0.059376951307058334, 0.17135855555534363, 0...  ...   \n",
       "1    [-0.1439182013273239, 0.22149936854839325, 0.6...  ...   \n",
       "2    [-0.15142026543617249, 0.11411778628826141, 0....  ...   \n",
       "3    [-0.36103835701942444, 0.06514844298362732, 0....  ...   \n",
       "4    [-0.10645909607410431, 0.17022578418254852, 0....  ...   \n",
       "..                                                 ...  ...   \n",
       "949  [0.09008561074733734, 0.16759826242923737, 0.6...  ...   \n",
       "950  [-0.20470425486564636, 0.20281416177749634, 0....  ...   \n",
       "951  [0.0035861318465322256, 0.07786554843187332, 0...  ...   \n",
       "952  [-0.08648061007261276, 0.11852650344371796, 0....  ...   \n",
       "953  [-0.09954569488763809, 0.13386352360248566, 0....  ...   \n",
       "\n",
       "                Category                                      Cleaned_mails  \\\n",
       "0           Cyber Alerts  microsoft ignite may be over but heres your ch...   \n",
       "1    Social Media Alerts  webinar with demos of windows 365 and vision f...   \n",
       "2    Social Media Alerts  why ai and why now we are witnessing and livin...   \n",
       "3    Social Media Alerts  episode 1 airs december 13 2022 having trouble...   \n",
       "4    Social Media Alerts  universal analytics will no longer process new...   \n",
       "..                   ...                                                ...   \n",
       "949       Service Alerts  youre not going to want to miss these savings ...   \n",
       "950       Service Alerts  imagine spending a lot of money on software li...   \n",
       "951       Service Alerts  zym helps business owners understand their mar...   \n",
       "952       Service Alerts  new videos coming in 2023 made to empower your...   \n",
       "953       Service Alerts  view it in browser instead free shipping on al...   \n",
       "\n",
       "                                      summary_TXTRNK_1  \\\n",
       "0    microsoft ignite may be over but heres your ch...   \n",
       "1                                           No Summary   \n",
       "2    we are witnessing and living through the rise ...   \n",
       "3    episode 1 airs december 13 2022\\nregister now ...   \n",
       "4    81 35 bounce rate\\nbreakdown of visitors acqui...   \n",
       "..                                                 ...   \n",
       "949  use promo code bf25\\nuse promo code bf25\\nunit...   \n",
       "950  would your project fail hopefully not but it n...   \n",
       "951  you may qualify for the grant meaning you coul...   \n",
       "952  our new videos coming in 2023 are made to empo...   \n",
       "953  workspace refresh with up to 40 discount until...   \n",
       "\n",
       "                                               Summary  \\\n",
       "0    microsoft ignite may be over but heres your ch...   \n",
       "1                                                        \n",
       "2    why ai and why now\\nwe are witnessing and livi...   \n",
       "3    episode 1 airs december 13 2022\\nregister now ...   \n",
       "4    81 35 bounce rate\\nbreakdown of visitors acqui...   \n",
       "..                                                 ...   \n",
       "949  were starting our black friday offers early wi...   \n",
       "950  imagine spending a lot of money on software li...   \n",
       "951  even better you can try zym for free for 14 da...   \n",
       "952  our new videos coming in 2023 are made to empo...   \n",
       "953  thinkpad p1 gen 4\\nthinkpad z13 gen 1\\nideapad...   \n",
       "\n",
       "                                          summary_BART index_number  \\\n",
       "0    Learn how microsoft empowers organisations to ...         1543   \n",
       "1    webinar with demos of windows 365 and vision f...          765   \n",
       "2    Artificial intelligence ai market is forecast ...          369   \n",
       "3    register now for microsoft envision season 3. ...          895   \n",
       "4    universal analytics will no longer process new...          740   \n",
       "..                                                 ...          ...   \n",
       "949  were starting our black friday offers early wi...         1130   \n",
       "950  iron software is a free open source solution t...          783   \n",
       "951  zym helps business owners understand their mar...          345   \n",
       "952  new videos coming in 2023  made to empower you...          958   \n",
       "953  Free shipping on all orders with up to 40% dis...          132   \n",
       "\n",
       "                                       Tokenized_Email  \\\n",
       "0    {'input_ids': tensor([[  101,  7513, 16270,  4...   \n",
       "1    {'input_ids': tensor([[  101,  4773,  3981,  2...   \n",
       "2    {'input_ids': tensor([[  101,  3193,  3784,  2...   \n",
       "3    {'input_ids': tensor([[  101,  2792,  1015, 14...   \n",
       "4    {'input_ids': tensor([[  101,  5415, 25095,  2...   \n",
       "..                                                 ...   \n",
       "949  {'input_ids': tensor([[  101,  2115,  2063,  2...   \n",
       "950  {'input_ids': tensor([[  101,  7632,  2957,  5...   \n",
       "951  {'input_ids': tensor([[  101,  1062, 24335,  7...   \n",
       "952  {'input_ids': tensor([[  101,  2047,  6876,  2...   \n",
       "953  {'input_ids': tensor([[  101,  3193,  2009,  1...   \n",
       "\n",
       "                                              Entities Cluster_retrieved  \\\n",
       "0    [('microsoft', 'ORG'), ('2022', 'DATE'), ('mic...                 0   \n",
       "1    [('thursday 17th', 'DATE'), ('2022 1400  1500'...                 0   \n",
       "2    [('500 billion', 'MONEY'), ('20231', 'DATE'), ...                 0   \n",
       "3    [('1', 'CARDINAL'), ('13 2022', 'DATE'), ('mic...                 0   \n",
       "4    [('2023', 'DATE'), ('4', 'CARDINAL'), ('septem...                 0   \n",
       "..                                                 ...               ...   \n",
       "949  [('friday', 'DATE'), ('25', 'CARDINAL'), ('25'...                 7   \n",
       "950  [('jeff fritz', 'PERSON'), ('net conf', 'ORG')...                 7   \n",
       "951  [('uk', 'GPE'), ('up to 5000', 'CARDINAL'), ('...                 7   \n",
       "952  [('2023', 'DATE'), ('2023', 'DATE'), ('10815',...                 7   \n",
       "953  [('up to 40', 'CARDINAL'), ('up to 40', 'CARDI...                 7   \n",
       "\n",
       "                                         Summary_human  \n",
       "0    The email discusses post-Microsoft Ignite 2022...  \n",
       "1    Webinar Announcement: \"Windows 365 for Your Hy...  \n",
       "2    The email discusses the rapid growth of artifi...  \n",
       "3    The email announces the premiere of Microsoft ...  \n",
       "4    Starting in 2023, Universal Analytics will no ...  \n",
       "..                                                 ...  \n",
       "949  The email advertises an early Black Friday off...  \n",
       "950  Jeff Fritz from .NET Conf reviewed IronPDF aga...  \n",
       "951  Zym aids business owners in comprehending mark...  \n",
       "952  Summary:\\n\\nClear Measure has announced an upc...  \n",
       "953  Email from Lenovo announces a workspace refres...  \n",
       "\n",
       "[954 rows x 21 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9fddd0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2994a99474084954bff9b8c11d8199e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\764883\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb5323ee3af7414ba0ec6cf9d6fab3c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87fd09a4d1db49c9b99cb5ae4b3fc1d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\models\\t5\\tokenization_t5.py:240: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from transformers import T5Tokenizer\n",
    "from dateutil.parser import parse\n",
    "\n",
    "def convert_html_to_text(html):\n",
    "    # Use BeautifulSoup to parse HTML and extract text\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    text = soup.get_text(separator=\" \")\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "def standardize_date_time(text):\n",
    "    \n",
    "    # Function to standardize dates\n",
    "    def replace_date(match):\n",
    "        try:\n",
    "            date = parse(match.group(), fuzzy=True)\n",
    "            return date.strftime(\"%Y-%m-%d\")\n",
    "        except ValueError:\n",
    "            # Return the original string if parsing fails\n",
    "            return match.group()\n",
    "\n",
    "    # Function to standardize times\n",
    "    def replace_time(match):\n",
    "        try:\n",
    "            time = parse(match.group(), fuzzy=True)\n",
    "            return time.strftime(\"%H:%M\")\n",
    "        except ValueError:\n",
    "            # Return the original string if parsing fails\n",
    "            return match.group()\n",
    "\n",
    "    # Standardize dates\n",
    "    text = re.sub(r'\\b\\d{1,2}[-/]\\d{1,2}[-/]\\d{2,4}\\b', replace_date, text)\n",
    "    # Standardize times\n",
    "    text = re.sub(r'\\b\\d{1,2}:\\d{2}\\b', replace_time, text)\n",
    "\n",
    "    return text\n",
    "\n",
    "def preprocess_email_for_t5(email_body, max_length=512):\n",
    "\n",
    "    # Convert HTML to text\n",
    "    email_body = convert_html_to_text(email_body)\n",
    "    # Initialize the T5 tokenizer\n",
    "    tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
    "    \n",
    "    # Remove extra whitespaces\n",
    "    email_body = ' '.join(email_body.split())\n",
    "    # Remove common greeting texts\n",
    "    greetings_pattern = r\"Hi\\s\\w+|Hello\\s\\w+|Hope\\syou\\sare\\sdoing\\swell|How\\sare\\syou\\sdoing|How\\sis\\sit\\sgoing|Dear\\s\\w+|Greetings|Good\\s[Mm]orning|Good\\s[Aa]fternoon|Good\\s[Ee]vening|Hey\\s\\w+|To\\swhom\\sit\\smay\\sconcern\"\n",
    "    email_body = re.sub(greetings_pattern, \"\", email_body, flags=re.IGNORECASE)\n",
    "    \n",
    "    # Remove common sign-offs\n",
    "    signoffs_pattern = r\"Best\\sregards|Best\\s\\w+|Sincerely|Yours\\sfaithfully|Yours\\ssincerely|Regards|Warm\\sregards|Kind\\sregards|Cheers|Thanks\\sand\\sregards|Thank\\syou|Take\\scare|Looking\\sforward\"\n",
    "    email_body = re.sub(signoffs_pattern, \"\", email_body, flags=re.IGNORECASE)\n",
    "    # Standardize dates and times\n",
    "    email_body = standardize_date_time(email_body)\n",
    "\n",
    "    # Remove email signatures and disclaimers\n",
    "    email_body = re.sub(r\"--\\s*[\\s\\S]*$\", \"\", email_body)\n",
    "    \n",
    "    # Standardize email addresses and URLs\n",
    "    email_body = re.sub(r\"\\S+@\\S+\\.\\S+\", \"<email>\", email_body)\n",
    "    email_body = re.sub(r\"http\\S+\", \"<url>\", email_body)\n",
    "\n",
    "    # Add task-specific prefix\n",
    "    email_body = \"summarize: \" + email_body\n",
    "\n",
    "    # Tokenize and truncate if necessary\n",
    "    tokens = tokenizer.encode(email_body, add_special_tokens=True, truncation=True, max_length=max_length)\n",
    "\n",
    "    # Decode tokens back to string\n",
    "    preprocessed_email = tokenizer.decode(tokens, skip_special_tokens=True)\n",
    "\n",
    "    return preprocessed_email\n",
    "\n",
    "df['Preprocessed_Body'] = df['body'].apply(lambda x: preprocess_email_for_t5(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbfea7cf",
   "metadata": {},
   "source": [
    "Considerations for target summaries ,Preserve Meaningful Special Characters:\n",
    "\n",
    "Characters like the dollar sign ('$') can be crucial in conveying the correct meaning, especially in contexts involving financial information.\n",
    "Only remove special characters if they are irrelevant or if they are known to cause issues with the model.\n",
    "Maintain Sentence Structure:\n",
    "\n",
    "Proper sentence structure, including punctuation, is important for the model to learn how to generate coherent and grammatically correct summaries.\n",
    "T5 is trained on a diverse corpus that includes proper sentence structures, so maintaining these in your target summaries is beneficial.\n",
    "Consistency with Input Data:\n",
    "\n",
    "The style and format of the target summaries should be consistent with the preprocessed input data.\n",
    "If you're standardizing certain elements in the input (like replacing email addresses with a placeholder), consider whether similar standardizations are necessary in the target summaries.\n",
    "Tokenization:\n",
    "\n",
    "Use the same T5 tokenizer for the target summaries as used for the input data.\n",
    "This ensures consistency in how the model interprets and generates text.\n",
    "Handling Length:\n",
    "\n",
    "Ensure that the length of the target summaries is within the model's capabilities.\n",
    "For T5, this typically means ensuring the summaries are not excessively long."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4347166d",
   "metadata": {},
   "source": [
    "## Target summaries preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba18557",
   "metadata": {},
   "source": [
    "Standardize Dates and Times: If dates and times are present in the summaries and need to be standardized, the standardize_date_time function (defined earlier) can be used.\n",
    "URL and Website Address Removal: If URLs or website addresses are not relevant to the summary content, they can be replaced with a <url> placeholder. This step can be omitted if these details are important for the summary.\n",
    "Tokenization and Truncation: The T5 tokenizer is used to encode the summary, ensuring it fits within the model's token limit. The max_length parameter can be adjusted based on the T5 model variant you are using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a991938",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_summary_for_t5(summary, max_length=512):\n",
    "    # Initialize the T5 tokenizer\n",
    "    tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
    "\n",
    "    # Standardize dates and times (if necessary)\n",
    "    summary = standardize_date_time(summary)\n",
    "\n",
    "    # Remove URLs and website addresses (if they are not relevant to the summary)\n",
    "    url_pattern = r\"http\\S+|www\\.\\S+\"\n",
    "    summary = re.sub(url_pattern, \"<url>\", summary)\n",
    "\n",
    "    # Tokenize and truncate if necessary\n",
    "    tokens = tokenizer.encode(summary, add_special_tokens=True, truncation=True, max_length=max_length)\n",
    "\n",
    "    # Decode tokens back to string\n",
    "    preprocessed_summary = tokenizer.decode(tokens, skip_special_tokens=True)\n",
    "\n",
    "    return preprocessed_summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788b144b",
   "metadata": {},
   "source": [
    "df= pd.read_json('sum_anno_human_proper.json', lines= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "880e6509",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\models\\t5\\tokenization_t5.py:240: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# Apply the preprocessing function to the 'Summary_human' column\n",
    "df['Preprocessed_Summary'] = df['Summary_human'].apply(lambda x: preprocess_summary_for_t5(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a51824",
   "metadata": {},
   "source": [
    "## Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53fccc0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\models\\t5\\tokenization_t5.py:240: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer, Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Define a custom dataset for the T5 model\n",
    "class EmailSummarizationDataset(Dataset):\n",
    "    def __init__(self, tokenizer, data, max_length=512):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.input_texts = data['Preprocessed_Body']\n",
    "        self.target_texts = data['Preprocessed_Summary']\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        source = self.tokenizer.encode_plus(\n",
    "            self.input_texts.iloc[idx], \n",
    "            max_length=self.max_length, \n",
    "            padding='max_length', \n",
    "            truncation=True, \n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        target = self.tokenizer.encode_plus(\n",
    "            self.target_texts.iloc[idx], \n",
    "            max_length=self.max_length, \n",
    "            padding='max_length', \n",
    "            truncation=True, \n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': source['input_ids'].flatten(),\n",
    "            'attention_mask': source['attention_mask'].flatten(),\n",
    "            'labels': target['input_ids'].flatten()\n",
    "        }\n",
    "\n",
    "# Initialize the tokenizer and model\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-base')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "578d5d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_df, test_df = train_test_split(df, test_size=0.2)\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "# Prepare the training and testing datasets\n",
    "train_dataset = EmailSummarizationDataset(tokenizer, train_df)\n",
    "test_dataset = EmailSummarizationDataset(tokenizer, test_df)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 8  # You can adjust this based on your GPU's memory\n",
    "\n",
    "# Create DataLoaders for training and testing datasets\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "2c10f36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## metrics for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "2028c2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback\n",
    "import numpy as np\n",
    "from rouge_score import rouge_scorer, scoring\n",
    "from bert_score import score\n",
    "import torch\n",
    "\n",
    "class CustomEvaluationCallback(TrainerCallback):\n",
    "    def __init__(self, eval_dataset, tokenizer, device):\n",
    "        self.eval_dataset = test_dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "\n",
    "    def on_evaluate(self, args, state, control, **kwargs):\n",
    "        model = kwargs[\"model\"]\n",
    "        predictions, references = [], []\n",
    "\n",
    "        for batch in self.eval_dataset:\n",
    "            inputs = batch['input_ids'].to(self.device)\n",
    "#             attention_mask = batch.get('attention_mask', None).to(self.device) if 'attention_mask' in batch else None\n",
    "            if 'attention_mask' in batch:\n",
    "                attention_mask = batch['attention_mask'].to(self.device)\n",
    "            else:\n",
    "                attention_mask = None\n",
    "\n",
    "            if len(inputs.shape) == 1:\n",
    "                inputs = inputs.unsqueeze(0)\n",
    "            if attention_mask is not None and len(attention_mask.shape) == 1:\n",
    "                attention_mask = attention_mask.unsqueeze(0)\n",
    "\n",
    "            outputs = model.generate(inputs, attention_mask=attention_mask)\n",
    "            batch_predictions = [self.tokenizer.decode(o, skip_special_tokens=True) for o in outputs]\n",
    "\n",
    "            for i in range(len(batch_predictions)):\n",
    "                predictions.append(batch_predictions[i])\n",
    "                references.append(self.tokenizer.decode(batch['labels'][i], skip_special_tokens=True))\n",
    "\n",
    "        # Compute ROUGE and BERTScore\n",
    "        rouge_scores = self.compute_rouge(references, predictions)\n",
    "        bert_scores = self.compute_bertScore(references, predictions)\n",
    "        \n",
    "\n",
    "        # Print formatted scores\n",
    "        self.print_formatted_scores(rouge_scores, bert_scores, state.epoch)\n",
    "\n",
    "        return control\n",
    "\n",
    "    def compute_rouge(self, targets, predictions, score_keys=None, use_stemmer=True):\n",
    "        if score_keys is None:\n",
    "            score_keys = [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]\n",
    "        scorer = rouge_scorer.RougeScorer(score_keys, use_stemmer=use_stemmer)\n",
    "        aggregator = scoring.BootstrapAggregator()\n",
    "\n",
    "        for target, prediction in zip(targets, predictions):\n",
    "            aggregator.add_scores(scorer.score(target=target, prediction=prediction))\n",
    "        result = aggregator.aggregate()\n",
    "\n",
    "        return {key: result[key].mid.fmeasure * 100 for key in score_keys}\n",
    "\n",
    "    def compute_bertScore(self, refs, cands, rescale_with_baseline=True):\n",
    "        P, R, F1 = score(cands, refs, lang=\"en\", rescale_with_baseline=rescale_with_baseline)\n",
    "        return {\"bertScore\": F1.mean().item() * 100}\n",
    "\n",
    "    def print_formatted_scores(self, rouge_scores, bert_scores, epoch):\n",
    "    # Headers for the scores\n",
    "        headers = [\"Epoch\"] + list(rouge_scores.keys()) + [\"bertScore\"]\n",
    "    \n",
    "    # Values for the scores\n",
    "        values = [f\"{epoch}\"] + [f\"{score:.2f}\" for score in rouge_scores.values()] + [f\"{bert_scores['bertScore']:.2f}\"]\n",
    "\n",
    "    # Calculate the max width for each column\n",
    "        column_widths = [max(len(header), len(value)) + 2 for header, value in zip(headers, values)]\n",
    "\n",
    "    # Print the header\n",
    "        header_row = \" | \".join(f\"{header:<{column_widths[idx]}}\" for idx, header in enumerate(headers))\n",
    "        print(header_row)\n",
    "        print(\"-\" * len(header_row))\n",
    "\n",
    "    # Print the scores\n",
    "        score_row = \" | \".join(f\"{value:<{column_widths[idx]}}\" for idx, value in enumerate(values))\n",
    "        print(score_row)\n",
    "        print(\"\\n\" + \"-\" * len(header_row))\n",
    "\n",
    "# Usage\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "custom_eval_callback = CustomEvaluationCallback(test_dataset, tokenizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "0a082489",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 768)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = T5ForConditionalGeneration.from_pretrained('t5-base')\n",
    "model.to(device) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "fbd30ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to('cuda')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "836b16bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([21603,    10,    96,  8313,   485,    13,  3455,   195,   121,  8496,\n",
      "         2270,   335,  1762,   460,  2773,  8747,   134,  3455,   195,  7914,\n",
      "         2488, 13591,     7,    16,  4639,  2505,    38, 12025,     3,  9951,\n",
      "           12,   416,   593,  1270,  8747,   134,  3001,     3, 12860,     3,\n",
      "         3995,    89, 19208,  9918,     8,   167,    13,     8,   296,    18,\n",
      "         4057,  2465,    44,     8,   636,    13,  3455,   195,     6,     8,\n",
      "          372,   808,   294,    16,     3,     9,   381,    13,  4639,  3830,\n",
      "           48,   471,    11,  9137,  3931,   233, 17016,    38, 26213, 14626,\n",
      "            7,     3,     9,   102,  2700,     7, 24612,     7,  4004,    40,\n",
      "           32,    15,  2018,  4102,     7,    11, 10457,     9, 30368,     3,\n",
      "           18, 31136,     3,   184,     3, 16627,   302, 31136,     3,   184,\n",
      "            3, 16627,   302,   621, 20220,    53,    45,     8,   636,    13,\n",
      "         3455,   195,    28,     3,     9,  1952,    16,  2392, 11366,     6,\n",
      "        10457,     9,  3311, 14626,     7,    16,  1233,    38,     3,     9,\n",
      "         1020,    11,  5856,  3856, 13856,   233, 17016,    38, 26213, 29169,\n",
      "         7977,  5085,  2700,     7,  2003,  8540,  7815,   106,    38,  2146,\n",
      "         3440,     6, 10107,  3003,    17,  1950,     3,   184,  3611,  5869,\n",
      "         2825,  1433,  1769,  1185,    15,   216,    19,     3,     9,  5213,\n",
      "           13,    37,   636,    13,  3455,   195,     6,  1270,     5,   105,\n",
      "         1326,    33, 10693,    12,  2222,  2003,    12, 29169, 10358,    15,\n",
      "           11, 11531,   112,     3, 18720,  1252,  2980,    11,   233, 17016,\n",
      "           38, 26213,   451,  4728,    16,   333,    28,     8,     3, 16547,\n",
      "          222,    63,    13, 23257,    15,     7,   318,   232,  2130,   149,\n",
      "         2056,   217,     7,   135,   868, 26588,    96,   196,    17,  2130,\n",
      "           69,  1418,    12,  4521, 23257,  2056,   976,   845, 23257,  2152,\n",
      "          343, 20570,  6060,    44,     8,   636,    13,  3455,   195,     6,\n",
      "           16,  2789,     6,   113,    47,     3,     9,   233, 17016,    38,\n",
      "        26213,  1610,    72,   772,  1820, 11664,    48,  5685,   148,    43,\n",
      "         1204,    48,   791,   250,    25,    43, 10006,    26,    12,  1163,\n",
      "        23951,     7,     5,   597,  7304, 19308,  1820,  4197,    66,    39,\n",
      "         5685,     7, 24083,    48,  5685,    38, 14348,  3305,  9384, 23485,\n",
      "            1,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), 'labels': tensor([ 8496,  2270,    10,  3455,   195,  7914,    31,     7,  2488, 13591,\n",
      "            7,    16,  4639,  2505,    38, 12025,    19,     3,  9951,    12,\n",
      "          416,   593,     5,    37,   372, 11411,   296,    18,  4057,  2465,\n",
      "           44,     8,   636,    13,  3455,   195,    21,  4639,  3830,     5,\n",
      "           86,   119,  1506,     6, 10457,     9, 30368,     6,     3,     9,\n",
      "          636,    13,  3455,   195,  5213,     6,  3311, 14626,     7,   264,\n",
      "        11192,   127,     7,     5, 29169, 10358,    15,  7817,  2003,  8540,\n",
      "         7815,   106,    38,  2146,  3440,     6,     3, 26072,   112,  1252,\n",
      "         2980,     5, 20570,  6060,    44,     8,   636,    13,  3455,   195,\n",
      "         2130, 23257,  2056,  1901,     5,   465,  1041,  1173,    16,    48,\n",
      "          791,     5,     1,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0])}\n",
      "{'input_ids': tensor([21603,    10,  7847,     8,  8562,   733,    13,     8,  4821, 21892,\n",
      "           15,    11,     8,  6524,    13,   878,  5515,    63,  4197,    16,\n",
      "         3509,  1820, 15151,     7,  1443,  1068,  3365,     5, 19539,    15,\n",
      "           26,  1929,   441,   220,  6832,   268,   477,   227,  3365,     5,\n",
      "         1610, 17652,    21,  1151,  1030,     5,  2149,   419,  1169,    60,\n",
      "         4128,    10,  2803,  3417,     7,     3,   184,  1424,  2311,     3,\n",
      "            9, 19276,  1396,  2135,    11,     3,     9,  2803,   905,     5,\n",
      "           37,  2803,  3417,     7,     3,   184,  1424,  1120,    19,  1083,\n",
      "          347,   163,    21,  1758, 11704,     5,     4,   201, 14480,   555,\n",
      "            6, 14480,  9181,    11, 14480,     3,     4,    87,   134,     5,\n",
      "          242,  1758,  1105,    41,     4,   345,     6, 16695,     6,  4306,\n",
      "            4,     6,  4848,     4,     6,  5477,     4,   201,     8,  2803,\n",
      "         4493,  2311,    10,  2803, 11690,     6, 19570,   507,  1220,    42,\n",
      "        10780,   997,  1220,     5,   242,  2143,    77,   235,     7,   107,\n",
      "         1105,  1180,  1758,  5477,   948,     5,   927,  1220,  5869,    18,\n",
      "          390,     6,     8,  2803,  4493,  2311,    10, 20082,     3, 22642,\n",
      "         1220,    11,  5642,  2242,   305,  1220,     5,   242,    72,   313,\n",
      "         4742,  2219,    11,  9076,     6,  1214,   270,     5,   597,  7304,\n",
      "        19308,  1820, 17865, 16836,  2803,  6708,   555,  2803,  5994,  1624,\n",
      "         6764,     6,  7896,   668,  2079,  5373,     1,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), 'labels': tensor([   37,   791, 17212,     7,     8,  4821, 21892,    15,    11,     8,\n",
      "         6524,    13,   878,  5515,    63,   733,    11,  2652,     7,   339,\n",
      "         1068,  3365,    28,     3,     9,  1929, 18775,    13,  6862,   268,\n",
      "          477,     5,    94,  8285,   358,  1502,    21,   338,  2803,  3417,\n",
      "            7,     3,   184,  1424,     6,   379,  1396,  2135,    11,  7441,\n",
      "         1904,    11,  3509,     7,     5,    94,  1223,     7,  1105,    12,\n",
      "            8, 17652,    21,  1151,  1030,    11,   313,  4742,  2219,     5,\n",
      "          465, 10839,  1052,    42,  1041,  2118,    47, 21119,  4568,     5,\n",
      "            1,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0])}\n",
      "{'input_ids': tensor([21603,    10,   148,    33, 10006,    26,    12, 14183, 19361, 17228,\n",
      "          725,    21, 14183, 19361,    11, 24076,  3684,  7038,     5,   100,\n",
      "          251,    65,  1310,   118,  3250,     6,    11,    19,   230,   347,\n",
      "            5,   205, 18161, 13048,     7, 12714,    53,   421,  9130,    10,\n",
      "         5793,    53,    12, 12165, 11010,   480,   104,  2122,  2371,    45,\n",
      "        14183, 19361, 30980,     7,   460,  2773, 14772, 14962, 14146,    10,\n",
      "         2517,  5422,     3,  6038,  8465,  1576,   833,    10,  1762, 14320,\n",
      "          460,  2773,  1960,     6,   205, 18161,  1883, 12714,    53,   421,\n",
      "         9130,    10,  5793,    53,    12, 12165, 11010,   480,   104,  2122,\n",
      "         2371,    45, 14183, 19361, 30980,     7,     5,    37,   934,   795,\n",
      "         5719,    11,  1438,    12,   199,   480,  5947,  2061,    11,   496,\n",
      "        14126,  1115,   358,   447, 28684,  1020,     5,    94,    92,   795,\n",
      "         6574,   139,     8,   750,  5888,  3283,   806,    12,     8,   480,\n",
      "         5947,   573,    11,   704,   650,  2245,   496,  2440,    54,   240,\n",
      "           12,  8726,    70, 28684,  2231,     5,    37,   934,    22,     7,\n",
      "         7469,   538,    24,   480,  5947,  2371,   174,  1438,     6, 16538,\n",
      "           11,  1884,   155,  1707,    12,  3762,  1428,    70, 28684,  1020,\n",
      "            5,   304,  1115,   175,   807,     6,   205, 18161,   795,   386,\n",
      "         5719,    16,     8,   934,    12,   199,   480,  5947,  2440,   918,\n",
      "            6,  4368,     6,    11,  1961, 24139, 28684,  1356,    10,  1300,\n",
      "            3, 13898,    16,     8,   167,  1113,  1329,  1034,  3629,    11,\n",
      "          918,  2957,     3,     9, 10281, 28684,   515,     5,  1682,  7136,\n",
      "        12905,   776,    11,  8609,  1115,  3487, 17765,     5,  1877, 11561,\n",
      "           30,  3561,    11,   251,    18, 22473,     5,  8529,    28,     8,\n",
      "          934,     6,    62,    33,  1260,    46,   367,  1464,  9229,    84,\n",
      "         7901,     7,  1438,    11,  1397,    12,   284,    13,   205, 18161,\n",
      "           22,     7,   386,  5719,   590,    28,  4864,    30,   149, 10588,\n",
      "           54,  4028,   284, 10919,     3,   390,    30,    70,   750,   523,\n",
      "            5,   304,   608,     8,   423,   934,    11,    12,   592,     8,\n",
      "         1464,  9229,     6,   719, 12714,    53,   421,  9130,    10,  5793,\n",
      "           53,    12, 12165, 11010,   480,   104,  2122,  2371,    45, 14183,\n",
      "        19361, 30980,     7,     5,   100,   556,    19,   937,  1426,    12,\n",
      "           48,   933,  2420,    11,    48, 17865,     3,   184,  2048,  1291,\n",
      "            5,     3,  3985,  3169,  7426,    48,  1569,    58,  4197,    34,\n",
      "           38,     3,     9, 17652,     5,   148,    33, 10006,    26,    12,\n",
      "         3864,    45,     8, 14183, 19361,    11, 24076,  3684,  7038,    41,\n",
      "        21134,   188,    61, 19607,  3325, 11830,     7,  1820, 17865,  7587,\n",
      "         1820,  5090,  7878,    28,   205, 18161,    10,  1376,  1820,  3046,\n",
      "         1820,  4601,  1820, 14225,  1820,  5343,     3,   834,   834,   834,\n",
      "          834, 22319, 22319, 22319, 22319,   100,   791,    47,  1622,    12,\n",
      "          791,  3155,   338, 17416,  2962,  7591,   651, 11538,  5713,     6,\n",
      "           30,  6089,    13,    10, 14183, 19361,    11, 24076,  3684,  7038,\n",
      "          489,  4560,  1003,   189,   472,     6,  9254,   314,  2313, 12154,\n",
      "            6,  2847,  2775, 19818,     1,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), 'labels': tensor([14183, 19361,    11, 24076,  3684,  7038,    41, 21134,   188,    61,\n",
      "           65,  1883,     3,     9,   934,     3, 10920,    96,  3174,  5822,\n",
      "         1222,   421,  9130,    10,  5793,    53,    12, 12165, 11010,   480,\n",
      "         5947,  9139,     7,    45, 14183, 19361, 30980,     7,   121,    30,\n",
      "         1762, 14320,   460,  2773,     5,    37,   934,   704,  5719,    12,\n",
      "         3391,     8, 28684,    13,   480,  5947,  2061,    11,   496, 14126,\n",
      "           57,     3, 14198,   358,   447,  5217,     6,  1705,     8,   806,\n",
      "            7,    13, 11262,    12,     8,   480,  5947,   573,     6,    11,\n",
      "         1260,  2245,    21,  2440,    12, 19452,    70, 28684,  2231,     5,\n",
      "         4420,   934,  7469,   560,     8,   174,    21,   480,  5947,  2371,\n",
      "           12,   592,  1438,    11,   992,    30, 16538,    11,  1884,   155,\n",
      "         1707,    12, 21002, 28684,  5217,  3762,     5,   205, 18161,    31,\n",
      "            7,     1,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0])}\n",
      "{'input_ids': tensor([21603,    10,  2847,  5596, 18795,     7,    10,    37, 10825,  2821,\n",
      "           13,  2747,   389,  2264,   257,    11, 23545,   257,   486,  5135,\n",
      "         1422,    16,     5,  9978,    57,  1838,   309,     5, 22119,  2747,\n",
      "        30729,     7,    33,    59,   163,    21,   169,    16,     3, 23380,\n",
      "            5,  9978,   765,  1564,     5,  2372,   686,    13,     5,  9978,\n",
      "          917,    54,   169,   331, 30729,     7,    21,  3982,  1014,   331,\n",
      "            5,    94,   163,  1217,    81,   335,  2356,    13,  1081,    12,\n",
      "          478,  4992,  1427, 16742,   331, 30729,     7,  5223,    12, 10409,\n",
      "         2287,     5,   290,    33,   186,  1192,    18,    77,   331, 30729,\n",
      "            7,  8794,    57,  2803,    24,    54, 16742,    39,   331,  1224,\n",
      "           11,    34,    31,     7,   514,    12,   482,    39,   293,   331,\n",
      "        30729, 12978,    12,  1581,    12,    39, 10409,  2287,     5,   156,\n",
      "           25,    43,   128,   182,   806, 16148,   523,     6,    25,   164,\n",
      "         4028,     8, 10869,     9,  8130,     9,  3869, 17057,  3459,    21,\n",
      "           39, 10409,  2287,     5,   156,    25,    31,    60,  2421,  1249,\n",
      "        25207,  1564,     6,    25,    54,   237,   888,    39,  3505,  4175,\n",
      "          139,  1438,    11, 11610,     8,   564,    13,   273,  1438,    30,\n",
      "          284,    13,    39, 12978,     5,    86,    48,  1108,     6,    25,\n",
      "           31,    60,   352,    12,  2075,   167,    13,     8,   331, 30729,\n",
      "            7,  8794,    57,  2803,     5,   148,    31,    60,   352,    12,\n",
      "         1344,     3,     9,   360,  1653, 16148, 12978,    12,   691,  5128,\n",
      "           11,   206, 17552,  2620,     5,   148,    31,   195,    92,   669,\n",
      "           12,   482,     3,     9,  1653, 16148, 15816,    12,  4048,     8,\n",
      "         2620,   344,   192,   315,  2605,     5,   148,    31,    60,    92,\n",
      "          352,    12,   217,   149,    12,  4028,     8, 10869,     9,  8130,\n",
      "            9,  3869, 17057,  3459,    12,  8000,    72,  6446, 16148, 13911,\n",
      "            5,  4213,     6,    25,    31,   195,   356,    95,     3,     9,\n",
      "         1158,    13,  3487,  2073,    11,   217,   149,   514,    34,    19,\n",
      "           12,   415,  1737,    39,  3505,  4175,     5, 16505, 23545,   257,\n",
      "         7717,     7,    86,     8, 10382,   657,     6,    12, 16742,     8,\n",
      "          331,     3,     9,  1139,  3785,     7,   139,     3,     9,   607,\n",
      "          133,    36,   612,  1461,    16,     8,  1081,    18,   346,  2907,\n",
      "           26,     8,   607,     5,    37,  2016,  4175,   130,  6099,    30,\n",
      "            8,  3785,   607,    12,   817,     8,  1139,   125,     3,    17,\n",
      "         1786,     5,   282,  3735,    18,  9442,  6020,    41,   667,  4652,\n",
      "           61,  1632,     8,  7982,     6,  5564,  2301,    24,  3785,   331,\n",
      "          139,  2605,    13,     3,     9,   853,    11,  2832,     3,     9,\n",
      "        23545,   342,  9960,  1573,    12,  1912,     8, 16148,     5,    71,\n",
      "         1232,    13,  4175,    19,  3666,    45,     8, 23545,   342,  9960,\n",
      "         1573,    11,   273,  4175,   130,  8120,    12,     8,  3785,   607,\n",
      "           12,    36,  6099,     5,  1563,    31,     7,   166,   320,    44,\n",
      "            8,  1435,   194,    13,  3982,  1014,   331,   274,    62,   888,\n",
      "         2400,   338,   331, 30729,     7,     5,  6357,     3,     9, 27513,\n",
      "         8566,   304,  1130,   590,    28,    48,  1108,     6,   539, 10893,\n",
      "         5929,    11,   482,     3,     9,  8990,   917,    28,     8,   564,\n",
      "        12474,     7,     5,   148,   164,   169,   893,     5,  9978,   431,\n",
      "           42,     5,  9978,   489,    21,     8,  5977,    16,    48,  1108,\n",
      "            5,  1377,    13,     8,  1081,    56,   161,   131,    38,   168,\n",
      "           16,  2283,  5204,    13,     5,  9978,   396,     5,  1447,    25,\n",
      "           43,     8,   917,   990,     6,   269,  8429,    18, 12570,    30,\n",
      "            8,   516,    11,   617,     3,     9,     1,     0,     0,     0,\n",
      "            0,     0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 0, 0, 0, 0, 0]), 'labels': tensor([  100,   791,  3792,     8,   169,    13,   331, 30729,     7,    16,\n",
      "            5,  9978,    21,   331, 16148,     6,   379,  1577,  1653, 16148,\n",
      "        12978,    11,     3, 10311,     8, 10869,     9,  8130,     9,  3869,\n",
      "        17057,  3459,     5,    94,   795,     3,     9,  1147,    18,   969,\n",
      "           18,  7910,  1539,    21,  6990,   331, 30729,     7,     6,  2421,\n",
      "         1653, 16148, 12978,     6,     3, 10311,     8, 10869,     9,  8130,\n",
      "            9,  3869, 17057,  3459,     6,    11,   415,  2610,  3505,  4175,\n",
      "            5,    94,    92,   963,  3909,    21,  1577,     3,     9,  8990,\n",
      "          917,    12,  1130,   590,     5,  5433,     6,    34,  2519,     7,\n",
      "            3,     9,  4696,    28,  7523,  8041,  2773,    11,   795,   251,\n",
      "           81,     3,  4685,  7523,  1034,   761,  2996,     5,    37,   791,\n",
      "           92,   963,   931,    21,  5037,  7644, 11177,    11,  3882,    53,\n",
      "           91,    13,   856,  5030,     5,     1,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0])}\n",
      "{'input_ids': tensor([21603,    10, 12489, 13926,    56,   150,  1200,   433,   126,   331,\n",
      "           16,  1068,  2605,  1684,    16,   460,  2773,     5, 16386,   230,\n",
      "           57,  1898,    95,    11, 14569,   147,    12,     3,     9,  1163,\n",
      "        13926,   314,   785,     5,  4117,     6,   270,    31,     7,    39,\n",
      "         1671, 23052,     3, 16137,  3155,   391, 24604, 15805,  3585,     3,\n",
      "        16597,    18, 22011,  4729, 16431, 17234,  1820,  4197,    10,   432,\n",
      "         1620,  3238,  2747,     3, 27640,  2368,  4834,  4959,   180,  5080,\n",
      "        27645,  4083, 14536,  2224,   821, 15905,  7135,     7,     3,  2172,\n",
      "           12,  1767,   847, 13504,     3, 27025, 10630,     5,  2712, 24984,\n",
      "            3, 24274,     3,  6932,   272,  7906, 13002,  4357,  2712,  1902,\n",
      "            5,  6370, 23836,   679,     7,  1938, 20610,     3,  1206,    10,\n",
      "         2688,     3, 19947,     5,  2712,  6674,  6566, 11429,  3035,    13,\n",
      "         2692,  7347,    57,  4245,  7143,  2775,     5,  7561, 15045,  4769,\n",
      "         9264,  5953,   432,  2502, 24984,  2853,  4704,     3, 16772, 19365,\n",
      "          421,   416,    18, 10877,  9753,  1127,     6,  1163, 13926,  6464,\n",
      "           19, 11906, 12489, 13926,     5, 14362,    16,   460,  2773,     6,\n",
      "        12489, 13926,    56,   150,  1200,   433,   331,     5,    86,  4537,\n",
      "            6,    62,  2454,    25,    12,  3615,   147,    12,     3,     9,\n",
      "         1163, 13926,   314,   785,    38,  1116,    38,   487,     5,   304,\n",
      "          199,   921,   143,     8,   888,     6,    62,    31,    60,     3,\n",
      "        14138,   126,  1438,    11,  1339,    12,   199,    25,   129,   708,\n",
      "           28,  1163, 13926,  2853,   242,     3,     9,    72, 10069,   351,\n",
      "            6,    25,    54,   456,     8,  3615,    28,     8,  2821,   413,\n",
      "         9255,    16,    39, 12489, 13926,   785,     5,  3403,    69,  1251,\n",
      "          875,   442,    12,   669,    72,    81,     8,   167,  1100,  1163,\n",
      "        13926,   314,  7298,     7,     5,     3, 20750,     3, 19481, 11430,\n",
      "         1609,  7639,    30,     8,   281,    57, 16891,     8,   339,  1163,\n",
      "        13926,  1120,     5,   460,  2884,  1163,  6516, 24046,   736, 11692,\n",
      "          532,     9,   929,  1061,  1343,     6,  5617,  4197,     6,  3087,\n",
      "          668,  2445,  4906,   100,   791,    47,  1622,    12,   791,  3155,\n",
      "          250,    25,  7972,    24,    25,    31,    26,   114,    12,   911,\n",
      "         3864,    11,  2316,    81,    39,  1163, 13926,   905,     5,   156,\n",
      "           25,   278,    31,    17,   241,    12,   911,   224,  7594,    16,\n",
      "            8,   647,     6,   754,    73,  7304, 19308,   270,    10,     3,\n",
      "        16137,  3155,   148,    54,    92,   483,    39, 11177,    44,    39,\n",
      "          905,    31,     7,  1163, 13926,  8601, 11538,   543,    57,     3,\n",
      "        12578,   139,     3, 16137,  3155,     1,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), 'labels': tensor([   37,   791,  9379,    24, 12489, 13926,    56,   150,  1200,   433,\n",
      "          126,   331,    16,  1068,  2605,    45,   460,  2773,     5,    94,\n",
      "         7786,     7,  1898,    95,    11, 14569,   147,    12,  1163, 13926,\n",
      "          314,   785,     5,    94,   795,     3,     9,  1309,    12,   903,\n",
      "          475,   331,    11,  2454,     7,   338,   126,  1438,    11,  1339,\n",
      "           21,     8,  3508,     5,    37,   791,    92,   963,  3909,    21,\n",
      "           46, 10069,   351,    11,     3,     9,  1309,    12,     3,     9,\n",
      "          875,   442,    21,    72,   251,     5,  5433,     6,    34,   704,\n",
      "         7639,    30,     8,   281,    57, 16891,     8,  1163, 13926,  1120,\n",
      "            5,     1,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0])}\n"
     ]
    }
   ],
   "source": [
    "# Quick check to ensure the dataset is accessible\n",
    "for i in range(min(5, len(train_dataset))):\n",
    "    print(train_dataset[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "2b7e668d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "5a66d03f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(model.parameters()).is_cuda\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "3635a958",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1920' max='1920' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1920/1920 13:22:09, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>9.445800</td>\n",
       "      <td>2.959740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.291800</td>\n",
       "      <td>0.481133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.497800</td>\n",
       "      <td>0.405812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.433500</td>\n",
       "      <td>0.374636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.395100</td>\n",
       "      <td>0.359168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.376600</td>\n",
       "      <td>0.349338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.347800</td>\n",
       "      <td>0.343033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.329000</td>\n",
       "      <td>0.339735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.317500</td>\n",
       "      <td>0.337368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.306500</td>\n",
       "      <td>0.334868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.292600</td>\n",
       "      <td>0.333456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.284800</td>\n",
       "      <td>0.332755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.279100</td>\n",
       "      <td>0.332819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.268900</td>\n",
       "      <td>0.333084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.262500</td>\n",
       "      <td>0.333386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.259100</td>\n",
       "      <td>0.333603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.258700</td>\n",
       "      <td>0.333911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.251100</td>\n",
       "      <td>0.334235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.254100</td>\n",
       "      <td>0.334104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.244200</td>\n",
       "      <td>0.334123</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "1.0     | 3.83     | 0.00     | 3.78     | 3.82        | -32.89     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "2.0     | 0.00     | 0.00     | 0.00     | 0.00        | -492.51    \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "3.0     | 0.00     | 0.00     | 0.00     | 0.00        | -490.02    \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "4.0     | 2.05     | 0.00     | 2.03     | 2.03        | -267.90    \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "5.0     | 4.89     | 0.00     | 4.89     | 4.93        | -42.06     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "6.0     | 5.40     | 0.00     | 5.40     | 5.38        | -29.36     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "7.0     | 5.56     | 0.00     | 5.56     | 5.52        | -29.18     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "8.0     | 6.01     | 0.00     | 6.00     | 6.00        | -28.76     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "9.0     | 6.10     | 0.00     | 6.11     | 6.14        | -28.83     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "10.0    | 5.76     | 0.00     | 5.75     | 5.75        | -28.91     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "11.0    | 5.68     | 0.00     | 5.71     | 5.70        | -28.35     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "12.0    | 5.91     | 0.00     | 5.91     | 5.91        | -28.92     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "13.0    | 5.86     | 0.00     | 5.90     | 5.83        | -29.04     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "14.0    | 5.83     | 0.00     | 5.87     | 5.84        | -28.61     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "15.0    | 5.54     | 0.00     | 5.53     | 5.54        | -28.45     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "16.0    | 5.87     | 0.00     | 5.84     | 5.84        | -28.36     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "17.0    | 5.75     | 0.00     | 5.77     | 5.74        | -28.66     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "18.0    | 5.86     | 0.00     | 5.82     | 5.85        | -28.19     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "19.0    | 5.83     | 0.00     | 5.81     | 5.85        | -28.28     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "20.0    | 5.74     | 0.00     | 5.73     | 5.72        | -28.49     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the T5 model\n",
    "model_name = 't5-base'  \n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "\n",
    "# Calculate the size of the training dataset\n",
    "training_dataset_size = len(train_dataset)\n",
    "\n",
    "N = training_dataset_size\n",
    "batch_size = 8  \n",
    "\n",
    "# Calculate steps per epoch\n",
    "steps_per_epoch = N // batch_size\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",                      # Directory for storing outputs\n",
    "    num_train_epochs=20,                         # Set number of epochs to 10\n",
    "    per_device_train_batch_size=8,               # Batch size for training\n",
    "    per_device_eval_batch_size=8,                # Batch size for evaluation\n",
    "    warmup_steps=500,                            # Number of warmup steps\n",
    "    weight_decay=0.01,                           # Weight decay for regularization\n",
    "    logging_dir=\"./logs\",                        # Directory for storing logs\n",
    "    logging_steps=steps_per_epoch,                            # Log every steps in epoch\n",
    "    do_train=True,                               # Enable training\n",
    "    do_eval=True,                                # Enable evaluation\n",
    "    evaluation_strategy=\"epoch\",                 # Evaluate at the end of each epoch\n",
    "    save_strategy=\"no\",                    # Disable saving the model\n",
    "    save_total_limit=0,                    # Limit on the total amount of checkpoints. Setting to 0 disables it\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,  # Use the dataset instance for training data\n",
    "    eval_dataset=test_dataset,     # Use the dataset instance for validation data\n",
    "    callbacks=[custom_eval_callback],  # Custom callback function for metrics\n",
    "    optimizers=(AdamW(model.parameters(), lr=5e-5), None)  # AdamW optimizer with a specified learning rate\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n",
    "# Plotting\n",
    "training_logs = trainer.state.log_history\n",
    "\n",
    "training_loss = [log['loss'] for log in training_logs if 'loss' in log]\n",
    "validation_loss = [log['eval_loss'] for log in training_logs if 'eval_loss' in log]\n",
    "epochs = range(1, len(training_loss) + 1)\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(epochs, training_loss, 'bo-', label='Training loss')\n",
    "plt.plot(epochs, validation_loss, 'ro-', label='Validation loss')\n",
    "plt.title('Training and Validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5472b853",
   "metadata": {},
   "source": [
    "## T5 base with textrank summaries as input and human annoted summary as target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61e747ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\models\\t5\\tokenization_t5.py:240: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Apply the preprocessing function to the 'body' column\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTxtrnk_Body\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSummary\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_email_for_t5\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Program Files\\Python311\\Lib\\site-packages\\pandas\\core\\series.py:4630\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4520\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4521\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4522\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4525\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4526\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4527\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4528\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4529\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4628\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4629\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4630\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Program Files\\Python311\\Lib\\site-packages\\pandas\\core\\apply.py:1025\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[0;32m   1024\u001b[0m \u001b[38;5;66;03m# self.f is Callable\u001b[39;00m\n\u001b[1;32m-> 1025\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Program Files\\Python311\\Lib\\site-packages\\pandas\\core\\apply.py:1076\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1074\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1075\u001b[0m         values \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;241m.\u001b[39m_values\n\u001b[1;32m-> 1076\u001b[0m         mapped \u001b[38;5;241m=\u001b[39m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1077\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1078\u001b[0m \u001b[43m            \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1079\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1080\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1082\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1083\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1084\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1085\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32mC:\\Program Files\\Python311\\Lib\\site-packages\\pandas\\_libs\\lib.pyx:2834\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[12], line 2\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Apply the preprocessing function to the 'body' column\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTxtrnk_Body\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSummary\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mpreprocess_email_for_t5\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[1;32mIn[8], line 45\u001b[0m, in \u001b[0;36mpreprocess_email_for_t5\u001b[1;34m(email_body, max_length)\u001b[0m\n\u001b[0;32m     43\u001b[0m email_body \u001b[38;5;241m=\u001b[39m convert_html_to_text(email_body)\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# Initialize the T5 tokenizer\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mT5Tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mt5-base\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# Remove extra whitespaces\u001b[39;00m\n\u001b[0;32m     48\u001b[0m email_body \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(email_body\u001b[38;5;241m.\u001b[39msplit())\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\tokenization_utils_base.py:1983\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   1981\u001b[0m             resolved_vocab_files[file_id] \u001b[38;5;241m=\u001b[39m download_url(file_path, proxies\u001b[38;5;241m=\u001b[39mproxies)\n\u001b[0;32m   1982\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1983\u001b[0m         resolved_vocab_files[file_id] \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1984\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1985\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1986\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1987\u001b[0m \u001b[43m            \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1988\u001b[0m \u001b[43m            \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1989\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1990\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1991\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1992\u001b[0m \u001b[43m            \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1993\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1994\u001b[0m \u001b[43m            \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1995\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_raise_exceptions_for_missing_entries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1996\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_raise_exceptions_for_connection_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1997\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1998\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1999\u001b[0m         commit_hash \u001b[38;5;241m=\u001b[39m extract_commit_hash(resolved_vocab_files[file_id], commit_hash)\n\u001b[0;32m   2001\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(unresolved_files) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\utils\\hub.py:430\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    427\u001b[0m user_agent \u001b[38;5;241m=\u001b[39m http_user_agent(user_agent)\n\u001b[0;32m    428\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    429\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[1;32m--> 430\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    431\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    432\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    433\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    434\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    435\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    436\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    437\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    438\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    439\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    440\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    441\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    442\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    443\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    444\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    445\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[0;32m    446\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to access a gated repo.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMake sure to request access at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    447\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and pass a token having permission to this repo either \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    448\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mby logging in with `huggingface-cli login` or by passing `token=<your_token>`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    449\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\huggingface_hub\\utils\\_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[0;32m    116\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\huggingface_hub\\file_download.py:1247\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[1;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout, endpoint)\u001b[0m\n\u001b[0;32m   1245\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1246\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1247\u001b[0m         metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1248\u001b[0m \u001b[43m            \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1249\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1250\u001b[0m \u001b[43m            \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1251\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1252\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1253\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n\u001b[0;32m   1254\u001b[0m         \u001b[38;5;66;03m# Cache the non-existence of the file and raise\u001b[39;00m\n\u001b[0;32m   1255\u001b[0m         commit_hash \u001b[38;5;241m=\u001b[39m http_error\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(HUGGINGFACE_HEADER_X_REPO_COMMIT)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\huggingface_hub\\utils\\_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[0;32m    116\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\huggingface_hub\\file_download.py:1624\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[1;34m(url, token, proxies, timeout)\u001b[0m\n\u001b[0;32m   1621\u001b[0m headers[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccept-Encoding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124midentity\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# prevent any compression => we want to know the real size of the file\u001b[39;00m\n\u001b[0;32m   1623\u001b[0m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[1;32m-> 1624\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1625\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHEAD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1626\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1627\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1628\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1629\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1630\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1631\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1632\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1633\u001b[0m hf_raise_for_status(r)\n\u001b[0;32m   1635\u001b[0m \u001b[38;5;66;03m# Return\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\huggingface_hub\\file_download.py:402\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[1;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[0;32m    400\u001b[0m \u001b[38;5;66;03m# 2. Force relative redirection\u001b[39;00m\n\u001b[0;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[1;32m--> 402\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    403\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    407\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    409\u001b[0m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[0;32m    410\u001b[0m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n\u001b[0;32m    411\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;241m300\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m399\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\huggingface_hub\\file_download.py:425\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[1;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[0;32m    422\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[0;32m    424\u001b[0m \u001b[38;5;66;03m# Perform request and return if status_code is not in the retry list.\u001b[39;00m\n\u001b[1;32m--> 425\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mget_session\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    426\u001b[0m hf_raise_for_status(response)\n\u001b[0;32m    427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mC:\\Program Files\\Python311\\Lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32mC:\\Program Files\\Python311\\Lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\huggingface_hub\\utils\\_http.py:63\u001b[0m, in \u001b[0;36mUniqueRequestIdAdapter.send\u001b[1;34m(self, request, *args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Catch any RequestException to append request id to the error message for debugging.\"\"\"\u001b[39;00m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 63\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mRequestException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     65\u001b[0m     request_id \u001b[38;5;241m=\u001b[39m request\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(X_AMZN_TRACE_ID)\n",
      "File \u001b[1;32mC:\\Program Files\\Python311\\Lib\\site-packages\\requests\\adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    483\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m    485\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 486\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    501\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[1;32mC:\\Program Files\\Python311\\Lib\\site-packages\\urllib3\\connectionpool.py:714\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_proxy(conn)\n\u001b[0;32m    713\u001b[0m \u001b[38;5;66;03m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[1;32m--> 714\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    715\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    716\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    717\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    718\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    719\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    720\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    721\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    722\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    724\u001b[0m \u001b[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[0;32m    725\u001b[0m \u001b[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[0;32m    726\u001b[0m \u001b[38;5;66;03m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[0;32m    727\u001b[0m \u001b[38;5;66;03m# mess.\u001b[39;00m\n\u001b[0;32m    728\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Program Files\\Python311\\Lib\\site-packages\\urllib3\\connectionpool.py:466\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    461\u001b[0m             httplib_response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[0;32m    462\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    463\u001b[0m             \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[0;32m    464\u001b[0m             \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[0;32m    465\u001b[0m             \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[1;32m--> 466\u001b[0m             \u001b[43msix\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_from\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    467\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    468\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[1;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "File \u001b[1;32mC:\\Program Files\\Python311\\Lib\\site-packages\\urllib3\\connectionpool.py:461\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    458\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m    459\u001b[0m     \u001b[38;5;66;03m# Python 3\u001b[39;00m\n\u001b[0;32m    460\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 461\u001b[0m         httplib_response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    462\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    463\u001b[0m         \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[0;32m    464\u001b[0m         \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[0;32m    465\u001b[0m         \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[0;32m    466\u001b[0m         six\u001b[38;5;241m.\u001b[39mraise_from(e, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32mC:\\Program Files\\Python311\\Lib\\http\\client.py:1375\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1373\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1374\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1375\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1376\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[0;32m   1377\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mC:\\Program Files\\Python311\\Lib\\http\\client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    316\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 318\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Program Files\\Python311\\Lib\\http\\client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 279\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mreadline(_MAXLINE \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mC:\\Program Files\\Python311\\Lib\\socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    704\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 706\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    707\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    708\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Program Files\\Python311\\Lib\\ssl.py:1278\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1274\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1275\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1276\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1277\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1278\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1279\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1280\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32mC:\\Program Files\\Python311\\Lib\\ssl.py:1134\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1132\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1133\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1134\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1135\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1136\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Apply the preprocessing function to the 'body' column\n",
    "df['Txtrnk_Body'] = df['Summary'].apply(lambda x: preprocess_email_for_t5(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c4bb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_summary_for_t5(summary):\n",
    "    # Standardize dates and times \n",
    "    summary = standardize_date_time(summary)\n",
    "\n",
    "    # Remove URLs and website addresses (if they are not relevant to the summary)\n",
    "    url_pattern = r\"http\\S+|www\\.\\S+\"\n",
    "    summary = re.sub(url_pattern, \"<url>\", summary)\n",
    "\n",
    "    return summary\n",
    "df['Preprocessed_Summary'] = df['Summary_human'].apply(lambda x: preprocess_email_for_t5(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a152fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the preprocessing function to the 'Summary_human' column\n",
    "df['Preprocessed_Summary'] = df['Summary_human'].apply(lambda x: preprocess_summary_for_t5(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b94da3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer, Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Define a custom dataset for the T5 model\n",
    "class EmailSummarizationDataset(Dataset):\n",
    "    def __init__(self, tokenizer, data, max_length=512):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.input_texts = data['Txtrnk_Body']\n",
    "        self.target_texts = data['Preprocessed_Summary']\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        source = self.tokenizer.encode_plus(\n",
    "            self.input_texts.iloc[idx], \n",
    "            max_length=self.max_length, \n",
    "            padding='max_length', \n",
    "            truncation=True, \n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        target = self.tokenizer.encode_plus(\n",
    "            self.target_texts.iloc[idx], \n",
    "            max_length=self.max_length, \n",
    "            padding='max_length', \n",
    "            truncation=True, \n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': source['input_ids'].flatten(),\n",
    "            'attention_mask': source['attention_mask'].flatten(),\n",
    "            'labels': target['input_ids'].flatten()\n",
    "        }\n",
    "\n",
    "# Initialize the tokenizer and model\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-base')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "e61e6668",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_df, test_df = train_test_split(df, test_size=0.2)\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "# Prepare the training and testing datasets\n",
    "train_dataset = EmailSummarizationDataset(tokenizer, train_df)\n",
    "test_dataset = EmailSummarizationDataset(tokenizer, test_df)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 8  # You can adjust this based on your GPU's memory\n",
    "\n",
    "# Create DataLoaders for training and testing datasets\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77427ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11' max='480' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 11/480 02:58 < 2:35:27, 0.05 it/s, Epoch 0.10/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the T5 model\n",
    "model_name = 't5-base'  \n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "\n",
    "# Calculate the size of the training dataset\n",
    "training_dataset_size = len(train_dataset)\n",
    "\n",
    "N = training_dataset_size\n",
    "batch_size = 8  \n",
    "# Calculate steps per epoch\n",
    "steps_per_epoch = N // batch_size\n",
    "\n",
    "\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",                      # Directory for storing outputs\n",
    "    num_train_epochs=5,                         # Set number of epochs to 10\n",
    "    per_device_train_batch_size=8,               # Batch size for training\n",
    "    per_device_eval_batch_size=8,                # Batch size for evaluation\n",
    "    warmup_steps=500,                            # Number of warmup steps\n",
    "    weight_decay=0.01,                           # Weight decay for regularization\n",
    "    logging_dir=\"./logs\",                        # Directory for storing logs\n",
    "    logging_steps=steps_per_epoch,                            # Log every steps in epoch\n",
    "    do_train=True,                               # Enable training\n",
    "    do_eval=True,                                # Enable evaluation\n",
    "    evaluation_strategy=\"epoch\",                 # Evaluate at the end of each epoch\n",
    "    save_strategy=\"no\",                    # Disable saving the model\n",
    "    save_total_limit=0,                    # Limit on the total amount of checkpoints. Setting to 0 disables it\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,  # Use the dataset instance for training data\n",
    "    eval_dataset=test_dataset,     # Use the dataset instance for validation data\n",
    "    callbacks=[custom_eval_callback],  # Custom callback function for metrics\n",
    "    optimizers=(AdamW(model.parameters(), lr=5e-5), None)  # AdamW optimizer with a specified learning rate\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n",
    "# Plotting\n",
    "training_logs = trainer.state.log_history\n",
    "\n",
    "training_loss = [log['loss'] for log in training_logs if 'loss' in log]\n",
    "validation_loss = [log['eval_loss'] for log in training_logs if 'eval_loss' in log]\n",
    "epochs = range(1, len(training_loss) + 1)\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(epochs, training_loss, 'bo-', label='Training loss')\n",
    "plt.plot(epochs, validation_loss, 'ro-', label='Validation loss')\n",
    "plt.title('Training and Validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579e4ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "## After running the t5 model with numerous learning rates and batch sizes we could see thatthe model is learning, but the rouge and bert scores are not improving\n",
    "## Thus it has to be the issue with the tokenisation, to address the issue, we are moving on with differernt aproaches for tokenisation,\n",
    "## Chunking is one of the way to do the tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d4a6c924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting sklearn\n",
      "  Downloading sklearn-0.0.post12.tar.gz (2.6 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  python setup.py egg_info did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [15 lines of output]\n",
      "  The 'sklearn' PyPI package is deprecated, use 'scikit-learn'\n",
      "  rather than 'sklearn' for pip commands.\n",
      "  \n",
      "  Here is how to fix this error in the main use cases:\n",
      "  - use 'pip install scikit-learn' rather than 'pip install sklearn'\n",
      "  - replace 'sklearn' by 'scikit-learn' in your pip requirements files\n",
      "    (requirements.txt, setup.py, setup.cfg, Pipfile, etc ...)\n",
      "  - if the 'sklearn' package is used by one of your dependencies,\n",
      "    it would be great if you take some time to track which package uses\n",
      "    'sklearn' instead of 'scikit-learn' and report it to their issue tracker\n",
      "  - as a last resort, set the environment variable\n",
      "    SKLEARN_ALLOW_DEPRECATED_SKLEARN_PACKAGE_INSTALL=True to avoid this error\n",
      "  \n",
      "  More information is available at\n",
      "  https://github.com/scikit-learn/sklearn-pypi-package\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: metadata-generation-failed\n",
      "\n",
      "Encountered error while generating package metadata.\n",
      "\n",
      "See above for output.\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for details.\n",
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install sklearn transformers torch\n",
    "\n",
    "import torch\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer, Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset, random_split\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fbf81e",
   "metadata": {},
   "source": [
    "## T5 base with textrank summary Input and Summary Human target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "22abac23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\764883\\AppData\\Local\\Temp\\ipykernel_2668\\266685611.py:8: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(html, \"html.parser\")\n"
     ]
    }
   ],
   "source": [
    "def preprocess_summary_for_t5(summary):\n",
    "    # Standardize dates and times \n",
    "    summary = standardize_date_time(summary)\n",
    "\n",
    "    # Remove URLs and website addresses (if they are not relevant to the summary)\n",
    "    url_pattern = r\"http\\S+|www\\.\\S+\"\n",
    "    summary = re.sub(url_pattern, \"<url>\", summary)\n",
    "\n",
    "    return summary\n",
    "df['Preprocessed_Summary'] = df['Summary_human'].apply(lambda x: preprocess_email_for_t5(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "f849d075",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      The email discusses post-Microsoft Ignite 2022...\n",
       "1      Webinar Announcement: \"Windows 365 for \" event...\n",
       "2      The email discusses the rapid growth of artifi...\n",
       "3      The email announces the premiere of Microsoft ...\n",
       "4      Starting in 2023, Universal Analytics will no ...\n",
       "                             ...                        \n",
       "949    The email advertises an early Black Friday off...\n",
       "950    Jeff Fritz from .NET Conf reviewed IronPDF aga...\n",
       "951    Zym aids business owners in comprehending mark...\n",
       "952    Summary: Clear Measure has announced an upcomi...\n",
       "953    Email from Lenovo announces a workspace refres...\n",
       "Name: Preprocessed_Summary, Length: 954, dtype: object"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Preprocessed_Summary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "de0765c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>from</th>\n",
       "      <th>to</th>\n",
       "      <th>date</th>\n",
       "      <th>body</th>\n",
       "      <th>Body_Length</th>\n",
       "      <th>Subject_Length</th>\n",
       "      <th>Cleaned_Body</th>\n",
       "      <th>Cleaned_Subject</th>\n",
       "      <th>BERT_Embeddings</th>\n",
       "      <th>...</th>\n",
       "      <th>Summary</th>\n",
       "      <th>summary_BART</th>\n",
       "      <th>index_number</th>\n",
       "      <th>Tokenized_Email</th>\n",
       "      <th>Entities</th>\n",
       "      <th>Cluster_retrieved</th>\n",
       "      <th>Summary_human</th>\n",
       "      <th>Preprocessed_Body</th>\n",
       "      <th>Tokenized_Summary</th>\n",
       "      <th>Tokenized_Input_Chunks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Thank you for attending Microsoft Ignite</td>\n",
       "      <td>Microsoft Team &lt;microsoft@email.microsoft.com&gt;</td>\n",
       "      <td>richard.potter@raddsolutions.co.uk</td>\n",
       "      <td>Wed, 19 Oct 2022 20:31:34 +0100</td>\n",
       "      <td>...</td>\n",
       "      <td>6232</td>\n",
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td>Thank you for attending Microsoft Ignite</td>\n",
       "      <td>[-0.059376951307058334, 0.17135855555534363, 0...</td>\n",
       "      <td>...</td>\n",
       "      <td>microsoft ignite may be over but heres your ch...</td>\n",
       "      <td>Learn how microsoft empowers organisations to ...</td>\n",
       "      <td>1543</td>\n",
       "      <td>{'input_ids': tensor([[  101,  7513, 16270,  4...</td>\n",
       "      <td>[('microsoft', 'ORG'), ('2022', 'DATE'), ('mic...</td>\n",
       "      <td>0</td>\n",
       "      <td>The email discusses post-Microsoft Ignite 2022...</td>\n",
       "      <td>Microsoft Ignite may be over, but here’s  cont...</td>\n",
       "      <td>[[tensor(37), tensor(791), tensor(17212), tens...</td>\n",
       "      <td>[[2803, 27, 122, 7980, 164, 36, 147, 6, 68, 27...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Invitation to learn how Windows 365 Cloud PCs ...</td>\n",
       "      <td>Microsoft &lt;replyto@email.microsoft.com&gt;</td>\n",
       "      <td>richard.potter@raddsolutions.co.uk</td>\n",
       "      <td>Tue, 01 Nov 2022 11:01:50 +0000</td>\n",
       "      <td>Webinar with demos of Windows 365 and vision f...</td>\n",
       "      <td>2943</td>\n",
       "      <td>90</td>\n",
       "      <td>webinar with demos of windows 365 and vision f...</td>\n",
       "      <td>Invitation to learn how Windows 365 Cloud PCs ...</td>\n",
       "      <td>[-0.1439182013273239, 0.22149936854839325, 0.6...</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td>webinar with demos of windows 365 and vision f...</td>\n",
       "      <td>765</td>\n",
       "      <td>{'input_ids': tensor([[  101,  4773,  3981,  2...</td>\n",
       "      <td>[('thursday 17th', 'DATE'), ('2022 1400  1500'...</td>\n",
       "      <td>0</td>\n",
       "      <td>Webinar Announcement: \"Windows 365 for Your Hy...</td>\n",
       "      <td>Webinar with demos of Windows 365 and vision f...</td>\n",
       "      <td>[[tensor(1620), tensor(77), tensor(291), tenso...</td>\n",
       "      <td>[[1620, 77, 291, 28, 8698, 7, 13, 1758, 3, 104...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Don’t fall behind – embrace AI with Dell Techn...</td>\n",
       "      <td>Dell Technologies Partner Program &lt;DellTechnol...</td>\n",
       "      <td>richard.potter@raddsolutions.co.uk</td>\n",
       "      <td>Tue, 15 Nov 2022 06:01:17 +0000</td>\n",
       "      <td>&lt;https://click.comm.delltechnologies.com/open...</td>\n",
       "      <td>4498</td>\n",
       "      <td>64</td>\n",
       "      <td>\\n   \\t\\r\\nview online \\n\\n  \\t\\r\\n \\t\\r\\nwhy...</td>\n",
       "      <td>Dont fall behind  embrace AI with Dell Technol...</td>\n",
       "      <td>[-0.15142026543617249, 0.11411778628826141, 0....</td>\n",
       "      <td>...</td>\n",
       "      <td>why ai and why now\\nwe are witnessing and livi...</td>\n",
       "      <td>Artificial intelligence ai market is forecast ...</td>\n",
       "      <td>369</td>\n",
       "      <td>{'input_ids': tensor([[  101,  3193,  3784,  2...</td>\n",
       "      <td>[('500 billion', 'MONEY'), ('20231', 'DATE'), ...</td>\n",
       "      <td>0</td>\n",
       "      <td>The email discusses the rapid growth of artifi...</td>\n",
       "      <td>View Online Why AI and why now ? Why AI and wh...</td>\n",
       "      <td>[[tensor(37), tensor(791), tensor(17212), tens...</td>\n",
       "      <td>[[4197, 1777, 1615, 7833, 11, 572, 230, 3, 58,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Microsoft Envision Season 3 begins December 13</td>\n",
       "      <td>Microsoft Team &lt;microsoft@email.microsoft.com&gt;</td>\n",
       "      <td>richard.potter@raddsolutions.co.uk</td>\n",
       "      <td>Wed, 09 Nov 2022 17:05:09 +0000</td>\n",
       "      <td>Episode 1 airs December 13, 2022 \\r\\nHaving tr...</td>\n",
       "      <td>3476</td>\n",
       "      <td>46</td>\n",
       "      <td>episode 1 airs december 13 2022 \\r\\nhaving tro...</td>\n",
       "      <td>Microsoft Envision Season 3 begins December 13</td>\n",
       "      <td>[-0.36103835701942444, 0.06514844298362732, 0....</td>\n",
       "      <td>...</td>\n",
       "      <td>episode 1 airs december 13 2022\\nregister now ...</td>\n",
       "      <td>register now for microsoft envision season 3. ...</td>\n",
       "      <td>895</td>\n",
       "      <td>{'input_ids': tensor([[  101,  2792,  1015, 14...</td>\n",
       "      <td>[('1', 'CARDINAL'), ('13 2022', 'DATE'), ('mic...</td>\n",
       "      <td>0</td>\n",
       "      <td>The email announces the premiere of Microsoft ...</td>\n",
       "      <td>Episode 1 airs December 13, 2022  email? | Vie...</td>\n",
       "      <td>[[tensor(37), tensor(791), tensor(6456), tenso...</td>\n",
       "      <td>[[16112, 209, 799, 7, 1882, 10670, 460, 2884, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In September, you had 71 users visit your webs...</td>\n",
       "      <td>Google Analytics &lt;analytics-noreply@google.com&gt;</td>\n",
       "      <td>richard.potter@raddsolutions.co.uk</td>\n",
       "      <td>Tue, 11 Oct 2022 06:02:01 +0100</td>\n",
       "      <td>&lt;https://www.google.com/images/branding/googl...</td>\n",
       "      <td>5559</td>\n",
       "      <td>68</td>\n",
       "      <td>\\n \\r\\nuniversal analytics will no longer pr...</td>\n",
       "      <td>In September you had 71 users visit your websi...</td>\n",
       "      <td>[-0.10645909607410431, 0.17022578418254852, 0....</td>\n",
       "      <td>...</td>\n",
       "      <td>81 35 bounce rate\\nbreakdown of visitors acqui...</td>\n",
       "      <td>universal analytics will no longer process new...</td>\n",
       "      <td>740</td>\n",
       "      <td>{'input_ids': tensor([[  101,  5415, 25095,  2...</td>\n",
       "      <td>[('2023', 'DATE'), ('4', 'CARDINAL'), ('septem...</td>\n",
       "      <td>0</td>\n",
       "      <td>Starting in 2023, Universal Analytics will no ...</td>\n",
       "      <td>Universal Analytics will no longer process new...</td>\n",
       "      <td>[[tensor(14362), tensor(16), tensor(460), tens...</td>\n",
       "      <td>[[12489, 13926, 56, 150, 1200, 433, 126, 331, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>949</th>\n",
       "      <td>Our Black Friday offers have landed!</td>\n",
       "      <td>IT Governance &lt;emailsupport@itgovernance.co.uk&gt;</td>\n",
       "      <td>richie.wynne@raddsolutions.co.uk</td>\n",
       "      <td>Mon, 21 Nov 2022 11:05:15 +0000</td>\n",
       "      <td>You’re not going to want to miss these savings...</td>\n",
       "      <td>3228</td>\n",
       "      <td>36</td>\n",
       "      <td>youre not going to want to miss these savings ...</td>\n",
       "      <td>Our Black Friday offers have landed</td>\n",
       "      <td>[0.09008561074733734, 0.16759826242923737, 0.6...</td>\n",
       "      <td>...</td>\n",
       "      <td>were starting our black friday offers early wi...</td>\n",
       "      <td>were starting our black friday offers early wi...</td>\n",
       "      <td>1130</td>\n",
       "      <td>{'input_ids': tensor([[  101,  2115,  2063,  2...</td>\n",
       "      <td>[('friday', 'DATE'), ('25', 'CARDINAL'), ('25'...</td>\n",
       "      <td>7</td>\n",
       "      <td>The email advertises an early Black Friday off...</td>\n",
       "      <td>You’re not going to want to miss these savings...</td>\n",
       "      <td>[[tensor(37), tensor(791), tensor(17123), tens...</td>\n",
       "      <td>[[148, 22, 60, 59, 352, 12, 241, 12, 3041, 175...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>950</th>\n",
       "      <td>Jeff Fritz reviews IronPDF vs SyncFusion, iTex...</td>\n",
       "      <td>\"Jacob, Head of Engineering\" &lt;developers@irons...</td>\n",
       "      <td>Richard Potter &lt;richard.potter@raddsolutions.c...</td>\n",
       "      <td>Tue, 13 Dec 2022 15:31:28 +0000</td>\n",
       "      <td>&lt;https://ironsoftware.lt.acemlnb.com/Prod/lin...</td>\n",
       "      <td>8587</td>\n",
       "      <td>55</td>\n",
       "      <td>\\t \\r\\n \\t \\r\\nhi richard\\r\\nimagine spendin...</td>\n",
       "      <td>Jeff Fritz reviews IronPDF vs SyncFusion iText...</td>\n",
       "      <td>[-0.20470425486564636, 0.20281416177749634, 0....</td>\n",
       "      <td>...</td>\n",
       "      <td>imagine spending a lot of money on software li...</td>\n",
       "      <td>iron software is a free open source solution t...</td>\n",
       "      <td>783</td>\n",
       "      <td>{'input_ids': tensor([[  101,  7632,  2957,  5...</td>\n",
       "      <td>[('jeff fritz', 'PERSON'), ('net conf', 'ORG')...</td>\n",
       "      <td>7</td>\n",
       "      <td>Jeff Fritz from .NET Conf reviewed IronPDF aga...</td>\n",
       "      <td>, Imagine spending a lot of money on software ...</td>\n",
       "      <td>[[tensor(8507), tensor(28748), tensor(45), ten...</td>\n",
       "      <td>[[3, 6, 11648, 2887, 3, 9, 418, 13, 540, 30, 8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>951</th>\n",
       "      <td>Don't Miss Out: Help to Grow: Digital Ends in ...</td>\n",
       "      <td>Zym &lt;rebecca@zymplify.com&gt;</td>\n",
       "      <td>Richard Potter &lt;richard.potter@raddsolutions.c...</td>\n",
       "      <td>Tue, 17 Jan 2023 12:01:49 +0000</td>\n",
       "      <td>ZYM helps business owners understand their mar...</td>\n",
       "      <td>6966</td>\n",
       "      <td>56</td>\n",
       "      <td>zym helps business owners understand their mar...</td>\n",
       "      <td>Dont Miss Out Help to Grow Digital Ends in 16 ...</td>\n",
       "      <td>[0.0035861318465322256, 0.07786554843187332, 0...</td>\n",
       "      <td>...</td>\n",
       "      <td>even better you can try zym for free for 14 da...</td>\n",
       "      <td>zym helps business owners understand their mar...</td>\n",
       "      <td>345</td>\n",
       "      <td>{'input_ids': tensor([[  101,  1062, 24335,  7...</td>\n",
       "      <td>[('uk', 'GPE'), ('up to 5000', 'CARDINAL'), ('...</td>\n",
       "      <td>7</td>\n",
       "      <td>Zym aids business owners in comprehending mark...</td>\n",
       "      <td>ZYM helps business owners understand their mar...</td>\n",
       "      <td>[[tensor(1027), tensor(63), tensor(51), tensor...</td>\n",
       "      <td>[[1027, 476, 329, 1691, 268, 2713, 734, 70, 10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>952</th>\n",
       "      <td>New videos coming in 2023</td>\n",
       "      <td>Clear Measure &lt;clearmeasure@clear-measure.com&gt;</td>\n",
       "      <td>richard.potter@raddsolutions.co.uk</td>\n",
       "      <td>Thu, 29 Dec 2022 10:00:02 +0000</td>\n",
       "      <td>New videos coming in 2023 … made to empower yo...</td>\n",
       "      <td>3404</td>\n",
       "      <td>25</td>\n",
       "      <td>new videos coming in 2023  made to empower you...</td>\n",
       "      <td>New videos coming in 2023</td>\n",
       "      <td>[-0.08648061007261276, 0.11852650344371796, 0....</td>\n",
       "      <td>...</td>\n",
       "      <td>our new videos coming in 2023 are made to empo...</td>\n",
       "      <td>new videos coming in 2023  made to empower you...</td>\n",
       "      <td>958</td>\n",
       "      <td>{'input_ids': tensor([[  101,  2047,  6876,  2...</td>\n",
       "      <td>[('2023', 'DATE'), ('2023', 'DATE'), ('10815',...</td>\n",
       "      <td>7</td>\n",
       "      <td>Summary:\\n\\nClear Measure has announced an upc...</td>\n",
       "      <td>New videos coming in 2023 … made to empower . ...</td>\n",
       "      <td>[[tensor(20698), tensor(10), tensor(8912), ten...</td>\n",
       "      <td>[[368, 3075, 1107, 16, 460, 2773, 3, 233, 263,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>953</th>\n",
       "      <td>Business PCs up to 40% off</td>\n",
       "      <td>Lenovo New beginnings! &lt;lenovo@ecomm.lenovo.com&gt;</td>\n",
       "      <td>richard.potter@raddsolutions.co.uk</td>\n",
       "      <td>Wed, 01 Feb 2023 09:04:42 +0000</td>\n",
       "      <td>&lt;https://trk.ecomm.lenovo.com/ss/o/k8leYzsS8M...</td>\n",
       "      <td>8108</td>\n",
       "      <td>26</td>\n",
       "      <td>\\n \\t\\r\\n\\tview it in browser instead \\n  fre...</td>\n",
       "      <td>Business PCs up to 40 off</td>\n",
       "      <td>[-0.09954569488763809, 0.13386352360248566, 0....</td>\n",
       "      <td>...</td>\n",
       "      <td>thinkpad p1 gen 4\\nthinkpad z13 gen 1\\nideapad...</td>\n",
       "      <td>Free shipping on all orders with up to 40% dis...</td>\n",
       "      <td>132</td>\n",
       "      <td>{'input_ids': tensor([[  101,  3193,  2009,  1...</td>\n",
       "      <td>[('up to 40', 'CARDINAL'), ('up to 40', 'CARDI...</td>\n",
       "      <td>7</td>\n",
       "      <td>Email from Lenovo announces a workspace refres...</td>\n",
       "      <td>View it in browser instead Free shipping on al...</td>\n",
       "      <td>[[tensor(8601), tensor(45), tensor(24340), ten...</td>\n",
       "      <td>[[4197, 34, 16, 3509, 1446, 1443, 3365, 30, 66...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>954 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               subject  \\\n",
       "0             Thank you for attending Microsoft Ignite   \n",
       "1    Invitation to learn how Windows 365 Cloud PCs ...   \n",
       "2    Don’t fall behind – embrace AI with Dell Techn...   \n",
       "3       Microsoft Envision Season 3 begins December 13   \n",
       "4    In September, you had 71 users visit your webs...   \n",
       "..                                                 ...   \n",
       "949               Our Black Friday offers have landed!   \n",
       "950  Jeff Fritz reviews IronPDF vs SyncFusion, iTex...   \n",
       "951  Don't Miss Out: Help to Grow: Digital Ends in ...   \n",
       "952                          New videos coming in 2023   \n",
       "953                         Business PCs up to 40% off   \n",
       "\n",
       "                                                  from  \\\n",
       "0       Microsoft Team <microsoft@email.microsoft.com>   \n",
       "1              Microsoft <replyto@email.microsoft.com>   \n",
       "2    Dell Technologies Partner Program <DellTechnol...   \n",
       "3       Microsoft Team <microsoft@email.microsoft.com>   \n",
       "4      Google Analytics <analytics-noreply@google.com>   \n",
       "..                                                 ...   \n",
       "949    IT Governance <emailsupport@itgovernance.co.uk>   \n",
       "950  \"Jacob, Head of Engineering\" <developers@irons...   \n",
       "951                         Zym <rebecca@zymplify.com>   \n",
       "952     Clear Measure <clearmeasure@clear-measure.com>   \n",
       "953   Lenovo New beginnings! <lenovo@ecomm.lenovo.com>   \n",
       "\n",
       "                                                    to  \\\n",
       "0                   richard.potter@raddsolutions.co.uk   \n",
       "1                   richard.potter@raddsolutions.co.uk   \n",
       "2                   richard.potter@raddsolutions.co.uk   \n",
       "3                   richard.potter@raddsolutions.co.uk   \n",
       "4                   richard.potter@raddsolutions.co.uk   \n",
       "..                                                 ...   \n",
       "949                   richie.wynne@raddsolutions.co.uk   \n",
       "950  Richard Potter <richard.potter@raddsolutions.c...   \n",
       "951  Richard Potter <richard.potter@raddsolutions.c...   \n",
       "952                 richard.potter@raddsolutions.co.uk   \n",
       "953                 richard.potter@raddsolutions.co.uk   \n",
       "\n",
       "                                date  \\\n",
       "0    Wed, 19 Oct 2022 20:31:34 +0100   \n",
       "1    Tue, 01 Nov 2022 11:01:50 +0000   \n",
       "2    Tue, 15 Nov 2022 06:01:17 +0000   \n",
       "3    Wed, 09 Nov 2022 17:05:09 +0000   \n",
       "4    Tue, 11 Oct 2022 06:02:01 +0100   \n",
       "..                               ...   \n",
       "949  Mon, 21 Nov 2022 11:05:15 +0000   \n",
       "950  Tue, 13 Dec 2022 15:31:28 +0000   \n",
       "951  Tue, 17 Jan 2023 12:01:49 +0000   \n",
       "952  Thu, 29 Dec 2022 10:00:02 +0000   \n",
       "953  Wed, 01 Feb 2023 09:04:42 +0000   \n",
       "\n",
       "                                                  body  Body_Length  \\\n",
       "0                                                  ...         6232   \n",
       "1    Webinar with demos of Windows 365 and vision f...         2943   \n",
       "2     <https://click.comm.delltechnologies.com/open...         4498   \n",
       "3    Episode 1 airs December 13, 2022 \\r\\nHaving tr...         3476   \n",
       "4     <https://www.google.com/images/branding/googl...         5559   \n",
       "..                                                 ...          ...   \n",
       "949  You’re not going to want to miss these savings...         3228   \n",
       "950   <https://ironsoftware.lt.acemlnb.com/Prod/lin...         8587   \n",
       "951  ZYM helps business owners understand their mar...         6966   \n",
       "952  New videos coming in 2023 … made to empower yo...         3404   \n",
       "953   <https://trk.ecomm.lenovo.com/ss/o/k8leYzsS8M...         8108   \n",
       "\n",
       "     Subject_Length                                       Cleaned_Body  \\\n",
       "0                40                                                ...   \n",
       "1                90  webinar with demos of windows 365 and vision f...   \n",
       "2                64   \\n   \\t\\r\\nview online \\n\\n  \\t\\r\\n \\t\\r\\nwhy...   \n",
       "3                46  episode 1 airs december 13 2022 \\r\\nhaving tro...   \n",
       "4                68    \\n \\r\\nuniversal analytics will no longer pr...   \n",
       "..              ...                                                ...   \n",
       "949              36  youre not going to want to miss these savings ...   \n",
       "950              55    \\t \\r\\n \\t \\r\\nhi richard\\r\\nimagine spendin...   \n",
       "951              56  zym helps business owners understand their mar...   \n",
       "952              25  new videos coming in 2023  made to empower you...   \n",
       "953              26   \\n \\t\\r\\n\\tview it in browser instead \\n  fre...   \n",
       "\n",
       "                                       Cleaned_Subject  \\\n",
       "0             Thank you for attending Microsoft Ignite   \n",
       "1    Invitation to learn how Windows 365 Cloud PCs ...   \n",
       "2    Dont fall behind  embrace AI with Dell Technol...   \n",
       "3       Microsoft Envision Season 3 begins December 13   \n",
       "4    In September you had 71 users visit your websi...   \n",
       "..                                                 ...   \n",
       "949                Our Black Friday offers have landed   \n",
       "950  Jeff Fritz reviews IronPDF vs SyncFusion iText...   \n",
       "951  Dont Miss Out Help to Grow Digital Ends in 16 ...   \n",
       "952                          New videos coming in 2023   \n",
       "953                          Business PCs up to 40 off   \n",
       "\n",
       "                                       BERT_Embeddings  ...  \\\n",
       "0    [-0.059376951307058334, 0.17135855555534363, 0...  ...   \n",
       "1    [-0.1439182013273239, 0.22149936854839325, 0.6...  ...   \n",
       "2    [-0.15142026543617249, 0.11411778628826141, 0....  ...   \n",
       "3    [-0.36103835701942444, 0.06514844298362732, 0....  ...   \n",
       "4    [-0.10645909607410431, 0.17022578418254852, 0....  ...   \n",
       "..                                                 ...  ...   \n",
       "949  [0.09008561074733734, 0.16759826242923737, 0.6...  ...   \n",
       "950  [-0.20470425486564636, 0.20281416177749634, 0....  ...   \n",
       "951  [0.0035861318465322256, 0.07786554843187332, 0...  ...   \n",
       "952  [-0.08648061007261276, 0.11852650344371796, 0....  ...   \n",
       "953  [-0.09954569488763809, 0.13386352360248566, 0....  ...   \n",
       "\n",
       "                                               Summary  \\\n",
       "0    microsoft ignite may be over but heres your ch...   \n",
       "1                                                        \n",
       "2    why ai and why now\\nwe are witnessing and livi...   \n",
       "3    episode 1 airs december 13 2022\\nregister now ...   \n",
       "4    81 35 bounce rate\\nbreakdown of visitors acqui...   \n",
       "..                                                 ...   \n",
       "949  were starting our black friday offers early wi...   \n",
       "950  imagine spending a lot of money on software li...   \n",
       "951  even better you can try zym for free for 14 da...   \n",
       "952  our new videos coming in 2023 are made to empo...   \n",
       "953  thinkpad p1 gen 4\\nthinkpad z13 gen 1\\nideapad...   \n",
       "\n",
       "                                          summary_BART index_number  \\\n",
       "0    Learn how microsoft empowers organisations to ...         1543   \n",
       "1    webinar with demos of windows 365 and vision f...          765   \n",
       "2    Artificial intelligence ai market is forecast ...          369   \n",
       "3    register now for microsoft envision season 3. ...          895   \n",
       "4    universal analytics will no longer process new...          740   \n",
       "..                                                 ...          ...   \n",
       "949  were starting our black friday offers early wi...         1130   \n",
       "950  iron software is a free open source solution t...          783   \n",
       "951  zym helps business owners understand their mar...          345   \n",
       "952  new videos coming in 2023  made to empower you...          958   \n",
       "953  Free shipping on all orders with up to 40% dis...          132   \n",
       "\n",
       "                                       Tokenized_Email  \\\n",
       "0    {'input_ids': tensor([[  101,  7513, 16270,  4...   \n",
       "1    {'input_ids': tensor([[  101,  4773,  3981,  2...   \n",
       "2    {'input_ids': tensor([[  101,  3193,  3784,  2...   \n",
       "3    {'input_ids': tensor([[  101,  2792,  1015, 14...   \n",
       "4    {'input_ids': tensor([[  101,  5415, 25095,  2...   \n",
       "..                                                 ...   \n",
       "949  {'input_ids': tensor([[  101,  2115,  2063,  2...   \n",
       "950  {'input_ids': tensor([[  101,  7632,  2957,  5...   \n",
       "951  {'input_ids': tensor([[  101,  1062, 24335,  7...   \n",
       "952  {'input_ids': tensor([[  101,  2047,  6876,  2...   \n",
       "953  {'input_ids': tensor([[  101,  3193,  2009,  1...   \n",
       "\n",
       "                                              Entities Cluster_retrieved  \\\n",
       "0    [('microsoft', 'ORG'), ('2022', 'DATE'), ('mic...                 0   \n",
       "1    [('thursday 17th', 'DATE'), ('2022 1400  1500'...                 0   \n",
       "2    [('500 billion', 'MONEY'), ('20231', 'DATE'), ...                 0   \n",
       "3    [('1', 'CARDINAL'), ('13 2022', 'DATE'), ('mic...                 0   \n",
       "4    [('2023', 'DATE'), ('4', 'CARDINAL'), ('septem...                 0   \n",
       "..                                                 ...               ...   \n",
       "949  [('friday', 'DATE'), ('25', 'CARDINAL'), ('25'...                 7   \n",
       "950  [('jeff fritz', 'PERSON'), ('net conf', 'ORG')...                 7   \n",
       "951  [('uk', 'GPE'), ('up to 5000', 'CARDINAL'), ('...                 7   \n",
       "952  [('2023', 'DATE'), ('2023', 'DATE'), ('10815',...                 7   \n",
       "953  [('up to 40', 'CARDINAL'), ('up to 40', 'CARDI...                 7   \n",
       "\n",
       "                                         Summary_human  \\\n",
       "0    The email discusses post-Microsoft Ignite 2022...   \n",
       "1    Webinar Announcement: \"Windows 365 for Your Hy...   \n",
       "2    The email discusses the rapid growth of artifi...   \n",
       "3    The email announces the premiere of Microsoft ...   \n",
       "4    Starting in 2023, Universal Analytics will no ...   \n",
       "..                                                 ...   \n",
       "949  The email advertises an early Black Friday off...   \n",
       "950  Jeff Fritz from .NET Conf reviewed IronPDF aga...   \n",
       "951  Zym aids business owners in comprehending mark...   \n",
       "952  Summary:\\n\\nClear Measure has announced an upc...   \n",
       "953  Email from Lenovo announces a workspace refres...   \n",
       "\n",
       "                                     Preprocessed_Body  \\\n",
       "0    Microsoft Ignite may be over, but here’s  cont...   \n",
       "1    Webinar with demos of Windows 365 and vision f...   \n",
       "2    View Online Why AI and why now ? Why AI and wh...   \n",
       "3    Episode 1 airs December 13, 2022  email? | Vie...   \n",
       "4    Universal Analytics will no longer process new...   \n",
       "..                                                 ...   \n",
       "949  You’re not going to want to miss these savings...   \n",
       "950  , Imagine spending a lot of money on software ...   \n",
       "951  ZYM helps business owners understand their mar...   \n",
       "952  New videos coming in 2023 … made to empower . ...   \n",
       "953  View it in browser instead Free shipping on al...   \n",
       "\n",
       "                                     Tokenized_Summary  \\\n",
       "0    [[tensor(37), tensor(791), tensor(17212), tens...   \n",
       "1    [[tensor(1620), tensor(77), tensor(291), tenso...   \n",
       "2    [[tensor(37), tensor(791), tensor(17212), tens...   \n",
       "3    [[tensor(37), tensor(791), tensor(6456), tenso...   \n",
       "4    [[tensor(14362), tensor(16), tensor(460), tens...   \n",
       "..                                                 ...   \n",
       "949  [[tensor(37), tensor(791), tensor(17123), tens...   \n",
       "950  [[tensor(8507), tensor(28748), tensor(45), ten...   \n",
       "951  [[tensor(1027), tensor(63), tensor(51), tensor...   \n",
       "952  [[tensor(20698), tensor(10), tensor(8912), ten...   \n",
       "953  [[tensor(8601), tensor(45), tensor(24340), ten...   \n",
       "\n",
       "                                Tokenized_Input_Chunks  \n",
       "0    [[2803, 27, 122, 7980, 164, 36, 147, 6, 68, 27...  \n",
       "1    [[1620, 77, 291, 28, 8698, 7, 13, 1758, 3, 104...  \n",
       "2    [[4197, 1777, 1615, 7833, 11, 572, 230, 3, 58,...  \n",
       "3    [[16112, 209, 799, 7, 1882, 10670, 460, 2884, ...  \n",
       "4    [[12489, 13926, 56, 150, 1200, 433, 126, 331, ...  \n",
       "..                                                 ...  \n",
       "949  [[148, 22, 60, 59, 352, 12, 241, 12, 3041, 175...  \n",
       "950  [[3, 6, 11648, 2887, 3, 9, 418, 13, 540, 30, 8...  \n",
       "951  [[1027, 476, 329, 1691, 268, 2713, 734, 70, 10...  \n",
       "952  [[368, 3075, 1107, 16, 460, 2773, 3, 233, 263,...  \n",
       "953  [[4197, 34, 16, 3509, 1446, 1443, 3365, 30, 66...  \n",
       "\n",
       "[954 rows x 24 columns]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de91aad7",
   "metadata": {},
   "source": [
    "# Tokenize\n",
    "        input_tokens = tokenizer.tokenize(\"summarize: \" + Body)\n",
    "        output_tokens = tokenizer.tokenize(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "2b26c7a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\models\\t5\\tokenization_t5_fast.py:160: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 768)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, T5ForConditionalGeneration\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, TensorDataset\n",
    "import pandas as pd\n",
    "from datasets import Dataset, load_metric\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Check if CUDA is available\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "# Load your data\n",
    "data = df\n",
    "data = data[['Txtrnk_Body', 'Preprocessed_Summary']].dropna()\n",
    "data = data.sample(frac=1, random_state=0)  # Shuffle the data\n",
    "dataset = Dataset.from_pandas(data)\n",
    "dataset = dataset.train_test_split(test_size=0.3)  # Splitting the data\n",
    "dataset[\"test\"] = dataset[\"test\"].train_test_split(test_size=0.5)\n",
    "\n",
    "# Prepare the data\n",
    "train_source = dataset[\"train\"][\"Txtrnk_Body\"]\n",
    "train_target = dataset[\"train\"][\"Preprocessed_Summary\"]\n",
    "val_source = dataset[\"test\"][\"train\"][\"Txtrnk_Body\"]\n",
    "val_target = dataset[\"test\"][\"train\"][\"Preprocessed_Summary\"]\n",
    "test_source = dataset[\"test\"][\"test\"][\"Txtrnk_Body\"]\n",
    "test_target = dataset[\"test\"][\"test\"][\"Preprocessed_Summary\"]\n",
    "device = 'cpu'\n",
    "# Initialize tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else torch.device(\"cpu\"))\n",
    "\n",
    "# Tokenize the data\n",
    "def tokenize_data(source, target, max_length=512, max_target_length=140):\n",
    "    encodings = tokenizer(source, truncation=True, max_length=max_length, padding=True)\n",
    "    decodings = tokenizer(target, truncation=True, max_length=max_target_length, padding=True)\n",
    "    return {\n",
    "        'input_ids': torch.tensor(encodings['input_ids']),\n",
    "        'attention_mask': torch.tensor(encodings['attention_mask']),\n",
    "        'labels': torch.tensor(decodings['input_ids'])\n",
    "    }\n",
    "\n",
    "train_encodings = tokenize_data(train_source, train_target)\n",
    "val_encodings = tokenize_data(val_source, val_target)\n",
    "test_encodings = tokenize_data(test_source, test_target)\n",
    "\n",
    "# Create TensorDatasets\n",
    "train_dataset = TensorDataset(train_encodings['input_ids'], train_encodings['attention_mask'], train_encodings['labels'])\n",
    "val_dataset = TensorDataset(val_encodings['input_ids'], val_encodings['attention_mask'], val_encodings['labels'])\n",
    "test_dataset = TensorDataset(test_encodings['input_ids'], test_encodings['attention_mask'], test_encodings['labels'])\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=2)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=2)\n",
    "\n",
    "# Load the T5 model\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "faeb0193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load metrics\n",
    "rouge_metric = load_metric(\"rouge\")\n",
    "bert_metric = load_metric(\"bertscore\")\n",
    "\n",
    "def validate_and_calculate_metrics(model, dataloader, tokenizer, device):\n",
    "    model.eval()\n",
    "    rouge_scores = []\n",
    "    bert_scores = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Validating\"):\n",
    "            input_ids, attention_masks, labels = [t.to(device) for t in batch]\n",
    "            generated_ids = model.generate(input_ids, attention_mask=attention_masks, max_length=150)\n",
    "            preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
    "            refs = [tokenizer.decode(l, skip_special_tokens=True, clean_up_tokenization_spaces=True) for l in labels]\n",
    "\n",
    "            # Calculate ROUGE\n",
    "            rouge_scores.append(rouge_metric.compute(predictions=preds, references=refs))\n",
    "            \n",
    "            # Calculate BERTScore\n",
    "            P, R, F1 = bert_score(preds, refs, lang=\"en\", rescale_with_baseline=True)\n",
    "            bert_scores.append({\"precision\": P.mean().item(), \"recall\": R.mean().item(), \"f1\": F1.mean().item()})\n",
    "\n",
    "    # Aggregate scores\n",
    "    rouge_final = {key: sum(score[key].mid.fmeasure for score in rouge_scores) / len(rouge_scores) for key in rouge_scores[0]}\n",
    "    bert_final = {\n",
    "        \"precision\": sum(score[\"precision\"] for score in bert_scores) / len(bert_scores),\n",
    "        \"recall\": sum(score[\"recall\"] for score in bert_scores) / len(bert_scores),\n",
    "        \"f1\": sum(score[\"f1\"] for score in bert_scores) / len(bert_scores)\n",
    "    }\n",
    "\n",
    "    return rouge_final, bert_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "d8cd0dd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|█████████████████████████████████████████████████████████| 334/334 [53:57<00:00,  9.69s/batch, loss=1.27]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.2748, grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|███████████████████████████████████████████████████████████| 72/72 [03:55<00:00,  3.27s/batch, loss=2.77]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.7684)\n",
      "train_loss = 2.1785604015081943\n",
      " test_loss = 1.8651749640703201\n",
      "1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  38%|██████████████████████▏                                   | 128/334 [20:25<32:52,  9.57s/batch, loss=1.1]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[149], line 21\u001b[0m\n\u001b[0;32m     19\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     20\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 21\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     23\u001b[0m tepoch\u001b[38;5;241m.\u001b[39mset_postfix(loss\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[1;32mC:\\Program Files\\Python311\\Lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Program Files\\Python311\\Lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n",
    "\n",
    "epochs = 5\n",
    "# device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = 0.0004)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(str(epoch)+\"\\n\")\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    with tqdm(train_dataloader, unit=\"batch\") as tepoch:    \n",
    "        for batch in tepoch:\n",
    "            tepoch.set_description(f\"Epoch {epoch}\")\n",
    "            input_ids, attention_masks, labels = tuple(t.to(device) for t in batch)\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_masks, labels=labels)\n",
    "\n",
    "            loss = outputs.loss\n",
    "            train_loss += loss.item()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            tepoch.set_postfix(loss= loss.item())\n",
    "        train_loss /= len(train_dataloader)\n",
    "        sleep(0.1)\n",
    "    print(loss)\n",
    "\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        with tqdm(test_dataloader, unit=\"batch\") as tepoch:\n",
    "            for batch in tepoch:\n",
    "                tepoch.set_description(f\"Epoch {epoch}\")\n",
    "                input_ids, attention_masks, labels = tuple(t.to(device) for t in batch)\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_masks, labels=labels)\n",
    "                loss = outputs.loss\n",
    "                test_loss += loss.item()\n",
    "                tepoch.set_postfix(loss= loss.item())\n",
    "\n",
    "            # Compute the average testing loss for the epoch\n",
    "            test_loss /= len(test_dataloader)\n",
    "    print(loss)\n",
    "    print(f'train_loss = {train_loss}\\n test_loss = {test_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "78891c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "941647ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "b3957228",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c53171",
   "metadata": {},
   "source": [
    "## T5 small with more batch size and less epochs, to sped up the training process has to be considered. Chunking the tokens to improve the summarisation context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c55404",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Adding more to ythe preprocessing logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "537e3ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('sum_anno_human_proper.json', lines= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "48b38671",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from transformers import T5Tokenizer\n",
    "from dateutil.parser import parse\n",
    "\n",
    "\n",
    "def convert_html_to_text(html):\n",
    "    # Use BeautifulSoup to parse HTML and extract text\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    text = soup.get_text(separator=\" \")\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "def standardize_date_time(text):\n",
    "\n",
    "    # Function to standardize dates\n",
    "    def replace_date(match):\n",
    "        try:\n",
    "            date = parse(match.group(), fuzzy=True)\n",
    "            return date.strftime(\"%Y-%m-%d\")\n",
    "        except ValueError:\n",
    "            # Return the original string if parsing fails\n",
    "            return match.group()\n",
    "\n",
    "    # Function to standardize times\n",
    "    def replace_time(match):\n",
    "        try:\n",
    "            time = parse(match.group(), fuzzy=True)\n",
    "            return time.strftime(\"%H:%M\")\n",
    "        except ValueError:\n",
    "            # Return the original string if parsing fails\n",
    "            return match.group()\n",
    "\n",
    "    # Standardize dates\n",
    "    text = re.sub(r'\\b\\d{1,2}[-/]\\d{1,2}[-/]\\d{2,4}\\b', replace_date, text)\n",
    "    # Standardize times\n",
    "    text = re.sub(r'\\b\\d{1,2}:\\d{2}\\b', replace_time, text)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_email_for_t5(email_body):\n",
    "    # Convert HTML to text\n",
    "    email_body = convert_html_to_text(email_body)\n",
    "\n",
    "    # Remove extra whitespaces\n",
    "    email_body = ' '.join(email_body.split())\n",
    "\n",
    "    # Remove common greeting texts\n",
    "    greetings_pattern = (\n",
    "    r\"Hi\\s\\w+|\"\n",
    "    r\"Hello\\s\\w+|\"\n",
    "    r\"Dear\\s\\w+|\"\n",
    "    r\"Dear\\s[Mm]r\\.\\s\\w+|\"\n",
    "    r\"Dear\\s[Mm]rs\\.\\s\\w+|\"\n",
    "    r\"Dear\\s[Mm]s\\.\\s\\w+|\"\n",
    "    r\"Dear\\s[MD]r(s)?\\.\\s\\w+|\"\n",
    "    r\"Dear\\sProf\\.\\s\\w+|\"\n",
    "    r\"Dear\\sDoctor\\.\\s\\w+|\"\n",
    "    r\"Greetings|\"\n",
    "    r\"Good\\s[Mm]orning|\"\n",
    "    r\"Good\\s[Aa]fternoon|\"\n",
    "    r\"Good\\s[Ee]vening|\"\n",
    "    r\"Hey\\s\\w+|\"\n",
    "    r\"Hey\\sthere|\"\n",
    "    r\"Hello\\severyone|\"\n",
    "    r\"Hope\\syou\\sare\\sdoing\\swell|\"\n",
    "    r\"Hope\\syou\\sare\\sdoing\\swell|\"\n",
    "    r\"Hope\\sthis\\semail\\sfinds\\syou\\swell|\"\n",
    "    r\"Hope\\sthis\\semail\\sfinds\\syou\\swell|\"\n",
    "    r\"Hope\\sthis\\semail\\sfinds\\syou\\sin\\sgood\\shealth|\"\n",
    "    r\"How\\sare\\syou\\sdoing|\"\n",
    "    r\"How\\sis\\sit\\sgoing|\"\n",
    "    r\"To\\swhom\\sit\\smay\\sconcern|\"\n",
    "    r\"To\\swhom\\sit\\smay\\sconcern|\"\n",
    "    r\"I\\sam\\swriting\\sto\\syou\\sbecause|\"\n",
    "    r\"I\\sam\\swriting\\sto\\syou\\sto|\"\n",
    "    r\"I\\shope\\sthat\\syou|\"\n",
    "    r\"I\\swanted\\sto\\sreach\\sout\\sto|\"\n",
    "    r\"I\\swanted\\sto\\slet\\syou\\sknow|\"\n",
    "    r\"I\\swould\\slike\\sto\\sinform\\syou|\"\n",
    "    r\"It\\spleases\\sme\\sto\\scontact\\syou|\"\n",
    "    r\"It\\shas\\scome\\sto\\smy\\sattention|\"\n",
    "    r\"I\\swas\\sjust\\sthinking\\sabout\\syou\\sand\\s|\"\n",
    "    r\"Allow\\sme\\sto\\sintroduce\\smyself|\"\n",
    "    r\"I\\shope\\sthat\\severything\\sis\\sgoing\\swell|\"\n",
    "    r\"Thank\\syou\\sfor\\syour\\semail|\"\n",
    "    r\"Thank\\syou\\sfor\\sreaching\\sout\")\n",
    "\n",
    "    email_body = re.sub(greetings_pattern, \"\", email_body, flags=re.IGNORECASE)\n",
    "\n",
    "    # Remove common sign-offs\n",
    "    signoffs_pattern = (\n",
    "    r\"Best\\sregards|\"\n",
    "    r\"Best\\s\\w+|\"\n",
    "    r\"Sincerely|\"\n",
    "    r\"Yours\\sfaithfully|\"\n",
    "    r\"Yours\\ssincerely|\"\n",
    "    r\"Regards|\"\n",
    "    r\"Warm\\sregards|\"\n",
    "    r\"Kind\\sregards|\"\n",
    "    r\"Cheers|\"\n",
    "    r\"Thanks\\sand\\sregards|\"\n",
    "    r\"Thank\\syou|\"\n",
    "    r\"Take\\scare|\"\n",
    "    r\"Looking\\sforward|\"\n",
    "    r\"All\\sbest|\"\n",
    "    r\"Best\\swishes|\"\n",
    "    r\"Best|\"\n",
    "    r\"Yours\\struly|\"\n",
    "    r\"Cordially|\"\n",
    "    r\"With\\sappreciation|\"\n",
    "    r\"Respectfully|\"\n",
    "    r\"With\\sregards|\"\n",
    "    r\"Many\\sthanks|\"\n",
    "    r\"Hope\\sto\\shear\\sfrom\\syou\\ssoon|\"\n",
    "    r\"Until\\snext\\stime|\"\n",
    "    r\"Yours\\svery\\struly|\"\n",
    "    r\"Yours|\"\n",
    "    r\"In\\sgratitude|\"\n",
    "    r\"In\\ssympathy|\"\n",
    "    r\"Thoughtfully|\"\n",
    "    r\"With\\saffection|\"\n",
    "    r\"Fond\\sregards|\"\n",
    "    r\"With\\santicipation|\"\n",
    "    r\"Stay\\swell|\"\n",
    "    r\"Stay\\ssafe|\"\n",
    "    r\"Peace|\"\n",
    "    r\"God\\sbless|\"\n",
    "    r\"Love|\"\n",
    "    r\"Sent\\sfrom\\smy\\s\\w+|\"\n",
    "    r\"Sent\\sfrom\\smy\\s\\w+\\s\\w+|\"\n",
    "    r\"Talk\\sto\\syou\\ssoon|\"\n",
    "    r\"See\\syou\\ssoon|\"\n",
    "    r\"See\\sya|\"\n",
    "    r\"Ciao|\"\n",
    "    r\"Adieu|\"\n",
    "    r\"Farewell|\"\n",
    "    r\"Good\\sbye|\"\n",
    "    r\"Bye\\sfor\\snow|\"\n",
    "    r\"Signing\\soff|\"\n",
    "    r\"Out|\"\n",
    "    r\"Yours\\s[in]\\s\\w+|\"\n",
    "    r\"Your\\sfriend|\"\n",
    "    r\"Your\\s\\w+\\s\\w+|\"\n",
    "    r\"Keep\\sin\\stouch|\"\n",
    "    r\"Yours\\sfaithfully|\"\n",
    "    r\"Yours\\ssincerely|\"\n",
    "    r\"Yours\\sobediently|\"\n",
    "    r\"Yours\\saffectionately|\"\n",
    "    r\"Yours\\scordially|\"\n",
    "    r\"Yours\\srespectfully|\"\n",
    "    r\"Yours\\struly|\"\n",
    "    r\"Yours\\sever\")\n",
    "\n",
    "    email_body = re.sub(signoffs_pattern, \"\", email_body, flags=re.IGNORECASE)\n",
    "\n",
    "    # Pattern to remove repetitive characters like dashes or underscores\n",
    "    repetitive_pattern = r\"[-_]{3,}\"  # Adjust the number based on your observation, here it's set to match 3 or more\n",
    "    email_body = re.sub(repetitive_pattern, \"\", email_body)\n",
    "\n",
    "    # Standardize dates and times\n",
    "    email_body = standardize_date_time(email_body)\n",
    "\n",
    "    # Remove email signatures and disclaimers\n",
    "    email_body = re.sub(r\"--\\s*[\\s\\S]*$\", \"\", email_body)\n",
    "\n",
    "    # Standardize email addresses and URLs\n",
    "    email_body = re.sub(r\"\\S+@\\S+\\.\\S+\", \"<email>\", email_body)\n",
    "    email_body = re.sub(r\"http\\S+\", \"<url>\", email_body)\n",
    "\n",
    "    # Remove phrases indicating difficulty in viewing images or links\n",
    "    irrelevant_phrases_pattern = (\n",
    "    r\"difficulty in viewing this image|\"\n",
    "    r\"click here|\"\n",
    "    r\"having trouble viewing this|\"\n",
    "    r\"view this email in your browser|\"\n",
    "    r\"to ensure delivery to your inbox|\"\n",
    "    r\"if you cannot see this message|\"\n",
    "    r\"message not displaying correctly|\"\n",
    "    r\"trouble seeing this email|\"\n",
    "    r\"can't see the images below|\"\n",
    "    r\"email not looking quite right|\"\n",
    "    r\"viewing this email on a mobile device|\"\n",
    "    r\"can't read this email|\"\n",
    "    r\"images not showing up|\"\n",
    "    r\"to view the online version of this email|\"\n",
    "    r\"email doesn't display correctly|\"\n",
    "    r\"problems seeing this email|\"\n",
    "    r\"to unsubscribe or change preferences|\"\n",
    "    r\"this message was sent to <email>|\"\n",
    "    r\"not interested in these emails|\"\n",
    "    r\"you're receiving this email because|\"\n",
    "    r\"to stop receiving these emails|\"\n",
    "    r\"unsubscribe from this list|\"\n",
    "    r\"manage your email preferences\")\n",
    "    email_body = re.sub(irrelevant_phrases_pattern, \"\", email_body, flags=re.IGNORECASE)\n",
    "\n",
    "    # Add task-specific prefix (if you are going to use this text directly for T5 summarization later)\n",
    "    email_body = \"summarize: \" + email_body\n",
    "\n",
    "    return email_body\n",
    "\n",
    "\n",
    "# Apply the preprocessing function to the 'body' column\n",
    "df['Preprocessed_Body'] = df['body'].apply(lambda x: preprocess_email_for_t5(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55bc6513",
   "metadata": {},
   "source": [
    "## Tokenising the Summary targets with the regular tokeniser,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dc553888",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer\n",
    "\n",
    "# Assuming you have already loaded the T5 model and tokenizer\n",
    "# load the tokenizer:\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "\n",
    "# Function to tokenize a text\n",
    "def tokenize_text(text):\n",
    "    return tokenizer.encode(text, add_special_tokens=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Apply tokenization to the 'Summary_human' column\n",
    "df['Tokenized_Summary'] = df['Summary_human'].apply(tokenize_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "dfbd7dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\models\\t5\\tokenization_t5.py:303: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    " # Load the model and tokenizer\n",
    "model_name = 't5-small'\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Function to chunk the text\n",
    "def chunk_text(text, max_length, stride):\n",
    "    \"\"\"\n",
    "    Breaks text into chunks of max_length with a stride.\n",
    "    \"\"\"\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=True, truncation=True)\n",
    "    chunk_start = 0\n",
    "    chunk_end = max_length  \n",
    "    chunks = []\n",
    "\n",
    "    while chunk_start < len(tokens):\n",
    "        chunks.append(tokens[chunk_start:chunk_end])\n",
    "        chunk_start += stride\n",
    "        chunk_end = min(chunk_start + max_length, len(tokens))\n",
    "    return chunks\n",
    "    \n",
    "    #Function to chunk and tokenize the input text\n",
    "def tokenize_input_chunks(text, max_length=512, stride=256):\n",
    "    \"\"\"\n",
    "    Tokenizes input text by chunking each chunk.\n",
    "    \"\"\"\n",
    "    # Chunk the text\n",
    "    chunks = chunk_text(text, max_length, stride)\n",
    "    tokenized_chunks = []\n",
    "\n",
    "    for chunk in chunks:\n",
    "        # Tokenize each chunk\n",
    "        input_ids = tokenizer.encode(chunk, add_special_tokens=True, truncation=True, max_length=max_length)\n",
    "        tokenized_chunks.append(input_ids)\n",
    "\n",
    "    return tokenized_chunks\n",
    "\n",
    "# Apply chunking and tokenization to 'Preprocessed_Body' column\n",
    "df['Tokenized_Input_Chunks'] = df['Preprocessed_Body'].apply(tokenize_input_chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "bb431a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Splitting the DataFrame into training and evaluation sets\n",
    "train_df, eval_df = train_test_split(df, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8b988d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\models\\t5\\tokenization_t5.py:303: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n",
      "  warnings.warn(\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\models\\t5\\tokenization_t5.py:303: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the input for training and evaluation data\n",
    "train_df['Tokenized_Input_Chunks'] = train_df['Preprocessed_Body'].apply(lambda x: tokenize_input_chunks(x))\n",
    "eval_df['Tokenized_Input_Chunks'] = eval_df['Preprocessed_Body'].apply(lambda x: tokenize_input_chunks(x))\n",
    "\n",
    "# Tokenize the summaries for training and evaluation data\n",
    "max_length_summary = 150  # Set the maximum length for the summary\n",
    "train_df['Tokenized_Summary'] = train_df['Summary_human'].apply(lambda x: tokenizer.encode(x, add_special_tokens=True, truncation=True, max_length=max_length_summary))\n",
    "eval_df['Tokenized_Summary'] = eval_df['Summary_human'].apply(lambda x: tokenizer.encode(x, add_special_tokens=True, truncation=True, max_length=max_length_summary))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0ecc3319",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmailSummarizationDataset(Dataset):\n",
    "    def __init__(self, tokenized_input_chunks, tokenized_summaries):\n",
    "        self.tokenized_input_chunks = tokenized_input_chunks\n",
    "        self.tokenized_summaries = tokenized_summaries\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokenized_input_chunks)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': torch.tensor(self.tokenized_input_chunks[idx], dtype=torch.long),\n",
    "            'labels': torch.tensor(self.tokenized_summaries[idx], dtype=torch.long)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "52b50dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the tokenized data for training and evaluation\n",
    "flat_input_chunks_train = [chunk for sublist in train_df['Tokenized_Input_Chunks'] for chunk in sublist]\n",
    "flat_summaries_train = train_df['Tokenized_Summary'].explode().tolist()\n",
    "\n",
    "flat_input_chunks_eval = [chunk for sublist in eval_df['Tokenized_Input_Chunks'] for chunk in sublist]\n",
    "flat_summaries_eval = eval_df['Tokenized_Summary'].explode().tolist()\n",
    "\n",
    "# Create the dataset objects\n",
    "train_dataset = EmailSummarizationDataset(flat_input_chunks_train, flat_summaries_train)\n",
    "eval_dataset = EmailSummarizationDataset(flat_input_chunks_eval, flat_summaries_eval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc65278e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## The number of chunks and the target summary would mismatch, To comphensate this, the summary targets will be made as many time as the summary chunks"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f8e95bdd",
   "metadata": {},
   "source": [
    "# Flatten the list of lists for input chunks and repeat summaries for each chunk\n",
    "flat_input_chunks = []\n",
    "flat_target_summaries = []\n",
    "\n",
    "for i in range(len(df)):\n",
    "    num_chunks = len(df.iloc[i]['Tokenized_Input_Chunks'])\n",
    "    flat_input_chunks.extend(df.iloc[i]['Tokenized_Input_Chunks'])\n",
    "    flat_target_summaries.extend([df.iloc[i]['Tokenized_Summary']] * num_chunks)\n",
    "\n",
    "# Train-test split\n",
    "input_train, input_test, summary_train, summary_test = train_test_split(flat_input_chunks, flat_target_summaries, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "02664e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "## dataset class"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6196541f",
   "metadata": {},
   "source": [
    "class EvaluationDataset(Dataset):\n",
    "    def __init__(self, tokenized_input_chunks, tokenized_summaries):\n",
    "        self.tokenized_input_chunks = tokenized_input_chunks\n",
    "        self.tokenized_summaries = tokenized_summaries\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokenized_input_chunks)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': torch.tensor(self.tokenized_input_chunks[idx], dtype=torch.long),\n",
    "            'labels': torch.tensor(self.tokenized_summaries[idx], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Flatten the tokenized data\n",
    "flat_input_chunks_eval = [chunk for sublist in eval_df['Tokenized_Input_Chunks'] for chunk in sublist]\n",
    "flat_summaries_eval = eval_df['Tokenized_Summary'].explode().tolist()\n",
    "\n",
    "# Create the evaluation dataset\n",
    "eval_dataset = EvaluationDataset(flat_input_chunks_eval, flat_summaries_eval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0347f7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## creating data objects for train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebee82dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating evaluation callback function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8ef2a92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback\n",
    "import numpy as np\n",
    "from rouge_score import rouge_scorer, scoring\n",
    "from bert_score import score\n",
    "import torch\n",
    "\n",
    "class CustomEvaluationCallback(TrainerCallback):\n",
    "    def __init__(self, test_dataset, tokenizer, device):\n",
    "        self.test_dataset = eval_dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "\n",
    "    def on_evaluate(self, args, state, control, **kwargs):\n",
    "        model = kwargs[\"model\"]\n",
    "        predictions, references = [], []\n",
    "\n",
    "        # Iterate through the test dataset\n",
    "        for i in range(len(self.test_dataset)):\n",
    "            batch = self.test_dataset[i]\n",
    "            inputs = batch['input_ids'].to(self.device).unsqueeze(0)  # Add batch dimension\n",
    "            attention_mask = torch.tensor([1] * len(batch['input_ids']), dtype=torch.long).to(self.device).unsqueeze(0)  # Create attention mask\n",
    "\n",
    "            outputs = model.generate(inputs, attention_mask=attention_mask, max_length=120)\n",
    "            batch_predictions = [self.tokenizer.decode(o, skip_special_tokens=True) for o in outputs]\n",
    "\n",
    "            predictions.append(batch_predictions[0])\n",
    "            references.append(self.tokenizer.decode(batch['labels'], skip_special_tokens=True))\n",
    "\n",
    "        # Compute ROUGE and BERTScore\n",
    "        rouge_scores = self.compute_rouge(references, predictions)\n",
    "        bert_scores = self.compute_bertScore(references, predictions)\n",
    "\n",
    "        # Print formatted scores\n",
    "        self.print_formatted_scores(rouge_scores, bert_scores, state.epoch)\n",
    "\n",
    "        return control\n",
    "\n",
    "    def compute_rouge(self, targets, predictions, score_keys=None, use_stemmer=True):\n",
    "        if score_keys is None:\n",
    "            score_keys = [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]\n",
    "        scorer = rouge_scorer.RougeScorer(score_keys, use_stemmer=use_stemmer)\n",
    "        aggregator = scoring.BootstrapAggregator()\n",
    "\n",
    "        for target, prediction in zip(targets, predictions):\n",
    "            aggregator.add_scores(scorer.score(target=target, prediction=prediction))\n",
    "        result = aggregator.aggregate()\n",
    "\n",
    "        return {key: result[key].mid.fmeasure * 100 for key in score_keys}\n",
    "\n",
    "    def compute_bertScore(self, refs, cands, rescale_with_baseline=True):\n",
    "        P, R, F1 = score(cands, refs, lang=\"en\", rescale_with_baseline=rescale_with_baseline)\n",
    "        return {\"bertScore\": F1.mean().item() * 100}\n",
    "\n",
    "    def print_formatted_scores(self, rouge_scores, bert_scores, epoch):\n",
    "    # Headers for the scores\n",
    "        headers = [\"Epoch\"] + list(rouge_scores.keys()) + [\"bertScore\"]\n",
    "    \n",
    "    # Values for the scores\n",
    "        values = [f\"{epoch}\"] + [f\"{score:.2f}\" for score in rouge_scores.values()] + [f\"{bert_scores['bertScore']:.2f}\"]\n",
    "\n",
    "    # Calculate the max width for each column\n",
    "        column_widths = [max(len(header), len(value)) + 2 for header, value in zip(headers, values)]\n",
    "\n",
    "    # Print the header\n",
    "        header_row = \" | \".join(f\"{header:<{column_widths[idx]}}\" for idx, header in enumerate(headers))\n",
    "        print(header_row)\n",
    "        print(\"-\" * len(header_row))\n",
    "\n",
    "    # Print the scores\n",
    "        score_row = \" | \".join(f\"{value:<{column_widths[idx]}}\" for idx, value in enumerate(values))\n",
    "        print(score_row)\n",
    "        print(\"\\n\" + \"-\" * len(header_row))\n",
    "\n",
    "# Usage\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "custom_eval_callback = CustomEvaluationCallback(test_dataset, tokenizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b4c2168c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "adfbaab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "329f4624",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='410' max='410' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [410/410 57:01, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>19.219600</td>\n",
       "      <td>21.450796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>16.918900</td>\n",
       "      <td>15.774423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>12.706000</td>\n",
       "      <td>10.947338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>10.559600</td>\n",
       "      <td>9.296342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>9.181600</td>\n",
       "      <td>8.211372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>8.351800</td>\n",
       "      <td>7.712661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>7.977000</td>\n",
       "      <td>7.537162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>7.764800</td>\n",
       "      <td>7.470317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>7.698400</td>\n",
       "      <td>7.416632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>7.561300</td>\n",
       "      <td>7.365152</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "1.0     | 0.42     | 0.00     | 0.41     | 0.42        | -63.02     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "2.0     | 0.39     | 0.00     | 0.40     | 0.40        | -63.24     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "3.0     | 0.39     | 0.00     | 0.39     | 0.39        | -68.52     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "4.0     | 0.39     | 0.00     | 0.39     | 0.40        | -67.78     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "5.0     | 0.31     | 0.00     | 0.32     | 0.31        | -73.22     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "6.0     | 0.27     | 0.00     | 0.27     | 0.28        | -79.91     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "7.0     | 0.15     | 0.00     | 0.15     | 0.14        | -79.17     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "8.0     | 0.15     | 0.00     | 0.15     | 0.15        | -72.86     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "9.0     | 0.13     | 0.00     | 0.13     | 0.13        | -69.61     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "10.0    | 0.20     | 0.00     | 0.20     | 0.20        | -63.87     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0oAAAHWCAYAAABJ4Xn8AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAB65klEQVR4nO3dd1yVZR/H8c9hiCCCW0Fwmyu3Vmqaljkyy5VpZlqWmZMcOcptmqa5crasTC0HpuVIy92yFNNyVbg1ywHiQIXz/HE9ICAo4xzO4fB9v17nxX3uc5/7/h3C5+HLdd2/y2K1Wq2IiIiIiIhIPDdHFyAiIiIiIuJsFJRERERERESSUFASERERERFJQkFJREREREQkCQUlERERERGRJBSUREREREREklBQEhERERERSUJBSUREREREJAkFJRERERERkSQUlEREnEjXrl0pUaJEut47atQoLBaLbQtyMkeOHMFisbBgwYJMv7bFYmHUqFHxzxcsWIDFYuHIkSN3fW+JEiXo2rWrTevJyM9KRjjyv4GISGZSUBIRSQWLxZKqx+bNmx1darbXt29fLBYLf/75Z4rHvP7661gsFn777bdMrCztTp06xahRowgLC3N0KSIi2Y6HowsQEckKPv3000TPP/nkEzZs2HDb/goVKmToOu+99x6xsbHpeu8bb7zBkCFDMnR9V9CpUydmzpzJokWLGDFiRLLHLF68mMqVK1OlSpV0X6dz58506NABLy+vdJ/jbk6dOsXo0aMpUaIE1apVS/RaRn5WRETk7hSURERS4dlnn030/Mcff2TDhg237U/qypUr+Pj4pPo6np6e6aoPwMPDAw8P/c/6/fffT5kyZVi8eHGyQemHH34gPDyct956K0PXcXd3x93dPUPnyIiM/KyIiMjdaeqdiIiNNGzYkHvvvZdff/2VBg0a4OPjw7BhwwD48ssvadGiBYGBgXh5eVG6dGnGjh1LTExMonMkve8k7n6QyZMnM3/+fEqXLo2Xlxe1a9dm586did6b3D1KFouF3r17s3LlSu699168vLyoVKkS69atu63+zZs3U6tWLXLmzEnp0qWZN29equ972rZtG0899RTFihXDy8uL4OBgXn31Va5evXrb5/P19eXkyZO0atUKX19fChYsyMCBA2/7Xly8eJGuXbvi7+9Pnjx56NKlCxcvXrxrLWBGlQ4cOMCuXbtue23RokVYLBY6duzI9evXGTFiBDVr1sTf359cuXJRv359Nm3adNdrJHePktVqZdy4cQQFBeHj40OjRo34/fffb3vv+fPnGThwIJUrV8bX1xc/Pz+aN2/Onj174o/ZvHkztWvXBuD555+Pn94Zd29QcvcoXb58mQEDBhAcHIyXlxflypVj8uTJWK3WRMel5ecitb777jvq169Prly5yJMnD08++ST79+9PdMylS5cICQmhRIkSeHl5UahQIR599NFE/50OHz5M27ZtKVKkCDlz5iQoKIgOHToQERGR7tpERNJDf3oUEbGhc+fO0bx5czp06MCzzz5L4cKFAfNLta+vL/3798fX15fvvvuOESNGEBkZydtvv33X8y5atIhLly7x8ssvY7FYmDRpEm3atOHvv/++68jC9u3bWbFiBT179iR37tzMmDGDtm3bcuzYMfLnzw/A7t27adasGQEBAYwePZqYmBjGjBlDwYIFU/W5ly5dypUrV3jllVfInz8/P//8MzNnzuTEiRMsXbo00bExMTE0bdqU+++/n8mTJ7Nx40amTJlC6dKleeWVVwATOJ588km2b99Ojx49qFChAqGhoXTp0iVV9XTq1InRo0ezaNEiatSokejaX3zxBfXr16dYsWL8999/vP/++3Ts2JGXXnqJS5cu8cEHH9C0aVN+/vnn26a73c2IESMYN24cjz32GI899hi7du2iSZMmXL9+PdFxf//9NytXruSpp56iZMmS/PPPP8ybN4+HHnqIP/74g8DAQCpUqMCYMWMYMWIE3bt3p379+gDUrVs32WtbrVaeeOIJNm3aRLdu3ahWrRrr169n0KBBnDx5kqlTpyY6PjU/F6m1ceNGmjdvTqlSpRg1ahRXr15l5syZ1KtXj127dsUHuh49erBs2TJ69+5NxYoVOXfuHNu3b2f//v3UqFGD69ev07RpU6Kjo+nTpw9FihTh5MmTfPXVV1y8eBF/f/801SUikiFWERFJs169elmT/k/oQw89ZAWsc+fOve34K1eu3Lbv5Zdftvr4+FivXbsWv69Lly7W4sWLxz8PDw+3Atb8+fNbz58/H7//yy+/tALW1atXx+8bOXLkbTUB1hw5clj//PPP+H179uyxAtaZM2fG72vZsqXVx8fHevLkyfh9hw8ftnp4eNx2zuQk9/kmTJhgtVgs1qNHjyb6fIB1zJgxiY6tXr26tWbNmvHPV65caQWskyZNit938+ZNa/369a2A9aOPPrprTbVr17YGBQVZY2Ji4vetW7fOCljnzZsXf87o6OhE77tw4YK1cOHC1hdeeCHRfsA6cuTI+OcfffSRFbCGh4dbrVar9ezZs9YcOXJYW7RoYY2NjY0/btiwYVbA2qVLl/h9165dS1SX1Wr+W3t5eSX63uzcuTPFz5v0ZyXuezZu3LhEx7Vr185qsVgS/Qyk9uciOXE/kwlrqlatmrVQoULWc+fOJTqfm5ub9bnnnovf5+/vb+3Vq1eK5969e7cVsC5duvSONYiIZAZNvRMRsSEvLy+ef/752/Z7e3vHb1+6dIn//vuP+vXrc+XKFQ4cOHDX8z799NPkzZs3/nnc6MLff/991/c2btyY0qVLxz+vUqUKfn5+8e+NiYlh48aNtGrVisDAwPjjypQpQ/Pmze96fkj8+S5fvsx///1H3bp1sVqt7N69+7bje/Tokeh5/fr1E32WNWvW4OHhET/CBOaeoD59+qSqHjD3lZ04cYKtW7fG71u0aBE5cuTgqaeeij9njhw5AIiNjeX8+fPcvHmTWrVqJTtt7042btzI9evX6dOnT6LpiiEhIbcd6+XlhZub+b/gmJgYzp07h6+vL+XKlUvzdeOsWbMGd3d3+vbtm2j/gAEDsFqtrF27NtH+u/1cpNbp06cJCwuja9eu5MuXL9H5Hn30UdasWRO/L0+ePPz000+cOnUq2XPFjRitX7+eK1eupKkOERFbU1ASEbGhokWLxv/indDvv/9O69at8ff3x8/Pj4IFC8Y3gkjNvRfFihVL9DwuNF24cCHN7417f9x7z549y9WrVylTpsxtxyW3LznHjh2L/0U57r6jhx56CLj98+XMmfO2KX0J6wE4evQoAQEB+Pr6JjquXLlyqaoHoEOHDri7u7No0SIArl27RmhoKM2bN08UOj/++GOqVKlCzpw5yZ8/PwULFuTrr79O8z0xR48eBaBs2bKJ9hcsWDDR9cCEsqlTp1K2bFm8vLwoUKAABQsW5Lfffkv3vThHjx4lMDCQ3LlzJ9of14kxrr44d/u5SMt1Ifn/NhUqVOC///7j8uXLAEyaNIl9+/YRHBzMfffdx6hRoxIFs5IlS9K/f3/ef/99ChQoQNOmTZk1a5buTxIRh1BQEhGxoYQjK3EuXrzIQw89xJ49exgzZgyrV69mw4YNTJw4ESBVLZ5T6q5mTXKTvq3fmxoxMTE8+uijfP311wwePJiVK1eyYcOG+KYDST9fZnWKi2sUsHz5cm7cuMHq1au5dOkSnTp1ij9m4cKFdO3aldKlS/PBBx+wbt06NmzYwMMPP2zX1tvjx4+nf//+NGjQgIULF7J+/Xo2bNhApUqVMq3lt71/LpLTvn17/v77b2bOnElgYCBvv/02lSpVSjTaNWXKFH777TeGDRvG1atX6du3L5UqVeLEiRN2q0tEJDlq5iAiYmebN2/m3LlzrFixggYNGsTvDw8Pd2BVtxQqVIicOXMmu0DrnRZtjbN3714OHTrExx9/zHPPPRe/f8OGDemuqXjx4nz77bdERUUlGlU6ePBgms7TqVMn1q1bx9q1a1m0aBF+fn60bNky/vVly5ZRqlQpVqxYkWi63MiRI9NVM5iubaVKlYrf/++//942SrNs2TIaNWrEBx98kGj/xYsXKVCgQPzz1HQcTHj9jRs3cunSpUSjSnFTO+Pqs7W48yb33+bAgQMUKFCAXLlyxe8LCAigZ8+e9OzZk7Nnz1KjRg3efPPNRNM8K1euTOXKlXnjjTf4/vvvqVevHnPnzmXcuHF2+QwiIsnRiJKIiJ3F/eU+4V/qr1+/zuzZsx1VUiLu7u40btyYlStXJrp35M8//7ztvpaU3g+JP5/VamX69Onprumxxx7j5s2bzJkzJ35fTEwMM2fOTNN5WrVqhY+PD7Nnz2bt2rW0adOGnDlz3rH2n376iR9++CHNNTdu3BhPT09mzpyZ6HzTpk277Vh3d/fbRm6WLl3KyZMnE+2LCxipaYv+2GOPERMTw7vvvpto/9SpU7FYLKm+3yytAgICqFatGh9//HGiOvft28c333zDY489Bpj/fkmn0BUqVIjAwECio6MBiIyM5ObNm4mOqVy5Mm5ubvHHiIhkFo0oiYjYWd26dcmbNy9dunShb9++WCwWPv30U7tOcUqrUaNG8c0331CvXj1eeeWV+F+47733XsLCwu743vLly1O6dGkGDhzIyZMn8fPzY/ny5Wm+1yWhli1bUq9ePYYMGcKRI0eoWLEiK1asSPO9Kr6+vrRq1Sr+PqWE0+4AHn/8cVasWEHr1q1p0aIF4eHhzJ07l4oVKxIVFZWma8WtBzVhwgQef/xxHnvsMXbv3s3atWsTjRLFXXfMmDE8//zz1K1bl7179/LZZ58lGokCKF26NHny5GHu3Lnkzp2bXLlycf/991OyZMnbrt+yZUsaNWrE66+/zpEjR6hatSrffPMNX375JSEhIYkaN9ja22+/TfPmzalTpw7dunWLbw/u7+/PqFGjANPEJCgoiHbt2lG1alV8fX3ZuHEjO3fuZMqUKYBZi6l379489dRT3HPPPdy8eZNPP/0Ud3d32rZta7f6RUSSoxElERE7y58/P1999RUBAQG88cYbTJ48mUcffZRJkyY5urR4NWvWZO3ateTNm5fhw4fzwQcfMGbMGB555JFEIzDJ8fT0ZPXq1VSrVo0JEyYwevRoypYtyyeffJLuetzc3Fi1ahWdOnVi4cKFvP766xQtWpSPP/44zeeKC0cBAQE8/PDDiV7r2rUr48ePZ8+ePfTt25f169ezcOFCatWqla66x40bx+jRo9m9ezeDBg3ir7/+4ptvvkk09Qxg2LBhDBgwgPXr19OvXz927drF119/TXBwcKLjPD09+fjjj3F3d6dHjx507NiRLVu2JHvtuO9ZSEgIX331FSEhIfzxxx+8/fbbvPPOO+n6PKnVuHFj1q1bR/78+RkxYgSTJ0/mgQceYMeOHfGhzsfHh549exIWFsbIkSN59dVXOXjwILNnz6Z///4AVK1alaZNm7J69Wr69+/PqFGj8PX1Ze3atTzwwAN2/QwiIklZrM70J00REXEqrVq14vfff+fw4cOOLkVERCRTaURJREQAuHr1aqLnhw8fZs2aNTRs2NAxBYmIiDiQRpRERAQwU9O6du1KqVKlOHr0KHPmzCE6Oprdu3fftjaQiIiIq1MzBxERAaBZs2YsXryYM2fO4OXlRZ06dRg/frxCkoiIZEsaURIREREREUlC9yiJiIiIiIgkoaAkIiIiIiKShMvfoxQbG8upU6fInTs3FovF0eWIiIiIiIiDWK1WLl26RGBgIG5udxkzsjrQ+PHjrbVq1bL6+vpaCxYsaH3yySetBw4ciH/93Llz1t69e1vvuecea86cOa3BwcHWPn36WC9evJjqaxw/ftwK6KGHHnrooYceeuihhx56WAHr8ePH75ojHDqitGXLFnr16kXt2rW5efMmw4YNo0mTJvzxxx/kypWLU6dOcerUKSZPnkzFihU5evQoPXr04NSpUyxbtixV18idOzcAx48fx8/Pz54fR0REREREnFhkZCTBwcHxGeFOnKrr3b///kuhQoXYsmULDRo0SPaYpUuX8uyzz3L58mU8PO6e8yIjI/H39yciIkJBSUREREQkG0tLNnCqe5QiIiIAyJcv3x2P8fPzSzEkRUdHEx0dHf88MjLStkWKiIiIiIjLc5qud7GxsYSEhFCvXj3uvffeZI/577//GDt2LN27d0/xPBMmTMDf3z/+ERwcbK+SRURERETERTnN1LtXXnmFtWvXsn37doKCgm57PTIykkcffZR8+fKxatUqPD09kz1PciNKwcHBmnonIiIiIpLNZbmpd7179+arr75i69atyYakS5cu0axZM3Lnzk1oaGiKIQnAy8sLLy8ve5YrIiIiIjZktVq5efMmMTExji5Fsjh3d3c8PDxssiyQQ4OS1WqlT58+hIaGsnnzZkqWLHnbMZGRkTRt2hQvLy9WrVpFzpw5HVCpiIiIiNjD9evXOX36NFeuXHF0KeIifHx8CAgIIEeOHBk6j0ODUq9evVi0aBFffvkluXPn5syZMwD4+/vj7e1NZGQkTZo04cqVKyxcuJDIyMj45gwFCxbE3d3dkeWLiIiISAbExsYSHh6Ou7s7gYGB5MiRwyYjAZI9Wa1Wrl+/zr///kt4eDhly5a9+6Kyd+DQe5RS+ofw0Ucf0bVrVzZv3kyjRo2SPSY8PJwSJUrc9RpqDy4iIiLinK5du0Z4eDjFixfHx8fH0eWIi7hy5QpHjx6lZMmSt81GyzL3KN0tozVs2PCux4iIiIhI1paRv/qLJGWrnyf9VIqIiIiIiCThFF3vsoWYGNi2DU6fhoAAqF8fdI+ViIiIiIhT0ohSZlixAkqUgEaN4JlnzNcSJcx+EREREcmwmBjYvBkWLzZfs2Kn8RIlSjBt2rRUH79582YsFgsXL160W00ACxYsIE+ePHa9hjNSULK3FSugXTs4cSLx/pMnzX6FJREREZEMyey/SVssljs+Ro0ala7z7ty5k+7du6f6+Lp163L69Gn8/f3TdT25M029s6eYGOjXD5JrSGG1gsUCISHw5JOahiciIiKSDnF/k07661bc36SXLYM2bWx7zdOnT8dvf/7554wYMYKDBw/G7/P19Y3ftlqtxMTE4OFx91+7CxYsmKY6cuTIQZEiRdL0Hkk9jSjZ07Ztt48kJWS1wvHj5jgRERERwWqFy5dT94iMhL59U/6bNJi/WUdGpu58qW22XKRIkfiHv78/Fosl/vmBAwfInTs3a9eupWbNmnh5ebF9+3b++usvnnzySQoXLoyvry+1a9dm48aNic6bdOqdxWLh/fffp3Xr1vj4+FC2bFlWrVoV/3rSqXdxU+TWr19PhQoV8PX1pVmzZomC3c2bN+nbty958uQhf/78DB48mC5dutCqVavUffj/mzNnDqVLlyZHjhyUK1eOTz/9NMH33sqoUaMoVqwYXl5eBAYG0rdv3/jXZ8+eTdmyZcmZMyeFCxemXbt2abp2ZlFQsqcEP5Q2OU5ERETExV25Ar6+qXv4+5uRo5RYreZv1v7+qTvflSu2+xxDhgzhrbfeYv/+/VSpUoWoqCgee+wxvv32W3bv3k2zZs1o2bIlx44du+N5Ro8eTfv27fntt9947LHH6NSpE+fPn0/x+CtXrjB58mQ+/fRTtm7dyrFjxxg4cGD86xMnTuSzzz7jo48+YseOHURGRrJy5co0fbbQ0FD69evHgAED2LdvHy+//DLPP/88mzZtAmD58uVMnTqVefPmcfjwYVauXEnlypUB+OWXX+jbty9jxozh4MGDrFu3jgYNGqTp+plFU+/sKSDAtseJiIiISJYwZswYHn300fjn+fLlo2rVqvHPx44dS2hoKKtWraJ3794pnqdr16507NgRgPHjxzNjxgx+/vlnmjVrluzxN27cYO7cuZQuXRqA3r17M2bMmPjXZ86cydChQ2ndujUA7777LmvWrEnTZ5s8eTJdu3alZ8+eAPTv358ff/yRyZMn06hRI44dO0aRIkVo3Lgxnp6eFCtWjPvuuw+AY8eOkStXLh5//HFy585N8eLFqV69epqun1k0omRP9etDUJC5Fyk5FgsEB5vjRERERAQfH4iKSt0jtb/fr1mTuvP5+Njuc9SqVSvR86ioKAYOHEiFChXIkycPvr6+7N+//64jSlWqVInfzpUrF35+fpw9ezbF4318fOJDEkBAQED88REREfzzzz/xoQXA3d2dmjVrpumz7d+/n3r16iXaV69ePfbv3w/AU089xdWrVylVqhQvvfQSoaGh3Lx5E4BHH32U4sWLU6pUKTp37sxnn33GFVsO5dmQgpI9ubvD9OlmO7mwZLXCtGlq5CAiIiLyfxYL5MqVukeTJqn7m3STJqk7X0rnSY9cuXIlej5w4EBCQ0MZP34827ZtIywsjMqVK3P9+vU7nsfT0zPJZ7IQGxubpuOtqb35ykaCg4M5ePAgs2fPxtvbm549e9KgQQNu3LhB7ty52bVrF4sXLyYgIIARI0ZQtWpVu7c4Tw8FJXtr08a0Wyla9PbXihc3He9EREREJM3u9DfpuOfO8jfpHTt20LVrV1q3bk3lypUpUqQIR44cydQa/P39KVy4MDt37ozfFxMTw65du9J0ngoVKrBjx45E+3bs2EHFihXjn3t7e9OyZUtmzJjB5s2b+eGHH9i7dy8AHh4eNG7cmEmTJvHbb79x5MgRvvvuuwx8MvvQPUqZoU0bE4i2bTONG3x84Lnn4OhR+PRT6NrV0RWKiIiIZElxf5Pu1y9xs+GgIBOSbN0aPL3Kli3LihUraNmyJRaLheHDh99xZMhe+vTpw4QJEyhTpgzly5dn5syZXLhwAUsahtMGDRpE+/btqV69Oo0bN2b16tWsWLEivovfggULiImJ4f7778fHx4eFCxfi7e1N8eLF+eqrr/j7779p0KABefPmZc2aNcTGxlKuXDl7feR0U1DKLO7u0LDhrefDh8OgQfD669C+vW0nxYqIiIhkI0n/Jh0QYG4Bd4aRpDjvvPMOL7zwAnXr1qVAgQIMHjyYyMjITK9j8ODBnDlzhueeew53d3e6d+9O06ZNcU/DN6tVq1ZMnz6dyZMn069fP0qWLMlHH31Ew///rpsnTx7eeust+vfvT0xMDJUrV2b16tXkz5+fPHnysGLFCkaNGsW1a9coW7YsixcvplKlSnb6xOlnsWb2pMVMFhkZib+/PxEREfj5+Tm6nFuuXYMKFeDIERg7Ft54w9EViYiIiGSqa9euER4eTsmSJcmZM6ejy8mWYmNjqVChAu3bt2fs2LGOLscm7vRzlZZsoHuUHCVnTpgwwWy/9RacOePYekRERETE5R09epT33nuPQ4cOsXfvXl555RXCw8N55plnHF2a01FQcqSnn4b77jNLQY8a5ehqRERERMTFubm5sWDBAmrXrk29evXYu3cvGzdupEKFCo4uzenoHiVHslhg8mRo0ADeew/69oUE3UJERERERGwpODj4to51kjyNKDla/frQujXExsJrrzm6GhERERERQUHJOUycCB4e8PXX8O23jq5GRERERCTbU1ByBmXLQs+eZnvgQDO6JCIiIiIiDqOg5CyGDwd/fwgLM4vQioiIiIiIwygoOYsCBczis2C+Xrni2HpERERERLIxBSVn0qcPFC8OJ0/C1KmOrkZEREREJNtSUHImSReh/ecfx9YjIiIiklXExMDmzbB4sfkaE+Poiu6qYcOGhISExD8vUaIE06ZNu+N7LBYLK1euzPC1bXWeOxk1ahTVqlWz6zXsSUHJ2Tz9NNSuDVFRWoRWREREJDVWrIASJaBRI3jmGfO1RAmz3w5atmxJs2bNkn1t27ZtWCwWfvvttzSfd+fOnXTv3j2j5SWSUlg5ffo0zZs3t+m1XI2CkrNxczOL0IJZhPaPPxxbj4iIiIgzW7EC2rWDEycS7z950uy3Q1jq1q0bGzZs4ETSawIfffQRtWrVokqVKmk+b8GCBfHx8bFFiXdVpEgRvLy8MuVaWZWCkjNq0ABatTJDxoMHO7oaERERkcxjtcLly6l7REZC377mPcmdB6BfP3Ncas6X3HmS8fjjj1OwYEEWLFiQaH9UVBRLly6lW7dunDt3jo4dO1K0aFF8fHyoXLkyixcvvuN5k069O3z4MA0aNCBnzpxUrFiRDRs23PaewYMHc8899+Dj40OpUqUYPnw4N27cAGDBggWMHj2aPXv2YLFYsFgs8TUnnXq3d+9eHn74Yby9vcmfPz/du3cnKioq/vWuXbvSqlUrJk+eTEBAAPnz56dXr17x10qN2NhYxowZQ1BQEF5eXlSrVo1169bFv379+nV69+5NQEAAOXPmpHjx4kz4/20pVquVUaNGUaxYMby8vAgMDKRv376pvnZ6eNj17JJ+EyfCV1+Zx3ffwcMPO7oiEREREfu7cgV8fW1zLqvVjDT5+6fu+KgoyJXrrod5eHjw3HPPsWDBAl5//XUsFgsAS5cuJSYmho4dOxIVFUXNmjUZPHgwfn5+fP3113Tu3JnSpUtz33333fUasbGxtGnThsKFC/PTTz8RERGR6H6mOLlz52bBggUEBgayd+9eXnrpJXLnzs1rr73G008/zb59+1i3bh0bN24EwD+Z78Xly5dp2rQpderUYefOnZw9e5YXX3yR3r17JwqDmzZtIiAggE2bNvHnn3/y9NNPU61aNV566aW7fh6A6dOnM2XKFObNm0f16tX58MMPeeKJJ/j9998pW7YsM2bMYNWqVXzxxRcUK1aM48ePc/z4cQCWL1/O1KlTWbJkCZUqVeLMmTPs2bMnVddNN6uLi4iIsALWiIgIR5eSdr17W61gtVavbrXGxDi6GhERERGbunr1qvWPP/6wXr169dbOqCjz+48jHlFRqa59//79VsC6adOm+H3169e3Pvvssym+p0WLFtYBAwbEP3/ooYes/fr1i39evHhx69SpU61Wq9W6fv16q4eHh/XkyZPxr69du9YKWENDQ1O8xttvv22tWbNm/PORI0daq1atettxCc8zf/58a968ea1RCT7/119/bXVzc7OeOXPGarVarV26dLEWL17cevPmzfhjnnrqKevTTz+dYi1Jrx0YGGh98803Ex1Tu3Zta8+ePa1Wq9Xap08f68MPP2yNjY297VxTpkyx3nPPPdbr16+neL04yf5c/V9asoGm3jmzESPAzw9274bPPnN0NSIiIiL25+NjRnZS81izJnXnXLMmdedLw/1B5cuXp27dunz44YcA/Pnnn2zbto1u3boBEBMTw9ixY6lcuTL58uXD19eX9evXc+zYsVSdf//+/QQHBxMYGBi/r06dOrcd9/nnn1OvXj2KFCmCr68vb7zxRqqvkfBaVatWJVeC0bR69eoRGxvLwYMH4/dVqlQJd3f3+OcBAQGcPXs2VdeIjIzk1KlT1KtXL9H+evXqsX//fsBM7wsLC6NcuXL07duXb775Jv64p556iqtXr1KqVCleeuklQkNDuXnzZpo+Z1opKDmzggVh2DCzPWwYXL3q2HpERERE7M1iMdPfUvNo0gSCgsx7UjpXcLA5LjXnS+k8KejWrRvLly/n0qVLfPTRR5QuXZqHHnoIgLfffpvp06czePBgNm3aRFhYGE2bNuX69esZ/Q7F++GHH+jUqROPPfYYX331Fbt37+b111+36TUS8vT0TPTcYrEQGxtrs/PXqFGD8PBwxo4dy9WrV2nfvj3t2rUDIDg4mIMHDzJ79my8vb3p2bMnDRo0SNM9UmmloOTs+vWDYsXM/Nq79NUXERERyVbc3WH6dLOdNOTEPZ82zRxnB+3bt8fNzY1FixbxySef8MILL8Tfr7Rjxw6efPJJnn32WapWrUqpUqU4dOhQqs9doUIFjh8/zunTp+P3/fjjj4mO+f777ylevDivv/46tWrVomzZshw9ejTRMTly5CDmLmtKVahQgT179nD58uX4fTt27MDNzY1y5cqluuY78fPzIzAwkB07diTav2PHDipWrJjouKeffpr33nuPzz//nOXLl3P+/HkAvL29admyJTNmzGDz5s388MMP7N271yb1JUdBydklXIR2wgRI5fCmiIiISLbQpg0sWwZFiybeHxRk9rdpY7dL+/r68vTTTzN06FBOnz5N165d418rW7YsGzZs4Pvvv2f//v28/PLL/PPPP6k+d+PGjbnnnnvo0qULe/bsYdu2bbz++uuJjilbtizHjh1jyZIl/PXXX8yYMYPQ0NBEx5QoUYLw8HDCwsL477//iI6Ovu1anTp1ImfOnHTp0oV9+/axadMm+vTpQ+fOnSlcuHDavil3MGjQICZOnMjnn3/OwYMHGTJkCGFhYfTr1w+Ad955h8WLF3PgwAEOHTrE0qVLKVKkCHny5GHBggV88MEH7Nu3j7///puFCxfi7e1N8eLFbVZfUgpKWUGHDlCrFly6pEVoRURERJJq0waOHIFNm2DRIvM1PNyuISlOt27duHDhAk2bNk10P9Ebb7xBjRo1aNq0KQ0bNqRIkSK0atUq1ed1c3MjNDSUq1evct999/Hiiy/y5ptvJjrmiSee4NVXX6V3795Uq1aN77//nuHDhyc6pm3btjRr1oxGjRpRsGDBZFuU+/j4sH79es6fP0/t2rVp164djzzyCO+++27avhl30bdvX/r378+AAQOoXLky69atY9WqVZQtWxYwHfwmTZpErVq1qF27NkeOHGHNmjW4ubmRJ08e3nvvPerVq0eVKlXYuHEjq1evJn/+/DatMSGL1ZrKhvFZVGRkJP7+/kRERODn5+foctJvyxZo2NAMHe/dCxUqOLoiERERkQy5du0a4eHhlCxZkpw5czq6HHERd/q5Sks20IhSVvHQQ/Dkk1qEVkREREQkEygoZSUTJ5oRpdWrzZCyiIiIiIjYhYJSVlKuHPToYbYHDgQbtmMUEREREZFbHBqUJkyYQO3atcmdOzeFChWiVatWiRa1AjPHsFevXuTPnx9fX1/atm2bpo4hLmfkSMidG3btMjcrioiIiIiIzTk0KG3ZsoVevXrx448/smHDBm7cuEGTJk0S9XB/9dVXWb16NUuXLmXLli2cOnWKNpnQwcRpaRFaERERcTEu3ltMMpmtfp6cquvdv//+S6FChdiyZQsNGjQgIiKCggULsmjRovhVeQ8cOECFChX44YcfeOCBB+56TpfpepfQ1atmGt7x42ZtpSFDHF2RiIiISJrFxMRw6NAhChUqZNc2z5K9nDt3jrNnz3LPPffgnmSx4bRkAw97FplWERERAOTLlw+AX3/9lRs3btC4ceP4Y8qXL0+xYsVSDErR0dGJFtKKjIy0c9UO4O0N48dD587ma7duZqRJREREJAtxd3cnT548nD17FjDr+VgsFgdXJVmV1WrlypUrnD17ljx58twWktLKaYJSbGwsISEh1KtXj3vvvReAM2fOkCNHDvLkyZPo2MKFC3PmzJlkzzNhwgRGjx5t73Id75lnYOpUc6/S6NFg4wXBRERERDJDkSJFAOLDkkhG5cmTJ/7nKiOcJij16tWLffv2sX379gydZ+jQofTv3z/+eWRkJMHBwRktz/m4ucGUKdCoEcydC336mOl4IiIiIlmIxWIhICCAQoUKcePGDUeXI1mcp6dnhkeS4jhFUOrduzdfffUVW7duJSgoKH5/kSJFuH79OhcvXkw0qvTPP/+kmBK9vLzw8vKyd8nOoWFDeOIJWLUKXnsNvvzS0RWJiIiIpIu7u7vNfsEVsQWHdr2zWq307t2b0NBQvvvuO0qWLJno9Zo1a+Lp6cm3334bv+/gwYMcO3aMOnXqZHa5ziluEdpVq2DzZkdXIyIiIiLiEhwalHr16sXChQtZtGgRuXPn5syZM5w5c4ar/2957e/vT7du3ejfvz+bNm3i119/5fnnn6dOnTqp6niXLZQvDy+/bLa1CK2IiIiIiE04tD14Sl1NPvroI7p27QqYBWcHDBjA4sWLiY6OpmnTpsyePTvVN2i5ZHvwpM6ehTJl4NIlWLgQOnVydEUiIiIiIk4nLdnAqdZRsodsEZTArKc0bBgUKwYHDpgW4iIiIiIiEi8t2cChU+/EhkJCICgIjh2DGTMcXY2IiIiISJamoOQq4hahBfP1338dW4+IiIiISBamoORKOnWC6tUhMhLGjHF0NSIiIiIiWZaCkitxc4PJk8323Llw8KBj6xERERERyaIUlFzNww/D44/DzZswZIijqxERERERyZIUlFzRpElmEdqVK2HrVkdXIyIiIiKS5SgouaIKFeCll8y2FqEVEREREUkzBSVXNWoU+PrCzp3w+eeOrkZEREREJEtRUMokMTGweTMsXmy+xsTY+YKFC8PQoWZ76FC4ds3OFxQRERERcR0KSplgxQooUQIaNYJnnjFfS5Qw++0qbhHao0e1CK2IiIiISBooKNnZihXQrh2cOJF4/8mTZr9dw5KPD7z5ptl+80347z87XkxERERExHUoKNlRTAz06wdW6+2vxe0LCbHzNLxnn4Vq1bQIrYiIiIhIGigo2dG2bbePJCVktcLx4+Y4u3FzgylTzPacOXDokB0vJiIiIiLiGhSU7Oj0adsel24PPwwtWmgRWhERERGRVFJQsqOAgNQdV7CgfesAzCK0bm4QGmrnISwRERERkaxPQcmO6tc3Tecsljsf9/rrEB5u52IqVry1CO2AAVqEVkRERETkDhSU7MjdHaZPN9tJw1Lccx8f+Pln02/B7uvCJlyE9osv7HwxEREREZGsS0HJztq0gWXLoGjRxPuDgmD5cvjjD6hb1zSl69DBDPpcvmynYooUgcGDzfaQIVqEVkREREQkBRarNbnm1a4jMjISf39/IiIi8PPzc1gdMTHm1qDTp829S/XrmxEnMD0WRo2C8eNNJ7zy5c3oUpUqdijkyhUoWxZOnYK334aBA+1wERERERER55OWbKCg5ES++84se3T6NHh5ma7ePXve/R6nNFuwAJ5/Hvz94a+/IH9+G19ARERERMT5pCUbaOqdE3n4Ydizx3Tyjo6G3r3N1L3z5218oc6dzU1RERFahFZEREREJBkKSk6mYEFYvRqmTYMcOWDlSqha1cYdvd3dYfJksz17Nhw+bMOTi4iIiIhkfQpKTshigX794IcfzO1EJ05Aw4YwerS518kmHnkEHntMi9CKiIiIiCRDQcmJ1agBv/4KXbqYZY9GjTLT806csNEF4hahXbECtm+30UlFRERERLI+BSUnlzu36b2wcKFZAmnrVjMV78svbXDySpXgxRfN9oABpuWeiIiIiIgoKGUVnTrB7t1Qs6Zp7tCqFfTpY4OlkEaPhly5zKq3WoRWRERERARQUMpSypSB7783gz8A774LDzwABw5k4KRJF6GNjs5wnSIiIiIiWZ2CUhaTI4dpWLdmjemQt2ePGWX64IMMzJzr3x8CA+HIEZO+RERERESyOQWlLKp5cxOSGjeGK1fMrUYdO5qlkdIsVy4YN85sjxsH587ZtFYRERERkaxGQSkLCwiA9ethwgSzNNLnn0P16vDTT+k42XPPQZUqcPHirdAkIiIiIpJNKShlcW5u5tai7duhRAkID4cHH4SJE01L8VRLuAjtrFnw55/2KFdEREREJEtQUHIRDzxguuK1b39rDdlmzeDMmTSc5NFHzZtu3IChQ+1Wq4iIiIiIs1NQciF58sCSJfD+++DtDRs2mDWX1q9Pw0neftsMUy1bBjt22KtUERERERGnpqDkYiwW6NYNfv3V3HJ09qwZJBo0CK5fT8UJ7r3XnAC0CK2IiIiIZFsKSi6qQgXT1KFXL/N88mRz79Jff6XizXGL0P70Eyxdatc6RURERESckYKSC8uZ0yyLFBoKefPCzp2mK96iRXd5Y0AAvPaa2dYitCIiIiKSDSkoZQOtWpk1l+rXh0uXoFMneP55iIq6w5sGDDCBKTzcdMETEREREclGFJSyieBg+O47GDnS9GpYsABq1oSwsBTekHAR2rFj4fz5TKpURERERMTxFJSyEQ8PGDXKBKaiReHQIbj/fpgxI4WeDV26QOXKWoRWRERERLIdhwalrVu30rJlSwIDA7FYLKxcuTLR61FRUfTu3ZugoCC8vb2pWLEic+fOdUyxLuShh8xUvCeeMJ3w+vWDJ5+E//5LcmDCRWjffTeVnSBERERERLI+hwaly5cvU7VqVWalcA9M//79WbduHQsXLmT//v2EhITQu3dvVq1alcmVup78+WHlSpg5E7y8YPVqs+bS5s1JDmzSBJo21SK0IiIiIpKtODQoNW/enHHjxtG6detkX//+++/p0qULDRs2pESJEnTv3p2qVavy888/Z3Klrsligd69TRfwcuXg1Cl4+GEYMQJu3kxwYNwitEuXwg8/OKxeEREREZHM4tT3KNWtW5dVq1Zx8uRJrFYrmzZt4tChQzRp0iTF90RHRxMZGZnoIXdWtapZoPaFF8y9SmPHQsOGcOzY/w+oXNm0yQMtQisiIiIi2YJTB6WZM2dSsWJFgoKCyJEjB82aNWPWrFk0aNAgxfdMmDABf3//+EdwcHAmVpx15coFH3wAixdD7tywY4cJUCtW/P+AMWPAx8eMKC1f7tBaRURERETszemD0o8//siqVav49ddfmTJlCr169WLjxo0pvmfo0KFERETEP44fP56JFWd9HTqYluH33Wea3bVtC6+8AlfzBsKgQeagwYNNFwgRERERERdlsVqdYx6VxWIhNDSUVq1aAXD16lX8/f0JDQ2lRYsW8ce9+OKLnDhxgnXr1qXqvJGRkfj7+xMREYGfn589SndJN27AG2/ApEnm+b33whcfRlHhibJw5gy88w68+qpjixQRERERSYO0ZAOnHVG6ceMGN27cwM0tcYnu7u7ExsY6qKrsw9MTJk6E9euhcGHYtw9qPuTLlke1CK2IiIiIuD6HBqWoqCjCwsIICwsDIDw8nLCwMI4dO4afnx8PPfQQgwYNYvPmzYSHh7NgwQI++eSTFLvkie01aWLWXGrSBK5ehYc/7coxv3vhwgV4801HlyciIiIiYhcOnXq3efNmGjVqdNv+Ll26sGDBAs6cOcPQoUP55ptvOH/+PMWLF6d79+68+uqrWCyWVF1DU+9sIzYWpkyBYcPg4ZvrWU8zYj08cTt4AEqVcnR5IiIiIiJ3lZZs4DT3KNmLgpJt7dxpGj7M/rspTfmGPyq3p9zuz3F3d3RlIiIiIiJ35hL3KIlzql0bdu+GTc3fJhYLFfd+Qcj9P3DqlKMrExERERGxHQUlSTM/P5jwdRX+rG8Woe3w60CqVrGyZo2DCxMRERERsREFJUkXiwXuWTyGWG8f6vE9Dc6toEUL6N8foqMdXZ2IiIiISMYoKEn6FS2K26CBAMz1H4wn15k6FerWhcOHHVybiIiIiEgGKChJxgwaBIULUzDiL3a9OIf8+WHXLqhRAz791NHFiYiIiIikj4KSZIyvr1l8Frh3xRh+23KBhx6CqCh47jno3BkuXXJwjSIiIiIiaaSgJBn3/PNQqRKcP0/ggvF8+y2MGQNubrBwoRld+vVXRxcpIiIiIpJ6CkqScR4e8PbbZnvGDNyPhTN8OGzZAsHB8OefUKcOvPOOWbhWRERERMTZKSiJbTRrBo0bw/XrMHQoAA8+CGFh0Lo13LgBAwZAy5Zw9qxjSxURERERuRsFJbENiwUmTzZfP/8cfvwRgHz5YPlymDMHvLxgzRqoWhW+/dbB9YqIiIiI3IGCkthO1arQtavZHjgQrFbAZKcePWDnTqhYEc6cgUcfhWHDzEiTiIiIiIizUVAS2xo7Fry9YccOCA1N9FLlyiYsde9uMtSECdCgARw54phSRURERERSoqAktlW0qBlNAhg82NyzlICPD8ybB198Af7+ZoZetWqwdGnmlyoiIiIikhIFJbG9/y9Cy59/wty5yR7y1FOm0UOdOhARAe3bm5GmK1cyt1QRERERkeQoKInt5c5tFlICGD0aLl5M9rASJUwL8WHDzH1M770HtWvD3r2ZVqmIiIiISLIUlMQ+XnjBdG44fx7Gj0/xME9PePNN2LABAgLgjz/gvvtMl7z/94IgJgY2b4bFi83XmJhM+QQiIiIiko0pKIl9JFyEdvr0u3ZseOQR2LMHHnsMrl2Dnj2hbVv4+GMz8tSoETzzjPlaogSsWGHvDyAiIiIi2ZmCkthP8+YmAV2/bubX3UXBgrB6NbzzjhlpCg013cZPnEh83MmT0K6dwpKIiIiI2I+CkthPwkVoFy+Gn3++61vc3ODVV2H7dnB3T/6YuCl5ISGahiciIiIi9qGgJPZVrRo895zZHjDgVsq5iytX7hyCrFY4fhy2bct4iSIiIiIiSSkoif2NG2cWod2+HVauTNVbTp9O3alTe5yIiIiISFooKIn9BQWZ0SSA1167bRHa5AQEpO7UqT1ORERERCQtFJQkc7z2GhQqZBahnTfvrofXr2/ylcWS/OsWCwQHm+NERERERGxNQUkyRyoXoY3j7m66ikPyYclqhWnTUm74ICIiIiKSEQpKknm6dYMKFeDcOZgw4a6Ht2kDy5ZB0aLJv66OdyIiIiJiLxarNZVtyLKoyMhI/P39iYiIwM/Pz9HlyNdfw+OPg5cXHDhgVo+9i5gY093u9GlzT9LatTBpkhmk2r0bSpe2f9kiIiIikvWlJRsoKEnmslqhcWP47jt45hn47LM0n+LmTWjYEHbsgBo14PvvTe4SEREREbmTtGQDTb2TzJVwEdpFi2DnzjSfwsMDliyB/Plh1y4YNMgOdYqIiIhItqagJJmvenXo3NlsDxyY6kVoEwoKgk8+MdszZ8Ly5TasT0RERESyPQUlcYxx4yBnTti6FVatStcpHnvMdB0H0yfi779tWJ+IiIiIZGsKSuIYwcHQv7/Zfu01uHEjXacZNw7q1oWICHj6aYiOtmGNIiIiIpJtKSiJ4wweDAULwqFDMH9+uk7h6WnuV8qXD375xZxSRERERCSjFJTEcfz8zOKzAKNGmWGhdAgOho8/NtvTp0NoqG3KExEREZHsS0FJHOull6B8efjvv1QtQpuSxx83fSEAXngBwsNtVJ+IiIiIZEsKSuJYHh7w9ttme9o0OHo03acaPx4eeAAuXjT3K12/bpMKRURERCQbUlASx2vRAho1Mp0YXn893aeJu18pb16zPNOQITasUURERESyFQUlcby4RWgBPvvMdGVIp+LFYcECsz11Knz5ZcbLExEREZHsR0FJnEONGhlehDbOE0/c6jzetWuGZvOJiIiISDaloCTOI24R2i1bYPXqDJ1qwgS47z7dryQiIiIi6ePQoLR161ZatmxJYGAgFouFlStX3nbM/v37eeKJJ/D39ydXrlzUrl2bY8eOZX6xYn/FisGrr5rtDCxCC5AjB3z+OeTJAz/9BMOG2aZEEREREckeHBqULl++TNWqVZk1a1ayr//11188+OCDlC9fns2bN/Pbb78xfPhwcubMmcmVSqYZMsQsQnvwILz3XoZOVaIEfPSR2Z4yJcODVCIiIiKSjVis1gzcDGJDFouF0NBQWrVqFb+vQ4cOeHp68umnn6b7vJGRkfj7+xMREYGfn58NKhW7mz0bevWCAgXgzz/B3z9Dp3v1VdN5PG9eCAszA1ciIiIikv2kJRs47T1KsbGxfP3119xzzz00bdqUQoUKcf/99yc7PS+h6OhoIiMjEz0ki3npJShXzixCO3Fihk83cSLUrg0XLkCHDhma0SciIiIi2YTTBqWzZ88SFRXFW2+9RbNmzfjmm29o3bo1bdq0YcuWLSm+b8KECfj7+8c/goODM7FqsQlPT5g0yWxPnQrh4bB5MyxebL7GxKTpdHH3K/n7ww8/wBtv2LxiEREREXExTjv17tSpUxQtWpSOHTuyaNGi+OOeeOIJcuXKxeLFi5M9T3R0NNHR0fHPIyMjCQ4O1tS7rMZqNYvQbtkCPj5w5cqt14KCYPp0aNMmTadcsQLatjXbX31l1rkVERERkezDJabeFShQAA8PDypWrJhof4UKFe7Y9c7Lyws/P79ED8mCLBZ47DGznTAkAZw8Ce3ameSTBm3aQJ8+ZrtLFzh+3AZ1ioiIiIhLctqglCNHDmrXrs3BgwcT7T906BDFixd3UFWSaWJiYObM5F+LGwQNCUnzNLy334aaNeHcOejYUfcriYiIiEjyHBqUoqKiCAsLIywsDIDw8HDCwsLiR4wGDRrE559/znvvvceff/7Ju+++y+rVq+nZs6cDq5ZMsW0bnDiR8utWqxkS2rYtTaf18oIvvgA/P9ixA0aMyGCdIiIiIuKSHBqUfvnlF6pXr0716tUB6N+/P9WrV2fE/397bd26NXPnzmXSpElUrlyZ999/n+XLl/Pggw86smzJDKdP2/a4BEqVgg8+MNtvvQVr16b5FCIiIiLi4pymmYO9aB2lLGrzZtPM4W42bYKGDdN1id69YdYss1zT7t2mR4SIiIiIuC6XaOYg2Vz9+ia5WCzJv26xQHCwOS6dJk+GGjXMck0dO8LNm+k+lYiIiIi4GAUlcU7u7qYFOKQclqZNM8elU86cZn2l3Llh+3YYOTLdpxIRERERF6OgJM6rTRtYtgyKFr39tZYt07yOUnLKlIH33zfbEybA+vUZPqWIiIiIuAAFJXFubdrAkSPmXqRFi2D8eLP/q69g506bXKJ9e3jlFdNIr3NnOHXKJqcVERERkSxMzRwk6+nUyYSme++FX3+FHDkyfMpr16BOHQgLg4cego0bwcMj46WKiIiIiPNQMwdxbdOnQ8GCsG/frRGmDMqZ06yv5OsLW7bA6NE2Oa2IiIiIZFEKSpL1FCgAM2ea7TffhL17bXLasmXhvfdunXbDBpucVkRERESyIAUlyZrat4cnnzQ9vV94wWa9vTt0gJdfNvcrdeqUrvVsRURERMQFKChJ1mSxwOzZ4O8Pv/wCU6fa7NRTp0KVKvDvv/DMMxATY7NTi4iIiEgWoaAkWVdgILzzjtkeMQIOHbLJab29b92vtHkzjBljk9OKiIiISBaioCRZ2/PPw6OPmrZ13bpBbKxNTluuHMybZ7bHjoVvv7XJaUVEREQki1BQkqzNYoH58yFXLti+HebMsdmpn3kGXnzx1v1KZ87Y7NQiIiIi4uQUlCTrK1EC3nrLbA8ZAkeP2uzUM2ZA5crwzz8mLOl+JREREZHsQUFJXEPPnvDggxAVBd27m2EgG4i7XylXLvjuO9M2XERERERcn4KSuAY3N3j/ffDygm++gY8/ttmpy5eHuXPN9qhRsGmTzU4tIiIiIk5KQUlcR7lyMHq02X71VZsugvTss2a5JqvV3Lv0zz82O7WIiIiIOCEFJXEtAwZAzZpw8SL06mWzKXgAM2dCpUqmqcOzz+p+JRERERFXpqAkrsXDAz74wHwNDYVly2x2ah8fWLrUfN24ESZMsNmpRURERMTJKCiJ66laFYYONdu9e8O5czY7dYUKtzqQjxwJW7bY7NQiIiIi4kQUlMQ1vf46VKwIZ89CSIhNT/3cc9C1q1nbtmNHcwkRERERcS0KSuKavLzgww9NN7yFC2HNGpue/t13TQ47fRo6dzahSURERERcR7qC0vHjxzlx4kT8859//pmQkBDmz59vs8JEMuz++033O4CXX4aICJudOlcus76St7fpRh633q2IiIiIuIZ0BaVnnnmGTf9fTObMmTM8+uij/Pzzz7z++uuMGTPGpgWKZMiYMVCmDJw4Aa+9ZtNTV6oEs2aZ7eHDYds2m55eRERERBwoXUFp37593HfffQB88cUX3HvvvXz//fd89tlnLFiwwJb1iWSMj49ZiBZg/nybrxbbtau5Zyk2Fjp0gH//tenpRURERMRB0hWUbty4gZeXFwAbN27kiSeeAKB8+fKctuEinyI28dBD0KOH2X7xRbh82WantljMqFL58nDq1K3QJCIiIiJZW7qCUqVKlZg7dy7btm1jw4YNNGvWDIBTp06RP39+mxYoYhMTJ0JwMPz9t5knZ0O+vmZ9JW9vWLcOJk2y6elFRERExAHSFZQmTpzIvHnzaNiwIR07dqRq1aoArFq1Kn5KnohT8fODefPM9rRp8OOPNj39vffCzJlm+403YPt2m55eRERERDKZxWq1WtPzxpiYGCIjI8mbN2/8viNHjuDj40OhQoVsVmBGRUZG4u/vT0REBH5+fo4uRxztuefg00/NyrG7d5s24jZitZrTL1wIRYtCWBgUKGCz04uIiIhIBqUlG6RrROnq1atER0fHh6SjR48ybdo0Dh486FQhSeQ2U6dCoUKwfz+MG2fTU1ssMGcOlCsHJ09Cly66X0lEREQkq0pXUHryySf55JNPALh48SL3338/U6ZMoVWrVsyZM8emBYrYVP78t3p6v/UW7Nlj09P7+pr1lXLmNGvcTpli09OLiIiISCZJV1DatWsX9evXB2DZsmUULlyYo0eP8sknnzBjxgybFihic+3aQZs2cPMmvPCC+WpDVapA3D+DoUPh++9tenoRERERyQTpCkpXrlwhd+7cAHzzzTe0adMGNzc3HnjgAY4ePWrTAkXsYtYsyJsXdu2CyZNtfvoXX4SOHSEmxqyvdO6czS8hIiIiInaUrqBUpkwZVq5cyfHjx1m/fj1NmjQB4OzZs2qYIFlDkSLmfiWAUaPg4EGbnt5iMU32ypaF48fNwrTpa5siIiIiIo6QrqA0YsQIBg4cSIkSJbjvvvuoU6cOYEaXqlevbtMCRezmueegWTOIjoZu3WzeeSF3brO+kpcXfPUVvPOOTU8vIiIiInaU7vbgZ86c4fTp01StWhU3N5O3fv75Z/z8/ChfvrxNi8wItQeXOzp2DCpVgqgoc2NRnz42v8S8edCjB3h4wLZt8MADNr+EiIiIiKRCWrJBuoNSnBMnTgAQFBSUkdPYjYKS3NWcOdCzJ/j4wL59ULKkTU9vtZr7lT7/HIoVM8s35ctn00uIiIiISCrYfR2l2NhYxowZg7+/P8WLF6d48eLkyZOHsWPHEquFYySrefllaNAArlyB7t1tfjORxQLz50OZMmYA6/nndb+SiIiIiLNLV1B6/fXXeffdd3nrrbfYvXs3u3fvZvz48cycOZPhw4fbukYR+3Jzg/ffN4sfbdwIH35o80v4+Zn1lXLkgFWrYNo0m19CRERERGwoXUHp448/5v333+eVV16hSpUqVKlShZ49e/Lee++xYMGCVJ9n69attGzZksDAQCwWCytXrkzx2B49emCxWJim3zDFHsqWhbFjzfaAAXDqlM0vUb36rUZ7gwfDzz/b/BIiIiIiYiPpCkrnz59PtmFD+fLlOX/+fKrPc/nyZapWrcqsWbPueFxoaCg//vgjgYGBaa5VJNVCQqB2bYiIgFdescv8uFdegaeeghs3oH17uHDB5pcQERERERtIV1CqWrUq77777m373333XapUqZLq8zRv3pxx48bRunXrFI85efIkffr04bPPPsPT0zM95YqkjoeHmXbn6Wnmx33+uc0vYbHAe+9BqVJw9Ci88ILuVxIRERFxRh7pedOkSZNo0aIFGzdujF9D6YcffuD48eOsWbPGZsXFxsbSuXNnBg0aRKVKlVL1nujoaKKjo+OfR0ZG2qweyQbuvRdef90sQtunDzzyCBQsaNNL+Pub+5Xq1oWVK01X8n79bHoJEREREcmgdI0oPfTQQxw6dIjWrVtz8eJFLl68SJs2bfj999/59NNPbVbcxIkT8fDwoG/fvql+z4QJE/D3949/BAcH26weySaGDoXKleG//+yWYGrWhClTzPagQbBzp10uIyIiIiLplOF1lBLas2cPNWrUICYmJu2FWCyEhobSqlUrAH799VdatGjBrl274u9NKlGiBCEhIYSEhKR4nuRGlIKDg7WOkqTNzp1mZdjYWDMNr2VLm1/CajX3Ky1fDiVKmPWV8uSx+WVERERE5P/svo5SZti2bRtnz56lWLFieHh44OHhwdGjRxkwYAAlSpRI8X1eXl74+fkleoikWe3aMHCg2e7RAy5etPklLBb44AOzvu2RI9Ctm+5XEhEREXEWThuUOnfuzG+//UZYWFj8IzAwkEGDBrF+/XpHlyfZwahRpm34qVNmfpwdxN2v5OkJK1ZAMj1SRERERMQB0tXMwVaioqL4888/45+Hh4cTFhZGvnz5KFasGPnz5090vKenJ0WKFKFcuXKZXapkR97eZsinQQOzIO3TT0Pjxja/TK1aMHmyuR1q4EDT5KFmTZtfRkRERETSIE1BqU2bNnd8/WIapyf98ssvNGrUKP55//79AejSpUuaFq4VsZv69aFXL5g1C156CfbuBV9fm1+mTx/YvBlCQ836Srt2mdEmEREREXGMNDVzeP7551N13EcffZTugmwtLTdsiSTr0iXTNvzYMejbF6ZPt8tlLlyAGjXM/Urt2pkpeRaLXS4lIiIiki2lJRvYtOudM1JQEpv45hto2tQkl23boF49u1zm55/hwQfhxg0ziNWzp10uIyIiIpItuUTXOxGn0qQJdO1q2tJ16wbXrtnlMvfdB5Mmme1XXzVT8EREREQk8ykoiaTWO+9AkSJw8CCMGWO3y/TrB08+Cdevm/uVIiPtdikRERERSYGCkkhq5c0Ls2eb7UmT7DbcY7HAhx9C8eLw11+mh4RrT5AVERERcT4KSiJp0bo1PPUUxMSYKXg3btjlMvnyweefg4eHaeowb55dLiMiIiIiKVBQEkmrmTNNkgkLu3VDkR3cfz+89ZbZDgkxlxMRERGRzKGgJJJWhQvDjBlme8wY+OMPu12qf39o2RKio839Spcu2e1SIiIiIpKAgpJIejzzDLRoYToudOtmpuLZgcUCCxZAcDAcPgwvv6z7lUREREQyg4KSSHpYLDB3Lvj5wY8/mul4dpLwfqXFi+G99+x2KRERERH5PwUlkfQKCoK33zbbw4aZFnV2UqcOjB9vtvv2hT177HYpEREREUFBSSRjXnoJGjWCq1ft3sd7wAAz20/3K4mIiIjYn4KSSEZYLGYunLc3bNpk13lxbm7w8cdmIOvQIejRQ/criYiIiNiLgpJIRpUuDW++abYHDoQTJ+x2qfz5YckScHeHRYvggw/sdikRERGRbE1BScQW+vaFBx4w8+HsPNRTr96tXNanD+zda7dLiYiIiGRbCkoituDuboZ3cuSAr782wz12NGgQNG8O166Z+5Wioux6OREREZFsR0FJxFYqVoThw812v35w9qzdLhV3v1LRonDgAPTsqfuVRERERGxJQUnElgYPhqpV4dw5My/OjgoWNOsqubvDp5+ahWlFRERExDYUlERsydMTPvzQpJcvvoCVK+16ufr1YexYs92rF/z+u10vJyIiIpJtKCiJ2FqNGvDaa2b7lVfgwgW7Xm7wYGja1Czl9NRTcPmyXS8nIiIiki0oKInYw4gRUL48nDljVoq1Izc3+OQTCAyE/fvNyJKIiIiIZIyCkog95MxpuuBZLPDRR/DNN3a9XKFC5n6luCYPul9JREREJGMUlETspW7dWw0dunc3ayzZUYMGMGaM2e7Vy6yvtHmzCVCbN0NMjF0vLyIiIuJSLFarazcVjoyMxN/fn4iICPz8/BxdjmQ3UVFQuTIcOWLSy7vv2vVysbHQrBls2AAeHnDz5q3XgoJg+nRo08auJYiIiIg4rbRkA40oidiTry+8957ZnjULtm2z6+Xc3KBDB7OdMCQBnDwJ7drBihV2LUFERETEJSgoidhb48bQrZvZ7tbNtKezk5gYGDky+dfixo5DQjQNT0RERORuFJREMsPkyaYt3eHDMGqU3S6zbRucOJHy61YrHD9u94EtERERkSxPQUkkM+TJA3PmmO3Jk+GXX+xymdOnbXuciIiISHaloCSSWZ54wtxAFBsLL7wA16/b/BIBAak7Ll8+m19aRERExKUoKIlkphkzoEAB07v7rbdsfvr69U13O4vlzsf17Qs//WTzy4uIiIi4DAUlkcxUsCDMnGm2x42Dfftsenp3d9MCHG4PS3HP8+SBQ4fMMk9vvGGXgS0RERGRLE9BSSSzPf20mYZ344bpgmfjFnRt2sCyZVC0aOL9QUGwfDn89Rc884yZAfjmm3DfffDbbzYtQURERCTL04KzIo5w6hRUrAgREaa5w4ABNr9ETIzpbnf6tLl3qX59M+IUZ9ky6NEDzp0DT08YPRoGDTIL1YqIiIi4orRkAwUlEUf54AN48UXImdPcs1SmTKaX8M8/0L07rFplnt9/P3z8MZQrl+mliIiIiNhdWrKBpt6JOMoLL5jFaK9dM4EpNjbTSyhcGFauNOHIz880eKhe3fSccEA5IiIiIk5DQUnEUSwWmD8ffHxgyxaYN89hZTz3nOkr0bgxXL0K/fqZ7aNHHVKSiIiIiMMpKIk4UsmSMGGC2X7tNTh2zGGlBAfD+vUwa5bJbps2QeXKZoaga0/QFREREbmdgpKIo/XubXp1R0XByy87NJW4uUHPnrBnD9SrB5cumVmBLVuaphAiIiIi2YWCkoijubmZYRsvL1i3Dj791NEVUaaMmQ04aRLkyAFffw2VKsGSJY6uTERERCRzKCiJOIPy5WHkSLMdEgJnzji0HDCtxAcNgl27oEYNuHABOnY0y0D995+jqxMRERGxL4cGpa1bt9KyZUsCAwOxWCysXLky/rUbN24wePBgKleuTK5cuQgMDOS5557j1KlTjitYxJ4GDjQt5y5cMNPxnESlSvDjjybHubvDF1/AvffC6tWOrkxERETEfhwalC5fvkzVqlWZNWvWba9duXKFXbt2MXz4cHbt2sWKFSs4ePAgTzzxhAMqFckEnp7w4Ydmxdfly83DSXh6wqhRJjBVqGDWX3riCdPhPCLC0dWJiIiI2J7TLDhrsVgIDQ2lVatWKR6zc+dO7rvvPo4ePUqxYsVSdV4tOCtZzvDhMG6cWeTojz8gXz5HV5TItWumxClTTN+JYsXgo4/g4YcdXZmIiIjInbnsgrMRERFYLBby5MmT4jHR0dFERkYmeohkKW+8ARUrmmGbV191dDW3yZkT3n7bNHsoVcp0NH/kEejbF65ccXR1IiIiIraRZYLStWvXGDx4MB07drxj+pswYQL+/v7xj+Dg4EysUsQGvLxMFzyLBT75BNaudXRFyapf37QR79HDPJ85E6pVgx9+cGhZIiIiIjaRJYLSjRs3aN++PVarlTlz5tzx2KFDhxIRERH/OH78eCZVKWJDDzxgut+BWVvJSUdGfX1hzhzT1bxoUTh8GB58EIYOhehoR1cnIiIikn5OH5TiQtLRo0fZsGHDXecSenl54efnl+ghkiWNG2fmth0/DkOGOLqaO2raFPbtg86dITYW3noLatWCsDBHVyYiIiKSPk4dlOJC0uHDh9m4cSP58+d3dEkimcfHB95/32zPmQObNzu0nLvJk8fMFFyxAgoWNMGpdm2T927edHR1IiIiImnj0KAUFRVFWFgYYf//s3N4eDhhYWEcO3aMGzdu0K5dO3755Rc+++wzYmJiOHPmDGfOnOH69euOLFsk8zRqBN27m+0XX8wS3RJatzYhqXVrE5CGD4e6deHAAUdXJiIiIpJ6Dm0PvnnzZho1anTb/i5dujBq1ChKliyZ7Ps2bdpEw4YNU3UNtQeXLC8iwqz6evIkDBgAkyc7uqJUsVrhs8/M2rkREaZb3vjx0K8fuDn1WLaIiIi4qrRkA6dZR8leFJTEJXz9NTz+uEkYP/wA993n6IpS7cQJMxi2fr153qABLFgAKfwdRERERMRuXHYdJZFsq0UL6NTJdEp44YUs1VIuKMh0OJ83D3Llgq1boXJlmD/fjDqJiIiIOCMFJZGsYto00yXh99/NHLYsxGIxt1r99ptZf+nyZdP1/LHHzIxCEREREWejoCSSVRQoALNmme3x403qyGJKlYJNm2DKFLOu7rp1cO+95l4mjS6JiIiIM1FQEslK2rW71U7uhReyZN9td3fo3x927zZrLV28CM8+C089Bf/+6+jqRERERAwFJZGsxGIxo0p58sCvv8I77zi6onSrUAG+/x7GjAEPD1i+3IwuffmloysTERERUVASyXoCAmDqVLM9ciQcOuTYejLA09Oss/TTT6YD+tmz0KoVdOliRppEREREHEVBSSQr6tIFmjaFa9egWzfTDS8Lq1HDDJC99poZNPvkE9MZb8MGR1cmIiIi2ZWCkkhWZLGYftu+vrB9O8yZ4+iKMszLCyZONB+nTBmz/lKTJtCzJ0RFObo6ERERyW4UlESyquLF4a23zPbgwXDkiEPLsZW6dSEsDHr3Ns/nzIFq1UyAEhEREcksCkoiWdkrr9xamKh7d5fpsZ0rF8ycaabeBQfDX39BgwZmat61a46uTkRERLIDBSWRrMzNDd5/H3LmNKliwQJHV2RTjRvD3r3QtavJgG+/DTVrmvuZREREROxJQUkkq7vnHhg92mz37w+nTzu2Hhvz94ePPjJtwwsXhj/+gAceMB/5xg1HVyciIiKuSkFJxBX072+GWi5eNN0PXGQKXkJPPAH79pk1d2/ehFGjoE4dE5xEREREbE1BScQVeHiYYRdPT1i5EpYudXRFdlGgAHzxBSxaBHnzmil4NWrA5MkQE+Po6kRERMSVKCiJuIrKlWHYMLPduzf88w9s3gyLF5uvLpIkLBbo2NGMLjVvDtHRMGgQNGxomj6IiIiI2ILFanXBOToJREZG4u/vT0REBH5+fo4uR8S+rl83U/D27QMfH7hy5dZrQUEwfTq0aeO4+mzMaoUPPoBXXzVrLfn4mNGlHj1MoBIRERFJKC3ZQCNKIq4kRw7o3NlsJwxJACdPmht8VqzI/LrsxGKBF180nfEaNjQfuWdPaNoUjh93dHUiIiKSlSkoibiSmBizAFFy4gaPQ0JcZhpenBIl4NtvYdq0W53SK1eGTz5xyb4WIiIikgkUlERcybZtcOJEyq9brWaoZdu2zKspk7i5Qb9+sHs33HcfRERAly5mpuHZs46uTkRERLIaBSURV5LaNZRcbK2lhMqXhx074M03bzUBrFTJpWYcioiISCZQUBJxJQEBqTsud2771uFgHh6mAeDOnVClCvz3H7RtC88+CxcuOLo6ERERyQoUlERcSf36prvd3Vq+de4MEyfe3vDBxVStCj//bEKTmxt89hncey+sW+foykRERMTZKSiJuBJ3d9MCHG4PS3HPg4Lg4kUYMgRKl4ZZs0xbcRfl5WWm4e3YAffcA6dOmfWXevSAS5ccXZ2IiIg4KwUlEVfTpg0sWwZFiybeHxQEy5fDkSOmHVzJknDmjFmctlw5+Phjl+uGl9ADD5hGD337mufz5pkRp61bHVuXiIiIOCctOCviqmJiTHe706fNvUv165sRpzjXr5vVWseOvdXcoUIFGDcOWrd26RVbN22C55+Ho0fNxwwJMaNO3t53/7aJiIhI1pWWbKCgJJLdXbkC774Lb711q9NBzZowfjw8+qjLBqbISOjf32RFMN3ynn/eLEOVsMN6UJCZzdimjWPqFBEREdtRUEpAQUkklSIiYMoUeOcduHzZ7HvoIROY6tZ1bG129PXX8OKLZhZicuJy4rJlCksiIiJZXVqyge5REhHD3x/GjIG//4ZXXzVdELZsgXr14PHHYc8eR1doFy1amI/m7Z3863F/SgoJcelbuERERCQJBSURSaxQITOqdPgwvPSSuUHn66+hWjXo2BEOHXJ0hTb3xx9w9WrKr1utcPy4uXdJREREsgcFJRFJXnAwzJ9vUkSHDmbfkiVQsaIJUMePO7Y+G4rrZXE3x47Ztw4RERFxHgpKInJn99wDixeb3tqPP27mn73/PpQta7oh/PuvoyvMsICA1B0XEgKjR8M//9i1HBEREXECCkoikjrVqsHq1Wbl1oceguhomDoVSpWCESNMM4gsqn59093uTg3+3N1NU8BRo6BYMdMhLywssyoUERGRzKagJCJpU7euWYho/XrTRjwqyqzFVLIkTJpk2o1nMe7upgU43B6WLBbzWLTIzDy8/36zBNWCBVC9OjRqBF9+qUYPIiIirkZBSUTSzmKBJk1g505YvtwsVHvhAgweDGXKwJw5Jk1kIW3amBbgRYsm3h8UZPa3bw9PPw0//gg//GC23d1h82Zo1crMUJw+3azPJCIiIlmf1lESkYyLiYHPPoORI+HIEbOvZElzQ88zz5hEkUXExJjudqdPm3uX6tdPufzjx2HWLNPzIm6t3ty5oVs36NPHzEoUERER56EFZxNQUBLJRNevw3vvmal4cR0PKlaEcePMsMudbgLKwi5fhk8/NSNKBw6YfRYLPPmkaQDRoIHLfnQREZEsRQvOiohj5MgBvXrBX3/BW29B3rymvXibNubmno0bb63g6kJy5YIePeD332HdOmjWzHzMlSuhYUOoUQM+/tj0vxAREZGsQUFJRGwvVy5zv9Lff8Mbb5jnO3fCo4/CI4+Ym3xckJsbNG0Ka9eafNijB3h7m+54XbuabnlqLy4iIpI1KCiJiP3kyWOm4f39t5mDliOH6ZhXty488QT89pujK7SbChVMT4sTJ8zgWtGicPas2ouLiIhkFQ4NSlu3bqVly5YEBgZisVhYuXJlotetVisjRowgICAAb29vGjduzOHDhx1TrIikX6FCZs2lw4dNpwM3N7MmU7Vq0KkT/Pmnoyu0m3z5zOBaeLhpL/7AA2ovLiIikhU4NChdvnyZqlWrMmvWrGRfnzRpEjNmzGDu3Ln89NNP5MqVi6ZNm3Lt2rVMrlREbKJYMXj/fTMv7emnzY08ixZB+fLw8stm+MVFeXqaj/zDD+bRoYPai4uIiDgzp+l6Z7FYCA0NpVWrVoAZTQoMDGTAgAEMHDgQgIiICAoXLsyCBQvo0KFDqs6rrnciTiwsDF5/HdasMc+9vEwziCFDoGBBh5aWGY4fh9mzYd48tRcXERHJDC7R9S48PJwzZ87QuHHj+H3+/v7cf//9/HCHG8Gjo6OJjIxM9BARJ1WtGnz9tVm4qH590xbunXdMQhg50uWHV4KDYcIEE5jmzjUDa5cuwbRpZt3e1q1hyxaXbBQoIiLi9Jw2KJ05cwaAwoULJ9pfuHDh+NeSM2HCBPz9/eMfwcHBdq1TRGzgwQdNIli3zvTSjoqCMWPMorWTJ8PVq46u0K5y5TIzD9VeXERExHk4bVBKr6FDhxIRERH/OH78uKNLEpHUsFhMb+1ffoFly8zwyvnzMGiQGV6ZOxdu3HB0lXal9uIiIiLOw2mDUpEiRQD4J8lvBP/880/8a8nx8vLCz88v0UNEshCLBdq2hb174aOPoHhxOHUKXnnFhKeFC7NFi7ik7cWDgtReXEREJDM5bVAqWbIkRYoU4dtvv43fFxkZyU8//USdOnUcWJmIZAoPDzOMcvAgzJwJhQub9Zg6dzb3Nn35Zba4eSeuvfjff6u9uIiISGZyaFCKiooiLCyMsP//WTQ8PJywsDCOHTuGxWIhJCSEcePGsWrVKvbu3ctzzz1HYGBgfGc8EckGvLygd2/46y/T+SBPHti3z/TUfuABSPDHFFeWmvbi06a5fP8LERGRTOPQ9uCbN2+mUaNGt+3v0qULCxYswGq1MnLkSObPn8/Fixd58MEHmT17Nvfcc0+qr6H24CIu5uJF0+Bh6lS4csXse/hhePNNE5yykZTai7/wgmkvXrq0Y+sTERFxNmnJBk6zjpK9KCiJuKh//oHx402Th+vXzb4nn4SxY6FyZcfWlsmuXIFPPzUjSgcOmH0WCzzxBISEwEMPmeciIiLZnUusoyQickeFC8P06XDokOls4OZmbtapWhWefdZM1csmfHySby/+5ZfmHia1FxcREUk7BSURydqKF4cPPzQp4amnTEL47DPTIa9HDzh50tEVZprUtBcfNUrtxUVERFJDQUlEXEP58vDFF/Drr9C8Ody8aW7eKVMGBg6E//5zdIWZKqX24qNHm8DUtavai4uIiNyJgpKIuJYaNWDNGti6FR58EK5dgylToFQpkxKyWVu4lNqLf/yxaS/esCGsXKn24iIiIkkpKImIa6pf34SlNWtMIrh0ycw7K1XKBKerVx1dYaZKqb34li3QujWULav24iIiIgmp652IuL7YWFi+HIYPNwvYAgQGwogRppe2p+etY2NiYNs2OH0aAgJM4HJ3d0zddnbiBMyapfbiIiKSfajrnYhIQm5uptHDvn2m8UOxYnDqlOl2UKECLFpkwtSKFVCihGkV98wz5muJEma/CwoKMmv4njhhuqxXqGAG3qZPNyNMrVqZBW1d+89pIiIiydOIkohkP9HRMH8+jBtnOhyACU/Hjt1+bNwCRMuWQZs2mVejA1itsGGDmYK3du2t/VWrmvWYOnSAnDkdVZ2IiEjGacHZBBSURCRFUVEwYwZMnHjnm3MsFjP8Eh7ustPwkjpwwHxrFiy4dTtXoULwyivmUbhw4uOz0YxFERHJwjT1TkQkNXx9YdgwWLjwzsdZrXD8uEkC2UT58jB79p3bi+/ebY7NZjMWRUQkm1BQEhGJikrdceHh9q3DCSVtL16nzq324jVqQKVK0LatCVQJnTwJ7dopLImISNaloCQiEhCQuuN69ID27c1v/9m0vfj338OPP0LHjmZq3R9/JH983KTukBCt0SQiIlmTgpKISP36Zm5ZXOOG5Hh4mKGUpUvNEErhwvDcc6brwY0bmVerE7j/ftMocPHiOx+XDWcsioiIC1FQEhFxdzc9seH2sGSxmMeSJfDrrzBoEAQHmz7an34Kjz0GRYrAyy/Dpk3Zavjk5s3UHff666Yre3JNBUVERJyVut6JiMRZsQL69Ut8w01wsOmXnbA1eGws/PCDCU9ffHGrxTiYaXzt25te2vfff+dRqixu82bTuCEtypSBxo3hkUfMe/Pnt0tpIiIiyVJ78AQUlEQkTdLa5/rmTdiyxYSm5cvhwoVbrxUvbgJThw5mMSIXC00xMaa73cmTyS9Ka7FAwYLw4otmsO3nnxMPuFksUL26CU2NG8ODD4KPT6aVLyIi2ZCCUgIKSiKSaa5fh2++MaFp5Uq4fPnWa+XK3QpN5cs7rERbW7HCdLeDxGEpuXV6IyNNpvz2W9i4EX7/PfG5cuSAunVNcHrkEahd29waJiIiYisKSgkoKImIQ1y5AmvWmND01VcQHX3rtWrVTGB6+mkzJJPFpXbGYlKnT8N3390KTsePJ37dzw8eeujWVL2KFV1uUE5ERDKZglICCkoi4nCRkfDllyY0ffNN4i4IDzxgQtNTT0FgoONqzKC0zlhMymqFP/+8FZq++y7xLEYwPTPiRpseecQsfCsiIpIWCkoJKCiJiFM5d84MwSxZYm7cifufYIsFGjY0oalt22zf5SAmBsLCbgWnbdvg2rXEx5Qtm7gxRL58DilVRESyEAWlBBSURMRpnT5t1mVassR00Yvj4QGPPmpCU6tWZg5aNnftmvkWffutefz8s2k+GMdigRo1bjWGqFdPjSFEROR2CkoJKCiJSJZw5IhpNb5kCezefWu/lxe0aGFCU4sW+u3//yIiTGOIjRtNcPrjj8Sv58hhwlLcNL1atdQYQkREFJQSUVASkSzn4EETmJYsgQMHbu3PlQuefNKEpqZNTRoQAE6dStwYImFjCTCDcg0b3pqqV6GCGkOIiGRHCkoJKCiJSJZltcJvv90KTUeO3HotTx5zL1OHDiYBaLgkntUKhw/fGm367ju4eDHxMQEBiRtDBAc7pFQREclkCkoJKCiJiEuwWs2NOUuWwOefm/ub4hQqZLrmdehgFiJyc3NcnU4oJsbMZowbbdq+/fbGEPfck7gxRN68jqlVRETsS0EpAQUlEXE5cb24lywxK7qeO3frtaAgsz5Tx46mu4Hml93m2jX4/vtbwemXX25vDFGzZuLGEN7ejqtXRERsR0EpAQUlEXFpN26Y3/gXL4bQULh06dZrZcqYUaYOHaBSJcfV6OQuXoTNm2911Nu/P/HrXl6JG0PUrKmZjiIiWZWCUgIKSiKSbVy7BmvXmpGm1avh6tVbr917763QVLq042rMAk6eNPc1xd3jdPJk4tf9/RM3hihfXgN3IiJZhYJSAgpKIpItRUWZsLRkiQlPN27ceq12bROY2rc3U/UkRVaraUIYN9r03XemNXlCgYGJG0PoWyoi4rwUlBJQUBKRbO/CBTMtb8kS89t+whty6tc3oaldO9MUQu4oJgZ27bo12rR9O0RHJz6mXLlbo00NG969MUTcLWenT5tufPXrg7u73T6CiEi2pqCUgIKSiEgCZ8+aBhBLlpjfzuO4u5vf7Dt0gNatTftxuaurVxM3hvj118Q51M3t9sYQOXPeen3FCujXL/G6T0FBMH06tGmTeZ9DRCS7UFBKQEFJRCQFx4/DF1+Y0PTLL7f258gBzZqZ0NSyJfj6Oq7GLObChVuNITZuNNP2EvLyggcfNMHJ3R2GDDHT+xKKu99p2TKFJRERW1NQSkBBSUQkFf7806zPtGQJ7Nt3a7+PjwlLHTqY8JRwOCQhzR9L1okTt+5v+vZbOHUqde+zWMzIUni4vo0iIrakoJSAgpKISBrt22cC05Il8Ndft/b7+ZlpeR06mCERT0+zX/PHUsVqhQMHTGBasgR27Lj7exo1Mr03iheHYsVuffX3t3+9IiKuSEEpAQUlEZF0slrNTTdLlpjRpoRBKH9+0wCiaFEYOVLzx9Jo8WJ45pn0v9/fP3Fwivsat12kiEaiRESSo6CUgIKSiIgNxMaargVLlpj7mv799+7v0fyxFG3ebEaL7qZHD3PL2NGjcOyY+Xr+/N3f5+lpvvUphalixcysShGR7EZBKQEFJRERG7t5EzZtgqlTzRpNd/PVV9Cihf3rykJiYqBECbOYbXL/L3ynjBkVZUJTXHBKun3ihDn/3RQocHuISvi1QAEtpCsirkdBKQEFJRERO0nL/LHAQChfHipUSPw1MDDb/ja+YoWZvQiJw1JGZy3evGl6aiQchUoYpo4eNWHrbnLmTDlEFStmglyOHGmvT0TEkRSUElBQEhGxk9TOH7uT3LlvBaeEIapUqVvNIlxYcn0wgoNh2jT73dpltcLFi4lDVNIwdfr03c9jsZgGh0nvj0r41Z7LcanRooikh8sEpZiYGEaNGsXChQs5c+YMgYGBdO3alTfeeANLKv8CqaAkImInqZ0/tnu3aT++f79p+xb39a+/Up4j5ukJZcrcPgpVrpwJVy7EGX/hj4424S2lMHXsGFy7dvfz+PnduelEQED6PqsaLYpIerlMUBo/fjzvvPMOH3/8MZUqVeKXX37h+eef580336Rv376pOoeCkoiIHWVk/lh0tAlQCcNT3NcrV1K+ZlBQ8tP4ihTJttP4MpvVavp5JA1RCcPUf//d/TweHik3nShe3Iyu5cqV+D1xP3JqtCgi6eEyQenxxx+ncOHCfPDBB/H72rZti7e3NwsXLkz2PdHR0URHR8c/j4yMJDg4WEFJRMRebD1/LDbWnCu5APXPPym/z9//9vBUoQKULGl+I5dMdfkyHD+ecpg6ccLcT3U3+fPfCk/BwfDJJxARkfyxarQoInfjMkFp/PjxzJ8/n2+++YZ77rmHPXv20KRJE9555x06deqU7HtGjRrF6NGjb9uvoCQiYkeZNX/s/Hk4eDBxgNq/3/xmHBub/Hty5ICyZZOfxpd0uEIyTUyM+XFJLkTFNZ24dCl9527UyPznzZcP8ua99Uj63NdXg5Ai2Y3LBKXY2FiGDRvGpEmTcHd3JyYmhjfffJOhQ4em+B6NKImIZEPXrsHhw7ePQh08CFevpvy+YsVuD1AVKkDBgvoN2glERCQOT+vWmW7ztuLhcecgdafn3t5Z80fEGe+JE8lMLhOUlixZwqBBg3j77bepVKkSYWFhhISE8M4779ClS5dUnUP3KImIZGOxsea37OSm8d1p0dy8eZOfxleihH6rdKDUNlrs1cusA3X+PFy4cOuR8Pn16xmrJUeO24NUasOWl1fGrp1eaoIh4kJBKTg4mCFDhtCrV6/4fePGjWPhwoUcOHAgVedQUBIRkWSdO5d4+l7c9pEjyXfxA/Mb7j333B6g7rkHfHzSV4f+xJ9qGVmoNyGr1Qw03ilI3el5ahb0vRNv7/SNYuXNm/6u+WqCkXH6p+oa0pINnPru1itXruDm5pZon7u7O7EpzUMXERFJrfz5oV4980jo6lU4dCj5aXzR0bB3r3kkZLGYjgPJrQlVoEDKNehP/Gni7m6+Ne3amW95co0Wp027+y+vFovJtT4+5tudFlarWbA3LcEq4SMupF29CqdOpe3aYO6rSkuwypvX9Dnp1y/5cGm1mu9HSAg8+aR+8U+J/qlmT049otS1a1c2btzIvHnzqFSpErt376Z79+688MILTJw4MVXn0IiSiIjYREyMuVEmaYDav9/8RpyS/PmTn8b366/Qvr3+xJ8Ojlio1xZiYyEyMu0jWBcupNzpz5YeecR8Hz09zdRCT8/E22ndl9rjPT2dO6BpNM61uMzUu0uXLjF8+HBCQ0M5e/YsgYGBdOzYkREjRpAjR45UnUNBSURE7O7ff2+/B2r/fhOs0qtwYTPPJ29es3JrKv9/L7vIbtOgYmLg4sX0TRWMinJ09Xfn5paxUGbrfXFf3dygRYuUVyZQS/q7c7Z/qy4TlGxBQUlERBzmyhUzZS+5ZhKpWUQooZw5TWCKe/j7p/55wu303uTibJztty8ntnEjPPro3Y975RUzg/TGDdPsIuFXW+9ztd8+g4NNs0xfX7PqQEqPO72e8DVXWfrNGacsKigloKAkIiJO57PP4Nln735cjhwZb8+WVM6caQtZyT13dOByxt++nJitmmDYuiZ7hzFb7IuMNCN5mS1HjrSFLGcMYc46ZVFBKQEFJRERcTqp7XO9aRM8+KBZeTUy0tyoEhl565GW51eu2PYzeHtnfHTLzy/tv7U5629fTi7u2+ZmjeFBthHAaU4TwHbqE2tx17ctBan9pzp1qml+efnyrUdUVOLnqXktox0VUyMzQlhcOE/4t4yEHDllUUEpAQUlERFxOo74E//Nm4lDVHqD150W8E0PH5/UhyxfX9Oe7b//kj+Xbhi5ox9fW0Gxd/oRGHPrt9dT7kEc6z+dByYpJSUn4T9VizWG+glC5jbqY7W42+xHzmo1o1jpDVnOFMLc3VP+Z5rQpk3QsKHdy0rEZdqDi4iIuCRb9blOCw8P0zs6X76MnefGjdSNcN0tdMUFritXzOPMmYx/RqsVjh83v63lzJm+dmyOei3ua9x/f1tbsYIHJrcj6d/HA2JPEji5HTygIaXkxP1T/aztCqbRj2BuhczjBBFinU6naW1s8k/VYjFLtXl5ZfyfaVJxISy9Ietur8WFsOvXb58t7MbtATMW8w07fdq2n9PWFJREREQcoU0bM00suXttnLnPtaenbQLX9eu3AldqR7MOH4Y//rj7uWNjbT/VMLO4u9s+hLm7w/z5YLWSNIZZ4oLTSy+Z8OrlZUK1h4d5b9Lt5Pbd6XV7Bb9M1IYVtKYdVhKHzKKcZBntsLAMcNJ/r/+XMITlz2/bcycXwrZuhR49oDUrmJ5MwOzHdEJpQ0CAbWuxNU29ExERcSR1b0u91N4w8sUXUKvW3e/YT81rtjompX2uzs3NdqHLlgEutce6uUHTpimPeFosULSomXvnKq3qbCAmBnoUXsG8c+0AK24JXov9f1zvkX8Zc/6xzWhcWugepQQUlERERFyEM7Zvywir1dw7Zs8wdv067N0La9bcvZ6KFc1ww82b5r03bybeTm5fwu3szt39VsBK+EgYvNK6z17H2uL97u4mSCYnJoYrhUuQ89wJkjsiFgvX8gfh80/m/1vVPUoiIiLiehxxb5c9WSy3ps3Z0+bNqQtKs2al/856q9VMeUxPwEpvMLPnOeK+RkWZKaKpERNjHtHR6fseZkVxI4ZJQ1VMDD7nUu7m4IYVn3PHzWh6ZndzSAMFJREREck6suq9XY5Uv775/txtJK5+/fRfw2IxATWrhNTUSu10z9BQuP/+W4EraQC70760HJvR92fkWsmJjU2+g0NqOXk3BwUlERERyVratIEnn9S9XanlaiNxmSm1IbNlS9f+/sWNGKY2VP34I3TvfvfzOnk3B92jJCIiIpIdrFhx+0hccLBG4u4mbrVeSD5karXe2znx/YRpyQYp3IElIiIiIi6lTRs4csSs8rlokfkaHq5f8u8mbrpn0aKJ9wcFKSSlJG4UE25vEZ+FRjE1oiQiIiIicjdq5Z92TjiKqfbgCSgoiYiIiIg4iJMFTLUHFxERERERx3N3d+oW4Heie5RERERERESSUFASERERERFJQkFJREREREQkCQUlERERERGRJBSUREREREREklBQEhERERERSUJBSUREREREJAkFJRERERERkSQUlERERERERJJQUBIREREREUnCw9EF2JvVagUgMjLSwZWIiIiIiIgjxWWCuIxwJy4flC5dugRAcHCwgysRERERERFncOnSJfz9/e94jMWamjiVhcXGxnLq1Cly586NxWJxdDmSTpGRkQQHB3P8+HH8/PwcXY64OP28SWbTz5xkJv28SWZzpp85q9XKpUuXCAwMxM3tznchufyIkpubG0FBQY4uQ2zEz8/P4f/AJPvQz5tkNv3MSWbSz5tkNmf5mbvbSFIcNXMQERERERFJQkFJREREREQkCQUlyRK8vLwYOXIkXl5eji5FsgH9vElm08+cZCb9vElmy6o/cy7fzEFERERERCStNKIkIiIiIiKShIKSiIiIiIhIEgpKIiIiIiIiSSgoiYiIiIiIJKGgJE5rwoQJ1K5dm9y5c1OoUCFatWrFwYMHHV2WZBNvvfUWFouFkJAQR5ciLuzkyZM8++yz5M+fH29vbypXrswvv/zi6LLERcXExDB8+HBKliyJt7c3pUuXZuzYsaivl9jK1q1badmyJYGBgVgsFlauXJnodavVyogRIwgICMDb25vGjRtz+PBhxxSbCgpK4rS2bNlCr169+PHHH9mwYQM3btygSZMmXL582dGliYvbuXMn8+bNo0qVKo4uRVzYhQsXqFevHp6enqxdu5Y//viDKVOmkDdvXkeXJi5q4sSJzJkzh3fffZf9+/czceJEJk2axMyZMx1dmriIy5cvU7VqVWbNmpXs65MmTWLGjBnMnTuXn376iVy5ctG0aVOuXbuWyZWmjtqDS5bx77//UqhQIbZs2UKDBg0cXY64qKioKGrUqMHs2bMZN24c1apVY9q0aY4uS1zQkCFD2LFjB9u2bXN0KZJNPP744xQuXJgPPvggfl/btm3x9vZm4cKFDqxMXJHFYiE0NJRWrVoBZjQpMDCQAQMGMHDgQAAiIiIoXLgwCxYsoEOHDg6sNnkaUZIsIyIiAoB8+fI5uBJxZb169aJFixY0btzY0aWIi1u1ahW1atXiqaeeolChQlSvXp333nvP0WWJC6tbty7ffvsthw4dAmDPnj1s376d5s2bO7gyyQ7Cw8M5c+ZMov9/9ff35/777+eHH35wYGUp83B0ASKpERsbS0hICPXq1ePee+91dDniopYsWcKuXbvYuXOno0uRbODvv/9mzpw59O/fn2HDhrFz50769u1Ljhw56NKli6PLExc0ZMgQIiMjKV++PO7u7sTExPDmm2/SqVMnR5cm2cCZM2cAKFy4cKL9hQsXjn/N2SgoSZbQq1cv9u3bx/bt2x1dirio48eP069fPzZs2EDOnDkdXY5kA7GxsdSqVYvx48cDUL16dfbt28fcuXMVlMQuvvjiCz777DMWLVpEpUqVCAsLIyQkhMDAQP3MiSRDU+/E6fXu3ZuvvvqKTZs2ERQU5OhyxEX9+uuvnD17lho1auDh4YGHhwdbtmxhxowZeHh4EBMT4+gSxcUEBARQsWLFRPsqVKjAsWPHHFSRuLpBgwYxZMgQOnToQOXKlencuTOvvvoqEyZMcHRpkg0UKVIEgH/++SfR/n/++Sf+NWejoCROy2q10rt3b0JDQ/nuu+8oWbKko0sSF/bII4+wd+9ewsLC4h+1atWiU6dOhIWF4e7u7ugSxcXUq1fvtiUPDh06RPHixR1Ukbi6K1eu4OaW+Fc/d3d3YmNjHVSRZCclS5akSJEifPvtt/H7IiMj+emnn6hTp44DK0uZpt6J0+rVqxeLFi3iyy+/JHfu3PHzV/39/fH29nZwdeJqcufOfdv9b7ly5SJ//vy6L07s4tVXX6Vu3bqMHz+e9u3b8/PPPzN//nzmz5/v6NLERbVs2ZI333yTYsWKUalSJXbv3s0777zDCy+84OjSxEVERUXx559/xj8PDw8nLCyMfPnyUaxYMUJCQhg3bhxly5alZMmSDB8+nMDAwPjOeM5G7cHFaVkslmT3f/TRR3Tt2jVzi5FsqWHDhmoPLnb11VdfMXToUA4fPkzJkiXp378/L730kqPLEhd16dIlhg8fTmhoKGfPniUwMJCOHTsyYsQIcuTI4ejyxAVs3ryZRo0a3ba/S5cuLFiwAKvVysiRI5k/fz4XL17kwQcfZPbs2dxzzz0OqPbuFJRERERERESS0D1KIiIiIiIiSSgoiYiIiIiIJKGgJCIiIiIikoSCkoiIiIiISBIKSiIiIiIiIkkoKImIiIiIiCShoCQiIiIiIpKEgpKIiIiIiEgSCkoiIiIJWCwWVq5c6egyRETEwRSURETEaXTt2hWLxXLbo1mzZo4uTUREshkPRxcgIiKSULNmzfjoo48S7fPy8nJQNSIikl1pRElERJyKl5cXRYoUSfTImzcvYKbFzZkzh+bNm+Pt7U2pUqVYtmxZovfv3buXhx9+GG9vb/Lnz0/37t2JiopKdMyHH35IpUqV8PLyIiAggN69eyd6/b///qN169b4+PhQtmxZVq1aFf/ahQsX6NSpEwULFsTb25uyZcveFuxERCTrU1ASEZEsZfjw4bRt25Y9e/bQqVMnOnTowP79+wG4fPkyTZs2JW/evOzcuZOlS5eycePGREFozpw59OrVi+7du7N3715WrVpFmTJlEl1j9OjRtG/fnt9++43HHnuMTp06cf78+fjr//HHH6xdu5b9+/czZ84cChQokHnfABERyRQWq9VqdXQRIiIiYO5RWrhwITlz5ky0f9iwYQwbNgyLxUKPHj2YM2dO/GsPPPAANWrUYPbs2bz33nsMHjyY48ePkytXLgDWrFlDy5YtOXXqFIULF6Zo0aI8//zzjBs3LtkaLBYLb7zxBmPHjgVM+PL19WXt2rU0a9aMJ554ggIFCvDhhx/a6bsgIiLOQPcoiYiIU2nUqFGiIASQL1+++O06deokeq1OnTqEhYUBsH//fqpWrRofkgDq1atHbGwsBw8exGKxcOrUKR555JE71lClSpX47Vy5cuHn58fZs2cBeOWVV2jbti27du2iSZMmtGrVirp166brs4qIiPNSUBIREaeSK1eu26bC2Yq3t3eqjvP09Ez03GKxEBsbC0Dz5s05evQoa9asYcOGDTzyyCP06tWLyZMn27xeERFxHN2jJCIiWcqPP/542/MKFSoAUKFCBfbs2cPly5fjX9+xYwdubm6UK1eO3LlzU6JECb799tsM1VCwYEG6dOnCwoULmTZtGvPnz8/Q+URExPloRElERJxKdHQ0Z86cSbTPw8MjvmHC0qVLqVWrFg8++CCfffYZP//8Mx988AEAnTp1YuTIkXTp0oVRo0bx77//0qdPHzp37kzhwoUBGDVqFD169KBQoUI0b96cS5cusWPHDvr06ZOq+kaMGEHNmjWpVKkS0dHRfPXVV/FBTUREXIeCkoiIOJV169YREBCQaF+5cuU4cOAAYDrSLVmyhJ49exIQEMDixYupWLEiAD4+Pqxfv55+/fpRu3ZtfHx8aNu2Le+88078ubp06cK1a9eYOnUqAwcOpECBArRr1y7V9eXIkYOhQ4dy5MgRvL29qV+/PkuWLLHBJxcREWeirnciIpJlWCwWQkNDadWqlaNLERERF6d7lERERERERJJQUBIREREREUlC9yiJiEiWodniIiKSWTSiJCIiIiIikoSCkoiIiIiISBIKSiIiIiIiIkkoKImIiIiIiCShoCQiIiIiIpKEgpKIiIiIiEgSCkoiIiIiIiJJKCiJiIiIiIgk8T/xlLdeMYPUHwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Calculate the size of the training dataset\n",
    "training_dataset_size = len(train_dataset)\n",
    "\n",
    "N = training_dataset_size\n",
    "batch_size = 32  # Your batch size\n",
    "\n",
    "# Calculate steps per epoch\n",
    "steps_per_epoch = N // batch_size\n",
    "\n",
    "\n",
    "def custom_data_collator(batch):\n",
    "    input_ids_list = []\n",
    "    labels_list = []\n",
    "    attention_mask_list = []\n",
    "\n",
    "    for item in batch:\n",
    "        # Ensure input_ids is not 0-dimensional\n",
    "        if item['input_ids'].dim() > 0:\n",
    "            input_ids_list.append(item['input_ids'])\n",
    "            attention_mask = torch.ones(item['input_ids'].shape, dtype=torch.long)\n",
    "            attention_mask_list.append(attention_mask)\n",
    "        else:\n",
    "            # Handle 0-dimensional input_ids\n",
    "            input_ids_list.append(item['input_ids'].unsqueeze(0))\n",
    "            attention_mask_list.append(torch.ones(1, dtype=torch.long))\n",
    "\n",
    "        # Ensure labels is not 0-dimensional\n",
    "        if item['labels'].dim() > 0:\n",
    "            labels_list.append(item['labels'])\n",
    "        else:\n",
    "            # Handle 0-dimensional labels\n",
    "            labels_list.append(item['labels'].unsqueeze(0))\n",
    "\n",
    "    # Padding sequences to have the same length\n",
    "    if len(input_ids_list) > 0 and len(labels_list) > 0:\n",
    "        input_ids = pad_sequence(input_ids_list, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "        labels = pad_sequence(labels_list, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "        attention_mask = pad_sequence(attention_mask_list, batch_first=True, padding_value=0)\n",
    "    else:\n",
    "        raise ValueError(\"Empty batch received in custom_data_collator\")\n",
    "\n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'labels': labels,\n",
    "        'attention_mask': attention_mask\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",                      # Directory for storing outputs\n",
    "    num_train_epochs=10,                         # Set number of epochs to 10\n",
    "    per_device_train_batch_size=32,               # Batch size for training\n",
    "    per_device_eval_batch_size=32,                # Batch size for evaluation\n",
    "    warmup_steps=500,                            # Number of warmup steps\n",
    "    weight_decay=0.01,                           # Weight decay for regularization\n",
    "    logging_dir=\"./logs\",                        # Directory for storing logs\n",
    "    logging_steps=steps_per_epoch,                            # Log every steps in epoch\n",
    "    do_train=True,                               # Enable training\n",
    "    do_eval=True,                                # Enable evaluation\n",
    "    evaluation_strategy=\"epoch\",                 # Evaluate at the end of each epoch\n",
    "    save_strategy=\"no\",                    # Disable saving the model\n",
    "    save_total_limit=0,                    # Limit on the total amount of checkpoints. Setting to 0 disables it\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,  # Use the dataset instance for training data\n",
    "    eval_dataset=eval_dataset,     # Use the dataset instance for validation data\n",
    "    callbacks=[custom_eval_callback],  # Custom callback function for metrics\n",
    "    optimizers=(AdamW(model.parameters(), lr=5e-5), None),  # AdamW optimizer with a specified learning rate\n",
    "    data_collator=custom_data_collator\n",
    "#     data_collator=lambda data: {'input_ids': torch.stack([f['input_ids'] for f in data]),\n",
    "#                                 'labels': torch.stack([f['labels'] for f in data]),\n",
    "#                                 'attention_mask': torch.stack([torch.tensor([1]*len(f['input_ids'])) for f in data])}\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n",
    "# Plotting\n",
    "training_logs = trainer.state.log_history\n",
    "\n",
    "training_loss = [log['loss'] for log in training_logs if 'loss' in log]\n",
    "validation_loss = [log['eval_loss'] for log in training_logs if 'eval_loss' in log]\n",
    "epochs = range(1, len(training_loss) + 1)\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(epochs, training_loss, 'bo-', label='Training loss')\n",
    "plt.plot(epochs, validation_loss, 'ro-', label='Validation loss')\n",
    "plt.title('Training and Validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c2a556",
   "metadata": {},
   "source": [
    "## with batch size 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "70142647",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='243' max='243' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [243/243 04:19, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>10.809500</td>\n",
       "      <td>10.307573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>9.823700</td>\n",
       "      <td>8.796066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>8.647500</td>\n",
       "      <td>7.890121</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "1.0     | 0.37     | 0.00     | 0.37     | 0.37        | -53.38     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "2.0     | 0.50     | 0.00     | 0.49     | 0.49        | -58.11     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "3.0     | 0.27     | 0.00     | 0.27     | 0.27        | -62.94     \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Calculate the size of the training dataset\n",
    "training_dataset_size = len(train_dataset)\n",
    "\n",
    "N = training_dataset_size\n",
    "batch_size = 16 \n",
    "\n",
    "# Calculate steps per epoch\n",
    "steps_per_epoch = N // batch_size\n",
    "\n",
    "\n",
    "def custom_data_collator(batch):\n",
    "    input_ids_list = []\n",
    "    labels_list = []\n",
    "    attention_mask_list = []\n",
    "\n",
    "    for item in batch:\n",
    "        # Ensure input_ids is not 0-dimensional\n",
    "        if item['input_ids'].dim() > 0:\n",
    "            input_ids_list.append(item['input_ids'])\n",
    "            attention_mask = torch.ones(item['input_ids'].shape, dtype=torch.long)\n",
    "            attention_mask_list.append(attention_mask)\n",
    "        else:\n",
    "            # Handle 0-dimensional input_ids\n",
    "            input_ids_list.append(item['input_ids'].unsqueeze(0))\n",
    "            attention_mask_list.append(torch.ones(1, dtype=torch.long))\n",
    "\n",
    "        # Ensure labels is not 0-dimensional\n",
    "        if item['labels'].dim() > 0:\n",
    "            labels_list.append(item['labels'])\n",
    "        else:\n",
    "            # Handle 0-dimensional labels\n",
    "            labels_list.append(item['labels'].unsqueeze(0))\n",
    "\n",
    "    # Padding sequences to have the same length\n",
    "    if len(input_ids_list) > 0 and len(labels_list) > 0:\n",
    "        input_ids = pad_sequence(input_ids_list, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "        labels = pad_sequence(labels_list, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "        attention_mask = pad_sequence(attention_mask_list, batch_first=True, padding_value=0)\n",
    "    else:\n",
    "        raise ValueError(\"Empty batch received in custom_data_collator\")\n",
    "\n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'labels': labels,\n",
    "        'attention_mask': attention_mask\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",                      # Directory for storing outputs\n",
    "    num_train_epochs=3,                         # Set number of epochs to 10\n",
    "    per_device_train_batch_size=16,               # Batch size for training\n",
    "    per_device_eval_batch_size=16,                # Batch size for evaluation\n",
    "    warmup_steps=500,                            # Number of warmup steps\n",
    "    weight_decay=0.01,                           # Weight decay for regularization\n",
    "    logging_dir=\"./logs\",                        # Directory for storing logs\n",
    "    logging_steps=steps_per_epoch,                            # Log every steps in epoch\n",
    "    do_train=True,                               # Enable training\n",
    "    do_eval=True,                                # Enable evaluation\n",
    "    evaluation_strategy=\"epoch\",                 # Evaluate at the end of each epoch\n",
    "    save_strategy=\"no\",                    # Disable saving the model\n",
    "    save_total_limit=0,                    # Limit on the total amount of checkpoints. Setting to 0 disables it\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,  # Use the dataset instance for training data\n",
    "    eval_dataset=eval_dataset,     # Use the dataset instance for validation data\n",
    "    callbacks=[custom_eval_callback],  # Custom callback function for metrics\n",
    "    optimizers=(AdamW(model.parameters(), lr=5e-5), None),  # AdamW optimizer with a specified learning rate\n",
    "    data_collator=custom_data_collator\n",
    "\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n",
    "# Plotting\n",
    "training_logs = trainer.state.log_history\n",
    "\n",
    "training_loss = [log['loss'] for log in training_logs if 'loss' in log]\n",
    "validation_loss = [log['eval_loss'] for log in training_logs if 'eval_loss' in log]\n",
    "epochs = range(1, len(training_loss) + 1)\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(epochs, training_loss, 'bo-', label='Training loss')\n",
    "plt.plot(epochs, validation_loss, 'ro-', label='Validation loss')\n",
    "plt.title('Training and Validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5c1613",
   "metadata": {},
   "source": [
    "## Another aproach of chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5f487234",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, tokenizer, max_length=512):\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=True, truncation=True)\n",
    "    chunk_size = max_length - tokenizer.num_special_tokens_to_add(pair=False)\n",
    "    chunks = [tokens[i:i + chunk_size] for i in range(0, len(tokens), chunk_size)]\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a905d0a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "\n",
    "def preprocess_data(df):\n",
    "    tokenized_inputs = []\n",
    "    tokenized_labels = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        # Chunk the 'Preprocessed_mail' text\n",
    "        chunks = chunk_text(row['Preprocessed_Body'], tokenizer)\n",
    "        tokenized_inputs.extend(chunks)\n",
    "\n",
    "        # Repeat the summary for each chunk of the text\n",
    "        summary_tokens = tokenizer.encode(row['Summary_human'], truncation=True, padding='max_length', max_length=512)\n",
    "        tokenized_labels.extend([summary_tokens] * len(chunks))\n",
    "\n",
    "    return tokenized_inputs, tokenized_labels\n",
    "\n",
    "tokenized_inputs, tokenized_labels = preprocess_data(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ca77d571",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SummarizationDataset(Dataset):\n",
    "    def __init__(self, tokenized_inputs, tokenized_labels):\n",
    "        self.tokenized_inputs = tokenized_inputs\n",
    "        self.tokenized_labels = tokenized_labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokenized_inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': torch.tensor(self.tokenized_inputs[idx], dtype=torch.long),\n",
    "            'labels': torch.tensor(self.tokenized_labels[idx], dtype=torch.long)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "50f0c286",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_train, input_test, labels_train, labels_test = train_test_split(tokenized_inputs, tokenized_labels, test_size=0.2)\n",
    "train_dataset = SummarizationDataset(input_train, labels_train)\n",
    "test_dataset = SummarizationDataset(input_test, labels_test)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6ad0ead9",
   "metadata": {},
   "source": [
    "model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
    "model = model.to(torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu'))\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6ba37177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the size of the training dataset\n",
    "training_dataset_size = len(train_dataset)\n",
    "\n",
    "N = training_dataset_size\n",
    "batch_size = 16  # Your batch size\n",
    "\n",
    "# Calculate steps per epoch\n",
    "steps_per_epoch = N // batch_size\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",                      # Directory for storing outputs\n",
    "    num_train_epochs=3,                         # Set number of epochs to 10\n",
    "    per_device_train_batch_size=16,               # Batch size for training\n",
    "    per_device_eval_batch_size=16,                # Batch size for evaluation\n",
    "    warmup_steps=500,                            # Number of warmup steps\n",
    "    weight_decay=0.01,                           # Weight decay for regularization\n",
    "    logging_dir=\"./logs\",                        # Directory for storing logs\n",
    "    logging_steps=steps_per_epoch,                            # Log every steps in epoch\n",
    "    do_train=True,                               # Enable training\n",
    "    do_eval=True,                                # Enable evaluation\n",
    "    evaluation_strategy=\"epoch\",                 # Evaluate at the end of each epoch\n",
    "    save_strategy=\"no\",                    # Disable saving the model\n",
    "    save_total_limit=0,                    # Limit on the total amount of checkpoints. Setting to 0 disables it\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "76d72c22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "C:\\Users\\764883\\AppData\\Local\\Temp\\ipykernel_2668\\2171881100.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_ids_list = [torch.tensor(item['input_ids'], dtype=torch.long) for item in batch]\n",
      "C:\\Users\\764883\\AppData\\Local\\Temp\\ipykernel_2668\\2171881100.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels_list = [torch.tensor(item['labels'], dtype=torch.long) for item in batch]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='189' max='189' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [189/189 22:44, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.733800</td>\n",
       "      <td>0.637802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.715300</td>\n",
       "      <td>0.631604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.723900</td>\n",
       "      <td>0.622731</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "1.0     | 0.00     | 0.00     | 0.00     | 0.00        | -489.50    \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Local\\Temp\\ipykernel_2668\\2171881100.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_ids_list = [torch.tensor(item['input_ids'], dtype=torch.long) for item in batch]\n",
      "C:\\Users\\764883\\AppData\\Local\\Temp\\ipykernel_2668\\2171881100.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels_list = [torch.tensor(item['labels'], dtype=torch.long) for item in batch]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "2.0     | 0.00     | 0.00     | 0.00     | 0.00        | -489.50    \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "C:\\Users\\764883\\AppData\\Local\\Temp\\ipykernel_2668\\2171881100.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_ids_list = [torch.tensor(item['input_ids'], dtype=torch.long) for item in batch]\n",
      "C:\\Users\\764883\\AppData\\Local\\Temp\\ipykernel_2668\\2171881100.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels_list = [torch.tensor(item['labels'], dtype=torch.long) for item in batch]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   | rouge1   | rouge2   | rougeL   | rougeLsum   | bertScore  \n",
      "--------------------------------------------------------------------\n",
      "3.0     | 0.00     | 0.00     | 0.00     | 0.00        | -475.38    \n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1cAAAHWCAYAAACbsXOkAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABjdElEQVR4nO3dd3wUdf7H8fcmJJue0NIgEHov0nLAoXAGaYciouChlLOcEJqoB1hoKqigBoED9WgWREFAlCYgoCIKJ6KAVMFQQyeBAAls5vfH/rKwZEPaJJvA6/l4zCPZ735n9jvjEPPOd+YzFsMwDAEAAAAA8sXD3QMAAAAAgFsB4QoAAAAATEC4AgAAAAATEK4AAAAAwASEKwAAAAAwAeEKAAAAAExAuAIAAAAAExCuAAAAAMAEhCsAAAAAMAHhCgCKuT59+ig6OjpP644ePVoWi8XcARUxf/75pywWi2bPnl3on22xWDR69GjH69mzZ8tisejPP//Mdt3o6Gj16dPH1PHk51zJD3f+NwCAwkS4AoACYrFYcrSsW7fO3UO97Q0aNEgWi0X79u3Lss8LL7wgi8Wi3377rRBHlntHjx7V6NGjtXXrVncPBQBuOyXcPQAAuFV9+OGHTq8/+OADrVq1KlN7rVq18vU577//vtLT0/O07osvvqjhw4fn6/NvBT179tTkyZM1d+5cjRw50mWfTz75RPXq1VP9+vXz/DmPPvqoevToIavVmudtZOfo0aMaM2aMoqOj1bBhQ6f38nOuAACyR7gCgALyyCOPOL3+8ccftWrVqkztN7p48aL8/Pxy/DleXl55Gp8klShRQiVK8L+CmJgYVa1aVZ988onLcLVx40YdOHBAr732Wr4+x9PTU56envnaRn7k51wBAGSPywIBwI1at26tunXr6ueff9add94pPz8/Pf/885KkL774Qp06dVJkZKSsVquqVKmil19+WTabzWkbN95Hk3F/y8SJE/Xee++pSpUqslqtatq0qTZv3uy0rqt7riwWiwYMGKDFixerbt26slqtqlOnjlasWJFp/OvWrVOTJk3k4+OjKlWq6N13383xfVzfffedHnzwQVWoUEFWq1VRUVF6+umndenSpUz7FxAQoCNHjqhLly4KCAhQ2bJl9eyzz2Y6FufOnVOfPn0UHByskJAQ9e7dW+fOnct2LJJ99mrXrl3asmVLpvfmzp0ri8Wihx9+WGlpaRo5cqQaN26s4OBg+fv7q1WrVlq7dm22n+HqnivDMPTKK6+ofPny8vPzU5s2bbRjx45M6545c0bPPvus6tWrp4CAAAUFBalDhw769ddfHX3WrVunpk2bSpL69u3ruPQ0414nV/dcpaSk6JlnnlFUVJSsVqtq1KihiRMnyjAMp365OS9y6ptvvlGrVq3k7++vkJAQ3Xfffdq5c6dTn/Pnz2vIkCGKjo6W1WpVaGio2rZt6/Tfae/evXrggQcUHh4uHx8flS9fXj169FBSUlKexwYAecGfKwHAzU6fPq0OHTqoR48eeuSRRxQWFibJ/ot4QECAhg4dqoCAAH3zzTcaOXKkkpOTNWHChGy3O3fuXJ0/f17/+te/ZLFY9MYbb6hr167av39/tjMY33//vRYuXKj+/fsrMDBQ77zzjh544AEdPHhQpUuXliT98ssvat++vSIiIjRmzBjZbDaNHTtWZcuWzdF+z58/XxcvXlS/fv1UunRpbdq0SZMnT9bhw4c1f/58p742m03t2rVTTEyMJk6cqNWrV+vNN99UlSpV1K9fP0n2kHLffffp+++/11NPPaVatWpp0aJF6t27d47G07NnT40ZM0Zz585Vo0aNnD77s88+U6tWrVShQgWdOnVK//3vf/Xwww/riSee0Pnz5zVjxgy1a9dOmzZtynQpXnZGjhypV155RR07dlTHjh21ZcsW3XPPPUpLS3Pqt3//fi1evFgPPvigKlWqpOPHj+vdd9/VXXfdpd9//12RkZGqVauWxo4dq5EjR+rJJ59Uq1atJEktWrRw+dmGYejee+/V2rVr9dhjj6lhw4ZauXKlnnvuOR05ckRvv/22U/+cnBc5tXr1anXo0EGVK1fW6NGjdenSJU2ePFktW7bUli1bHCHwqaee0oIFCzRgwADVrl1bp0+f1vfff6+dO3eqUaNGSktLU7t27ZSamqqBAwcqPDxcR44c0VdffaVz584pODg4V+MCgHwxAACFIi4uzrjxx+5dd91lSDKmT5+eqf/Fixcztf3rX/8y/Pz8jMuXLzvaevfubVSsWNHx+sCBA4Yko3Tp0saZM2cc7V988YUhyfjyyy8dbaNGjco0JkmGt7e3sW/fPkfbr7/+akgyJk+e7Gjr3Lmz4efnZxw5csTRtnfvXqNEiRKZtumKq/0bP368YbFYjISEBKf9k2SMHTvWqe8dd9xhNG7c2PF68eLFhiTjjTfecLRdvXrVaNWqlSHJmDVrVrZjatq0qVG+fHnDZrM52lasWGFIMt59913HNlNTU53WO3v2rBEWFmb885//dGqXZIwaNcrxetasWYYk48CBA4ZhGMaJEycMb29vo1OnTkZ6erqj3/PPP29IMnr37u1ou3z5stO4DMP+39pqtTodm82bN2e5vzeeKxnH7JVXXnHq161bN8NisTidAzk9L1zJOCevH1PDhg2N0NBQ4/Tp007b8/DwMHr16uVoCw4ONuLi4rLc9i+//GJIMubPn3/TMQBAYeCyQABwM6vVqr59+2Zq9/X1dXx//vx5nTp1Sq1atdLFixe1a9eubLfbvXt3lSxZ0vE6YxZj//792a4bGxurKlWqOF7Xr19fQUFBjnVtNptWr16tLl26KDIy0tGvatWq6tChQ7bbl5z3LyUlRadOnVKLFi1kGIZ++eWXTP2feuopp9etWrVy2pdly5apRIkSjpksyX6P08CBA3M0Hsl+n9zhw4f17bffOtrmzp0rb29vPfjgg45tent7S5LS09N15swZXb16VU2aNHF5SeHNrF69WmlpaRo4cKDTpZRDhgzJ1NdqtcrDw/6/bZvNptOnTysgIEA1atTI9edmWLZsmTw9PTVo0CCn9meeeUaGYWj58uVO7dmdFzl17Ngxbd26VX369FGpUqWctte2bVstW7bM0RYSEqKffvpJR48edbmtjJmplStX6uLFi7kaBwCYjXAFAG5Wrlw5xy/r19uxY4fuv/9+BQcHKygoSGXLlnUUw8jJvSQVKlRwep0RtM6ePZvrdTPWz1j3xIkTunTpkqpWrZqpn6s2Vw4ePOj45TrjPqq77rpLUub98/HxyXS54fXjkaSEhARFREQoICDAqV+NGjVyNB5J6tGjhzw9PTV37lxJ0uXLl7Vo0SJ16NDBKajOmTNH9evXl4+Pj0qXLq2yZctq6dKlub7HJyEhQZJUrVo1p/ayZcs6fZ5kD3Jvv/22qlWrJqvVqjJlyqhs2bL67bff8nxvUUJCgiIjIxUYGOjUnlHBMmN8GbI7L3LzuZLr/za1atXSqVOnlJKSIkl64403tH37dkVFRalZs2YaPXq0U5irVKmShg4dqv/+978qU6aM2rVrp6lTp3K/FQC3IFwBgJtdP4OT4dy5c7rrrrv066+/auzYsfryyy+1atUqvf7665KUo3LaWVWlM24oVGD2ujlhs9nUtm1bLV26VMOGDdPixYu1atUqR+GFG/evsCrsZRRL+Pzzz3XlyhV9+eWXOn/+vHr27Ono89FHH6lPnz6qUqWKZsyYoRUrVmjVqlX629/+VqBlzseNG6ehQ4fqzjvv1EcffaSVK1dq1apVqlOnTqGVVy/o88KVhx56SPv379fkyZMVGRmpCRMmqE6dOk6zam+++aZ+++03Pf/887p06ZIGDRqkOnXq6PDhwwU2LgBwhYIWAFAErVu3TqdPn9bChQt15513OtoPHDjgxlFdExoaKh8fH5cP3b3Zg3gzbNu2TXv27NGcOXPUq1cvR/uqVavyPKaKFStqzZo1unDhgtPs1e7du3O1nZ49e2rFihVavny55s6dq6CgIHXu3Nnx/oIFC1S5cmUtXLjQ6VK+UaNG5WnMkr3aXeXKlR3tJ0+ezDQbtGDBArVp00YzZsxwaj937pzKlCnjeJ2TSo3Xf/7q1at1/vx5p9mrjMtOM8Zntoztuvpvs2vXLpUpU0b+/v6OtoiICPXv31/9+/fXiRMn1KhRI7366qtOl6DWq1dP9erV04svvqgffvhBLVu21PTp0/XKK68UyD4AgCvMXAFAEZQxQ3D9jEBaWpr+85//uGtITjw9PRUbG6vFixc73Quzb9++TPfpZLW+5Lx/hmFo0qRJeR5Tx44ddfXqVU2bNs3RZrPZNHny5Fxtp0uXLvLz89N//vMfLV++XF27dpWPj89Nx/7TTz9p48aNuR5zbGysvLy8NHnyZKftxcfHZ+rr6emZaYZo/vz5OnLkiFNbRijJSQn6jh07ymazacqUKU7tb7/9tiwWS47vn8utiIgINWzYUHPmzHEa5/bt2/X111+rY8eOkuz//W68vC80NFSRkZFKTU2VJCUnJ+vq1atOferVqycPDw9HHwAoLMxcAUAR1KJFC5UsWVK9e/fWoEGDZLFY9OGHHxbo5Ve5NXr0aH399ddq2bKl+vXr5/glvW7dutq6detN161Zs6aqVKmiZ599VkeOHFFQUJA+//zzXN+7c73OnTurZcuWGj58uP7880/Vrl1bCxcuzPW9NwEBAerSpYvjvqvrLwmUpL///e9auHCh7r//fnXq1EkHDhzQ9OnTVbt2bV24cCFXn5XxvK7x48fr73//uzp27KhffvlFy5cvd5qNyvjcsWPHqm/fvmrRooW2bdumjz/+2GnGS5KqVKmikJAQTZ8+XYGBgfL391dMTIwqVaqU6fM7d+6sNm3a6IUXXtCff/6pBg0a6Ouvv9YXX3yhIUOGOBWvMNuECRPUoUMHNW/eXI899pijFHtwcLBGjx4tyV7IpXz58urWrZsaNGiggIAArV69Wps3b9abb74pyf6srAEDBujBBx9U9erVdfXqVX344Yfy9PTUAw88UGDjBwBXmLkCgCKodOnS+uqrrxQREaEXX3xREydOVNu2bfXGG2+4e2gOjRs31vLly1WyZEm99NJLmjFjhsaOHau7777baabHFS8vL3355Zdq2LChxo8frzFjxqhatWr64IMP8jweDw8PLVmyRD179tRHH32kF154QeXKldOcOXNyva2MQBUREaG//e1vTu/16dNH48aN06+//qpBgwZp5cqV+uijj9SkSZM8jfuVV17RmDFj9Msvv+i5557TH3/8oa+//trpsjhJev755/XMM89o5cqVGjx4sLZs2aKlS5cqKirKqZ+Xl5fmzJkjT09PPfXUU3r44Ye1fv16l5+dccyGDBmir776SkOGDNHvv/+uCRMm6K233srT/uRUbGysVqxYodKlS2vkyJGaOHGi/vKXv2jDhg2OIOjn56f+/ftr69atGjVqlJ5++mnt3r1b//nPfzR06FBJUoMGDdSuXTt9+eWXGjp0qEaPHq2AgAAtX75cf/nLXwp0HwDgRhajKP0ZFABQ7HXp0kU7duzQ3r173T0UAAAKFTNXAIA8u3TpktPrvXv3atmyZWrdurV7BgQAgBsxcwUAyLOIiAj16dNHlStXVkJCgqZNm6bU1FT98ssvmZ7dBADArY6CFgCAPGvfvr0++eQTJSYmymq1qnnz5ho3bhzBCgBwW2LmCgAAAABMwD1XAAAAAGACwhUAAAAAmIB7rlxIT0/X0aNHFRgYKIvF4u7hAAAAAHATwzB0/vx5RUZGysPj5nNThCsXjh49mumhjAAAAABuX4cOHVL58uVv2odw5UJgYKAk+wEMCgpy82gAAAAAuEtycrKioqIcGeFmCFcuZFwKGBQURLgCAAAAkKPbhShoAQAAAAAmIFwBAAAAgAkIVwAAAABgAsIVAAAAAJiAcAUAAAAAJiBcAQAAAIAJCFcAAAAAYALCFQAAAACYgHAFAAAAACYo4e4BIGs2m/Tdd9KxY1JEhNSqleTp6e5RAQAAAHCFcFVELVwoDR4sHT58ra18eWnSJKlrV/eNCwAAAIBrXBZYBC1cKHXr5hysJOnIEXv7woXuGRcAAACArBGuihibzT5jZRiZ38toGzLE3g8AAABA0UG4KmK++y7zjNX1DEM6dEhat67QhgQAAAAgB7jnqog5dixn/dq1k2rVkmrXlurUsX+tXVuqVk3y8irYMQIAAADIjHBVxERE5KyfzSZt325frleihFS9unPgqlPHHrq8vc0fLwAAAAA7wlUR06qVvSrgkSOu77uyWOzvr1sn7dol/f67fdmxw/71woVrbdfz9LQHrBtDV/XqktVaKLsGAAAA3NIshuHqV/jbW3JysoKDg5WUlKSgoKBC//yMaoGSc8CyWOxfFyxwXY49436sGwPX779LycmuP8vTU6pa1Tlw1a4t1agh+fiYu18AAABAcZObbEC4csHd4Upy/ZyrqCgpPj73z7kyDPtM2I2Ba8cOKSnJ9ToeHlKVKpnv6apZU/L1zfNuAQAAAMUK4SqfikK4kuz3VX33nb3IRUSE/ZJBT0/ztm8Y9m27Cl1nz7pex2KRKld2Dl116thDl5+feWMDAAAAigLCVT4VlXDlLoYhHT+eOXDt2CGdOeN6HYtFio7OfE9XzZpSQEChDh8AAAAwDeEqn273cJUVw5BOnswcun7/3d6elYoVnWe5ate2l5EPDCy8sQMAAAB5QbjKJ8JV7p086bqQxvHjWa9ToULmQhq1aknBwYU3bgAAAOBmCFf5RLgyz+nTmUPXjh1SYmLW65Qvn7mQRu3aUkhIoQ0bAAAAkES4yjfCVcE7c0bauTPzJYZHj2a9TmRk5tBVp45UsmThjRsAAAC3F8JVPhGu3OfcOefQlfH1+pL0NwoPzxy4ateWSpcutGEDAADgFkW4yifCVdGTlGQPXTfe03XwYNbrhIZmLqRRu7ZUtmzhjRsAAADFG+EqnwhXxcf5865D159/Zr1O2bKZZ7lq17aHMYul0IYOAACAYoBwlU+Eq+LvwgVp167M93QdOJD1OqVLu76nKyyM0AUAAHC7IlzlE+Hq1pWSIu3enTl07d9vf46XKyVLug5dERGELgAAgFsd4SqfCFe3n0uXnENXxtc//pDS012vExLiXCo+I3yVK0foAgAAuFUQrvKJcIUMly/bQ9eNz+rat0+y2VyvExTkOnRFRRG6AAAAihvCVT4RrpCd1FRpz57MhTT27pWuXnW9TkCA60IaFSpIHh6FO34AAADkDOEqnwhXyKu0NHvAujF07d6ddejy95dq1cp8T1fFioQuAAAAdyNc5RPhCma7csV+KeGNhTR277a/54qvr3PoyvgaHS15ehbq8AEAAG5bhKt8IlyhsFy9ai+acWMhjV277LNgrvj42EPXjZcYVq5M6AIAADAb4SqfCFdwt6tX7eXhbyyksWuXvciGK1arVLNm5nu6qlSRSpQo3PEDAADcKghX+US4QlFls9kfhHzjPV07d9rLybvi7S3VqJE5dFWtKnl5Fe74AQAAihvCVT4RrlDc2GxSQoJz6Nqxwx66Ll50vY6Xl1S9euZCGlWr2gMZAAAACFf5RrjCrSI9XTp4MHMhjd9/l1JSXK9TooRUrVrm0FWtmv3SQwAAgNsJ4SqfCFe41aWnS4cPZy6k8fvv0vnzrtfx9LQHrBsLaVSvbi+yAQAAYAabTfruO+nYMSkiQmrVyr1FuwhX+US4wu3KMOyh68ZZrh07pORk1+t4eNgvJbzxnq6aNQldAAAgdxYulAYPtv8+kqF8eWnSJKlrV/eMiXCVT4QrwJlhSEePZp7l2rFDOnfO9ToeHvby8NeHrjp17MU1/PwKdfgAAKAYWLhQ6tbN/nvH9SwW+9cFC9wTsAhX+US4AnLGMKTERNeh68wZ1+tYLFKlSpnv6apZU/L3L9zxAwCAosFmk6KjnWesrmex2GewDhwo/EsECVf5RLgC8scwpBMnMgeuHTuk06ezXi86OnPoqlVLCggotKEDAIBCYBj2Ww5On5ZOnZK++UYaMSL79daulVq3LvDhOclNNuDRogBMZ7FIYWH25W9/c37vxInM93T9/ru9/c8/7cvSpc7rVKyYuZBGrVoSf/sAAMD9bgxKp07l7PurV3P/WceOmT9+MxGuABSq0FD7cuNfnU6dcl1I4/hx+zO8EhKk5cud14mKyhy6ateWgoMLbXcAALilFGZQkuz3YZcubX/cy7592fePiMjb5xQWLgt0gcsCgaLj9Gn7w5BvvMTwZn+5KlcucyGN2rWlkJBCGzYAAG5nGPZHrGQEoMIKSmXKXFuuf33j96VLXytylXHP1ZEjmQtaSNxzVawRroCi7+xZ59CV8fXIkazXiYjIfE9X7dpSqVKFN24AAPKiOAWlvMqoFpixvxmoFljMEa6A4isp6doM1/Wh69ChrNcJC8s8y1W7tv1/GAAAmO3GoHR9GMoqKJ0+LV25krfPc0dQyitXz7mKipLi43nOVbFFuAJuPcnJ9pmuGwtpJCRkvU5oqOt7ukJDC2/cAICijaBkPptN+u47+y0AERFSq1aFfyng9QhX+US4Am4f589Lu3ZlDl0HDmS9TpkymQNXnTr20JVx6QIAoPi5WVDKKjSZGZSyC03FMSjdCghX+US4ApCSYg9dNxbSOHDA9Y22kv3eLVehKzyc0AUAhY2gBLMQrvKJcAUgKxcvSrt3Zw5df/yRdegKCXFdSCMyktAFADmR06B04/cEJZih2IWrqVOnasKECUpMTFSDBg00efJkNWvWzGXf1q1ba/369ZnaO3bsqKVLl+rKlSt68cUXtWzZMu3fv1/BwcGKjY3Va6+9psjIyByNh3AFILcuXbKHrhsLaezbJ6Wnu14nOPha4Lo+dJUvT+gCcOsiKKG4KVbh6tNPP1WvXr00ffp0xcTEKD4+XvPnz9fu3bsV6uKu8TNnzigtLc3x+vTp02rQoIH++9//qk+fPkpKSlK3bt30xBNPqEGDBjp79qwGDx4sm82m//3vfzkaE+EKgFkuX5b27Ml8T9fevfYbdl0JDHRdSKNCBUIXgKKlKAUlV6GJoAQzFKtwFRMTo6ZNm2rKlCmSpPT0dEVFRWngwIEaPnx4tuvHx8dr5MiROnbsmPz9/V322bx5s5o1a6aEhARVqFAh220SrgAUtNRUe8C6MXTt2ZP1M0kCAqRatTJfYlihguThUbjjB3DrISgBruUmG5QopDG5lJaWpp9//lkjRoxwtHl4eCg2NlYbN27M0TZmzJihHj16ZBmsJCkpKUkWi0UhISEu309NTVVqaqrjdXJycs52AADyyGqV6ta1L9dLS7NfSnjjPV179kgXLkibN9uX6/n5OYeujK/R0YQu4HZ1fVDKSWlwghJgDreGq1OnTslmsyksLMypPSwsTLt27cp2/U2bNmn79u2aMWNGln0uX76sYcOG6eGHH84yaY4fP15jxozJ3eABoAB4e1+blbrelSv2ohk3hq7du+1FNn7+2b5cz9fXHrpuvMSwUiX3Pi8EQO4QlIDiw63hKr9mzJihevXqZVn84sqVK3rooYdkGIamTZuW5XZGjBihoUOHOl4nJycrKirK9PECQF55eUk1a9qXBx641n71qj103VhIY9cue5GNLVvsy/V8fOzbufGeripVCF1AQcsqKGUXmgo6KGV8T1AC8set4apMmTLy9PTU8ePHndqPHz+u8PDwm66bkpKiefPmaezYsS7fzwhWCQkJ+uabb256faTVapXVas39DgCAm5UoIdWoYV/uv/9au80m7d+fOXTt3GkvsrF1q325ntVq386NoatqVfvnAHBGUAJwI7f+79Lb21uNGzfWmjVr1KVLF0n2ghZr1qzRgAEDbrru/PnzlZqaqkceeSTTexnBau/evVq7dq1Kly5dEMMHgCLL01OqVs2+3HfftXabTfrzz8yFNHbutF9e+Ntv9uV6Xl720HVjIY2qVe3vAbcCghIAM7i9WuCnn36q3r17691331WzZs0UHx+vzz77TLt27VJYWJh69eqlcuXKafz48U7rtWrVSuXKldO8efOc2q9cuaJu3bppy5Yt+uqrr5zu5ypVqpS8vb2zHRPVAgHcbtLTpYQE59C1Y4c9dKWkuF6nRAmpevXMhTSqVbPfOwa4S06CkqvQlNeg5Ot78+cmEZSA4q3YVAuUpO7du+vkyZMaOXKkEhMT1bBhQ61YscIRig4ePCiPG8pd7d69W99//72+/vrrTNs7cuSIlixZIklq2LCh03tr165V69atC2Q/AKA48/CwF7qoVEnq1Olae3q6dOhQ5kIav/9ur16Y0Xa9EiXsAevGQhrVq9svPQRyg6AEoDhx+8xVUcTMFQDcnGHYQ9eNgev336Wsnmbh6Wm/lPDGe7pq1LAX2cCtL7dBKeN7ghIAdypWDxEuighXAJA3hiEdOZI5cO3YISUluV7Hw8NeqfDG0FWzpv2XYxRNBCUAtwvCVT4RrgDAXIYhHTvmOnSdPet6HYtFqlw5cyGNmjX5pdlsGUEpN89QMisouXpuEkEJQFFCuMonwhUAFA7DkI4fzxy4duyQzpxxvY7FIkVHuw5dAQGFOvwiiaAEAOYiXOUT4QoA3MswpJMnXRfSOHky6/WiozMX0qhVSwoMzP0YbDbpu+/sM24REVKrVoX/kGVXQSknoYmgBADmIVzlE+EKAIqukyddF9K44Xn0TipUyBy6ateWsvoRv3ChNHiwdPjwtbby5aVJk6SuXfM2boISABRPhKt8IlwBQPFz+nTm0LVjh5SYmPU65cs7B646daR9+6Teve1h6HoWi/3rggXS/fffPChlFZrMDkpZhSaCEgCYh3CVT4QrALh1nDljfxjyjZcYHj2at+15eNiXq1fztj5BCQCKl2L1EGEAAApSqVJSy5b25XrnzjmHrh07pC1b7DNMN5Oebl8kghIAwBnhCgBwWwoJkZo3ty8ZPvlE+sc/sl938mTpn/8kKAEAnHm4ewAAABQVERE561e3LsEKAJAZ4QoAgP/XqpW9yEVG8YobWSxSVJS9HwAANyJcAQDw/zw97eXWpcwBK+N1fHzhP+8KAFA8EK4AALhO1672cuvlyjm3ly9vb8/rc64AALc+CloAAHCDrl2l++6TvvtOOnbMfi9Wq1bMWAEAbo5wBQCAC56eUuvW7h4FAKA44bJAAAAAADAB4QoAAAAATEC4AgAAAAATEK4AAAAAwASEKwAAAAAwAeEKAAAAAExAuAIAAAAAExCuAAAAAMAEhCsAAAAAMAHhCgAAAABMQLgCAAAAABMQrgAAAADABIQrAAAAADAB4QoAAAAATEC4AgAAAAATEK4AAAAAwASEKwAAAAAwAeEKAAAAAExAuAIAAAAAExCuAAAAAMAEhCsAAAAAMAHhCgAAAABMQLgCAAAAABMQrgAAAADABIQrAAAAADAB4QoAAAAATEC4AgAAAAATEK4AAAAAwASEKwAAAAAwAeEKAAAAAExAuAIAAAAAExCuAAAAAMAEhCsAAAAAMAHhCgAAAABMQLgCAAAAABMQrgAAAADABIQrAAAAADBBkQhXU6dOVXR0tHx8fBQTE6NNmzZl2bd169ayWCyZlk6dOjn6GIahkSNHKiIiQr6+voqNjdXevXsLY1cAAAAA3KbcHq4+/fRTDR06VKNGjdKWLVvUoEEDtWvXTidOnHDZf+HChTp27Jhj2b59uzw9PfXggw86+rzxxht65513NH36dP3000/y9/dXu3btdPny5cLaLQAAAAC3GYthGIY7BxATE6OmTZtqypQpkqT09HRFRUVp4MCBGj58eLbrx8fHa+TIkTp27Jj8/f1lGIYiIyP1zDPP6Nlnn5UkJSUlKSwsTLNnz1aPHj2y3WZycrKCg4OVlJSkoKCg/O0gAAAAgGIrN9nArTNXaWlp+vnnnxUbG+to8/DwUGxsrDZu3JijbcyYMUM9evSQv7+/JOnAgQNKTEx02mZwcLBiYmKy3GZqaqqSk5OdFgAAAADIDbeGq1OnTslmsyksLMypPSwsTImJidmuv2nTJm3fvl2PP/64oy1jvdxsc/z48QoODnYsUVFRud0VAAAAALc5t99zlR8zZsxQvXr11KxZs3xtZ8SIEUpKSnIshw4dMmmEAAAAAG4Xbg1XZcqUkaenp44fP+7Ufvz4cYWHh9903ZSUFM2bN0+PPfaYU3vGernZptVqVVBQkNMCAAAAALnh1nDl7e2txo0ba82aNY629PR0rVmzRs2bN7/puvPnz1dqaqoeeeQRp/ZKlSopPDzcaZvJycn66aefst0mAAAAAORVCXcPYOjQoerdu7eaNGmiZs2aKT4+XikpKerbt68kqVevXipXrpzGjx/vtN6MGTPUpUsXlS5d2qndYrFoyJAheuWVV1StWjVVqlRJL730kiIjI9WlS5fC2i0AAAAAtxm3h6vu3bvr5MmTGjlypBITE9WwYUOtWLHCUZDi4MGD8vBwnmDbvXu3vv/+e3399dcut/nvf/9bKSkpevLJJ3Xu3Dn99a9/1YoVK+Tj41Pg+wMAAADg9uT251wVRTznCgAAAIBUjJ5zBQAAAAC3CsIVAAAAAJiAcAUAAAAAJiBcAQAAAIAJCFcAAAAAYALCFQAAAACYgHAFAAAAACYgXAEAAACACQhXAAAAAGACwhUAAAAAmIBwBQAAAAAmIFwBAAAAgAkIVwAAAABgAsIVAAAAAJiAcAUAAAAAJiBcAQAAAIAJCFcAAAAAYALCFQAAAACYgHAFAAAAACYgXAEAAACACQhXAAAAAGACwhUAAAAAmIBwBQAAAAAmIFwBAAAAgAkIVwAAAABgAsIVAAAAAJiAcAUAAAAAJiBcAQAAAIAJCFcAAAAAYALCFQAAAACYgHAFAAAAACYgXAEAAACACQhXAAAAAGACwhUAAAAAmIBwBQAAAAAmIFwBAAAAgAkIVwAAAABgAsIVAAAAAJiAcAUAAAAAJiBcAQAAAIAJCFcAAAAAYALCFQAAAACYgHAFAAAAACYgXAEAAACACQhXAAAAAGACwhUAAAAAmIBwBQAAAAAmIFwBAAAAgAkIVwAAAABgAsIVAAAAAJggT+Hq0KFDOnz4sOP1pk2bNGTIEL333numDQwAAAAAipM8hat//OMfWrt2rSQpMTFRbdu21aZNm/TCCy9o7Nixpg4QAAAAAIqDPIWr7du3q1mzZpKkzz77THXr1tUPP/ygjz/+WLNnz87VtqZOnaro6Gj5+PgoJiZGmzZtumn/c+fOKS4uThEREbJarapevbqWLVvmeN9ms+mll15SpUqV5OvrqypVqujll1+WYRi53k8AAAAAyKkSeVnpypUrslqtkqTVq1fr3nvvlSTVrFlTx44dy/F2Pv30Uw0dOlTTp09XTEyM4uPj1a5dO+3evVuhoaGZ+qelpalt27YKDQ3VggULVK5cOSUkJCgkJMTR5/XXX9e0adM0Z84c1alTR//73//Ut29fBQcHa9CgQXnZXQAAAADIVp7CVZ06dTR9+nR16tRJq1at0ssvvyxJOnr0qEqXLp3j7bz11lt64okn1LdvX0nS9OnTtXTpUs2cOVPDhw/P1H/mzJk6c+aMfvjhB3l5eUmSoqOjnfr88MMPuu+++9SpUyfH+5988km2M2IAAAAAkB95uizw9ddf17vvvqvWrVvr4YcfVoMGDSRJS5YscVwumJ20tDT9/PPPio2NvTYYDw/FxsZq48aNLtdZsmSJmjdvrri4OIWFhalu3boaN26cbDabo0+LFi20Zs0a7dmzR5L066+/6vvvv1eHDh2yHEtqaqqSk5OdFgAAAADIjTzNXLVu3VqnTp1ScnKySpYs6Wh/8skn5efnl6NtnDp1SjabTWFhYU7tYWFh2rVrl8t19u/fr2+++UY9e/bUsmXLtG/fPvXv319XrlzRqFGjJEnDhw9XcnKyatasKU9PT9lsNr366qvq2bNnlmMZP368xowZk6NxAwAAAIAreZq5unTpklJTUx3BKiEhQfHx8VneK2WW9PR0hYaG6r333lPjxo3VvXt3vfDCC5o+fbqjz2effaaPP/5Yc+fO1ZYtWzRnzhxNnDhRc+bMyXK7I0aMUFJSkmM5dOhQge0DAAAAgFtTnmau7rvvPnXt2lVPPfWUzp07p5iYGHl5eenUqVN666231K9fv2y3UaZMGXl6eur48eNO7cePH1d4eLjLdSIiIuTl5SVPT09HW61atZSYmKi0tDR5e3vrueee0/Dhw9WjRw9JUr169ZSQkKDx48erd+/eLrdrtVodBToAAAAAIC/yNHO1ZcsWtWrVSpK0YMEChYWFKSEhQR988IHeeeedHG3D29tbjRs31po1axxt6enpWrNmjZo3b+5ynZYtW2rfvn1KT093tO3Zs0cRERHy9vaWJF28eFEeHs675enp6bQOAAAAAJgtT+Hq4sWLCgwMlCR9/fXX6tq1qzw8PPSXv/xFCQkJOd7O0KFD9f7772vOnDnauXOn+vXrp5SUFEf1wF69emnEiBGO/v369dOZM2c0ePBg7dmzR0uXLtW4ceMUFxfn6NO5c2e9+uqrWrp0qf78808tWrRIb731lu6///687CoAAAAA5EieLgusWrWqFi9erPvvv18rV67U008/LUk6ceKEgoKCcryd7t276+TJkxo5cqQSExPVsGFDrVixwlHk4uDBg06zUFFRUY7Pq1+/vsqVK6fBgwdr2LBhjj6TJ0/WSy+9pP79++vEiROKjIzUv/71L40cOTIvuwoAAAAAOWIxDMPI7UoLFizQP/7xD9lsNv3tb3/TqlWrJNmr7n377bdavny56QMtTMnJyQoODlZSUlKuwiIAAACAW0tuskGewpUkJSYm6tixY2rQoIFjdmnTpk0KCgpSzZo187LJIoNwBQAAAEDKXTbI02WBkhQeHq7w8HAdPnxYklS+fPkcP0AYAAAAAG41eSpokZ6errFjxyo4OFgVK1ZUxYoVFRISopdffpmqfAAAAABuS3mauXrhhRc0Y8YMvfbaa2rZsqUk6fvvv9fo0aN1+fJlvfrqq6YOEgAAAACKujzdcxUZGanp06fr3nvvdWr/4osv1L9/fx05csS0AboD91wBAAAAkHKXDfJ0WeCZM2dcFq2oWbOmzpw5k5dNAgAAAECxlqdw1aBBA02ZMiVT+5QpU1S/fv18DwoAAAAAips83XP1xhtvqFOnTlq9erWaN28uSdq4caMOHTqkZcuWmTpAAAAAACgO8jRzddddd2nPnj26//77de7cOZ07d05du3bVjh079OGHH5o9RgAAAAAo8vL8EGFXfv31VzVq1Eg2m82sTboFBS0AAAAASIVQ0AIAAAAA4IxwBQAAAAAmIFwBAAAAgAlyVS2wa9euN33/3Llz+RkLAAAAABRbuQpXwcHB2b7fq1evfA0IAAAAAIqjXIWrWbNmFdQ4AAAAAKBY454rAAAAADAB4QoAAAAATEC4AgAAAAATEK4AAAAAwASEKwAAAAAwAeEKAAAAAExAuAIAAAAAExCuAAAAAMAEhCsAAAAAMAHhCgAAAABMQLgCAAAAABMQrgAAAADABIQrAAAAADAB4QoAAAAATEC4AgAAAAATEK4AAAAAwASEKwAAAAAwAeEKAAAAAExAuAIAAAAAExCuAAAAAMAEhCsAAAAAMAHhCgAAAABMQLgCAAAAABMQrgAAAADABIQrAAAAADAB4QoAAAAATEC4AgAAAAATEK4AAAAAwASEKwAAAAAwAeEKAAAAAExAuAIAAAAAExCuAAAAAMAEhCsAAAAAMAHhCgAAAABMQLgCAAAAABMQrgAAAADABG4PV1OnTlV0dLR8fHwUExOjTZs23bT/uXPnFBcXp4iICFmtVlWvXl3Lli1z6nPkyBE98sgjKl26tHx9fVWvXj3973//K8jdAAAAAHCbK+HOD//00081dOhQTZ8+XTExMYqPj1e7du20e/duhYaGZuqflpamtm3bKjQ0VAsWLFC5cuWUkJCgkJAQR5+zZ8+qZcuWatOmjZYvX66yZctq7969KlmyZCHuGQAAAIDbjcUwDMNdHx4TE6OmTZtqypQpkqT09HRFRUVp4MCBGj58eKb+06dP14QJE7Rr1y55eXm53Obw4cO1YcMGfffdd3keV3JysoKDg5WUlKSgoKA8bwcAAABA8ZabbOC2ywLT0tL0888/KzY29tpgPDwUGxurjRs3ulxnyZIlat68ueLi4hQWFqa6detq3LhxstlsTn2aNGmiBx98UKGhobrjjjv0/vvv33QsqampSk5OdloAAAAAIDfcFq5OnTolm82msLAwp/awsDAlJia6XGf//v1asGCBbDabli1bppdeeklvvvmmXnnlFac+06ZNU7Vq1bRy5Ur169dPgwYN0pw5c7Icy/jx4xUcHOxYoqKizNlJAAAAALcNt95zlVvp6ekKDQ3Ve++9J09PTzVu3FhHjhzRhAkTNGrUKEefJk2aaNy4cZKkO+64Q9u3b9f06dPVu3dvl9sdMWKEhg4d6nidnJxMwAIAAACQK24LV2XKlJGnp6eOHz/u1H78+HGFh4e7XCciIkJeXl7y9PR0tNWqVUuJiYlKS0uTt7e3IiIiVLt2baf1atWqpc8//zzLsVitVlmt1nzsDQAAAIDbndsuC/T29lbjxo21Zs0aR1t6errWrFmj5s2bu1ynZcuW2rdvn9LT0x1te/bsUUREhLy9vR19du/e7bTenj17VLFixQLYCwAAAACwc+tzroYOHar3339fc+bM0c6dO9WvXz+lpKSob9++kqRevXppxIgRjv79+vXTmTNnNHjwYO3Zs0dLly7VuHHjFBcX5+jz9NNP68cff9S4ceO0b98+zZ07V++9955THwAAAAAwm1vvuerevbtOnjypkSNHKjExUQ0bNtSKFSscRS4OHjwoD49r+S8qKkorV67U008/rfr166tcuXIaPHiwhg0b5ujTtGlTLVq0SCNGjNDYsWNVqVIlxcfHq2fPnoW+fwAAAABuH259zlVRxXOuAAAAAEjF5DlXAAAAAHArIVwBAAAAgAkIVwAAAABgAsIVAAAAAJiAcAUAAAAAJiBcAQAAAIAJCFcAAAAAYALCFQAAAACYgHAFAAAAACYgXAEAAACACQhXAAAAAGACwhUAAAAAmIBwBQAAAAAmIFwBAAAAgAkIVwAAAABgAsIVAAAAAJiAcAUAAAAAJiBcAQAAAIAJCFcAAAAAYALCFQAAAACYgHAFAAAAACYgXAEAAACACQhXAAAAAGACwhUAAAAAmIBwBQAAAAAmIFwBAAAAgAkIVwAAAABgAsIVAAAAAJiAcAUAAAAAJiBcAQAAAIAJCFcAAAAAYALCFQAAAACYgHAFAAAAACYgXAEAAACACQhXAAAAAGACwhUAAAAAmIBwBQAAAAAmIFwBAAAAgAkIVwAAAABgAsIVAAAAAJiAcAUAAAAAJiBcAQAAAIAJCFcAAAAAYALCFQAAAACYgHAFAAAAACYgXAEAAACACQhXAAAAAGACwhUAAAAAmIBwBQAAAAAmIFwBAAAAgAkIVwAAAABgAsIVAAAAAJigSISrqVOnKjo6Wj4+PoqJidGmTZtu2v/cuXOKi4tTRESErFarqlevrmXLlrns+9prr8lisWjIkCEFMHIAAAAAsCvh7gF8+umnGjp0qKZPn66YmBjFx8erXbt22r17t0JDQzP1T0tLU9u2bRUaGqoFCxaoXLlySkhIUEhISKa+mzdv1rvvvqv69esXwp4AAAAAuJ25febqrbfe0hNPPKG+ffuqdu3amj59uvz8/DRz5kyX/WfOnKkzZ85o8eLFatmypaKjo3XXXXepQYMGTv0uXLignj176v3331fJkiULY1cAAAAA3MbcGq7S0tL0888/KzY21tHm4eGh2NhYbdy40eU6S5YsUfPmzRUXF6ewsDDVrVtX48aNk81mc+oXFxenTp06OW07K6mpqUpOTnZaAAAAACA33HpZ4KlTp2Sz2RQWFubUHhYWpl27drlcZ//+/frmm2/Us2dPLVu2TPv27VP//v115coVjRo1SpI0b948bdmyRZs3b87ROMaPH68xY8bkb2cAAAAA3NbcfllgbqWnpys0NFTvvfeeGjdurO7du+uFF17Q9OnTJUmHDh3S4MGD9fHHH8vHxydH2xwxYoSSkpIcy6FDhwpyFwAAAADcgtw6c1WmTBl5enrq+PHjTu3Hjx9XeHi4y3UiIiLk5eUlT09PR1utWrWUmJjouMzwxIkTatSokeN9m82mb7/9VlOmTFFqaqrTupJktVpltVpN3DOT2GzSd99Jx45JERFSq1bSDWMHAAAAUDS4debK29tbjRs31po1axxt6enpWrNmjZo3b+5ynZYtW2rfvn1KT093tO3Zs0cRERHy9vbW3XffrW3btmnr1q2OpUmTJurZs6e2bt2aKVgVWQsXStHRUps20j/+Yf8aHW1vBwAAAFDkuP2ywKFDh+r999/XnDlztHPnTvXr108pKSnq27evJKlXr14aMWKEo3+/fv105swZDR48WHv27NHSpUs1btw4xcXFSZICAwNVt25dp8Xf31+lS5dW3bp13bKPubZwodStm3T4sHP7kSP2dgIWAAAAUOS4/TlX3bt318mTJzVy5EglJiaqYcOGWrFihaPIxcGDB+XhcS0DRkVFaeXKlXr66adVv359lStXToMHD9awYcPctQvmstmkwYMlw8j8nmFIFos0cKDUrp3k52d/DQAAAMDtLIbh6rf421tycrKCg4OVlJSkoKCgwv3wdevslwDmhJeXFBQkBQfbv16/5KbNaiWkAQAAAC7kJhu4feYKNzh2LOd9r1yRTp+2L/mREdJuFsJyEth8fAhpAAAAuG0RroqaiIic9fvqK6lBAyk52b4kJV37Pqdt58/bLzU0K6SVKJH/mbSgIMnXl5AGAACAYodwVdS0aiWVL28vXuHqik2Lxf5++/b5L8ueni6lpOQtmF3flhHSrl6VzpyxL/mREdLyepkjIQ0AAABuQLgqajw9pUmT7FUBLRbngJURFOLjzXnelYeHFBhoX/IjI6TlNZxd/9rMkObpac5MGoVDAAAAkAOEq6Koa1dpwQJ71cDry7GXL28PVl27um1oLl0f0sqVy/t20tOlixfzP5OWEdJsNunsWfuSHxkhLb8zaYQ0AACAWxrVAl1wa7XA69ls0nff2YtcRETYLxksLg9BdifDMG8m7bqHVeebh0fuZ81ctfv7E9IAAAAKCdUCbxWenlLr1u4eRfFjsUgBAfYlMjLv2zEM82bS0tPty7lz9iU/MkJafmfSCGkAAACmIlwBWbFY7AHE39+ckGbGTJrNZm5ICwzMe+n9jCUggJAGAAAgwhVQ8K4PaTktte+KYUiXLpkzk5YR0pKS7Et+98+smTQPj/yNBQAAwI0IV0BxYbHYi2L4+ZkT0m4WwnIS2JKS7CHNMMwLaWbNpBHSAACAGxCugNvN9SEtPDzv2zEM6fJlc2bSrl61by+jPb/7FxiY99L7Ge2ENAAAkEuEq3yw2Wy6cuWKu4eBYs7Ly0uexbEKpMVif1Czr685IS23wczVTJqZIU26FtLy86y0wEBCGgAAtwnCVR4YhqHExESdy29BAeD/hYSEKDw8XJbbsTDE9SEtLCzv2zEMKTU1/zNpGSFNks6fty9HjuRvH82aSSuOIRwAgNsI4SoPMoJVaGio/Pz8bs9fiGEKwzB08eJFnThxQpIUkZ97qW53Fovk42NfzAhpZsykZcxsmxXSAgLMmUkjpAEAUCAIV7lks9kcwap06dLuHg5uAb6+vpKkEydOKDQ0tHheIngruT6khYbmb1tmzaRlhLQLF+zL0aP5G1dGSMvLQ6wz2m6HkMaD3AEAuUS4yqWMe6z8/PzcPBLcSjLOpytXrhCubiVWqz2gmRHS8vuMtORk+3Yk80Kav3/+Z9KCgopmYFm4UBo8WDp8+Fpb+fLSpElS167uGxcAoEgjXOURlwLCTJxPuCmrVSpb1r7kx/UhLT/VHTNCWkqKfTl2LH/jyghp+XlWWmCgVMKk/6UtXCh162a/RPR6R47Y2xcsIGABAFwiXAHA7cLMkHb+fP5n0i5ftm/PrJDm55f/mTR/f/uM1Y3BSrK3WSzSkCHSffcVzRk3AIBbEa7c6Fa4nD86OlpDhgzRkCFDctR/3bp1atOmjc6ePauQkJACG9fs2bM1ZMgQKjoCBcFqtS9lyuRvO2lp5sykZYS0ixftS2Ji/vcxK4YhHTokvfqq1Lq1/RiULSuVKlX8foADAExHuHKTwr6cP7vLzkaNGqXRo0fnerubN2+Wv79/jvu3aNFCx44dU3BwcK4/C8AtxtvbHk7MCGlmzKRdupTzzxw1yvm1xSKVLGkPWhn7lN33AQH29QAAtwzClRu443L+Y9ddbvPpp59q5MiR2r17t6MtICDA8b1hGLLZbCqRg/sXyuby8iJvb2+F5+eBswBwI29vqXRp+5IfV65Iy5ZJXbpk37dOHXv/kyels2ftP9DPnLEv1/1svSlvb+fQlV0gK13avg4AoMjycPcAbgWGce2WgeyW5GRp0KCsL+eX7DNaycnZb8vVNrISHh7uWIKDg2WxWByvd+3apcDAQC1fvlyNGzeW1WrV999/rz/++EP33XefwsLCFBAQoKZNm2r16tVO242OjlZ8fLzjtcVi0X//+1/df//98vPzU7Vq1bRkyRLH++vWrZPFYnFcrjd79myFhIRo5cqVqlWrlgICAtS+fXunMHj16lUNGjRIISEhKl26tIYNG6bevXurS05+AbrOtGnTVKVKFXl7e6tGjRr68MMPHe8ZhqHRo0erQoUKslqtioyM1KBBgxzv/+c//1G1atXk4+OjsLAwdevWLVefDaAY8PKS/v53+2UEWc0oWSxSVJT066/2EHXmjD1kHT8ubd8urVtn/wvZ9OnSyy/bf6D37Cndc4/UqJFUoYL9gdmSfcbtyBH7ttaskT79VJoyRRo9WhowQOreXbr7bqlBAyky0n4pZnCwVLWq9Je/2Mfat6/03HPS669LM2dKS5ZIP/wg7d1rD33p6YV19AAAYubKFBcv2q/uMINh2C8VzMlVcxcu2O+9Nsvw4cM1ceJEVa5cWSVLltShQ4fUsWNHvfrqq7Jarfrggw/UuXNn7d69WxUqVMhyO2PGjNEbb7yhCRMmaPLkyerZs6cSEhJUqlQpl/0vXryoiRMn6sMPP5SHh4ceeeQRPfvss/r4448lSa+//ro+/vhjzZo1S7Vq1dKkSZO0ePFitWnTJsf7tmjRIg0ePFjx8fGKjY3VV199pb59+6p8+fJq06aNPv/8c7399tuaN2+e6tSpo8TERP3666+SpP/9738aNGiQPvzwQ7Vo0UJnzpzRd999l4sjC6DY8PS0X5/drZs9SF3/V6yMwBUf73x/VYkSuS+5f/GidOqUfebr1Knsvz992h6UMi5h/OOPnO9P6dK5u1wxI/wBAHKNcAWHsWPHqm3bto7XpUqVUoMGDRyvX375ZS1atEhLlizRgAEDstxOnz599PDDD0uSxo0bp3feeUebNm1S+/btXfa/cuWKpk+fripVqkiSBgwYoLFjxzrenzx5skaMGKH7779fkjRlyhQtW7YsV/s2ceJE9enTR/3795ckDR06VD/++KMmTpyoNm3a6ODBgwoPD1dsbKy8vLxUoUIFNWvWTJJ08OBB+fv76+9//7sCAwNVsWJF3XHHHbn6fADFSNeu9tknVzfGxsebc922n599Fusmf6hykp4unTuXdQBzFcjOn7dXTjpxwr7kZmxZBTBXgYxiHgDgQLgygZ+ffRYpJ779VurYMft+y5ZJd96Z/eeaqUmTJk6vL1y4oNGjR2vp0qU6duyYrl69qkuXLungwYM33U79+vUd3/v7+ysoKEgnbvI/dj8/P0ewkqSIiAhH/6SkJB0/ftwRdCTJ09NTjRs3VnouLnfZuXOnnnzySae2li1batKkSZKkBx98UPHx8apcubLat2+vjh07qnPnzipRooTatm2rihUrOt5r376947JHALeorl3t5daLSklXDw97iClVSqpRI2frpKbaZ7xyOjt26pT9EseLF6WEBPuSExTzAAAHwpUJLJacX553zz32P34eOeL6nimLxf7+PfcU/v/Db6z69+yzz2rVqlWaOHGiqlatKl9fX3Xr1k1paWk33Y6Xl5fTa4vFctMg5Kq/kZsbykwQFRWl3bt3a/Xq1Vq1apX69++vCRMmaP369QoMDNSWLVu0bt06ff311xo5cqRGjx6tzZs3F2g5eQBu5ulpL7deXFmt9nu1IiNz1t8w7LNd2c2IXf99Xot5ZJTyz+nsGMU8ABQThKtClpfL+d1lw4YN6tOnj+NyvAsXLujPP/8s1DEEBwcrLCxMmzdv1p3/P5Vns9m0ZcsWNWzYMMfbqVWrljZs2KDevXs72jZs2KDatWs7Xvv6+qpz587q3Lmz4uLiVLNmTW3btk2NGjVSiRIlFBsbq9jYWI0aNUohISH65ptv1LUg6uYDgDtYLNcepnzd1QQ3dfWqPVTldHbs5En7c8lSU+1/ZTxyJOfjCwrK3exYcLB9xg8AChHhyg0K43J+M1SrVk0LFy5U586dZbFY9NJLL+XqUjyzDBw4UOPHj1fVqlVVs2ZNTZ48WWfPns322V3Xe+655/TQQw/pjjvuUGxsrL788kstXLjQUf1w9uzZstlsiomJkZ+fnz766CP5+vqqYsWK+uqrr7R//37deeedKlmypJYtW6b09HTVyOmlOQBwq8prMY/c3DuWn2Ie18+GUcwDQCEgXLlJUbuc35W33npL//znP9WiRQuVKVNGw4YNU3JycqGPY9iwYUpMTFSvXr3k6empJ598Uu3atZNnLg5Wly5dNGnSJE2cOFGDBw9WpUqVNGvWLLX+/0t+QkJC9Nprr2no0KGy2WyqV6+evvzyS5UuXVohISFauHChRo8ercuXL6tatWr65JNPVKdOnQLaYwC4hfn5SRUr2pecyK6Yh6vvM4p5HD9uX3IzttzMjlHMA8ANLEZh39xSDCQnJys4OFhJSUkKCgpyeu/y5cs6cOCAKlWqJB8fHzeN8PaWnp6uWrVq6aGHHtLLL7/s7uGYgvMKAEyUmpr1bNjNinnkFsU8gNvCzbLBjZi5QpGXkJCgr7/+WnfddZdSU1M1ZcoUHThwQP/4xz/cPTQAQFFktUrlytmXnHBVzCO77wuimIerwh4U8wCKFcIVijwPDw/Nnj1bzz77rAzDUN26dbV69WrVqlXL3UMDANwK8lrM4/TpnFdWpJgHcFsgXKHIi4qK0oYNG9w9DAAArilRQgoLsy85dbNiHq6+N6uYR3al7inmAZiGcAUAAFAY8lLM4+zZnM+OUcwDcDvCFQAAQFHk4WG/56p0aSmnj/+4WTGPrL6/csU+q5aQYF9ywmKxB6zczI5RzAO3AcIVAADArSIvxTySk3MXyDKKeZw+bV/MLuaR8T3FPFAMEa4AAABuVxaLvQBGcHDei3lkF8byU8wjODh3gYxiHnAzwhUAAAByLi/FPFJScjc7llHMIynJvphRzCOr7ynmARMRrgAAAFCw/P3tS16LeeTk3jGKeaAIIFy5k80mffeddOyYFBEhtWpV5P+xtm7dWg0bNlR8fLwkKTo6WkOGDNGQIUOyXMdisWjRokXq0qVLvj7brO3czOjRo7V48WJt3bq1wD4DAABko7gW88iqsAfFPG4bhCt3WbhQGjxYOnz4Wlv58tKkSVLXrqZ/XOfOnXXlyhWtWLEi03vfffed7rzzTv3666+qX79+rra7efNm+fv7mzVMSVkHnGPHjqlkyZKmfhYAALhF5LeYR07CGMU8kA3ClTssXCh162b/x3m9I0fs7QsWmB6wHnvsMT3wwAM6fPiwypcv7/TerFmz1KRJk1wHK0kqW7asWUPMVnh4eKF9FgAAuMXlpZjHlSvSmTMU80CWCFdmMAz7lHJO2GzSoEGZg1XGdiwW+4xWbGz2lwj6+eV4ivnvf/+7ypYtq9mzZ+vFF190tF+4cEHz58/XhAkTdPr0aQ0YMEDffvutzp49qypVquj555/Xww8/nOV2b7wscO/evXrssce0adMmVa5cWZMmTcq0zrBhw7Ro0SIdPnxY4eHh6tmzp0aOHCkvLy/Nnj1bY8aMkWS/DFCyh78+ffpkuixw27ZtGjx4sDZu3Cg/Pz898MADeuuttxQQECBJ6tOnj86dO6e//vWvevPNN5WWlqYePXooPj5eXl5eOTpu6enpeuWVV/Tee+/p5MmTqlWrll577TW1b99ekpSWlqahQ4fq888/19mzZxUWFqannnpKI0aMkGEYGjNmjGbOnKnjx4+rdOnS6tatm955550cfTYAAChivLzyV8wjJ7NjFPMolrfOZCBcmeHiRfu1tGYwDPulgsHB2fe9cMF+c2gOlChRQr169dLs2bP1wgsvOILL/PnzZbPZ9PDDD+vChQtq3Lixhg0bpqCgIC1dulSPPvqoqlSpombNmmX7Genp6eratavCwsL0008/KSkpyeW9WIGBgZo9e7YiIyO1bds2PfHEEwoMDNS///1vde/eXdu3b9eKFSu0evVqSVKwi2ORkpKidu3aqXnz5tq8ebNOnDihxx9/XAMGDNDs2bMd/dauXauIiAitXbtW+/btU/fu3dWwYUM98cQTOTpukyZN0ptvvql3331Xd9xxh2bOnKl7771XO3bsULVq1fTOO+9oyZIl+uyzz1ShQgUdOnRIhw4dkiR9/vnnevvttzVv3jzVqVNHiYmJ+vXXX3P0uQAA4BZRlIt5+Pvn7kHQhVHMo5BvnTEb4eo28s9//lMTJkzQ+vXr1bp1a0n2WaEHHnhAwcHBCg4O1rPPPuvoP3DgQK1cuVKfffZZjsLV6tWrtWvXLq1cuVKRkZGSpHHjxqlDhw5O/a6fOYuOjtazzz6refPm6d///rd8fX0VEBCgEiVK3PQywLlz5+ry5cv64IMPHPd8TZkyRZ07d9brr7+usP//i1LJkiU1ZcoUeXp6qmbNmurUqZPWrFmT43A1ceJEDRs2TD169JAkvf7661q7dq3i4+M1depUHTx4UNWqVdNf//pXWSwWVbzuB+fBgwcVHh6u2NhYeXl5qUKFCjk6jgAA4DaWl2Iely9fe/ZYTsJYRjGPlBT7YnYxj+u/z00xDzfcOmM2wpUZ/Pzss0g58e23UseO2fdbtky6887sPzcXatasqRYtWmjmzJlq3bq19u3bp++++05jx46VJNlsNo0bN06fffaZjhw5orS0NKWmpsovh5+zc+dORUVFOYKVJDVv3jxTv08//VTvvPOO/vjjD124cEFXr15VUFBQrvZl586datCggVMxjZYtWyo9PV27d+92hKs6derI87q/sERERGjbtm05+ozk5GQdPXpULVu2dGpv2bKlYwaqT58+atu2rWrUqKH27dvr73//u+655x5J0oMPPqj4+HhVrlxZ7du3V8eOHdW5c2eVKME/OwAAYCIfn+JfzKNUKWnAgJvfOjNkiHTffUX6EkF+yzODxZLjy/N0zz32qc0jR1yfPBaL/f177imQE+exxx7TwIEDNXXqVM2aNUtVqlTRXXfdJUmaMGGCJk2apPj4eNWrV0/+/v4aMmSI0tLSTPv8jRs3qmfPnhozZozatWun4OBgzZs3T2+++aZpn3G9G++tslgsSk9PN237jRo10oEDB7R8+XKtXr1aDz30kGJjY7VgwQJFRUVp9+7dWr16tVatWqX+/fs7Zg5zes8XAACA6fJbzCMnYSw/xTxcMQzp0CH7vVj/fwVWUUS4KmyenvZrRrt1s5/Y1wesjCnT+PgCS+QPPfSQBg8erLlz5+qDDz5Qv379HPdfbdiwQffdd58eeeQRSfZ7qPbs2aPatWvnaNu1atXSoUOHdOzYMUVEREiSfvzxR6c+P/zwgypWrKgXXnjB0ZZww1S0t7e3bDZbtp81e/ZspaSkOGavNmzYIA8PD9XI6RR6NoKCghQZGakNGzY4AmjG51x/eV9QUJC6d++u7t27q1u3bmrfvr3OnDmjUqVKydfXV507d1bnzp0VFxenmjVratu2bWrUqJEpYwQAACgUBV3M4/Bh+71j2Tl2LO/7UAgIV+7Qtav9mlFXN+vFxxfotaQBAQHq3r27RowYoeTkZPXp08fxXrVq1bRgwQL98MMPKlmypN566y0dP348x+EqNjZW1atXV+/evTVhwgQlJyc7haiMzzh48KDmzZunpk2baunSpVq0aJFTn+joaB04cEBbt25V+fLlFRgYKKvV6tSnZ8+eGjVqlHr37q3Ro0fr5MmTGjhwoB599FHHJYFmeO655zRq1ChVqVJFDRs21KxZs7R161Z9/PHHkqS33npLERERuuOOO+Th4aH58+crPDxcISEhmj17tmw2m2JiYuTn56ePPvpIvr6+TvdlAQAA3LJyU8xj3TqpTZvs+/3/H/CLKorhu0vXrtKff0pr10pz59q/HjhQKDfpPfbYYzp79qzatWvndH/Uiy++qEaNGqldu3Zq3bq1wsPDHWXPc8LDw0OLFi3SpUuX1KxZMz3++ON69dVXnfrce++9evrppzVgwAA1bNhQP/zwg1566SWnPg888IDat2+vNm3aqGzZsvrkk08yfZafn59WrlypM2fOqGnTpurWrZvuvvtuTZkyJXcHIxuDBg3S0KFD9cwzz6hevXpasWKFlixZomrVqkmyVz5844031KRJEzVt2lR//vmnli1bJg8PD4WEhOj9999Xy5YtVb9+fa1evVpffvmlSpcubeoYAQAAir1WrewTDVkVv7BYpKgoe78izGIYrm78ub0lJycrODhYSUlJmQotXL58WQcOHFClSpXk4+PjphHiVsN5BQAAbnsZ1QIl17fOuKla4M2ywY2YuQIAAADgfhm3ztxY9bB8+WJRhl0qIuFq6tSpio6Olo+Pj2JiYrRp06ab9j937pzi4uIUEREhq9Wq6tWra9myZY73x48fr6ZNmyowMFChoaHq0qWLdue0XCQAAAAA93DjrTNmcHtBi08//VRDhw7V9OnTFRMTo/j4eLVr1067d+9WaGhopv5paWlq27atQkNDtWDBApUrV04JCQkKCQlx9Fm/fr3i4uLUtGlTXb16Vc8//7zuuece/f77707PRQIAAABQxHh6Fuly6zfj9nuuYmJi1LRpU0chgvT0dEVFRWngwIEaPnx4pv7Tp0/XhAkTtGvXrhw/K+jkyZMKDQ3V+vXrdWd2D+YV91yh8HFeAQAAFE3F5p6rtLQ0/fzzz4qNjXW0eXh4KDY2Vhs3bnS5zpIlS9S8eXPFxcUpLCxMdevW1bhx4276XKSkpCRJUqlSpVy+n5qaquTkZKclO9QBgZk4nwAAAIo/t4arU6dOyWazZXouUVhYmBITE12us3//fi1YsEA2m03Lli3TSy+9pDfffFOvvPKKy/7p6ekaMmSIWrZsqbp167rsM378eAUHBzuWqKioLMecMVt28eLFnOwikCMZ51NOZ2MBAABQ9Lj9nqvcSk9PV2hoqN577z15enqqcePGOnLkiCZMmKBRo0Zl6h8XF6ft27fr+++/z3KbI0aM0NChQx2vk5OTswxYnp6eCgkJ0YkTJyTZn7dkyaoeP5ANwzB08eJFnThxQiEhIfL09HT3kAAAAJBHbg1XZcqUkaenp44fP+7Ufvz4cYWHh7tcJyIiQl5eXk6/hNaqVUuJiYlKS0uTt7e3o33AgAH66quv9O2336p8+fJZjsNqtcpqteZ43BljywhYQH6FhIRkec4DAACgeHBruPL29lbjxo21Zs0adenSRZJ9ZmrNmjUaMGCAy3VatmypuXPnKj09XR4e9qsa9+zZo4iICEewMgxDAwcO1KJFi7Ru3TpVqlTJ1HFbLBZFREQoNDRUV65cMXXbuP3c+McCAAAAFE9uvyxw6NCh6t27t5o0aaJmzZopPj5eKSkp6tu3rySpV69eKleunMaPHy9J6tevn6ZMmaLBgwdr4MCB2rt3r8aNG6dBgwY5thkXF6e5c+fqiy++UGBgoOP+reDgYPn6+po2dk9PT34pBgAAACCpCISr7t276+TJkxo5cqQSExPVsGFDrVixwlHk4uDBg44ZKkmKiorSypUr9fTTT6t+/foqV66cBg8erGHDhjn6TJs2TZLU+ob6+LNmzVKfPn0KfJ8AAAAA3H7c/pyroig3tewBAAAA3LqKzXOuAAAAAOBW4fbLAouijMm8nDxMGAAAAMCtKyMT5OSCP8KVC+fPn5ekmz5MGAAAAMDt4/z58woODr5pH+65ciE9PV1Hjx5VYGCg2x8QnPFA40OHDnH/VwHg+BY8jnHB4vgWLI5vweL4FiyOb8Hi+BasonR8DcPQ+fPnFRkZ6VRozxVmrlzw8PC46UOH3SEoKMjtJ9atjONb8DjGBYvjW7A4vgWL41uwOL4Fi+NbsIrK8c1uxioDBS0AAAAAwASEKwAAAAAwAeGqiLNarRo1apSsVqu7h3JL4vgWPI5xweL4FiyOb8Hi+BYsjm/B4vgWrOJ6fCloAQAAAAAmYOYKAAAAAExAuAIAAAAAExCuAAAAAMAEhCsAAAAAMAHhqpB9++236ty5syIjI2WxWLR48eJs11m3bp0aNWokq9WqqlWravbs2Zn6TJ06VdHR0fLx8VFMTIw2bdpk/uCLgdwe34ULF6pt27YqW7asgoKC1Lx5c61cudKpz+jRo2WxWJyWmjVrFuBeFF25Pb7r1q3LdOwsFosSExOd+nH+2uX2+Pbp08fl8a1Tp46jD+ev3fjx49W0aVMFBgYqNDRUXbp00e7du7Ndb/78+apZs6Z8fHxUr149LVu2zOl9wzA0cuRIRUREyNfXV7Gxsdq7d29B7UaRlZfj+/7776tVq1YqWbKkSpYsqdjY2Ez/9l2d4+3bty/IXSmS8nJ8Z8+enenY+fj4OPXh/L0mL8e4devWLn8Gd+rUydGHc9hu2rRpql+/vuOBwM2bN9fy5ctvuk5x/flLuCpkKSkpatCggaZOnZqj/gcOHFCnTp3Upk0bbd26VUOGDNHjjz/uFAA+/fRTDR06VKNGjdKWLVvUoEEDtWvXTidOnCio3Siycnt8v/32W7Vt21bLli3Tzz//rDZt2qhz58765ZdfnPrVqVNHx44dcyzff/99QQy/yMvt8c2we/dup+MXGhrqeI/z95rcHt9JkyY5HddDhw6pVKlSevDBB536cf5K69evV1xcnH788UetWrVKV65c0T333KOUlJQs1/nhhx/08MMP67HHHtMvv/yiLl26qEuXLtq+fbujzxtvvKF33nlH06dP108//SR/f3+1a9dOly9fLozdKjLycnzXrVunhx9+WGvXrtXGjRsVFRWle+65R0eOHHHq1759e6fz95NPPino3Sly8nJ8JSkoKMjp2CUkJDi9z/l7TV6O8cKFC52O7/bt2+Xp6ZnpZzDnsFS+fHm99tpr+vnnn/W///1Pf/vb33Tfffdpx44dLvsX65+/BtxGkrFo0aKb9vn3v/9t1KlTx6mte/fuRrt27RyvmzVrZsTFxTle22w2IzIy0hg/fryp4y1ucnJ8Xaldu7YxZswYx+tRo0YZDRo0MG9gt4icHN+1a9cakoyzZ89m2Yfz17W8nL+LFi0yLBaL8eeffzraOH9dO3HihCHJWL9+fZZ9HnroIaNTp05ObTExMca//vUvwzAMIz093QgPDzcmTJjgeP/cuXOG1Wo1Pvnkk4IZeDGRk+N7o6tXrxqBgYHGnDlzHG29e/c27rvvvgIYYfGWk+M7a9YsIzg4OMv3OX9vLi/n8Ntvv20EBgYaFy5ccLRxDmetZMmSxn//+1+X7xXnn7/MXBVxGzduVGxsrFNbu3bttHHjRklSWlqafv75Z6c+Hh4eio2NdfRBzqWnp+v8+fMqVaqUU/vevXsVGRmpypUrq2fPnjp48KCbRlg8NWzYUBEREWrbtq02bNjgaOf8NdeMGTMUGxurihUrOrVz/maWlJQkSZn+rV8vu5+/Bw4cUGJiolOf4OBgxcTE3Pbnb06O740uXryoK1euZFpn3bp1Cg0NVY0aNdSvXz+dPn3a1LEWRzk9vhcuXFDFihUVFRWVaZaA8/fm8nIOz5gxQz169JC/v79TO+ewM5vNpnnz5iklJUXNmzd32ac4//wlXBVxiYmJCgsLc2oLCwtTcnKyLl26pFOnTslms7nsc+N9LcjexIkTdeHCBT300EOOtpiYGM2ePVsrVqzQtGnTdODAAbVq1Urnz59340iLh4iICE2fPl2ff/65Pv/8c0VFRal169basmWLJHH+mujo0aNavny5Hn/8cad2zt/M0tPTNWTIELVs2VJ169bNsl9WP38zzs2Mr5y/znJ6fG80bNgwRUZGOv2y1L59e33wwQdas2aNXn/9da1fv14dOnSQzWYriKEXCzk9vjVq1NDMmTP1xRdf6KOPPlJ6erpatGihw4cPS+L8vZm8nMObNm3S9u3bM/0M5hy+Ztu2bQoICJDVatVTTz2lRYsWqXbt2i77FuefvyXc+ulAETJ37lyNGTNGX3zxhdM9QR06dHB8X79+fcXExKhixYr67LPP9Nhjj7ljqMVGjRo1VKNGDcfrFi1a6I8//tDbb7+tDz/80I0ju/XMmTNHISEh6tKli1M7529mcXFx2r59+21571lhyMvxfe211zRv3jytW7fOqehCjx49HN/Xq1dP9evXV5UqVbRu3Trdfffdpo67uMjp8W3evLnTrECLFi1Uq1Ytvfvuu3r55ZcLepjFWl7O4RkzZqhevXpq1qyZUzvn8DU1atTQ1q1blZSUpAULFqh3795av359lgGruGLmqogLDw/X8ePHndqOHz+uoKAg+fr6qkyZMvL09HTZJzw8vDCHWqzNmzdPjz/+uD777LNM09A3CgkJUfXq1bVv375CGt2tpVmzZo5jx/lrDsMwNHPmTD366KPy9va+ad/b/fwdMGCAvvrqK61du1bly5e/ad+sfv5mnJsZXzl/r8nN8c0wceJEvfbaa/r6669Vv379m/atXLmyypQpw/mbi+ObwcvLS3fccYfj2HH+upaXY5ySkqJ58+bl6A9Wt/M57O3trapVq6px48YaP368GjRooEmTJrnsW5x//hKuirjmzZtrzZo1Tm2rVq1y/DXK29tbjRs3duqTnp6uNWvWZHkdK5x98skn6tu3rz755BOn8qlZuXDhgv744w9FREQUwuhuPVu3bnUcO85fc6xfv1779u3L0f/Yb9fz1zAMDRgwQIsWLdI333yjSpUqZbtOdj9/K1WqpPDwcKc+ycnJ+umnn2678zcvx1eyV/t6+eWXtWLFCjVp0iTb/ocPH9bp06c5f3N4fK9ns9m0bds2x7Hj/HWWn2M8f/58paam6pFHHsm27+16DruSnp6u1NRUl+8V65+/bi2ncRs6f/688csvvxi//PKLIcl46623jF9++cVISEgwDMMwhg8fbjz66KOO/vv37zf8/PyM5557zti5c6cxdepUw9PT01ixYoWjz7x58wyr1WrMnj3b+P33340nn3zSCAkJMRITEwt9/9wtt8f3448/NkqUKGFMnTrVOHbsmGM5d+6co88zzzxjrFu3zjhw4ICxYcMGIzY21ihTpoxx4sSJQt8/d8vt8X377beNxYsXG3v37jW2bdtmDB482PDw8DBWr17t6MP5e01uj2+GRx55xIiJiXG5Tc5fu379+hnBwcHGunXrnP6tX7x40dHn0UcfNYYPH+54vWHDBqNEiRLGxIkTjZ07dxqjRo0yvLy8jG3btjn6vPbaa0ZISIjxxRdfGL/99ptx3333GZUqVTIuXbpUqPvnbnk5vq+99prh7e1tLFiwwGmd8+fPG4Zh//fw7LPPGhs3bjQOHDhgrF692mjUqJFRrVo14/Lly4W+j+6Ul+M7ZswYY+XKlcYff/xh/Pzzz0aPHj0MHx8fY8eOHY4+nL/X5OUYZ/jrX/9qdO/ePVM75/A1w4cPN9avX28cOHDA+O2334zhw4cbFovF+Prrrw3DuLV+/hKuCllGaeobl969exuGYS/Zedddd2Vap2HDhoa3t7dRuXJlY9asWZm2O3nyZKNChQqGt7e30axZM+PHH38s+J0pgnJ7fO+6666b9jcMe+n7iIgIw9vb2yhXrpzRvXt3Y9++fYW7Y0VEbo/v66+/blSpUsXw8fExSpUqZbRu3dr45ptvMm2X89cuLz8fzp07Z/j6+hrvvfeey21y/tq5Oq6SnH6e3nXXXU7/9g3DMD777DOjevXqhre3t1GnTh1j6dKlTu+np6cbL730khEWFmZYrVbj7rvvNnbv3l0Ie1S05OX4VqxY0eU6o0aNMgzDMC5evGjcc889RtmyZQ0vLy+jYsWKxhNPPHFb/uElL8d3yJAhjp+rYWFhRseOHY0tW7Y4bZfz95q8/ozYtWuXIckREq7HOXzNP//5T6NixYqGt7e3UbZsWePuu+92Oma30s9fi2EYhkmTYAAAAABw2+KeKwAAAAAwAeEKAAAAAExAuAIAAAAAExCuAAAAAMAEhCsAAAAAMAHhCgAAAABMQLgCAAAAABMQrgAAAADABIQrAADyyWKxaPHixe4eBgDAzQhXAIBirU+fPrJYLJmW9u3bu3toAIDbTAl3DwAAgPxq3769Zs2a5dRmtVrdNBoAwO2KmSsAQLFntVoVHh7utJQsWVKS/ZK9adOmqUOHDvL19VXlypW1YMECp/W3bdumv/3tb/L19VXp0qX15JNP6sKFC059Zs6cqTp16shqtSoiIkIDBgxwev/UqVO6//775efnp2rVqmnJkiWO986ePauePXuqbNmy8vX1VbVq1TKFQQBA8Ue4AgDc8l566SU98MAD+vXXX9WzZ0/16NFDO3fulCSlpKSoXbt2KlmypDZv3qz58+dr9erVTuFp2rRpiouL05NPPqlt27ZpyZIlqlq1qtNnjBkzRg899JB+++03dezYUT179tSZM2ccn//7779r+fLl2rlzp6ZNm6YyZcoU3gEAABQKi2EYhrsHAQBAXvXp00cfffSRfHx8nNqff/55Pf/887JYLHrqqac0bdo0x3t/+ctf1KhRI/3nP//R+++/r2HDhunQoUPy9/eXJC1btkydO3fW0aNHFRYWpnLlyqlv37565ZVXXI7BYrHoxRdf1MsvvyzJHtgCAgK0fPlytW/fXvfee6/KlCmjmTNnFtBRAAAUBdxzBQAo9tq0aeMUniSpVKlSju+bN2/u9F7z5s21detWSdLOnTvVoEEDR7CSpJYtWyo9PV27d++WxWLR0aNHdffdd990DPXr13d87+/vr6CgIJ04cUKS1K9fPz3wwAPasmWL7rnnHnXp0kUtWrTI074CAIouwhUAoNjz9/fPdJmeWXx9fXPUz8vLy+m1xWJRenq6JKlDhw5KSEjQsmXLtGrVKt19992Ki4vTxIkTTR8vAMB9uOcKAHDL+/HHHzO9rlWrliSpVq1a+vXXX5WSkuJ4f8OGDfLw8FCNGjUUGBio6OhorVmzJl9jKFu2rHr37q2PPvpI8fHxeu+99/K1PQBA0cPMFQCg2EtNTVViYqJTW4kSJRxFI+bPn68mTZror3/9qz7++GNt2rRJM2bMkCT17NlTo0aNUu/evTV69GidPHlSAwcO1KOPPqqwsDBJ0ujRo/XUU08pNDRUHTp00Pnz57VhwwYNHDgwR+MbOXKkGjdurDp16ig1NVVfffWVI9wBAG4dhCsAQLG3YsUKRUREOLXVqFFDu3btkmSv5Ddv3jz1799fERER+uSTT1S7dm1Jkp+fn1auXKnBgweradOm8vPz0wMPPKC33nrLsa3evXvr8uXLevvtt/Xss8+qTJky6tatW47H5+3trREjRujPP/+Ur6+vWrVqpXnz5pmw5wCAooRqgQCAW5rFYtGiRYvUpUsXdw8FAHCL454rAAAAADAB4QoAAAAATMA9VwCAWxpXvwMACgszVwAAAABgAsIVAAAAAJiAcAUAAAAAJiBcAQAAAIAJCFcAAAAAYALCFQAAAACYgHAFAAAAACYgXAEAAACACf4Py0lblX0+wg4AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def custom_data_collator(batch):\n",
    "    input_ids_list = [torch.tensor(item['input_ids'], dtype=torch.long) for item in batch]\n",
    "    labels_list = [torch.tensor(item['labels'], dtype=torch.long) for item in batch]\n",
    "\n",
    "    # Padding\n",
    "    input_ids = pad_sequence(input_ids_list, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    labels = pad_sequence(labels_list, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    attention_masks = pad_sequence([torch.ones_like(input_id) for input_id in input_ids_list], batch_first=True, padding_value=0)\n",
    "\n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_masks,\n",
    "        'labels': labels\n",
    "    }\n",
    "data_collator=custom_data_collator\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,  # Use the dataset instance for training data\n",
    "    eval_dataset=test_dataset,     # Use the dataset instance for validation data\n",
    "    callbacks=[custom_eval_callback],  # Custom callback function for metrics\n",
    "    optimizers=(AdamW(model.parameters(), lr=5e-5), None),  # AdamW optimizer with a specified learning rate\n",
    "    data_collator=custom_data_collator\n",
    "#     data_collator=lambda data: {'input_ids': torch.stack([f['input_ids'] for f in data]),\n",
    "#                                 'labels': torch.stack([f['labels'] for f in data]),\n",
    "#                                 'attention_mask': torch.stack([torch.tensor([1]*len(f['input_ids'])) for f in data])}\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n",
    "# Plotting\n",
    "training_logs = trainer.state.log_history\n",
    "\n",
    "training_loss = [log['loss'] for log in training_logs if 'loss' in log]\n",
    "validation_loss = [log['eval_loss'] for log in training_logs if 'eval_loss' in log]\n",
    "epochs = range(1, len(training_loss) + 1)\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(epochs, training_loss, 'bo-', label='Training loss')\n",
    "plt.plot(epochs, validation_loss, 'ro-', label='Validation loss')\n",
    "plt.title('Training and Validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "dde94c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## The limitations of t5 small based on the input tpkens in can handle and the context loss during chunking and summarisation is a real issue\n",
    "## So being that, we can now fine tune a more good and capable model longt5 for summarisation considering the size of the inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f863d55",
   "metadata": {},
   "source": [
    "## LongT5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "9852030f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in c:\\users\\764883\\appdata\\roaming\\python\\python311\\site-packages (4.35.2)\n",
      "Requirement already satisfied: torch in c:\\program files\\python311\\lib\\site-packages (2.0.1+cu118)\n",
      "Requirement already satisfied: filelock in c:\\program files\\python311\\lib\\site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\users\\764883\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (0.19.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\program files\\python311\\lib\\site-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\program files\\python311\\lib\\site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\program files\\python311\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\764883\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in c:\\program files\\python311\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\764883\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\764883\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\program files\\python311\\lib\\site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\program files\\python311\\lib\\site-packages (from torch) (4.5.0)\n",
      "Requirement already satisfied: sympy in c:\\program files\\python311\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\program files\\python311\\lib\\site-packages (from torch) (3.0)\n",
      "Requirement already satisfied: jinja2 in c:\\program files\\python311\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\764883\\appdata\\roaming\\python\\python311\\site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.10.0)\n",
      "Requirement already satisfied: colorama in c:\\program files\\python311\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\program files\\python311\\lib\\site-packages (from jinja2->torch) (2.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\program files\\python311\\lib\\site-packages (from requests->transformers) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\program files\\python311\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\program files\\python311\\lib\\site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\program files\\python311\\lib\\site-packages (from requests->transformers) (2023.5.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\program files\\python311\\lib\\site-packages (from sympy->torch) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "3a387066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in c:\\users\\764883\\appdata\\roaming\\python\\python311\\site-packages (4.35.2)\n",
      "Requirement already satisfied: filelock in c:\\program files\\python311\\lib\\site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\users\\764883\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (0.19.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\program files\\python311\\lib\\site-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\program files\\python311\\lib\\site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\program files\\python311\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\764883\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in c:\\program files\\python311\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\764883\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\764883\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\program files\\python311\\lib\\site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\764883\\appdata\\roaming\\python\\python311\\site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\program files\\python311\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
      "Requirement already satisfied: colorama in c:\\program files\\python311\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\program files\\python311\\lib\\site-packages (from requests->transformers) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\program files\\python311\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\program files\\python311\\lib\\site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\program files\\python311\\lib\\site-packages (from requests->transformers) (2023.5.7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install -U transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "cb51c133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: bert-score in c:\\users\\764883\\appdata\\roaming\\python\\python311\\site-packages (0.3.13)\n",
      "Requirement already satisfied: torch>=1.0.0 in c:\\program files\\python311\\lib\\site-packages (from bert-score) (2.0.1+cu118)\n",
      "Requirement already satisfied: pandas>=1.0.1 in c:\\program files\\python311\\lib\\site-packages (from bert-score) (2.0.2)\n",
      "Requirement already satisfied: transformers>=3.0.0 in c:\\users\\764883\\appdata\\roaming\\python\\python311\\site-packages (from bert-score) (4.35.2)\n",
      "Requirement already satisfied: numpy in c:\\program files\\python311\\lib\\site-packages (from bert-score) (1.23.5)\n",
      "Requirement already satisfied: requests in c:\\program files\\python311\\lib\\site-packages (from bert-score) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.31.1 in c:\\program files\\python311\\lib\\site-packages (from bert-score) (4.65.0)\n",
      "Requirement already satisfied: matplotlib in c:\\program files\\python311\\lib\\site-packages (from bert-score) (3.7.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\program files\\python311\\lib\\site-packages (from bert-score) (23.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\program files\\python311\\lib\\site-packages (from pandas>=1.0.1->bert-score) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\program files\\python311\\lib\\site-packages (from pandas>=1.0.1->bert-score) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\program files\\python311\\lib\\site-packages (from pandas>=1.0.1->bert-score) (2023.3)\n",
      "Requirement already satisfied: filelock in c:\\program files\\python311\\lib\\site-packages (from torch>=1.0.0->bert-score) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\program files\\python311\\lib\\site-packages (from torch>=1.0.0->bert-score) (4.5.0)\n",
      "Requirement already satisfied: sympy in c:\\program files\\python311\\lib\\site-packages (from torch>=1.0.0->bert-score) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\program files\\python311\\lib\\site-packages (from torch>=1.0.0->bert-score) (3.0)\n",
      "Requirement already satisfied: jinja2 in c:\\program files\\python311\\lib\\site-packages (from torch>=1.0.0->bert-score) (3.1.2)\n",
      "Requirement already satisfied: colorama in c:\\program files\\python311\\lib\\site-packages (from tqdm>=4.31.1->bert-score) (0.4.6)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\users\\764883\\appdata\\roaming\\python\\python311\\site-packages (from transformers>=3.0.0->bert-score) (0.19.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\program files\\python311\\lib\\site-packages (from transformers>=3.0.0->bert-score) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\764883\\appdata\\roaming\\python\\python311\\site-packages (from transformers>=3.0.0->bert-score) (2023.10.3)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\764883\\appdata\\roaming\\python\\python311\\site-packages (from transformers>=3.0.0->bert-score) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\764883\\appdata\\roaming\\python\\python311\\site-packages (from transformers>=3.0.0->bert-score) (0.4.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\program files\\python311\\lib\\site-packages (from matplotlib->bert-score) (1.0.7)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\program files\\python311\\lib\\site-packages (from matplotlib->bert-score) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\program files\\python311\\lib\\site-packages (from matplotlib->bert-score) (4.39.4)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\program files\\python311\\lib\\site-packages (from matplotlib->bert-score) (1.4.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\program files\\python311\\lib\\site-packages (from matplotlib->bert-score) (9.5.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\764883\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib->bert-score) (2.4.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\program files\\python311\\lib\\site-packages (from requests->bert-score) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\program files\\python311\\lib\\site-packages (from requests->bert-score) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\program files\\python311\\lib\\site-packages (from requests->bert-score) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\program files\\python311\\lib\\site-packages (from requests->bert-score) (2023.5.7)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\764883\\appdata\\roaming\\python\\python311\\site-packages (from huggingface-hub<1.0,>=0.16.4->transformers>=3.0.0->bert-score) (2023.10.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\program files\\python311\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->bert-score) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\program files\\python311\\lib\\site-packages (from jinja2->torch>=1.0.0->bert-score) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\program files\\python311\\lib\\site-packages (from sympy->torch>=1.0.0->bert-score) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install bert-score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "b7890c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\models\\t5\\tokenization_t5_fast.py:160: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 768)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if CUDA is available\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "# Load your data\n",
    "data = df\n",
    "data = data[['Preprocessed_Body', 'Summary_human']].dropna()\n",
    "data = data.sample(frac=1, random_state=0)  # Shuffle the data\n",
    "dataset = Dataset.from_pandas(data)\n",
    "dataset = dataset.train_test_split(test_size=0.3)  # Splitting the data\n",
    "dataset[\"test\"] = dataset[\"test\"].train_test_split(test_size=0.5)\n",
    "\n",
    "# Prepare the data\n",
    "train_source = dataset[\"train\"][\"Preprocessed_Body\"]\n",
    "train_target = dataset[\"train\"][\"Summary_human\"]\n",
    "val_source = dataset[\"test\"][\"train\"][\"Preprocessed_Body\"]\n",
    "val_target = dataset[\"test\"][\"train\"][\"Summary_human\"]\n",
    "test_source = dataset[\"test\"][\"test\"][\"Preprocessed_Body\"]\n",
    "test_target = dataset[\"test\"][\"test\"][\"Summary_human\"]\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else torch.device(\"cpu\"))\n",
    "\n",
    "# Tokenize the data\n",
    "def tokenize_data(source, target, max_length=512, max_target_length=128):\n",
    "    encodings = tokenizer(source, truncation=True, max_length=max_length, padding=True)\n",
    "    decodings = tokenizer(target, truncation=True, max_length=max_target_length, padding=True)\n",
    "    return {\n",
    "        'input_ids': torch.tensor(encodings['input_ids']),\n",
    "        'attention_mask': torch.tensor(encodings['attention_mask']),\n",
    "        'labels': torch.tensor(decodings['input_ids'])\n",
    "    }\n",
    "\n",
    "train_encodings = tokenize_data(train_source, train_target)\n",
    "val_encodings = tokenize_data(val_source, val_target)\n",
    "test_encodings = tokenize_data(test_source, test_target)\n",
    "\n",
    "# Create TensorDatasets\n",
    "train_dataset = TensorDataset(train_encodings['input_ids'], train_encodings['attention_mask'], train_encodings['labels'])\n",
    "val_dataset = TensorDataset(val_encodings['input_ids'], val_encodings['attention_mask'], val_encodings['labels'])\n",
    "test_dataset = TensorDataset(test_encodings['input_ids'], test_encodings['attention_mask'], test_encodings['labels'])\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=8)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=8)\n",
    "\n",
    "# Load the T5 model\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n",
    "model.to(device)\n",
    "\n",
    "# Training Setup (Placeholder)\n",
    "# Define your training loop, loss function, optimizer, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "14f20b49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|███████████████████████████████████████████████████████████| 84/84 [00:26<00:00,  3.17batch/s, loss=1.58]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.5788, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|███████████████████████████████████████████████████████████| 18/18 [00:01<00:00,  9.55batch/s, loss=1.32]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.3157, device='cuda:0')\n",
      "train_loss = 1.8088899183840979\n",
      " test_loss = 1.4358011020554438\n",
      "1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|███████████████████████████████████████████████████████████| 84/84 [00:27<00:00,  3.08batch/s, loss=1.36]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.3551, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|███████████████████████████████████████████████████████████| 18/18 [00:01<00:00,  9.33batch/s, loss=1.27]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.2734, device='cuda:0')\n",
      "train_loss = 1.2416178286075592\n",
      " test_loss = 1.3947661916414897\n",
      "2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|███████████████████████████████████████████████████████████| 84/84 [00:26<00:00,  3.13batch/s, loss=1.01]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0108, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|███████████████████████████████████████████████████████████| 18/18 [00:01<00:00,  9.46batch/s, loss=1.31]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.3133, device='cuda:0')\n",
      "train_loss = 0.9971998525517327\n",
      " test_loss = 1.4186555412080553\n",
      "3\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████████████████████████████████████████████████████| 84/84 [00:26<00:00,  3.12batch/s, loss=0.889]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8892, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|███████████████████████████████████████████████████████████| 18/18 [00:01<00:00,  9.46batch/s, loss=1.32]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.3205, device='cuda:0')\n",
      "train_loss = 0.8176996147348767\n",
      " test_loss = 1.4403615196545918\n",
      "4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|███████████████████████████████████████████████████████████| 84/84 [00:27<00:00,  3.11batch/s, loss=0.48]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4798, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|███████████████████████████████████████████████████████████| 18/18 [00:01<00:00,  9.41batch/s, loss=1.36]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.3630, device='cuda:0')\n",
      "train_loss = 0.6818676945709047\n",
      " test_loss = 1.5122286412451003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n",
    "\n",
    "epochs = 5\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = 0.0004)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(str(epoch)+\"\\n\")\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    with tqdm(train_dataloader, unit=\"batch\") as tepoch:    \n",
    "        for batch in tepoch:\n",
    "            tepoch.set_description(f\"Epoch {epoch}\")\n",
    "            input_ids, attention_masks, labels = tuple(t.to(device) for t in batch)\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_masks, labels=labels)\n",
    "\n",
    "            loss = outputs.loss\n",
    "            train_loss += loss.item()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            tepoch.set_postfix(loss= loss.item())\n",
    "        train_loss /= len(train_dataloader)\n",
    "        sleep(0.1)\n",
    "    print(loss)\n",
    "\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        with tqdm(test_dataloader, unit=\"batch\") as tepoch:\n",
    "            for batch in tepoch:\n",
    "                tepoch.set_description(f\"Epoch {epoch}\")\n",
    "                input_ids, attention_masks, labels = tuple(t.to(device) for t in batch)\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_masks, labels=labels)\n",
    "                loss = outputs.loss\n",
    "                test_loss += loss.item()\n",
    "                tepoch.set_postfix(loss= loss.item())\n",
    "\n",
    "            # Compute the average testing loss for the epoch\n",
    "            test_loss /= len(test_dataloader)\n",
    "    print(loss)\n",
    "    print(f'train_loss = {train_loss}\\n test_loss = {test_loss}')\n",
    "\n",
    "#     train_loss_his.append(train_loss)\n",
    "#     test_loss_his.append(test_loss)\n",
    "# model.save_pretrained(f'/home4/s4236599/t5_models/t5_epoch{epoch+1}')\n",
    "# loss_df = pd.DataFrame(list(zip(train_loss_his, test_loss_his)))\n",
    "# loss_df.to_csv(\"t5_loss_final_0.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56210703",
   "metadata": {},
   "source": [
    "## T5 Long for big texts summarisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "845d04cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, LongT5ForConditionalGeneration\n",
    "import torch\n",
    "import sentencepiece\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "from time import sleep\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset as HFDataset, load_metric\n",
    "from bert_score import score as bert_score\n",
    "from torch.utils.data import TensorDataset, RandomSampler, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "db8d140b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Some weights of LongT5ForConditionalGeneration were not initialized from the model checkpoint at google/long-t5-local-base and are newly initialized: ['lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LongT5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 768)\n",
       "  (encoder): LongT5Stack(\n",
       "    (embed_tokens): Embedding(32128, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): LongT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): LongT5LayerLocalSelfAttention(\n",
       "            (LocalSelfAttention): LongT5LocalAttention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): LongT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): LongT5LayerFF(\n",
       "            (DenseReluDense): LongT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): LongT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x LongT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): LongT5LayerLocalSelfAttention(\n",
       "            (LocalSelfAttention): LongT5LocalAttention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): LongT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): LongT5LayerFF(\n",
       "            (DenseReluDense): LongT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): LongT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LongT5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): LongT5Stack(\n",
       "    (embed_tokens): Embedding(32128, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): LongT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): LongT5LayerSelfAttention(\n",
       "            (SelfAttention): LongT5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): LongT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): LongT5LayerCrossAttention(\n",
       "            (EncDecAttention): LongT5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): LongT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): LongT5LayerFF(\n",
       "            (DenseReluDense): LongT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): LongT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x LongT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): LongT5LayerSelfAttention(\n",
       "            (SelfAttention): LongT5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): LongT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): LongT5LayerCrossAttention(\n",
       "            (EncDecAttention): LongT5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): LongT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): LongT5LayerFF(\n",
       "            (DenseReluDense): LongT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): LongT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LongT5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, LongT5ForConditionalGeneration\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from datasets import Dataset as HFDataset  \n",
    "import sentencepiece \n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset as HFDataset, load_metric\n",
    "from bert_score import score as bert_score\n",
    "# Check if CUDA is available\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "# Load your data\n",
    "data = df\n",
    "data = data[['Preprocessed_Body', 'Summary_human']].dropna()\n",
    "data = data.sample(frac=1, random_state=0)\n",
    "data = HFDataset.from_pandas(data)\n",
    "\n",
    "# Splitting the data\n",
    "data = data.train_test_split(test_size=0.3)\n",
    "data[\"test\"] = data[\"test\"].train_test_split(test_size=0.5)\n",
    "\n",
    "# Prepare the data\n",
    "train_source = data[\"train\"][\"Preprocessed_Body\"]\n",
    "train_target = data[\"train\"][\"Summary_human\"]\n",
    "val_source = data[\"test\"][\"train\"][\"Preprocessed_Body\"]\n",
    "val_target = data[\"test\"][\"train\"][\"Summary_human\"]\n",
    "test_source = data[\"test\"][\"test\"][\"Preprocessed_Body\"]\n",
    "test_target = data[\"test\"][\"test\"][\"Summary_human\"]\n",
    "\n",
    "# Initialize tokenizer for LongT5\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/long-t5-local-base\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Tokenize the data\n",
    "train_encodings = tokenizer(train_source, truncation=True, max_length=1024, padding=True)\n",
    "train_decodings = tokenizer(train_target, truncation=True, padding=True)\n",
    "test_encodings = tokenizer(test_source, truncation=True, max_length=1024, padding=True)\n",
    "test_decodings = tokenizer(test_target, truncation=True, padding=True)\n",
    "\n",
    "# Convert to PyTorch tensors and create DataLoader\n",
    "def create_data_loader(encodings, decodings, batch_size=2):\n",
    "    input_ids = torch.tensor(encodings[\"input_ids\"])\n",
    "    attention_masks = torch.tensor(encodings['attention_mask'])\n",
    "    labels = torch.tensor(decodings['input_ids'])\n",
    "    data = TensorDataset(input_ids, attention_masks, labels)\n",
    "    sampler = RandomSampler(data)\n",
    "    return DataLoader(data, sampler=sampler, batch_size=batch_size)\n",
    "\n",
    "train_dataloader = create_data_loader(train_encodings, train_decodings)\n",
    "test_dataloader = create_data_loader(test_encodings, test_decodings)\n",
    "\n",
    "# Load LongT5 model\n",
    "model = LongT5ForConditionalGeneration.from_pretrained(\"google/long-t5-local-base\")\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "48ba8221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|                                                                              | 0/334 [00:00<?, ?batch/s]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Epoch 0: 100%|█████████████████████████████████████████████████████████| 334/334 [01:34<00:00,  3.53batch/s, loss=1.45]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.4494, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████████████████████████████████████████████████████| 72/72 [00:07<00:00, 10.20batch/s, loss=0.967]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9666, device='cuda:0')\n",
      "train_loss = 3.3653023705154124\n",
      " test_loss = 1.1250489089224074\n",
      "1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|█████████████████████████████████████████████████████████| 334/334 [01:34<00:00,  3.52batch/s, loss=1.01]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0107, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|████████████████████████████████████████████████████████████| 72/72 [00:07<00:00, 10.04batch/s, loss=1.3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.3014, device='cuda:0')\n",
      "train_loss = 1.3555148277810947\n",
      " test_loss = 1.075018631087409\n",
      "2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|████████████████████████████████████████████████████████| 334/334 [01:34<00:00,  3.53batch/s, loss=0.499]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4993, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|███████████████████████████████████████████████████████████| 72/72 [00:07<00:00, 10.27batch/s, loss=1.17]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.1725, device='cuda:0')\n",
      "train_loss = 1.2226583299165714\n",
      " test_loss = 1.036266220940484\n",
      "3\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|████████████████████████████████████████████████████████| 334/334 [01:34<00:00,  3.54batch/s, loss=0.717]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7169, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████████████████████████████████████████████████████| 72/72 [00:07<00:00, 10.09batch/s, loss=0.782]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7823, device='cuda:0')\n",
      "train_loss = 1.1487620879790026\n",
      " test_loss = 1.0082557333840265\n",
      "4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|█████████████████████████████████████████████████████████| 334/334 [01:33<00:00,  3.57batch/s, loss=1.54]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.5449, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████████████████████████████████████████████████████| 72/72 [00:07<00:00, 10.28batch/s, loss=0.998]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9977, device='cuda:0')\n",
      "train_loss = 1.0806527964904638\n",
      " test_loss = 0.9868896529078484\n",
      "5\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|█████████████████████████████████████████████████████████| 334/334 [01:33<00:00,  3.57batch/s, loss=1.09]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0893, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|███████████████████████████████████████████████████████████| 72/72 [00:07<00:00, 10.00batch/s, loss=1.13]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.1347, device='cuda:0')\n",
      "train_loss = 1.0202158319200585\n",
      " test_loss = 0.9673494944969813\n",
      "6\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|████████████████████████████████████████████████████████| 334/334 [01:34<00:00,  3.55batch/s, loss=0.965]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9646, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████████████████████████████████████████████████████| 72/72 [00:07<00:00, 10.03batch/s, loss=0.834]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8339, device='cuda:0')\n",
      "train_loss = 0.9685258050700147\n",
      " test_loss = 0.9503071183959643\n",
      "7\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|████████████████████████████████████████████████████████| 334/334 [01:33<00:00,  3.56batch/s, loss=0.783]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7833, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████████████████████████████████████████████████████| 72/72 [00:07<00:00, 10.09batch/s, loss=0.439]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4387, device='cuda:0')\n",
      "train_loss = 0.914780591954728\n",
      " test_loss = 0.9440601310796208\n",
      "8\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|█████████████████████████████████████████████████████████| 334/334 [01:33<00:00,  3.55batch/s, loss=0.75]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7498, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████████████████████████████████████████████████████| 72/72 [00:07<00:00,  9.95batch/s, loss=0.898]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8978, device='cuda:0')\n",
      "train_loss = 0.8641326439951709\n",
      " test_loss = 0.9423946357435651\n",
      "9\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|████████████████████████████████████████████████████████| 334/334 [01:33<00:00,  3.59batch/s, loss=0.374]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3739, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████████████████████████████████████████████████████| 72/72 [00:07<00:00, 10.08batch/s, loss=0.703]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7026, device='cuda:0')\n",
      "train_loss = 0.8147892466116095\n",
      " test_loss = 0.9432890199952655\n",
      "10\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|███████████████████████████████████████████████████████| 334/334 [01:33<00:00,  3.58batch/s, loss=0.555]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5545, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|███████████████████████████████████████████████████████████| 72/72 [00:07<00:00,  9.92batch/s, loss=1.3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.3015, device='cuda:0')\n",
      "train_loss = 0.7716458776218449\n",
      " test_loss = 0.9435017440054152\n",
      "11\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|███████████████████████████████████████████████████████| 334/334 [01:34<00:00,  3.55batch/s, loss=0.837]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8369, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████████████████████████████████████████████████████| 72/72 [00:07<00:00,  9.95batch/s, loss=1.04]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0367, device='cuda:0')\n",
      "train_loss = 0.7229953930228056\n",
      " test_loss = 0.9562921615110503\n",
      "12\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|███████████████████████████████████████████████████████| 334/334 [01:33<00:00,  3.57batch/s, loss=0.847]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8467, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|█████████████████████████████████████████████████████████| 72/72 [00:07<00:00, 10.13batch/s, loss=0.648]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6478, device='cuda:0')\n",
      "train_loss = 0.6781272618713493\n",
      " test_loss = 0.9595261017481486\n",
      "13\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|███████████████████████████████████████████████████████| 334/334 [01:32<00:00,  3.60batch/s, loss=0.659]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6593, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|██████████████████████████████████████████████████████████| 72/72 [00:07<00:00, 10.25batch/s, loss=0.97]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9698, device='cuda:0')\n",
      "train_loss = 0.6396644812322663\n",
      " test_loss = 0.9704824052751064\n",
      "14\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|███████████████████████████████████████████████████████| 334/334 [01:33<00:00,  3.57batch/s, loss=0.509]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5093, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|█████████████████████████████████████████████████████████| 72/72 [00:07<00:00, 10.08batch/s, loss=0.827]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8270, device='cuda:0')\n",
      "train_loss = 0.5985441199409034\n",
      " test_loss = 0.9827089707056681\n",
      "15\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|███████████████████████████████████████████████████████| 334/334 [01:34<00:00,  3.54batch/s, loss=0.495]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4955, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|██████████████████████████████████████████████████████████| 72/72 [00:07<00:00,  9.06batch/s, loss=0.69]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6896, device='cuda:0')\n",
      "train_loss = 0.5578695167592186\n",
      " test_loss = 0.9979588277637959\n",
      "16\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|███████████████████████████████████████████████████████| 334/334 [01:34<00:00,  3.52batch/s, loss=0.424]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4239, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|█████████████████████████████████████████████████████████| 72/72 [00:07<00:00, 10.00batch/s, loss=0.696]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6965, device='cuda:0')\n",
      "train_loss = 0.5177745546944841\n",
      " test_loss = 1.0193291265103552\n",
      "17\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|███████████████████████████████████████████████████████| 334/334 [01:34<00:00,  3.53batch/s, loss=0.371]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3710, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|█████████████████████████████████████████████████████████| 72/72 [00:07<00:00, 10.13batch/s, loss=0.881]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8810, device='cuda:0')\n",
      "train_loss = 0.48660557824159095\n",
      " test_loss = 1.0365775086813502\n",
      "18\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|███████████████████████████████████████████████████████| 334/334 [01:34<00:00,  3.55batch/s, loss=0.481]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4806, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|██████████████████████████████████████████████████████████| 72/72 [00:07<00:00, 10.13batch/s, loss=1.73]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.7255, device='cuda:0')\n",
      "train_loss = 0.45251504777969714\n",
      " test_loss = 1.0707462864617507\n",
      "19\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|███████████████████████████████████████████████████████| 334/334 [01:33<00:00,  3.58batch/s, loss=0.701]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7010, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|█████████████████████████████████████████████████████████| 72/72 [00:07<00:00,  9.86batch/s, loss=0.862]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8618, device='cuda:0')\n",
      "train_loss = 0.42160424304579547\n",
      " test_loss = 1.0871215524772804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = LongT5ForConditionalGeneration.from_pretrained(\"google/long-t5-tglobal-base\")\n",
    "\n",
    "epochs = 20\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = 1e-4)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(str(epoch)+\"\\n\")\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    with tqdm(train_dataloader, unit=\"batch\") as tepoch:    \n",
    "        for batch in tepoch:\n",
    "            tepoch.set_description(f\"Epoch {epoch}\")\n",
    "            input_ids, attention_masks, labels = tuple(t.to(device) for t in batch)\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_masks, labels=labels)\n",
    "\n",
    "            loss = outputs.loss\n",
    "            train_loss += loss.item()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            tepoch.set_postfix(loss= loss.item())\n",
    "        train_loss /= len(train_dataloader)\n",
    "        sleep(0.1)\n",
    "    print(loss)\n",
    "\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        with tqdm(test_dataloader, unit=\"batch\") as tepoch:\n",
    "            for batch in tepoch:\n",
    "                tepoch.set_description(f\"Epoch {epoch}\")\n",
    "                input_ids, attention_masks, labels = tuple(t.to(device) for t in batch)\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_masks, labels=labels)\n",
    "                loss = outputs.loss\n",
    "                test_loss += loss.item()\n",
    "                tepoch.set_postfix(loss= loss.item())\n",
    "\n",
    "            # Compute the average testing loss for the epoch\n",
    "            test_loss /= len(test_dataloader)\n",
    "    print(loss)\n",
    "    print(f'train_loss = {train_loss}\\n test_loss = {test_loss}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d39c2095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: bert-score in c:\\users\\764883\\appdata\\roaming\\python\\python311\\site-packages (0.3.13)\n",
      "Requirement already satisfied: torch>=1.0.0 in c:\\program files\\python311\\lib\\site-packages (from bert-score) (2.0.1+cu118)\n",
      "Requirement already satisfied: pandas>=1.0.1 in c:\\program files\\python311\\lib\\site-packages (from bert-score) (2.0.2)\n",
      "Requirement already satisfied: transformers>=3.0.0 in c:\\users\\764883\\appdata\\roaming\\python\\python311\\site-packages (from bert-score) (4.35.2)\n",
      "Requirement already satisfied: numpy in c:\\program files\\python311\\lib\\site-packages (from bert-score) (1.23.5)\n",
      "Requirement already satisfied: requests in c:\\program files\\python311\\lib\\site-packages (from bert-score) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.31.1 in c:\\program files\\python311\\lib\\site-packages (from bert-score) (4.65.0)\n",
      "Requirement already satisfied: matplotlib in c:\\program files\\python311\\lib\\site-packages (from bert-score) (3.7.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\program files\\python311\\lib\\site-packages (from bert-score) (23.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\program files\\python311\\lib\\site-packages (from pandas>=1.0.1->bert-score) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\program files\\python311\\lib\\site-packages (from pandas>=1.0.1->bert-score) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\program files\\python311\\lib\\site-packages (from pandas>=1.0.1->bert-score) (2023.3)\n",
      "Requirement already satisfied: filelock in c:\\program files\\python311\\lib\\site-packages (from torch>=1.0.0->bert-score) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\program files\\python311\\lib\\site-packages (from torch>=1.0.0->bert-score) (4.5.0)\n",
      "Requirement already satisfied: sympy in c:\\program files\\python311\\lib\\site-packages (from torch>=1.0.0->bert-score) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\program files\\python311\\lib\\site-packages (from torch>=1.0.0->bert-score) (3.0)\n",
      "Requirement already satisfied: jinja2 in c:\\program files\\python311\\lib\\site-packages (from torch>=1.0.0->bert-score) (3.1.2)\n",
      "Requirement already satisfied: colorama in c:\\program files\\python311\\lib\\site-packages (from tqdm>=4.31.1->bert-score) (0.4.6)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\users\\764883\\appdata\\roaming\\python\\python311\\site-packages (from transformers>=3.0.0->bert-score) (0.19.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\program files\\python311\\lib\\site-packages (from transformers>=3.0.0->bert-score) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\764883\\appdata\\roaming\\python\\python311\\site-packages (from transformers>=3.0.0->bert-score) (2023.10.3)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\764883\\appdata\\roaming\\python\\python311\\site-packages (from transformers>=3.0.0->bert-score) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\764883\\appdata\\roaming\\python\\python311\\site-packages (from transformers>=3.0.0->bert-score) (0.4.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\program files\\python311\\lib\\site-packages (from matplotlib->bert-score) (1.0.7)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\program files\\python311\\lib\\site-packages (from matplotlib->bert-score) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\program files\\python311\\lib\\site-packages (from matplotlib->bert-score) (4.39.4)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\program files\\python311\\lib\\site-packages (from matplotlib->bert-score) (1.4.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\program files\\python311\\lib\\site-packages (from matplotlib->bert-score) (9.5.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\764883\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib->bert-score) (2.4.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\program files\\python311\\lib\\site-packages (from requests->bert-score) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\program files\\python311\\lib\\site-packages (from requests->bert-score) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\program files\\python311\\lib\\site-packages (from requests->bert-score) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\program files\\python311\\lib\\site-packages (from requests->bert-score) (2023.5.7)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\764883\\appdata\\roaming\\python\\python311\\site-packages (from huggingface-hub<1.0,>=0.16.4->transformers>=3.0.0->bert-score) (2023.10.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\program files\\python311\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->bert-score) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\program files\\python311\\lib\\site-packages (from jinja2->torch>=1.0.0->bert-score) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\program files\\python311\\lib\\site-packages (from sympy->torch>=1.0.0->bert-score) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install bert-score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb7f1f0",
   "metadata": {},
   "source": [
    "## Training with the tokens limit increased and learning rate to 5e-4 and incresed epoch, batch size = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80218f9a",
   "metadata": {},
   "source": [
    "## Some more addition to preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0b2ed3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_html_to_text(html):\n",
    "    # Use BeautifulSoup to parse HTML and extract text\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    text = soup.get_text(separator=\" \")\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "def standardize_date_time(text):\n",
    "\n",
    "    # Function to standardize dates\n",
    "    def replace_date(match):\n",
    "        try:\n",
    "            date = parse(match.group(), fuzzy=True)\n",
    "            return date.strftime(\"%Y-%m-%d\")\n",
    "        except ValueError:\n",
    "            # Return the original string if parsing fails\n",
    "            return match.group()\n",
    "\n",
    "    # Function to standardize times\n",
    "    def replace_time(match):\n",
    "        try:\n",
    "            time = parse(match.group(), fuzzy=True)\n",
    "            return time.strftime(\"%H:%M\")\n",
    "        except ValueError:\n",
    "            # Return the original string if parsing fails\n",
    "            return match.group()\n",
    "\n",
    "    # Standardize dates\n",
    "    text = re.sub(r'\\b\\d{1,2}[-/]\\d{1,2}[-/]\\d{2,4}\\b', replace_date, text)\n",
    "    # Standardize times\n",
    "    text = re.sub(r'\\b\\d{1,2}:\\d{2}\\b', replace_time, text)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def preprocess_email_for_t5(email_body):\n",
    "    # Convert HTML to text\n",
    "    email_body = convert_html_to_text(email_body)\n",
    "\n",
    "    # Remove extra whitespaces\n",
    "    email_body = ' '.join(email_body.split())\n",
    "\n",
    "    # Remove common greeting texts\n",
    "    greetings_pattern = (\n",
    "    r\"Hi\\s\\w+|\"\n",
    "    r\"Hello\\s\\w+|\"\n",
    "    r\"Dear\\s\\w+|\"\n",
    "    r\"Dear\\s[Mm]r\\.\\s\\w+|\"\n",
    "    r\"Dear\\s[Mm]rs\\.\\s\\w+|\"\n",
    "    r\"Dear\\s[Mm]s\\.\\s\\w+|\"\n",
    "    r\"Dear\\s[MD]r(s)?\\.\\s\\w+|\"\n",
    "    r\"Dear\\sProf\\.\\s\\w+|\"\n",
    "    r\"Dear\\sDoctor\\.\\s\\w+|\"\n",
    "    r\"Greetings|\"\n",
    "    r\"Good\\s[Mm]orning|\"\n",
    "    r\"Good\\s[Aa]fternoon|\"\n",
    "    r\"Good\\s[Ee]vening|\"\n",
    "    r\"Hey\\s\\w+|\"\n",
    "    r\"Hey\\sthere|\"\n",
    "    r\"Hello\\severyone|\"\n",
    "    r\"Hope\\syou\\sare\\sdoing\\swell|\"\n",
    "    r\"Hope\\syou\\sare\\sdoing\\swell|\"\n",
    "    r\"Hope\\sthis\\semail\\sfinds\\syou\\swell|\"\n",
    "    r\"Hope\\sthis\\semail\\sfinds\\syou\\swell|\"\n",
    "    r\"Hope\\sthis\\semail\\sfinds\\syou\\sin\\sgood\\shealth|\"\n",
    "    r\"How\\sare\\syou\\sdoing|\"\n",
    "    r\"How\\sis\\sit\\sgoing|\"\n",
    "    r\"To\\swhom\\sit\\smay\\sconcern|\"\n",
    "    r\"To\\swhom\\sit\\smay\\sconcern|\"\n",
    "    r\"I\\sam\\swriting\\sto\\syou\\sbecause|\"\n",
    "    r\"I\\sam\\swriting\\sto\\syou\\sto|\"\n",
    "    r\"I\\shope\\sthat\\syou|\"\n",
    "    r\"I\\swanted\\sto\\sreach\\sout\\sto|\"\n",
    "    r\"I\\swanted\\sto\\slet\\syou\\sknow|\"\n",
    "    r\"I\\swould\\slike\\sto\\sinform\\syou|\"\n",
    "    r\"It\\spleases\\sme\\sto\\scontact\\syou|\"\n",
    "    r\"It\\shas\\scome\\sto\\smy\\sattention|\"\n",
    "    r\"I\\swas\\sjust\\sthinking\\sabout\\syou\\sand\\s|\"\n",
    "    r\"Allow\\sme\\sto\\sintroduce\\smyself|\"\n",
    "    r\"I\\shope\\sthat\\severything\\sis\\sgoing\\swell|\"\n",
    "    r\"Thank\\syou\\sfor\\syour\\semail|\"\n",
    "    r\"Thank\\syou\\sfor\\sreaching\\sout\")\n",
    "\n",
    "    email_body = re.sub(greetings_pattern, \"\", email_body, flags=re.IGNORECASE)\n",
    "\n",
    "    # Remove common sign-offs\n",
    "    signoffs_pattern = (\n",
    "    r\"Best\\sregards|\"\n",
    "    r\"Best\\s\\w+|\"\n",
    "    r\"Sincerely|\"\n",
    "    r\"Yours\\sfaithfully|\"\n",
    "    r\"Yours\\ssincerely|\"\n",
    "    r\"Regards|\"\n",
    "    r\"Warm\\sregards|\"\n",
    "    r\"Kind\\sregards|\"\n",
    "    r\"Cheers|\"\n",
    "    r\"Thanks\\sand\\sregards|\"\n",
    "    r\"Thank\\syou|\"\n",
    "    r\"Take\\scare|\"\n",
    "    r\"Looking\\sforward|\"\n",
    "    r\"All\\sbest|\"\n",
    "    r\"Best\\swishes|\"\n",
    "    r\"Best|\"\n",
    "    r\"Yours\\struly|\"\n",
    "    r\"Cordially|\"\n",
    "    r\"With\\sappreciation|\"\n",
    "    r\"Respectfully|\"\n",
    "    r\"With\\sregards|\"\n",
    "    r\"Many\\sthanks|\"\n",
    "    r\"Hope\\sto\\shear\\sfrom\\syou\\ssoon|\"\n",
    "    r\"Until\\snext\\stime|\"\n",
    "    r\"Yours\\svery\\struly|\"\n",
    "    r\"Yours|\"\n",
    "    r\"In\\sgratitude|\"\n",
    "    r\"In\\ssympathy|\"\n",
    "    r\"Thoughtfully|\"\n",
    "    r\"With\\saffection|\"\n",
    "    r\"Fond\\sregards|\"\n",
    "    r\"With\\santicipation|\"\n",
    "    r\"Stay\\swell|\"\n",
    "    r\"Stay\\ssafe|\"\n",
    "    r\"Peace|\"\n",
    "    r\"God\\sbless|\"\n",
    "    r\"Love|\"\n",
    "    r\"Sent\\sfrom\\smy\\s\\w+|\"\n",
    "    r\"Sent\\sfrom\\smy\\s\\w+\\s\\w+|\"\n",
    "    r\"Talk\\sto\\syou\\ssoon|\"\n",
    "    r\"See\\syou\\ssoon|\"\n",
    "    r\"See\\sya|\"\n",
    "    r\"Ciao|\"\n",
    "    r\"Adieu|\"\n",
    "    r\"Farewell|\"\n",
    "    r\"Good\\sbye|\"\n",
    "    r\"Bye\\sfor\\snow|\"\n",
    "    r\"Signing\\soff|\"\n",
    "    r\"Out|\"\n",
    "    r\"Yours\\s[in]\\s\\w+|\"\n",
    "    r\"Your\\sfriend|\"\n",
    "    r\"Your\\s\\w+\\s\\w+|\"\n",
    "    r\"Keep\\sin\\stouch|\"\n",
    "    r\"Yours\\sfaithfully|\"\n",
    "    r\"Yours\\ssincerely|\"\n",
    "    r\"Yours\\sobediently|\"\n",
    "    r\"Yours\\saffectionately|\"\n",
    "    r\"Yours\\scordially|\"\n",
    "    r\"Yours\\srespectfully|\"\n",
    "    r\"Yours\\struly|\"\n",
    "    r\"Yours\\sever\")\n",
    "\n",
    "    email_body = re.sub(signoffs_pattern, \"\", email_body, flags=re.IGNORECASE)\n",
    "\n",
    "    # Pattern to remove repetitive characters like dashes or underscores\n",
    "    repetitive_pattern = r\"[-_]{3,}\"  \n",
    "    email_body = re.sub(repetitive_pattern, \"\", email_body)\n",
    "\n",
    "    # Standardize dates and times\n",
    "    email_body = standardize_date_time(email_body)\n",
    "\n",
    "    # Remove email signatures and disclaimers\n",
    "    email_body = re.sub(r\"--\\s*[\\s\\S]*$\", \"\", email_body)\n",
    "\n",
    "    # Standardize email addresses and URLs\n",
    "    email_body = re.sub(r\"\\S+@\\S+\\.\\S+\", \"<email>\", email_body)\n",
    "    email_body = re.sub(r\"http\\S+\", \"<url>\", email_body)\n",
    "\n",
    "    # Remove phrases indicating difficulty in viewing images or links\n",
    "    irrelevant_phrases_pattern = (\n",
    "    r\"difficulty in viewing this image|\"\n",
    "    r\"click here|\"\n",
    "    r\"having trouble viewing this|\"\n",
    "    r\"view this email in your browser|\"\n",
    "    r\"to ensure delivery to your inbox|\"\n",
    "    r\"if you cannot see this message|\"\n",
    "    r\"message not displaying correctly|\"\n",
    "    r\"trouble seeing this email|\"\n",
    "    r\"can't see the images below|\"\n",
    "    r\"email not looking quite right|\"\n",
    "    r\"viewing this email on a mobile device|\"\n",
    "    r\"can't read this email|\"\n",
    "    r\"images not showing up|\"\n",
    "    r\"to view the online version of this email|\"\n",
    "    r\"email doesn't display correctly|\"\n",
    "    r\"problems seeing this email|\"\n",
    "    r\"to unsubscribe or change preferences|\"\n",
    "    r\"this message was sent to <email>|\"\n",
    "    r\"not interested in these emails|\"\n",
    "    r\"you're receiving this email because|\"\n",
    "    r\"to stop receiving these emails|\"\n",
    "    r\"unsubscribe from this list|\"\n",
    "    r\"manage your email preferences\")\n",
    "    email_body = re.sub(irrelevant_phrases_pattern, \"\", email_body, flags=re.IGNORECASE)\n",
    "\n",
    "    # Add task-specific prefix (if you are going to use this text directly for T5 summarization later)\n",
    "    email_body = \"summarize: \" + email_body\n",
    "\n",
    "    return email_body\n",
    "\n",
    "\n",
    "# Apply the preprocessing function to the 'body' column\n",
    "df['Preprocessed_Body'] = df['body'].apply(lambda x: preprocess_email_for_t5(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b4bfed80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\764883\\AppData\\Local\\Temp\\ipykernel_5588\\4234254842.py:8: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(html, \"html.parser\")\n"
     ]
    }
   ],
   "source": [
    "def preprocess_summary_for_t5(summary):\n",
    "    # Standardize dates and times \n",
    "    summary = standardize_date_time(summary)\n",
    "\n",
    "    # Remove URLs and website addresses (if they are not relevant to the summary)\n",
    "    url_pattern = r\"http\\S+|www\\.\\S+\"\n",
    "    summary = re.sub(url_pattern, \"<url>\", summary)\n",
    "\n",
    "    return summary\n",
    "df['Preprocessed_Summary'] = df['Summary_human'].apply(lambda x: preprocess_email_for_t5(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c2ac8e29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Some weights of LongT5ForConditionalGeneration were not initialized from the model checkpoint at google/long-t5-local-base and are newly initialized: ['lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LongT5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 768)\n",
       "  (encoder): LongT5Stack(\n",
       "    (embed_tokens): Embedding(32128, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): LongT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): LongT5LayerLocalSelfAttention(\n",
       "            (LocalSelfAttention): LongT5LocalAttention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): LongT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): LongT5LayerFF(\n",
       "            (DenseReluDense): LongT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): LongT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x LongT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): LongT5LayerLocalSelfAttention(\n",
       "            (LocalSelfAttention): LongT5LocalAttention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): LongT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): LongT5LayerFF(\n",
       "            (DenseReluDense): LongT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): LongT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LongT5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): LongT5Stack(\n",
       "    (embed_tokens): Embedding(32128, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): LongT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): LongT5LayerSelfAttention(\n",
       "            (SelfAttention): LongT5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): LongT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): LongT5LayerCrossAttention(\n",
       "            (EncDecAttention): LongT5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): LongT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): LongT5LayerFF(\n",
       "            (DenseReluDense): LongT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): LongT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x LongT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): LongT5LayerSelfAttention(\n",
       "            (SelfAttention): LongT5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): LongT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): LongT5LayerCrossAttention(\n",
       "            (EncDecAttention): LongT5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): LongT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): LongT5LayerFF(\n",
       "            (DenseReluDense): LongT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): LongT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LongT5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load your data\n",
    "data = df\n",
    "data = data[['Preprocessed_Body', 'Preprocessed_Summary']].dropna()\n",
    "data = data.sample(frac=1, random_state=0)\n",
    "data = HFDataset.from_pandas(data)\n",
    "\n",
    "# Splitting the data\n",
    "data = data.train_test_split(test_size=0.3)\n",
    "data[\"test\"] = data[\"test\"].train_test_split(test_size=0.5)\n",
    "\n",
    "# Prepare the data\n",
    "train_source = data[\"train\"][\"Preprocessed_Body\"]\n",
    "train_target = data[\"train\"][\"Preprocessed_Summary\"]\n",
    "val_source = data[\"test\"][\"train\"][\"Preprocessed_Body\"]\n",
    "val_target = data[\"test\"][\"train\"][\"Preprocessed_Summary\"]\n",
    "test_source = data[\"test\"][\"test\"][\"Preprocessed_Body\"]\n",
    "test_target = data[\"test\"][\"test\"][\"Preprocessed_Summary\"]\n",
    "\n",
    "# Initialize tokenizer for LongT5\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/long-t5-local-base\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Tokenize the data\n",
    "train_encodings = tokenizer(train_source, truncation=True, max_length=3000, padding=True)\n",
    "train_decodings = tokenizer(train_target, truncation=True, padding=True)\n",
    "test_encodings = tokenizer(test_source, truncation=True, max_length=3000, padding=True)\n",
    "test_decodings = tokenizer(test_target, truncation=True, padding=True)\n",
    "\n",
    "# Convert to PyTorch tensors and create DataLoader\n",
    "def create_data_loader(encodings, decodings, batch_size=2):\n",
    "    input_ids = torch.tensor(encodings[\"input_ids\"])\n",
    "    attention_masks = torch.tensor(encodings['attention_mask'])\n",
    "    labels = torch.tensor(decodings['input_ids'])\n",
    "    data = TensorDataset(input_ids, attention_masks, labels)\n",
    "    sampler = RandomSampler(data)\n",
    "    return DataLoader(data, sampler=sampler, batch_size=batch_size)\n",
    "\n",
    "train_dataloader = create_data_loader(train_encodings, train_decodings)\n",
    "test_dataloader = create_data_loader(test_encodings, test_decodings)\n",
    "\n",
    "# Load LongT5 model\n",
    "model = LongT5ForConditionalGeneration.from_pretrained(\"google/long-t5-local-base\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c0f87f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LongT5ForConditionalGeneration were not initialized from the model checkpoint at google/long-t5-local-base and are newly initialized: ['lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LongT5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 768)\n",
       "  (encoder): LongT5Stack(\n",
       "    (embed_tokens): Embedding(32128, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): LongT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): LongT5LayerLocalSelfAttention(\n",
       "            (LocalSelfAttention): LongT5LocalAttention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): LongT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): LongT5LayerFF(\n",
       "            (DenseReluDense): LongT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): LongT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x LongT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): LongT5LayerLocalSelfAttention(\n",
       "            (LocalSelfAttention): LongT5LocalAttention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): LongT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): LongT5LayerFF(\n",
       "            (DenseReluDense): LongT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): LongT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LongT5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): LongT5Stack(\n",
       "    (embed_tokens): Embedding(32128, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): LongT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): LongT5LayerSelfAttention(\n",
       "            (SelfAttention): LongT5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): LongT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): LongT5LayerCrossAttention(\n",
       "            (EncDecAttention): LongT5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): LongT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): LongT5LayerFF(\n",
       "            (DenseReluDense): LongT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): LongT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x LongT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): LongT5LayerSelfAttention(\n",
       "            (SelfAttention): LongT5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): LongT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): LongT5LayerCrossAttention(\n",
       "            (EncDecAttention): LongT5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): LongT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): LongT5LayerFF(\n",
       "            (DenseReluDense): LongT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): LongT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LongT5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Corrected data loader\n",
    "# Check if CUDA is available\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "# Load your data\n",
    "data = df\n",
    "data = data[['Preprocessed_Body', 'Preprocessed_Summary']].dropna()\n",
    "data = data.sample(frac=1, random_state=0)  # Shuffle the data\n",
    "dataset = Dataset.from_pandas(data)\n",
    "dataset = dataset.train_test_split(test_size=0.3)  # Splitting the data\n",
    "dataset[\"test\"] = dataset[\"test\"].train_test_split(test_size=0.5)\n",
    "\n",
    "# Prepare the data\n",
    "train_source = dataset[\"train\"][\"Preprocessed_Body\"]\n",
    "train_target = dataset[\"train\"][\"Preprocessed_Summary\"]\n",
    "val_source = dataset[\"test\"][\"train\"][\"Preprocessed_Body\"]\n",
    "val_target = dataset[\"test\"][\"train\"][\"Preprocessed_Summary\"]\n",
    "test_source = dataset[\"test\"][\"test\"][\"Preprocessed_Body\"]\n",
    "test_target = dataset[\"test\"][\"test\"][\"Preprocessed_Summary\"]\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/long-t5-local-base\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else torch.device(\"cpu\"))\n",
    "\n",
    "# Tokenize the data\n",
    "def tokenize_data(source, target, max_length=1500, max_target_length=128):\n",
    "    encodings = tokenizer(source, truncation=True, max_length=max_length, padding=True)\n",
    "    decodings = tokenizer(target, truncation=True, max_length=max_target_length, padding=True)\n",
    "    return {\n",
    "        'input_ids': torch.tensor(encodings['input_ids']),\n",
    "        'attention_mask': torch.tensor(encodings['attention_mask']),\n",
    "        'labels': torch.tensor(decodings['input_ids'])\n",
    "    }\n",
    "\n",
    "train_encodings = tokenize_data(train_source, train_target)\n",
    "val_encodings = tokenize_data(val_source, val_target)\n",
    "test_encodings = tokenize_data(test_source, test_target)\n",
    "\n",
    "# Create TensorDatasets\n",
    "train_dataset = TensorDataset(train_encodings['input_ids'], train_encodings['attention_mask'], train_encodings['labels'])\n",
    "val_dataset = TensorDataset(val_encodings['input_ids'], val_encodings['attention_mask'], val_encodings['labels'])\n",
    "test_dataset = TensorDataset(test_encodings['input_ids'], test_encodings['attention_mask'], test_encodings['labels'])\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=2)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=2)\n",
    "\n",
    "# Load LongT5 model\n",
    "model = LongT5ForConditionalGeneration.from_pretrained(\"google/long-t5-local-base\")\n",
    "model.to(device)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b0be39",
   "metadata": {},
   "source": [
    "## Ading the metrics score for the model rouge and bert score(Body and SUmmary_human)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "40a23ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load metrics\n",
    "rouge_metric = load_metric(\"rouge\")\n",
    "bert_metric = load_metric(\"bertscore\")\n",
    "\n",
    "def validate_and_calculate_metrics(model, dataloader, tokenizer, device):\n",
    "    model.eval()\n",
    "    rouge_scores = []\n",
    "    bert_scores = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Validating\"):\n",
    "            input_ids, attention_masks, labels = [t.to(device) for t in batch]\n",
    "            generated_ids = model.generate(input_ids, attention_mask=attention_masks, max_length=150)\n",
    "            preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
    "            refs = [tokenizer.decode(l, skip_special_tokens=True, clean_up_tokenization_spaces=True) for l in labels]\n",
    "\n",
    "            # Calculate ROUGE\n",
    "            rouge_scores.append(rouge_metric.compute(predictions=preds, references=refs))\n",
    "            \n",
    "            # Calculate BERTScore\n",
    "            P, R, F1 = bert_score(preds, refs, lang=\"en\", rescale_with_baseline=True)\n",
    "            bert_scores.append({\"precision\": P.mean().item(), \"recall\": R.mean().item(), \"f1\": F1.mean().item()})\n",
    "\n",
    "    # Aggregate scores\n",
    "    rouge_final = {key: sum(score[key].mid.fmeasure for score in rouge_scores) / len(rouge_scores) for key in rouge_scores[0]}\n",
    "    bert_final = {\n",
    "        \"precision\": sum(score[\"precision\"] for score in bert_scores) / len(bert_scores),\n",
    "        \"recall\": sum(score[\"recall\"] for score in bert_scores) / len(bert_scores),\n",
    "        \"f1\": sum(score[\"f1\"] for score in bert_scores) / len(bert_scores)\n",
    "    }\n",
    "\n",
    "    return rouge_final, bert_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4b1311",
   "metadata": {},
   "source": [
    "## Training the loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "c27f7316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████████████████████████████████████████████████| 334/334 [1:53:37<00:00, 20.41s/batch, loss=0.515]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5153, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████████████████████████████████████████████████████| 72/72 [10:10<00:00,  8.48s/batch, loss=0.902]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9019, device='cuda:0')\n",
      "train_loss = 3.2859567430561887\n",
      " test_loss = 1.1876772766311963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:   0%|                                                                               | 0/18 [00:00<?, ?it/s]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:   6%|███▉                                                                   | 1/18 [00:17<05:05, 17.95s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  11%|███████▉                                                               | 2/18 [00:41<05:38, 21.19s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  17%|███████████▊                                                           | 3/18 [01:03<05:21, 21.45s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  22%|███████████████▊                                                       | 4/18 [01:26<05:07, 22.00s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  28%|███████████████████▋                                                   | 5/18 [01:43<04:23, 20.28s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  33%|███████████████████████▋                                               | 6/18 [02:03<04:02, 20.18s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  39%|███████████████████████████▌                                           | 7/18 [02:24<03:47, 20.70s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  44%|███████████████████████████████▌                                       | 8/18 [02:44<03:24, 20.43s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  50%|███████████████████████████████████▌                                   | 9/18 [03:07<03:09, 21.09s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  56%|██████████████████████████████████████▉                               | 10/18 [03:29<02:50, 21.26s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  61%|██████████████████████████████████████████▊                           | 11/18 [03:48<02:25, 20.78s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  67%|██████████████████████████████████████████████▋                       | 12/18 [04:09<02:05, 20.89s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  72%|██████████████████████████████████████████████████▌                   | 13/18 [04:31<01:45, 21.20s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  78%|██████████████████████████████████████████████████████▍               | 14/18 [04:50<01:22, 20.54s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  83%|██████████████████████████████████████████████████████████▎           | 15/18 [05:13<01:03, 21.18s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  89%|██████████████████████████████████████████████████████████████▏       | 16/18 [05:35<00:42, 21.46s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  94%|██████████████████████████████████████████████████████████████████    | 17/18 [05:59<00:22, 22.14s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating: 100%|██████████████████████████████████████████████████████████████████████| 18/18 [06:23<00:00, 21.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Validation Scores: ROUGE: {'rouge1': 0.300572756442322, 'rouge2': 0.1250635022380962, 'rougeL': 0.21993467387087434, 'rougeLsum': 0.2200938703528401}, BERTScore: {'precision': 0.07842443179753092, 'recall': 0.07768883217229611, 'f1': 0.0763859724263764}\n",
      "1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|                                                                              | 0/334 [00:00<?, ?batch/s]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Epoch 1: 100%|██████████████████████████████████████████████████████| 334/334 [1:59:06<00:00, 21.40s/batch, loss=0.764]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7640, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|███████████████████████████████████████████████████████████| 72/72 [13:45<00:00, 11.47s/batch, loss=1.37]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.3729, device='cuda:0')\n",
      "train_loss = 1.2523615304046047\n",
      " test_loss = 1.0855205837223265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:   0%|                                                                               | 0/18 [00:00<?, ?it/s]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:   6%|███▉                                                                   | 1/18 [00:29<08:24, 29.70s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  11%|███████▉                                                               | 2/18 [00:58<07:49, 29.34s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  17%|███████████▊                                                           | 3/18 [01:28<07:24, 29.65s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  22%|███████████████▊                                                       | 4/18 [01:58<06:53, 29.51s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  28%|███████████████████▋                                                   | 5/18 [02:29<06:31, 30.11s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  33%|███████████████████████▋                                               | 6/18 [02:58<05:57, 29.79s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  39%|███████████████████████████▌                                           | 7/18 [03:26<05:19, 29.09s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  44%|███████████████████████████████▌                                       | 8/18 [03:55<04:50, 29.09s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  50%|███████████████████████████████████▌                                   | 9/18 [04:24<04:22, 29.12s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  56%|██████████████████████████████████████▉                               | 10/18 [04:48<03:41, 27.65s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  61%|██████████████████████████████████████████▊                           | 11/18 [05:20<03:22, 28.92s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  67%|██████████████████████████████████████████████▋                       | 12/18 [05:49<02:53, 28.95s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  72%|██████████████████████████████████████████████████▌                   | 13/18 [06:17<02:23, 28.75s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  78%|██████████████████████████████████████████████████████▍               | 14/18 [06:46<01:54, 28.66s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  83%|██████████████████████████████████████████████████████████▎           | 15/18 [07:15<01:26, 28.89s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  89%|██████████████████████████████████████████████████████████████▏       | 16/18 [07:47<00:59, 29.77s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  94%|██████████████████████████████████████████████████████████████████    | 17/18 [08:18<00:30, 30.18s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating: 100%|██████████████████████████████████████████████████████████████████████| 18/18 [08:44<00:00, 29.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Validation Scores: ROUGE: {'rouge1': 0.36418940967063146, 'rouge2': 0.1653560015423712, 'rougeL': 0.2735332247535098, 'rougeLsum': 0.2738158958592043}, BERTScore: {'precision': 0.19800987467169762, 'recall': 0.15925039040545622, 'f1': 0.1782585403157605}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = LongT5ForConditionalGeneration.from_pretrained(\"google/long-t5-tglobal-base\")\n",
    "\n",
    "epochs = 2\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = 1e-4)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(str(epoch)+\"\\n\")\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    with tqdm(train_dataloader, unit=\"batch\") as tepoch:    \n",
    "        for batch in tepoch:\n",
    "            tepoch.set_description(f\"Epoch {epoch}\")\n",
    "            input_ids, attention_masks, labels = tuple(t.to(device) for t in batch)\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_masks, labels=labels)\n",
    "\n",
    "            loss = outputs.loss\n",
    "            train_loss += loss.item()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            tepoch.set_postfix(loss= loss.item())\n",
    "        train_loss /= len(train_dataloader)\n",
    "        sleep(0.1)\n",
    "    print(loss)\n",
    "\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        with tqdm(test_dataloader, unit=\"batch\") as tepoch:\n",
    "            for batch in tepoch:\n",
    "                tepoch.set_description(f\"Epoch {epoch}\")\n",
    "                input_ids, attention_masks, labels = tuple(t.to(device) for t in batch)\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_masks, labels=labels)\n",
    "                loss = outputs.loss\n",
    "                test_loss += loss.item()\n",
    "                tepoch.set_postfix(loss= loss.item())\n",
    "\n",
    "            # Compute the average testing loss for the epoch\n",
    "            test_loss /= len(test_dataloader)\n",
    "    print(loss)\n",
    "    print(f'train_loss = {train_loss}\\n test_loss = {test_loss}')\n",
    "    # Validation and Metric Calculation\n",
    "    rouge_scores, bert_scores = validate_and_calculate_metrics(model, val_dataloader, tokenizer, device)\n",
    "    print(f\"Epoch {epoch} Validation Scores: ROUGE: {rouge_scores}, BERTScore: {bert_scores}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2728bbe7",
   "metadata": {},
   "source": [
    "## Training to more epochs 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4b1b5eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|                                                                              | 0/334 [00:00<?, ?batch/s]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Epoch 0: 100%|█████████████████████████████████████████████████████████| 334/334 [01:52<00:00,  2.97batch/s, loss=2.27]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.2717, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|███████████████████████████████████████████████████████████| 72/72 [00:07<00:00, 10.10batch/s, loss=1.01]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0095, device='cuda:0')\n",
      "train_loss = 3.366656947992519\n",
      " test_loss = 1.2942167205943003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:   0%|                                                                               | 0/72 [00:00<?, ?it/s]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:   1%|▉                                                                      | 1/72 [00:05<06:27,  5.46s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:   3%|█▉                                                                     | 2/72 [00:09<05:22,  4.61s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:   4%|██▉                                                                    | 3/72 [00:12<04:33,  3.96s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:   6%|███▉                                                                   | 4/72 [00:16<04:33,  4.02s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:   7%|████▉                                                                  | 5/72 [00:20<04:27,  3.99s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:   8%|█████▉                                                                 | 6/72 [00:24<04:09,  3.78s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  10%|██████▉                                                                | 7/72 [00:28<04:09,  3.84s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  11%|███████▉                                                               | 8/72 [00:32<04:08,  3.88s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  12%|████████▉                                                              | 9/72 [00:35<03:48,  3.63s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  14%|█████████▋                                                            | 10/72 [00:38<03:48,  3.69s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  15%|██████████▋                                                           | 11/72 [00:42<03:48,  3.74s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  17%|███████████▋                                                          | 12/72 [00:46<03:48,  3.81s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  18%|████████████▋                                                         | 13/72 [00:50<03:37,  3.69s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  19%|█████████████▌                                                        | 14/72 [00:54<03:47,  3.93s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  21%|██████████████▌                                                       | 15/72 [00:58<03:46,  3.98s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  22%|███████████████▌                                                      | 16/72 [01:02<03:42,  3.97s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  24%|████████████████▌                                                     | 17/72 [01:06<03:37,  3.96s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  25%|█████████████████▌                                                    | 18/72 [01:10<03:36,  4.01s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  26%|██████████████████▍                                                   | 19/72 [01:15<03:37,  4.10s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  28%|███████████████████▍                                                  | 20/72 [01:19<03:34,  4.13s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  29%|████████████████████▍                                                 | 21/72 [01:22<03:12,  3.78s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  31%|█████████████████████▍                                                | 22/72 [01:26<03:09,  3.79s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  32%|██████████████████████▎                                               | 23/72 [01:29<03:05,  3.79s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  33%|███████████████████████▎                                              | 24/72 [01:32<02:49,  3.53s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  35%|████████████████████████▎                                             | 25/72 [01:36<02:48,  3.58s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  36%|█████████████████████████▎                                            | 26/72 [01:39<02:36,  3.40s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  38%|██████████████████████████▎                                           | 27/72 [01:43<02:38,  3.52s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  39%|███████████████████████████▏                                          | 28/72 [01:46<02:38,  3.59s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  40%|████████████████████████████▏                                         | 29/72 [01:49<02:26,  3.41s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  42%|█████████████████████████████▏                                        | 30/72 [01:53<02:24,  3.45s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  43%|██████████████████████████████▏                                       | 31/72 [01:56<02:19,  3.41s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  44%|███████████████████████████████                                       | 32/72 [02:00<02:25,  3.63s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  46%|████████████████████████████████                                      | 33/72 [02:05<02:30,  3.87s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  47%|█████████████████████████████████                                     | 34/72 [02:08<02:21,  3.74s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  49%|██████████████████████████████████                                    | 35/72 [02:14<02:36,  4.22s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  50%|███████████████████████████████████                                   | 36/72 [02:18<02:29,  4.16s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  51%|███████████████████████████████████▉                                  | 37/72 [02:22<02:27,  4.22s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  53%|████████████████████████████████████▉                                 | 38/72 [02:26<02:19,  4.09s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  54%|█████████████████████████████████████▉                                | 39/72 [02:30<02:12,  4.03s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  56%|██████████████████████████████████████▉                               | 40/72 [02:34<02:06,  3.97s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  57%|███████████████████████████████████████▊                              | 41/72 [02:38<02:03,  3.97s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  58%|████████████████████████████████████████▊                             | 42/72 [02:42<02:00,  4.01s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  60%|█████████████████████████████████████████▊                            | 43/72 [02:45<01:51,  3.86s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  61%|██████████████████████████████████████████▊                           | 44/72 [02:49<01:44,  3.72s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  62%|███████████████████████████████████████████▊                          | 45/72 [02:52<01:37,  3.61s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  64%|████████████████████████████████████████████▋                         | 46/72 [02:56<01:36,  3.70s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  65%|█████████████████████████████████████████████▋                        | 47/72 [03:00<01:35,  3.83s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  67%|██████████████████████████████████████████████▋                       | 48/72 [03:04<01:32,  3.87s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  68%|███████████████████████████████████████████████▋                      | 49/72 [03:08<01:29,  3.88s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  69%|████████████████████████████████████████████████▌                     | 50/72 [03:12<01:24,  3.85s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  71%|█████████████████████████████████████████████████▌                    | 51/72 [03:16<01:22,  3.91s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  72%|██████████████████████████████████████████████████▌                   | 52/72 [03:19<01:13,  3.66s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  74%|███████████████████████████████████████████████████▌                  | 53/72 [03:23<01:14,  3.90s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  75%|████████████████████████████████████████████████████▌                 | 54/72 [03:26<01:04,  3.56s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  76%|█████████████████████████████████████████████████████▍                | 55/72 [03:29<01:00,  3.54s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  78%|██████████████████████████████████████████████████████▍               | 56/72 [03:32<00:52,  3.29s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  79%|███████████████████████████████████████████████████████▍              | 57/72 [03:36<00:51,  3.45s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  81%|████████████████████████████████████████████████████████▍             | 58/72 [03:40<00:49,  3.56s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  82%|█████████████████████████████████████████████████████████▎            | 59/72 [03:44<00:47,  3.65s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  83%|██████████████████████████████████████████████████████████▎           | 60/72 [03:47<00:43,  3.66s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  85%|███████████████████████████████████████████████████████████▎          | 61/72 [03:52<00:42,  3.84s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  86%|████████████████████████████████████████████████████████████▎         | 62/72 [03:55<00:38,  3.83s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  88%|█████████████████████████████████████████████████████████████▎        | 63/72 [04:00<00:35,  3.93s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  89%|██████████████████████████████████████████████████████████████▏       | 64/72 [04:04<00:31,  3.98s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  90%|███████████████████████████████████████████████████████████████▏      | 65/72 [04:08<00:29,  4.16s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  92%|████████████████████████████████████████████████████████████████▏     | 66/72 [04:12<00:24,  4.04s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  93%|█████████████████████████████████████████████████████████████████▏    | 67/72 [04:16<00:19,  3.94s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  94%|██████████████████████████████████████████████████████████████████    | 68/72 [04:20<00:16,  4.08s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  96%|███████████████████████████████████████████████████████████████████   | 69/72 [04:24<00:11,  3.94s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  97%|████████████████████████████████████████████████████████████████████  | 70/72 [04:28<00:07,  3.96s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  99%|█████████████████████████████████████████████████████████████████████ | 71/72 [04:32<00:04,  4.11s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating: 100%|██████████████████████████████████████████████████████████████████████| 72/72 [04:36<00:00,  3.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Validation Scores: ROUGE: {'rouge1': 0.3436750427351627, 'rouge2': 0.15225615487426353, 'rougeL': 0.25606862955439763, 'rougeLsum': 0.25606862955439763}, BERTScore: {'precision': 0.16765150817162874, 'recall': 0.13597483281046152, 'f1': 0.15117136487323377}\n",
      "1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|                                                                              | 0/334 [00:00<?, ?batch/s]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Epoch 1: 100%|█████████████████████████████████████████████████████████| 334/334 [01:54<00:00,  2.93batch/s, loss=1.07]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0670, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████████████████████████████████████████████████████| 72/72 [00:08<00:00,  8.04batch/s, loss=0.918]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9184, device='cuda:0')\n",
      "train_loss = 1.6608157056177448\n",
      " test_loss = 1.1933775573141046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:   0%|                                                                               | 0/72 [00:00<?, ?it/s]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:   1%|▉                                                                      | 1/72 [00:05<06:01,  5.09s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:   3%|█▉                                                                     | 2/72 [00:08<04:56,  4.23s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:   4%|██▉                                                                    | 3/72 [00:12<04:25,  3.85s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:   6%|███▉                                                                   | 4/72 [00:16<04:37,  4.08s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:   7%|████▉                                                                  | 5/72 [00:19<04:17,  3.85s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:   8%|█████▉                                                                 | 6/72 [00:23<04:04,  3.71s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  10%|██████▉                                                                | 7/72 [00:27<04:07,  3.82s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  11%|███████▉                                                               | 8/72 [00:31<04:03,  3.80s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  12%|████████▉                                                              | 9/72 [00:35<04:02,  3.85s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  14%|█████████▋                                                            | 10/72 [00:39<04:06,  3.97s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  15%|██████████▋                                                           | 11/72 [00:42<03:49,  3.77s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  17%|███████████▋                                                          | 12/72 [00:46<03:47,  3.79s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  18%|████████████▋                                                         | 13/72 [00:49<03:35,  3.66s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  19%|█████████████▌                                                        | 14/72 [00:53<03:32,  3.67s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  21%|██████████████▌                                                       | 15/72 [00:57<03:30,  3.70s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  22%|███████████████▌                                                      | 16/72 [01:01<03:27,  3.71s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  24%|████████████████▌                                                     | 17/72 [01:04<03:15,  3.55s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  25%|█████████████████▌                                                    | 18/72 [01:08<03:16,  3.64s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  26%|██████████████████▍                                                   | 19/72 [01:12<03:17,  3.72s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  28%|███████████████████▍                                                  | 20/72 [01:16<03:19,  3.83s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  29%|████████████████████▍                                                 | 21/72 [01:20<03:17,  3.87s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  31%|█████████████████████▍                                                | 22/72 [01:24<03:15,  3.91s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  32%|██████████████████████▎                                               | 23/72 [01:27<03:06,  3.81s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  33%|███████████████████████▎                                              | 24/72 [01:31<02:59,  3.73s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  35%|████████████████████████▎                                             | 25/72 [01:35<02:58,  3.80s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  36%|█████████████████████████▎                                            | 26/72 [01:39<02:56,  3.84s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  38%|██████████████████████████▎                                           | 27/72 [01:43<02:53,  3.87s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  39%|███████████████████████████▏                                          | 28/72 [01:46<02:40,  3.65s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  40%|████████████████████████████▏                                         | 29/72 [01:49<02:32,  3.54s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  42%|█████████████████████████████▏                                        | 30/72 [01:53<02:35,  3.70s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  43%|██████████████████████████████▏                                       | 31/72 [01:57<02:32,  3.73s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  44%|███████████████████████████████                                       | 32/72 [02:01<02:30,  3.76s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  46%|████████████████████████████████                                      | 33/72 [02:04<02:24,  3.71s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  47%|█████████████████████████████████                                     | 34/72 [02:08<02:17,  3.61s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  49%|██████████████████████████████████                                    | 35/72 [02:11<02:12,  3.59s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  50%|███████████████████████████████████                                   | 36/72 [02:15<02:13,  3.70s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  51%|███████████████████████████████████▉                                  | 37/72 [02:19<02:10,  3.73s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  53%|████████████████████████████████████▉                                 | 38/72 [02:23<02:08,  3.77s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  54%|█████████████████████████████████████▉                                | 39/72 [02:27<02:06,  3.84s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  56%|██████████████████████████████████████▉                               | 40/72 [02:31<02:02,  3.82s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  57%|███████████████████████████████████████▊                              | 41/72 [02:35<02:00,  3.90s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  58%|████████████████████████████████████████▊                             | 42/72 [02:39<02:00,  4.02s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  60%|█████████████████████████████████████████▊                            | 43/72 [02:44<02:02,  4.24s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  61%|██████████████████████████████████████████▊                           | 44/72 [02:47<01:52,  4.01s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  62%|███████████████████████████████████████████▊                          | 45/72 [02:51<01:45,  3.92s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  64%|████████████████████████████████████████████▋                         | 46/72 [02:55<01:39,  3.84s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  65%|█████████████████████████████████████████████▋                        | 47/72 [02:58<01:31,  3.64s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  67%|██████████████████████████████████████████████▋                       | 48/72 [03:02<01:28,  3.69s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  68%|███████████████████████████████████████████████▋                      | 49/72 [03:05<01:25,  3.72s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  69%|████████████████████████████████████████████████▌                     | 50/72 [03:09<01:18,  3.55s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  71%|█████████████████████████████████████████████████▌                    | 51/72 [03:13<01:18,  3.72s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  72%|██████████████████████████████████████████████████▌                   | 52/72 [03:16<01:11,  3.56s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  74%|███████████████████████████████████████████████████▌                  | 53/72 [03:20<01:11,  3.77s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  75%|████████████████████████████████████████████████████▌                 | 54/72 [03:24<01:08,  3.81s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  76%|█████████████████████████████████████████████████████▍                | 55/72 [03:28<01:05,  3.84s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  78%|██████████████████████████████████████████████████████▍               | 56/72 [03:32<01:01,  3.87s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  79%|███████████████████████████████████████████████████████▍              | 57/72 [03:36<00:59,  3.96s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  81%|████████████████████████████████████████████████████████▍             | 58/72 [03:40<00:55,  3.99s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  82%|█████████████████████████████████████████████████████████▎            | 59/72 [03:44<00:50,  3.90s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  83%|██████████████████████████████████████████████████████████▎           | 60/72 [03:48<00:47,  3.93s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  85%|███████████████████████████████████████████████████████████▎          | 61/72 [03:52<00:44,  4.07s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  86%|████████████████████████████████████████████████████████████▎         | 62/72 [03:55<00:37,  3.79s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  88%|█████████████████████████████████████████████████████████████▎        | 63/72 [03:59<00:33,  3.77s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  89%|██████████████████████████████████████████████████████████████▏       | 64/72 [04:02<00:28,  3.60s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  90%|███████████████████████████████████████████████████████████████▏      | 65/72 [04:06<00:25,  3.66s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  92%|████████████████████████████████████████████████████████████████▏     | 66/72 [04:09<00:21,  3.54s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  93%|█████████████████████████████████████████████████████████████████▏    | 67/72 [04:13<00:17,  3.50s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  94%|██████████████████████████████████████████████████████████████████    | 68/72 [04:16<00:14,  3.54s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  96%|███████████████████████████████████████████████████████████████████   | 69/72 [04:21<00:11,  3.87s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  97%|████████████████████████████████████████████████████████████████████  | 70/72 [04:25<00:07,  3.81s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  99%|█████████████████████████████████████████████████████████████████████ | 71/72 [04:29<00:03,  3.88s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating: 100%|██████████████████████████████████████████████████████████████████████| 72/72 [04:32<00:00,  3.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Validation Scores: ROUGE: {'rouge1': 0.392774939547419, 'rouge2': 0.17970340626352457, 'rougeL': 0.2938329776236527, 'rougeLsum': 0.2938329776236527}, BERTScore: {'precision': 0.2585226000390119, 'recall': 0.20302891706685638, 'f1': 0.23075546774392328}\n",
      "2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:   0%|                                                                              | 0/334 [00:00<?, ?batch/s]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Epoch 2: 100%|█████████████████████████████████████████████████████████| 334/334 [01:53<00:00,  2.95batch/s, loss=1.12]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.1157, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████████████████████████████████████████████████████| 72/72 [00:08<00:00,  8.03batch/s, loss=0.864]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8643, device='cuda:0')\n",
      "train_loss = 1.4588645807283367\n",
      " test_loss = 1.1567724959717856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:   0%|                                                                               | 0/72 [00:00<?, ?it/s]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:   1%|▉                                                                      | 1/72 [00:04<04:52,  4.12s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:   3%|█▉                                                                     | 2/72 [00:08<04:53,  4.19s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:   4%|██▉                                                                    | 3/72 [00:11<04:30,  3.93s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:   6%|███▉                                                                   | 4/72 [00:15<04:23,  3.87s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:   7%|████▉                                                                  | 5/72 [00:18<04:03,  3.63s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:   8%|█████▉                                                                 | 6/72 [00:22<03:57,  3.60s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  10%|██████▉                                                                | 7/72 [00:25<03:46,  3.48s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  11%|███████▉                                                               | 8/72 [00:28<03:35,  3.37s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  12%|████████▉                                                              | 9/72 [00:32<03:30,  3.35s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  14%|█████████▋                                                            | 10/72 [00:35<03:36,  3.49s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  15%|██████████▋                                                           | 11/72 [00:39<03:42,  3.64s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  17%|███████████▋                                                          | 12/72 [00:44<03:53,  3.90s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  18%|████████████▋                                                         | 13/72 [00:47<03:43,  3.79s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  19%|█████████████▌                                                        | 14/72 [00:51<03:40,  3.80s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  21%|██████████████▌                                                       | 15/72 [00:56<03:48,  4.01s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  22%|███████████████▌                                                      | 16/72 [01:00<03:43,  3.98s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  24%|████████████████▌                                                     | 17/72 [01:03<03:32,  3.86s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  25%|█████████████████▌                                                    | 18/72 [01:07<03:33,  3.95s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  26%|██████████████████▍                                                   | 19/72 [01:11<03:28,  3.94s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  28%|███████████████████▍                                                  | 20/72 [01:15<03:13,  3.73s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  29%|████████████████████▍                                                 | 21/72 [01:18<03:06,  3.67s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  31%|█████████████████████▍                                                | 22/72 [01:22<03:01,  3.62s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  32%|██████████████████████▎                                               | 23/72 [01:25<02:55,  3.58s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  33%|███████████████████████▎                                              | 24/72 [01:28<02:48,  3.51s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  35%|████████████████████████▎                                             | 25/72 [01:32<02:49,  3.61s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  36%|█████████████████████████▎                                            | 26/72 [01:36<02:51,  3.73s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  38%|██████████████████████████▎                                           | 27/72 [01:40<02:42,  3.61s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  39%|███████████████████████████▏                                          | 28/72 [01:43<02:35,  3.54s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  40%|████████████████████████████▏                                         | 29/72 [01:47<02:31,  3.52s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  42%|█████████████████████████████▏                                        | 30/72 [01:50<02:26,  3.48s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  43%|██████████████████████████████▏                                       | 31/72 [01:53<02:22,  3.49s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  44%|███████████████████████████████                                       | 32/72 [01:57<02:15,  3.39s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  46%|████████████████████████████████                                      | 33/72 [02:01<02:21,  3.63s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  47%|█████████████████████████████████                                     | 34/72 [02:04<02:10,  3.44s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  49%|██████████████████████████████████                                    | 35/72 [02:08<02:11,  3.55s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  50%|███████████████████████████████████                                   | 36/72 [02:11<02:09,  3.60s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  51%|███████████████████████████████████▉                                  | 37/72 [02:16<02:13,  3.81s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  53%|████████████████████████████████████▉                                 | 38/72 [02:19<02:07,  3.76s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  54%|█████████████████████████████████████▉                                | 39/72 [02:23<02:02,  3.72s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  56%|██████████████████████████████████████▉                               | 40/72 [02:27<02:01,  3.80s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  57%|███████████████████████████████████████▊                              | 41/72 [02:31<01:58,  3.82s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  58%|████████████████████████████████████████▊                             | 42/72 [02:34<01:52,  3.77s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  60%|█████████████████████████████████████████▊                            | 43/72 [02:38<01:47,  3.72s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  61%|██████████████████████████████████████████▊                           | 44/72 [02:41<01:41,  3.61s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  62%|███████████████████████████████████████████▊                          | 45/72 [02:45<01:35,  3.55s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  64%|████████████████████████████████████████████▋                         | 46/72 [02:49<01:36,  3.71s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  65%|█████████████████████████████████████████████▋                        | 47/72 [02:52<01:28,  3.52s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  67%|██████████████████████████████████████████████▋                       | 48/72 [02:56<01:30,  3.77s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  68%|███████████████████████████████████████████████▋                      | 49/72 [03:00<01:28,  3.85s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  69%|████████████████████████████████████████████████▌                     | 50/72 [03:04<01:25,  3.90s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  71%|█████████████████████████████████████████████████▌                    | 51/72 [03:08<01:21,  3.87s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  72%|██████████████████████████████████████████████████▌                   | 52/72 [03:12<01:16,  3.84s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  74%|███████████████████████████████████████████████████▌                  | 53/72 [03:16<01:14,  3.90s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  75%|████████████████████████████████████████████████████▌                 | 54/72 [03:19<01:03,  3.54s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  76%|█████████████████████████████████████████████████████▍                | 55/72 [03:23<01:02,  3.66s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  78%|██████████████████████████████████████████████████████▍               | 56/72 [03:26<00:56,  3.54s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  79%|███████████████████████████████████████████████████████▍              | 57/72 [03:29<00:53,  3.55s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  81%|████████████████████████████████████████████████████████▍             | 58/72 [03:33<00:50,  3.60s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  82%|█████████████████████████████████████████████████████████▎            | 59/72 [03:37<00:46,  3.60s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  83%|██████████████████████████████████████████████████████████▎           | 60/72 [03:41<00:45,  3.79s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  85%|███████████████████████████████████████████████████████████▎          | 61/72 [03:45<00:42,  3.87s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  86%|████████████████████████████████████████████████████████████▎         | 62/72 [03:49<00:37,  3.77s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  88%|█████████████████████████████████████████████████████████████▎        | 63/72 [03:52<00:33,  3.69s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  89%|██████████████████████████████████████████████████████████████▏       | 64/72 [03:55<00:28,  3.54s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  90%|███████████████████████████████████████████████████████████████▏      | 65/72 [04:00<00:26,  3.77s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  92%|████████████████████████████████████████████████████████████████▏     | 66/72 [04:03<00:21,  3.64s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  93%|█████████████████████████████████████████████████████████████████▏    | 67/72 [04:06<00:17,  3.55s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  94%|██████████████████████████████████████████████████████████████████    | 68/72 [04:10<00:14,  3.71s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  96%|███████████████████████████████████████████████████████████████████   | 69/72 [04:14<00:11,  3.81s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  97%|████████████████████████████████████████████████████████████████████  | 70/72 [04:18<00:07,  3.68s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  99%|█████████████████████████████████████████████████████████████████████ | 71/72 [04:22<00:03,  3.73s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating: 100%|██████████████████████████████████████████████████████████████████████| 72/72 [04:25<00:00,  3.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Validation Scores: ROUGE: {'rouge1': 0.4341991051058425, 'rouge2': 0.21214478630746364, 'rougeL': 0.3316140250933234, 'rougeLsum': 0.3316140250933234}, BERTScore: {'precision': 0.2999956905712477, 'recall': 0.24383456777367327, 'f1': 0.27184555488121176}\n",
      "3\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:   0%|                                                                              | 0/334 [00:00<?, ?batch/s]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Epoch 3: 100%|█████████████████████████████████████████████████████████| 334/334 [01:53<00:00,  2.93batch/s, loss=1.06]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0617, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|███████████████████████████████████████████████████████████| 72/72 [00:08<00:00,  8.29batch/s, loss=0.87]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8704, device='cuda:0')\n",
      "train_loss = 1.3195348676450238\n",
      " test_loss = 1.1601277047561274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:   0%|                                                                               | 0/72 [00:00<?, ?it/s]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:   1%|▉                                                                      | 1/72 [00:03<03:36,  3.05s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:   3%|█▉                                                                     | 2/72 [00:06<03:38,  3.12s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:   4%|██▉                                                                    | 3/72 [00:09<03:29,  3.03s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:   6%|███▉                                                                   | 4/72 [00:12<03:46,  3.34s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:   7%|████▉                                                                  | 5/72 [00:16<03:45,  3.36s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:   8%|█████▉                                                                 | 6/72 [00:20<03:58,  3.61s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  10%|██████▉                                                                | 7/72 [00:23<03:42,  3.42s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  11%|███████▉                                                               | 8/72 [00:26<03:35,  3.36s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  12%|████████▉                                                              | 9/72 [00:30<03:41,  3.51s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  14%|█████████▋                                                            | 10/72 [00:35<03:58,  3.84s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  15%|██████████▋                                                           | 11/72 [00:38<03:41,  3.63s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  17%|███████████▋                                                          | 12/72 [00:41<03:35,  3.60s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  18%|████████████▋                                                         | 13/72 [00:44<03:21,  3.42s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  19%|█████████████▌                                                        | 14/72 [00:47<03:11,  3.30s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  21%|██████████████▌                                                       | 15/72 [00:51<03:16,  3.45s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  22%|███████████████▌                                                      | 16/72 [00:54<03:08,  3.37s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  24%|████████████████▌                                                     | 17/72 [00:58<03:02,  3.31s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  25%|█████████████████▌                                                    | 18/72 [01:01<03:08,  3.50s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  26%|██████████████████▍                                                   | 19/72 [01:05<03:14,  3.67s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  28%|███████████████████▍                                                  | 20/72 [01:09<03:08,  3.62s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  29%|████████████████████▍                                                 | 21/72 [01:13<03:13,  3.80s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  31%|█████████████████████▍                                                | 22/72 [01:18<03:27,  4.16s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  32%|██████████████████████▎                                               | 23/72 [01:22<03:14,  3.97s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  33%|███████████████████████▎                                              | 24/72 [01:25<03:01,  3.78s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  35%|████████████████████████▎                                             | 25/72 [01:29<02:57,  3.77s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  36%|█████████████████████████▎                                            | 26/72 [01:32<02:45,  3.60s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  38%|██████████████████████████▎                                           | 27/72 [01:35<02:37,  3.50s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  39%|███████████████████████████▏                                          | 28/72 [01:39<02:41,  3.67s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  40%|████████████████████████████▏                                         | 29/72 [01:43<02:35,  3.61s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  42%|█████████████████████████████▏                                        | 30/72 [01:47<02:36,  3.72s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  43%|██████████████████████████████▏                                       | 31/72 [01:50<02:25,  3.55s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  44%|███████████████████████████████                                       | 32/72 [01:53<02:13,  3.35s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  46%|████████████████████████████████                                      | 33/72 [01:57<02:19,  3.58s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  47%|█████████████████████████████████                                     | 34/72 [02:00<02:10,  3.43s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  49%|██████████████████████████████████                                    | 35/72 [02:03<02:04,  3.37s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  50%|███████████████████████████████████                                   | 36/72 [02:07<02:04,  3.47s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  51%|███████████████████████████████████▉                                  | 37/72 [02:11<02:07,  3.63s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  53%|████████████████████████████████████▉                                 | 38/72 [02:15<02:07,  3.75s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  54%|█████████████████████████████████████▉                                | 39/72 [02:19<02:04,  3.79s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  56%|██████████████████████████████████████▉                               | 40/72 [02:23<02:04,  3.89s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  57%|███████████████████████████████████████▊                              | 41/72 [02:27<02:00,  3.88s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  58%|████████████████████████████████████████▊                             | 42/72 [02:30<01:52,  3.76s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  60%|█████████████████████████████████████████▊                            | 43/72 [02:34<01:46,  3.66s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  61%|██████████████████████████████████████████▊                           | 44/72 [02:37<01:39,  3.56s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  62%|███████████████████████████████████████████▊                          | 45/72 [02:40<01:34,  3.49s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  64%|████████████████████████████████████████████▋                         | 46/72 [02:44<01:33,  3.60s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  65%|█████████████████████████████████████████████▋                        | 47/72 [02:47<01:23,  3.35s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  67%|██████████████████████████████████████████████▋                       | 48/72 [02:51<01:24,  3.50s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  68%|███████████████████████████████████████████████▋                      | 49/72 [02:55<01:21,  3.53s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  69%|████████████████████████████████████████████████▌                     | 50/72 [02:58<01:18,  3.58s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  71%|█████████████████████████████████████████████████▌                    | 51/72 [03:02<01:19,  3.76s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  72%|██████████████████████████████████████████████████▌                   | 52/72 [03:06<01:15,  3.77s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  74%|███████████████████████████████████████████████████▌                  | 53/72 [03:10<01:09,  3.67s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  75%|████████████████████████████████████████████████████▌                 | 54/72 [03:13<01:02,  3.49s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  76%|█████████████████████████████████████████████████████▍                | 55/72 [03:16<00:59,  3.49s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  78%|██████████████████████████████████████████████████████▍               | 56/72 [03:20<00:55,  3.45s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  79%|███████████████████████████████████████████████████████▍              | 57/72 [03:24<00:57,  3.84s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  81%|████████████████████████████████████████████████████████▍             | 58/72 [03:29<00:55,  3.95s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  82%|█████████████████████████████████████████████████████████▎            | 59/72 [03:32<00:50,  3.88s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  83%|██████████████████████████████████████████████████████████▎           | 60/72 [03:37<00:48,  4.02s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  85%|███████████████████████████████████████████████████████████▎          | 61/72 [03:41<00:44,  4.02s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  86%|████████████████████████████████████████████████████████████▎         | 62/72 [03:44<00:37,  3.72s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  88%|█████████████████████████████████████████████████████████████▎        | 63/72 [03:47<00:32,  3.61s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  89%|██████████████████████████████████████████████████████████████▏       | 64/72 [03:50<00:27,  3.45s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  90%|███████████████████████████████████████████████████████████████▏      | 65/72 [03:54<00:25,  3.62s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  92%|████████████████████████████████████████████████████████████████▏     | 66/72 [03:57<00:20,  3.47s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  93%|█████████████████████████████████████████████████████████████████▏    | 67/72 [04:01<00:17,  3.48s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  94%|██████████████████████████████████████████████████████████████████    | 68/72 [04:05<00:14,  3.58s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  96%|███████████████████████████████████████████████████████████████████   | 69/72 [04:08<00:10,  3.65s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  97%|████████████████████████████████████████████████████████████████████  | 70/72 [04:13<00:07,  3.97s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  99%|█████████████████████████████████████████████████████████████████████ | 71/72 [04:18<00:04,  4.22s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating: 100%|██████████████████████████████████████████████████████████████████████| 72/72 [04:21<00:00,  3.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Validation Scores: ROUGE: {'rouge1': 0.43331292498850815, 'rouge2': 0.2167319495138679, 'rougeL': 0.3332696531200922, 'rougeLsum': 0.3332696531200922}, BERTScore: {'precision': 0.3098932278678856, 'recall': 0.22929391872862148, 'f1': 0.2691844085283164}\n",
      "4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4:   0%|                                                                              | 0/334 [00:00<?, ?batch/s]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Epoch 4: 100%|████████████████████████████████████████████████████████| 334/334 [01:53<00:00,  2.95batch/s, loss=0.697]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6974, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████████████████████████████████████████████████████| 72/72 [00:08<00:00,  8.06batch/s, loss=0.885]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8848, device='cuda:0')\n",
      "train_loss = 1.2127800692698198\n",
      " test_loss = 1.1386823679010074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:   0%|                                                                               | 0/72 [00:00<?, ?it/s]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:   1%|▉                                                                      | 1/72 [00:03<04:38,  3.92s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:   3%|█▉                                                                     | 2/72 [00:07<04:22,  3.75s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:   4%|██▉                                                                    | 3/72 [00:12<04:56,  4.29s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:   6%|███▉                                                                   | 4/72 [00:16<04:37,  4.08s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:   7%|████▉                                                                  | 5/72 [00:19<04:12,  3.77s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:   8%|█████▉                                                                 | 6/72 [00:23<04:07,  3.75s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  10%|██████▉                                                                | 7/72 [00:26<03:47,  3.50s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  11%|███████▉                                                               | 8/72 [00:29<03:39,  3.43s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  12%|████████▉                                                              | 9/72 [00:32<03:36,  3.44s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  14%|█████████▋                                                            | 10/72 [00:36<03:37,  3.51s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  15%|██████████▋                                                           | 11/72 [00:40<03:33,  3.50s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  17%|███████████▋                                                          | 12/72 [00:43<03:31,  3.53s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  18%|████████████▋                                                         | 13/72 [00:47<03:30,  3.56s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  19%|█████████████▌                                                        | 14/72 [00:50<03:20,  3.45s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  21%|██████████████▌                                                       | 15/72 [00:55<03:37,  3.82s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  22%|███████████████▌                                                      | 16/72 [00:58<03:25,  3.67s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  24%|████████████████▌                                                     | 17/72 [01:02<03:27,  3.77s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  25%|█████████████████▌                                                    | 18/72 [01:07<03:35,  4.00s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  26%|██████████████████▍                                                   | 19/72 [01:11<03:35,  4.07s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  28%|███████████████████▍                                                  | 20/72 [01:14<03:24,  3.93s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  29%|████████████████████▍                                                 | 21/72 [01:18<03:17,  3.87s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  31%|█████████████████████▍                                                | 22/72 [01:22<03:14,  3.88s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  32%|██████████████████████▎                                               | 23/72 [01:25<03:04,  3.77s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  33%|███████████████████████▎                                              | 24/72 [01:29<02:53,  3.62s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  35%|████████████████████████▎                                             | 25/72 [01:33<02:52,  3.67s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  36%|█████████████████████████▎                                            | 26/72 [01:36<02:44,  3.57s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  38%|██████████████████████████▎                                           | 27/72 [01:39<02:36,  3.48s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  39%|███████████████████████████▏                                          | 28/72 [01:43<02:33,  3.48s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  40%|████████████████████████████▏                                         | 29/72 [01:46<02:29,  3.49s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  42%|█████████████████████████████▏                                        | 30/72 [01:50<02:30,  3.58s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  43%|██████████████████████████████▏                                       | 31/72 [01:54<02:28,  3.63s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  44%|███████████████████████████████                                       | 32/72 [01:57<02:21,  3.54s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  46%|████████████████████████████████                                      | 33/72 [02:01<02:25,  3.74s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  47%|█████████████████████████████████                                     | 34/72 [02:05<02:18,  3.64s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  49%|██████████████████████████████████                                    | 35/72 [02:08<02:12,  3.58s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  50%|███████████████████████████████████                                   | 36/72 [02:12<02:13,  3.71s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  51%|███████████████████████████████████▉                                  | 37/72 [02:16<02:09,  3.71s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  53%|████████████████████████████████████▉                                 | 38/72 [02:20<02:06,  3.73s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  54%|█████████████████████████████████████▉                                | 39/72 [02:23<02:01,  3.67s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  56%|██████████████████████████████████████▉                               | 40/72 [02:27<01:59,  3.74s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  57%|███████████████████████████████████████▊                              | 41/72 [02:30<01:50,  3.57s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  58%|████████████████████████████████████████▊                             | 42/72 [02:34<01:45,  3.51s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  60%|█████████████████████████████████████████▊                            | 43/72 [02:37<01:39,  3.44s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  61%|██████████████████████████████████████████▊                           | 44/72 [02:41<01:38,  3.52s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  62%|███████████████████████████████████████████▊                          | 45/72 [02:44<01:34,  3.50s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  64%|████████████████████████████████████████████▋                         | 46/72 [02:48<01:33,  3.59s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  65%|█████████████████████████████████████████████▋                        | 47/72 [02:51<01:29,  3.59s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  67%|██████████████████████████████████████████████▋                       | 48/72 [02:56<01:32,  3.87s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  68%|███████████████████████████████████████████████▋                      | 49/72 [03:00<01:31,  3.96s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  69%|████████████████████████████████████████████████▌                     | 50/72 [03:04<01:28,  4.03s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  71%|█████████████████████████████████████████████████▌                    | 51/72 [03:08<01:25,  4.06s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  72%|██████████████████████████████████████████████████▌                   | 52/72 [03:13<01:23,  4.20s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  74%|███████████████████████████████████████████████████▌                  | 53/72 [03:17<01:19,  4.21s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  75%|████████████████████████████████████████████████████▌                 | 54/72 [03:20<01:08,  3.80s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  76%|█████████████████████████████████████████████████████▍                | 55/72 [03:23<01:02,  3.67s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  78%|██████████████████████████████████████████████████████▍               | 56/72 [03:27<00:57,  3.58s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  79%|███████████████████████████████████████████████████████▍              | 57/72 [03:30<00:54,  3.63s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  81%|████████████████████████████████████████████████████████▍             | 58/72 [03:34<00:50,  3.60s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  82%|█████████████████████████████████████████████████████████▎            | 59/72 [03:38<00:47,  3.64s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  83%|██████████████████████████████████████████████████████████▎           | 60/72 [03:41<00:43,  3.67s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  85%|███████████████████████████████████████████████████████████▎          | 61/72 [03:45<00:40,  3.70s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  86%|████████████████████████████████████████████████████████████▎         | 62/72 [03:49<00:37,  3.71s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  88%|█████████████████████████████████████████████████████████████▎        | 63/72 [03:53<00:33,  3.70s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  89%|██████████████████████████████████████████████████████████████▏       | 64/72 [03:56<00:28,  3.62s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  90%|███████████████████████████████████████████████████████████████▏      | 65/72 [04:01<00:28,  4.02s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  92%|████████████████████████████████████████████████████████████████▏     | 66/72 [04:05<00:24,  4.05s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  93%|█████████████████████████████████████████████████████████████████▏    | 67/72 [04:09<00:19,  3.90s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  94%|██████████████████████████████████████████████████████████████████    | 68/72 [04:13<00:16,  4.08s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  96%|███████████████████████████████████████████████████████████████████   | 69/72 [04:17<00:12,  4.05s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  97%|████████████████████████████████████████████████████████████████████  | 70/72 [04:20<00:07,  3.76s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  99%|█████████████████████████████████████████████████████████████████████ | 71/72 [04:24<00:03,  3.80s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating: 100%|██████████████████████████████████████████████████████████████████████| 72/72 [04:28<00:00,  3.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Validation Scores: ROUGE: {'rouge1': 0.45476049604244245, 'rouge2': 0.22281187076178602, 'rougeL': 0.343806066386882, 'rougeLsum': 0.343806066386882}, BERTScore: {'precision': 0.33643206188248265, 'recall': 0.2742686634365883, 'f1': 0.3052771755804618}\n",
      "5\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5:   0%|                                                                              | 0/334 [00:00<?, ?batch/s]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Epoch 5: 100%|█████████████████████████████████████████████████████████| 334/334 [01:53<00:00,  2.93batch/s, loss=1.18]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.1822, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████████████████████████████████████████████████████| 72/72 [00:08<00:00,  8.06batch/s, loss=0.889]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8885, device='cuda:0')\n",
      "train_loss = 1.1028135111053547\n",
      " test_loss = 1.1436062248216734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:   0%|                                                                               | 0/72 [00:00<?, ?it/s]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:   1%|▉                                                                      | 1/72 [00:03<03:53,  3.29s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:   3%|█▉                                                                     | 2/72 [00:06<03:47,  3.25s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:   4%|██▉                                                                    | 3/72 [00:09<03:37,  3.15s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:   6%|███▉                                                                   | 4/72 [00:12<03:38,  3.21s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:   7%|████▉                                                                  | 5/72 [00:16<03:46,  3.39s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:   8%|█████▉                                                                 | 6/72 [00:21<04:13,  3.84s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  10%|██████▉                                                                | 7/72 [00:24<04:00,  3.71s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  11%|███████▉                                                               | 8/72 [00:27<03:45,  3.53s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  12%|████████▉                                                              | 9/72 [00:30<03:34,  3.40s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  14%|█████████▋                                                            | 10/72 [00:34<03:31,  3.42s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  15%|██████████▋                                                           | 11/72 [00:37<03:25,  3.37s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  17%|███████████▋                                                          | 12/72 [00:41<03:30,  3.50s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  18%|████████████▋                                                         | 13/72 [00:45<03:29,  3.55s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  19%|█████████████▌                                                        | 14/72 [00:48<03:21,  3.47s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  21%|██████████████▌                                                       | 15/72 [00:52<03:20,  3.52s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  22%|███████████████▌                                                      | 16/72 [00:55<03:14,  3.47s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  24%|████████████████▌                                                     | 17/72 [00:58<02:59,  3.27s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  25%|█████████████████▌                                                    | 18/72 [01:01<03:03,  3.39s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  26%|██████████████████▍                                                   | 19/72 [01:05<03:05,  3.49s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  28%|███████████████████▍                                                  | 20/72 [01:08<02:58,  3.42s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  29%|████████████████████▍                                                 | 21/72 [01:12<02:59,  3.52s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  31%|█████████████████████▍                                                | 22/72 [01:16<02:59,  3.60s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  32%|██████████████████████▎                                               | 23/72 [01:19<02:55,  3.59s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  33%|███████████████████████▎                                              | 24/72 [01:23<02:49,  3.53s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  35%|████████████████████████▎                                             | 25/72 [01:27<02:57,  3.77s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  36%|█████████████████████████▎                                            | 26/72 [01:31<02:47,  3.65s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  38%|██████████████████████████▎                                           | 27/72 [01:35<02:54,  3.88s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  39%|███████████████████████████▏                                          | 28/72 [01:39<02:53,  3.94s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  40%|████████████████████████████▏                                         | 29/72 [01:43<02:43,  3.81s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  42%|█████████████████████████████▏                                        | 30/72 [01:46<02:33,  3.65s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  43%|██████████████████████████████▏                                       | 31/72 [01:49<02:27,  3.59s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  44%|███████████████████████████████                                       | 32/72 [01:52<02:17,  3.43s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  46%|████████████████████████████████                                      | 33/72 [01:56<02:17,  3.51s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  47%|█████████████████████████████████                                     | 34/72 [01:59<02:08,  3.39s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  49%|██████████████████████████████████                                    | 35/72 [02:03<02:05,  3.38s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  50%|███████████████████████████████████                                   | 36/72 [02:06<02:00,  3.35s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  51%|███████████████████████████████████▉                                  | 37/72 [02:09<01:56,  3.31s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  53%|████████████████████████████████████▉                                 | 38/72 [02:13<01:57,  3.47s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  54%|█████████████████████████████████████▉                                | 39/72 [02:16<01:55,  3.49s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  56%|██████████████████████████████████████▉                               | 40/72 [02:20<01:57,  3.66s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  57%|███████████████████████████████████████▊                              | 41/72 [02:24<01:48,  3.49s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  58%|████████████████████████████████████████▊                             | 42/72 [02:27<01:47,  3.60s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  60%|█████████████████████████████████████████▊                            | 43/72 [02:31<01:42,  3.54s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  61%|██████████████████████████████████████████▊                           | 44/72 [02:35<01:42,  3.66s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  62%|███████████████████████████████████████████▊                          | 45/72 [02:38<01:37,  3.60s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  64%|████████████████████████████████████████████▋                         | 46/72 [02:42<01:34,  3.64s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  65%|█████████████████████████████████████████████▋                        | 47/72 [02:45<01:24,  3.37s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  67%|██████████████████████████████████████████████▋                       | 48/72 [02:48<01:23,  3.46s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  68%|███████████████████████████████████████████████▋                      | 49/72 [02:52<01:19,  3.44s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  69%|████████████████████████████████████████████████▌                     | 50/72 [02:56<01:17,  3.54s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  71%|█████████████████████████████████████████████████▌                    | 51/72 [03:00<01:16,  3.67s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  72%|██████████████████████████████████████████████████▌                   | 52/72 [03:03<01:10,  3.53s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  74%|███████████████████████████████████████████████████▌                  | 53/72 [03:07<01:09,  3.67s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  75%|████████████████████████████████████████████████████▌                 | 54/72 [03:09<01:00,  3.38s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  76%|█████████████████████████████████████████████████████▍                | 55/72 [03:14<01:01,  3.65s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  78%|██████████████████████████████████████████████████████▍               | 56/72 [03:17<00:58,  3.67s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  79%|███████████████████████████████████████████████████████▍              | 57/72 [03:22<01:00,  4.00s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  81%|████████████████████████████████████████████████████████▍             | 58/72 [03:27<00:58,  4.19s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  82%|█████████████████████████████████████████████████████████▎            | 59/72 [03:31<00:56,  4.34s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  83%|██████████████████████████████████████████████████████████▎           | 60/72 [03:36<00:52,  4.40s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  85%|███████████████████████████████████████████████████████████▎          | 61/72 [03:40<00:48,  4.40s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  86%|████████████████████████████████████████████████████████████▎         | 62/72 [03:44<00:40,  4.05s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  88%|█████████████████████████████████████████████████████████████▎        | 63/72 [03:47<00:34,  3.83s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  89%|██████████████████████████████████████████████████████████████▏       | 64/72 [03:50<00:29,  3.66s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  90%|███████████████████████████████████████████████████████████████▏      | 65/72 [03:54<00:26,  3.74s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  92%|████████████████████████████████████████████████████████████████▏     | 66/72 [03:57<00:21,  3.54s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  93%|█████████████████████████████████████████████████████████████████▏    | 67/72 [04:01<00:17,  3.49s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  94%|██████████████████████████████████████████████████████████████████    | 68/72 [04:05<00:14,  3.61s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  96%|███████████████████████████████████████████████████████████████████   | 69/72 [04:08<00:11,  3.69s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  97%|████████████████████████████████████████████████████████████████████  | 70/72 [04:11<00:06,  3.48s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  99%|█████████████████████████████████████████████████████████████████████ | 71/72 [04:15<00:03,  3.49s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating: 100%|██████████████████████████████████████████████████████████████████████| 72/72 [04:18<00:00,  3.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Validation Scores: ROUGE: {'rouge1': 0.4355809416861971, 'rouge2': 0.21227214750115528, 'rougeL': 0.33510565453142943, 'rougeLsum': 0.33510565453142943}, BERTScore: {'precision': 0.33389977138075566, 'recall': 0.23940340931423837, 'f1': 0.2861260389909148}\n",
      "6\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6:   0%|                                                                              | 0/334 [00:00<?, ?batch/s]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Epoch 6: 100%|█████████████████████████████████████████████████████████| 334/334 [01:53<00:00,  2.95batch/s, loss=1.99]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.9898, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████████████████████████████████████████████████████| 72/72 [00:08<00:00,  8.09batch/s, loss=0.899]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8987, device='cuda:0')\n",
      "train_loss = 1.014085433618751\n",
      " test_loss = 1.1518146495024364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:   0%|                                                                               | 0/72 [00:00<?, ?it/s]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:   1%|▉                                                                      | 1/72 [00:03<04:14,  3.58s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:   3%|█▉                                                                     | 2/72 [00:06<03:57,  3.39s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:   4%|██▉                                                                    | 3/72 [00:10<03:52,  3.38s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:   6%|███▉                                                                   | 4/72 [00:13<03:55,  3.47s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:   7%|████▉                                                                  | 5/72 [00:16<03:42,  3.32s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:   8%|█████▉                                                                 | 6/72 [00:21<04:01,  3.66s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  10%|██████▉                                                                | 7/72 [00:24<03:55,  3.62s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  11%|███████▉                                                               | 8/72 [00:27<03:39,  3.43s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  12%|████████▉                                                              | 9/72 [00:30<03:29,  3.32s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  14%|█████████▋                                                            | 10/72 [00:34<03:29,  3.38s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  15%|██████████▋                                                           | 11/72 [00:37<03:22,  3.32s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  17%|███████████▋                                                          | 12/72 [00:41<03:28,  3.47s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  18%|████████████▋                                                         | 13/72 [00:44<03:24,  3.46s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  19%|█████████████▌                                                        | 14/72 [00:48<03:32,  3.66s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  21%|██████████████▌                                                       | 15/72 [00:52<03:27,  3.63s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  22%|███████████████▌                                                      | 16/72 [00:56<03:23,  3.63s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  24%|████████████████▌                                                     | 17/72 [00:59<03:20,  3.64s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  25%|█████████████████▌                                                    | 18/72 [01:03<03:22,  3.74s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  26%|██████████████████▍                                                   | 19/72 [01:07<03:13,  3.66s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  28%|███████████████████▍                                                  | 20/72 [01:11<03:13,  3.73s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  29%|████████████████████▍                                                 | 21/72 [01:14<03:08,  3.70s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  31%|█████████████████████▍                                                | 22/72 [01:18<03:08,  3.76s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  32%|██████████████████████▎                                               | 23/72 [01:22<02:58,  3.65s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  33%|███████████████████████▎                                              | 24/72 [01:25<02:51,  3.58s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  35%|████████████████████████▎                                             | 25/72 [01:29<02:49,  3.62s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  36%|█████████████████████████▎                                            | 26/72 [01:32<02:38,  3.45s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  38%|██████████████████████████▎                                           | 27/72 [01:36<02:40,  3.57s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  39%|███████████████████████████▏                                          | 28/72 [01:39<02:37,  3.57s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  40%|████████████████████████████▏                                         | 29/72 [01:42<02:28,  3.46s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  42%|█████████████████████████████▏                                        | 30/72 [01:46<02:23,  3.41s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  43%|██████████████████████████████▏                                       | 31/72 [01:49<02:24,  3.51s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  44%|███████████████████████████████                                       | 32/72 [01:53<02:16,  3.42s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  46%|████████████████████████████████                                      | 33/72 [01:57<02:24,  3.71s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  47%|█████████████████████████████████                                     | 34/72 [02:00<02:16,  3.60s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  49%|██████████████████████████████████                                    | 35/72 [02:04<02:14,  3.62s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  50%|███████████████████████████████████                                   | 36/72 [02:08<02:10,  3.62s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  51%|███████████████████████████████████▉                                  | 37/72 [02:11<02:03,  3.51s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  53%|████████████████████████████████████▉                                 | 38/72 [02:15<02:06,  3.73s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  54%|█████████████████████████████████████▉                                | 39/72 [02:19<02:05,  3.81s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  56%|██████████████████████████████████████▉                               | 40/72 [02:23<02:01,  3.79s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  57%|███████████████████████████████████████▊                              | 41/72 [02:27<01:58,  3.82s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  58%|████████████████████████████████████████▊                             | 42/72 [02:30<01:50,  3.68s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  60%|█████████████████████████████████████████▊                            | 43/72 [02:33<01:42,  3.54s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  61%|██████████████████████████████████████████▊                           | 44/72 [02:37<01:40,  3.61s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  62%|███████████████████████████████████████████▊                          | 45/72 [02:40<01:33,  3.46s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  64%|████████████████████████████████████████████▋                         | 46/72 [02:44<01:32,  3.55s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  65%|█████████████████████████████████████████████▋                        | 47/72 [02:47<01:24,  3.39s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  67%|██████████████████████████████████████████████▋                       | 48/72 [02:51<01:24,  3.53s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  68%|███████████████████████████████████████████████▋                      | 49/72 [02:54<01:19,  3.47s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  69%|████████████████████████████████████████████████▌                     | 50/72 [02:58<01:20,  3.64s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  71%|█████████████████████████████████████████████████▌                    | 51/72 [03:02<01:19,  3.79s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  72%|██████████████████████████████████████████████████▌                   | 52/72 [03:06<01:13,  3.69s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  74%|███████████████████████████████████████████████████▌                  | 53/72 [03:10<01:14,  3.90s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  75%|████████████████████████████████████████████████████▌                 | 54/72 [03:13<01:06,  3.70s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  76%|█████████████████████████████████████████████████████▍                | 55/72 [03:17<01:02,  3.65s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  78%|██████████████████████████████████████████████████████▍               | 56/72 [03:20<00:57,  3.62s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  79%|███████████████████████████████████████████████████████▍              | 57/72 [03:24<00:53,  3.59s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  81%|████████████████████████████████████████████████████████▍             | 58/72 [03:28<00:50,  3.59s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  82%|█████████████████████████████████████████████████████████▎            | 59/72 [03:31<00:47,  3.67s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  83%|██████████████████████████████████████████████████████████▎           | 60/72 [03:35<00:43,  3.58s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  85%|███████████████████████████████████████████████████████████▎          | 61/72 [03:38<00:38,  3.53s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  86%|████████████████████████████████████████████████████████████▎         | 62/72 [03:41<00:34,  3.40s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  88%|█████████████████████████████████████████████████████████████▎        | 63/72 [03:45<00:31,  3.47s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  89%|██████████████████████████████████████████████████████████████▏       | 64/72 [03:48<00:27,  3.47s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  90%|███████████████████████████████████████████████████████████████▏      | 65/72 [03:52<00:25,  3.58s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  92%|████████████████████████████████████████████████████████████████▏     | 66/72 [03:55<00:20,  3.46s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  93%|█████████████████████████████████████████████████████████████████▏    | 67/72 [03:59<00:18,  3.60s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  94%|██████████████████████████████████████████████████████████████████    | 68/72 [04:04<00:15,  3.82s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  96%|███████████████████████████████████████████████████████████████████   | 69/72 [04:08<00:11,  3.89s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  97%|████████████████████████████████████████████████████████████████████  | 70/72 [04:11<00:07,  3.68s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  99%|█████████████████████████████████████████████████████████████████████ | 71/72 [04:15<00:03,  3.76s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating: 100%|██████████████████████████████████████████████████████████████████████| 72/72 [04:18<00:00,  3.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Validation Scores: ROUGE: {'rouge1': 0.4417819707737161, 'rouge2': 0.2110195334966848, 'rougeL': 0.33179836780626776, 'rougeLsum': 0.33179836780626776}, BERTScore: {'precision': 0.3306504320870671, 'recall': 0.26208203192800283, 'f1': 0.2962837043321795}\n",
      "7\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7:   0%|                                                                              | 0/334 [00:00<?, ?batch/s]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Epoch 7: 100%|█████████████████████████████████████████████████████████| 334/334 [01:54<00:00,  2.93batch/s, loss=1.04]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0425, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████████████████████████████████████████████████████| 72/72 [00:08<00:00,  8.03batch/s, loss=0.934]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9343, device='cuda:0')\n",
      "train_loss = 0.9278731931469397\n",
      " test_loss = 1.1709975885848205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:   0%|                                                                               | 0/72 [00:00<?, ?it/s]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:   1%|▉                                                                      | 1/72 [00:03<03:44,  3.17s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:   3%|█▉                                                                     | 2/72 [00:06<03:56,  3.38s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:   4%|██▉                                                                    | 3/72 [00:10<04:01,  3.50s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:   6%|███▉                                                                   | 4/72 [00:14<04:15,  3.76s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:   7%|████▉                                                                  | 5/72 [00:17<03:57,  3.54s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:   8%|█████▉                                                                 | 6/72 [00:21<03:56,  3.58s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  10%|██████▉                                                                | 7/72 [00:24<03:43,  3.44s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  11%|███████▉                                                               | 8/72 [00:27<03:32,  3.31s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  12%|████████▉                                                              | 9/72 [00:30<03:28,  3.31s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  14%|█████████▋                                                            | 10/72 [00:35<03:44,  3.62s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  15%|██████████▋                                                           | 11/72 [00:38<03:38,  3.59s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  17%|███████████▋                                                          | 12/72 [00:42<03:43,  3.72s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  18%|████████████▋                                                         | 13/72 [00:46<03:34,  3.63s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  19%|█████████████▌                                                        | 14/72 [00:49<03:26,  3.57s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  21%|██████████████▌                                                       | 15/72 [00:53<03:24,  3.59s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  22%|███████████████▌                                                      | 16/72 [00:56<03:21,  3.60s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  24%|████████████████▌                                                     | 17/72 [01:01<03:32,  3.87s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  25%|█████████████████▌                                                    | 18/72 [01:05<03:27,  3.84s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  26%|██████████████████▍                                                   | 19/72 [01:08<03:15,  3.68s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  28%|███████████████████▍                                                  | 20/72 [01:11<03:08,  3.62s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  29%|████████████████████▍                                                 | 21/72 [01:15<03:05,  3.63s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  31%|█████████████████████▍                                                | 22/72 [01:19<03:05,  3.72s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  32%|██████████████████████▎                                               | 23/72 [01:23<03:04,  3.77s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  33%|███████████████████████▎                                              | 24/72 [01:26<02:52,  3.59s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  35%|████████████████████████▎                                             | 25/72 [01:30<02:51,  3.65s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  36%|█████████████████████████▎                                            | 26/72 [01:33<02:39,  3.47s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  38%|██████████████████████████▎                                           | 27/72 [01:37<02:43,  3.63s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  39%|███████████████████████████▏                                          | 28/72 [01:40<02:36,  3.55s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  40%|████████████████████████████▏                                         | 29/72 [01:44<02:36,  3.63s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  42%|█████████████████████████████▏                                        | 30/72 [01:47<02:29,  3.57s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  43%|██████████████████████████████▏                                       | 31/72 [01:52<02:36,  3.81s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  44%|███████████████████████████████                                       | 32/72 [01:55<02:27,  3.69s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  46%|████████████████████████████████                                      | 33/72 [02:00<02:32,  3.90s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  47%|█████████████████████████████████                                     | 34/72 [02:03<02:26,  3.86s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  49%|██████████████████████████████████                                    | 35/72 [02:07<02:16,  3.69s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  50%|███████████████████████████████████                                   | 36/72 [02:10<02:13,  3.72s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  51%|███████████████████████████████████▉                                  | 37/72 [02:14<02:11,  3.77s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  53%|████████████████████████████████████▉                                 | 38/72 [02:18<02:07,  3.76s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  54%|█████████████████████████████████████▉                                | 39/72 [02:21<01:56,  3.54s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  56%|██████████████████████████████████████▉                               | 40/72 [02:25<01:53,  3.53s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  57%|███████████████████████████████████████▊                              | 41/72 [02:28<01:48,  3.50s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  58%|████████████████████████████████████████▊                             | 42/72 [02:31<01:43,  3.46s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  60%|█████████████████████████████████████████▊                            | 43/72 [02:35<01:44,  3.59s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  61%|██████████████████████████████████████████▊                           | 44/72 [02:39<01:42,  3.66s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  62%|███████████████████████████████████████████▊                          | 45/72 [02:44<01:50,  4.08s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  64%|████████████████████████████████████████████▋                         | 46/72 [02:48<01:45,  4.07s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  65%|█████████████████████████████████████████████▋                        | 47/72 [02:51<01:35,  3.80s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  67%|██████████████████████████████████████████████▋                       | 48/72 [02:55<01:28,  3.70s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  68%|███████████████████████████████████████████████▋                      | 49/72 [02:59<01:28,  3.85s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  69%|████████████████████████████████████████████████▌                     | 50/72 [03:03<01:23,  3.81s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  71%|█████████████████████████████████████████████████▌                    | 51/72 [03:07<01:20,  3.83s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  72%|██████████████████████████████████████████████████▌                   | 52/72 [03:10<01:14,  3.73s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  74%|███████████████████████████████████████████████████▌                  | 53/72 [03:14<01:10,  3.71s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  75%|████████████████████████████████████████████████████▌                 | 54/72 [03:17<01:03,  3.53s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  76%|█████████████████████████████████████████████████████▍                | 55/72 [03:21<01:01,  3.59s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  78%|██████████████████████████████████████████████████████▍               | 56/72 [03:24<00:57,  3.61s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  79%|███████████████████████████████████████████████████████▍              | 57/72 [03:28<00:54,  3.64s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  81%|████████████████████████████████████████████████████████▍             | 58/72 [03:32<00:51,  3.67s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  82%|█████████████████████████████████████████████████████████▎            | 59/72 [03:36<00:48,  3.77s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  83%|██████████████████████████████████████████████████████████▎           | 60/72 [03:40<00:47,  3.92s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  85%|███████████████████████████████████████████████████████████▎          | 61/72 [03:44<00:43,  3.96s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  86%|████████████████████████████████████████████████████████████▎         | 62/72 [03:48<00:39,  3.91s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  88%|█████████████████████████████████████████████████████████████▎        | 63/72 [03:51<00:34,  3.82s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  89%|██████████████████████████████████████████████████████████████▏       | 64/72 [03:55<00:30,  3.80s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  90%|███████████████████████████████████████████████████████████████▏      | 65/72 [03:59<00:27,  3.89s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  92%|████████████████████████████████████████████████████████████████▏     | 66/72 [04:03<00:22,  3.68s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  93%|█████████████████████████████████████████████████████████████████▏    | 67/72 [04:06<00:17,  3.52s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  94%|██████████████████████████████████████████████████████████████████    | 68/72 [04:09<00:14,  3.60s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  96%|███████████████████████████████████████████████████████████████████   | 69/72 [04:13<00:10,  3.60s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  97%|████████████████████████████████████████████████████████████████████  | 70/72 [04:16<00:06,  3.37s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  99%|█████████████████████████████████████████████████████████████████████ | 71/72 [04:19<00:03,  3.37s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating: 100%|██████████████████████████████████████████████████████████████████████| 72/72 [04:22<00:00,  3.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 Validation Scores: ROUGE: {'rouge1': 0.4494794710753568, 'rouge2': 0.21934081637753045, 'rougeL': 0.3415317964947422, 'rougeLsum': 0.3415317964947422}, BERTScore: {'precision': 0.3394577394032644, 'recall': 0.27083323103660306, 'f1': 0.3049719835528069}\n",
      "8\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8:   0%|                                                                              | 0/334 [00:00<?, ?batch/s]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Epoch 8: 100%|████████████████████████████████████████████████████████| 334/334 [01:41<00:00,  3.28batch/s, loss=0.669]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6693, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|███████████████████████████████████████████████████████████| 72/72 [00:09<00:00,  7.54batch/s, loss=0.93]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9301, device='cuda:0')\n",
      "train_loss = 0.8552034502614758\n",
      " test_loss = 1.1922425710492663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:   0%|                                                                               | 0/72 [00:00<?, ?it/s]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:   1%|▉                                                                      | 1/72 [00:03<04:02,  3.42s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:   3%|█▉                                                                     | 2/72 [00:06<03:53,  3.34s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:   4%|██▉                                                                    | 3/72 [00:10<04:02,  3.52s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:   6%|███▉                                                                   | 4/72 [00:14<04:05,  3.61s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:   7%|████▉                                                                  | 5/72 [00:17<03:54,  3.51s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:   8%|█████▉                                                                 | 6/72 [00:21<03:58,  3.61s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  10%|██████▉                                                                | 7/72 [00:24<03:48,  3.52s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  11%|███████▉                                                               | 8/72 [00:28<04:01,  3.77s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  12%|████████▉                                                              | 9/72 [00:32<03:51,  3.68s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  14%|█████████▋                                                            | 10/72 [00:36<03:56,  3.82s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  15%|██████████▋                                                           | 11/72 [00:40<03:51,  3.79s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  17%|███████████▋                                                          | 12/72 [00:44<04:00,  4.01s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  18%|████████████▋                                                         | 13/72 [00:48<03:46,  3.83s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  19%|█████████████▌                                                        | 14/72 [00:51<03:36,  3.73s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  21%|██████████████▌                                                       | 15/72 [00:55<03:27,  3.65s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  22%|███████████████▌                                                      | 16/72 [00:58<03:21,  3.60s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  24%|████████████████▌                                                     | 17/72 [01:02<03:21,  3.66s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  25%|█████████████████▌                                                    | 18/72 [01:06<03:27,  3.83s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  26%|██████████████████▍                                                   | 19/72 [01:10<03:23,  3.85s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  28%|███████████████████▍                                                  | 20/72 [01:13<03:12,  3.71s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  29%|████████████████████▍                                                 | 21/72 [01:17<03:13,  3.80s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  31%|█████████████████████▍                                                | 22/72 [01:23<03:32,  4.25s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  32%|██████████████████████▎                                               | 23/72 [01:27<03:22,  4.12s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  33%|███████████████████████▎                                              | 24/72 [01:31<03:15,  4.07s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  35%|████████████████████████▎                                             | 25/72 [01:35<03:16,  4.18s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  36%|█████████████████████████▎                                            | 26/72 [01:39<03:08,  4.09s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  38%|██████████████████████████▎                                           | 27/72 [01:43<03:00,  4.01s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  39%|███████████████████████████▏                                          | 28/72 [01:47<02:59,  4.09s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  40%|████████████████████████████▏                                         | 29/72 [01:50<02:46,  3.86s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  42%|█████████████████████████████▏                                        | 30/72 [01:54<02:36,  3.73s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  43%|██████████████████████████████▏                                       | 31/72 [01:58<02:34,  3.76s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  44%|███████████████████████████████                                       | 32/72 [02:01<02:24,  3.62s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  46%|████████████████████████████████                                      | 33/72 [02:05<02:25,  3.72s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  47%|█████████████████████████████████                                     | 34/72 [02:08<02:13,  3.52s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  49%|██████████████████████████████████                                    | 35/72 [02:11<02:09,  3.50s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  50%|███████████████████████████████████                                   | 36/72 [02:15<02:09,  3.61s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  51%|███████████████████████████████████▉                                  | 37/72 [02:19<02:03,  3.53s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  53%|████████████████████████████████████▉                                 | 38/72 [02:24<02:17,  4.03s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  54%|█████████████████████████████████████▉                                | 39/72 [02:27<02:06,  3.85s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  56%|██████████████████████████████████████▉                               | 40/72 [02:31<02:03,  3.85s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  57%|███████████████████████████████████████▊                              | 41/72 [02:35<02:00,  3.90s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  58%|████████████████████████████████████████▊                             | 42/72 [02:39<02:01,  4.05s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  60%|█████████████████████████████████████████▊                            | 43/72 [02:43<01:55,  3.97s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  61%|██████████████████████████████████████████▊                           | 44/72 [02:47<01:47,  3.83s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  62%|███████████████████████████████████████████▊                          | 45/72 [02:50<01:39,  3.69s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  64%|████████████████████████████████████████████▋                         | 46/72 [02:54<01:36,  3.73s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  65%|█████████████████████████████████████████████▋                        | 47/72 [02:57<01:29,  3.59s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  67%|██████████████████████████████████████████████▋                       | 48/72 [03:01<01:29,  3.71s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  68%|███████████████████████████████████████████████▋                      | 49/72 [03:05<01:27,  3.80s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  69%|████████████████████████████████████████████████▌                     | 50/72 [03:08<01:19,  3.64s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  71%|█████████████████████████████████████████████████▌                    | 51/72 [03:13<01:19,  3.78s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  72%|██████████████████████████████████████████████████▌                   | 52/72 [03:16<01:15,  3.76s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  74%|███████████████████████████████████████████████████▌                  | 53/72 [03:20<01:11,  3.75s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  75%|████████████████████████████████████████████████████▌                 | 54/72 [03:23<01:05,  3.62s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  76%|█████████████████████████████████████████████████████▍                | 55/72 [03:27<01:01,  3.60s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  78%|██████████████████████████████████████████████████████▍               | 56/72 [03:30<00:56,  3.52s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  79%|███████████████████████████████████████████████████████▍              | 57/72 [03:34<00:53,  3.54s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  81%|████████████████████████████████████████████████████████▍             | 58/72 [03:37<00:50,  3.60s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  82%|█████████████████████████████████████████████████████████▎            | 59/72 [03:41<00:46,  3.58s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  83%|██████████████████████████████████████████████████████████▎           | 60/72 [03:45<00:44,  3.73s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  85%|███████████████████████████████████████████████████████████▎          | 61/72 [03:49<00:40,  3.73s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  86%|████████████████████████████████████████████████████████████▎         | 62/72 [03:52<00:35,  3.52s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  88%|█████████████████████████████████████████████████████████████▎        | 63/72 [03:55<00:31,  3.46s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  89%|██████████████████████████████████████████████████████████████▏       | 64/72 [03:59<00:27,  3.45s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  90%|███████████████████████████████████████████████████████████████▏      | 65/72 [04:03<00:25,  3.59s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  92%|████████████████████████████████████████████████████████████████▏     | 66/72 [04:05<00:20,  3.38s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  93%|█████████████████████████████████████████████████████████████████▏    | 67/72 [04:08<00:16,  3.27s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  94%|██████████████████████████████████████████████████████████████████    | 68/72 [04:12<00:13,  3.48s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  96%|███████████████████████████████████████████████████████████████████   | 69/72 [04:16<00:10,  3.56s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  97%|████████████████████████████████████████████████████████████████████  | 70/72 [04:19<00:06,  3.40s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  99%|█████████████████████████████████████████████████████████████████████ | 71/72 [04:23<00:03,  3.61s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating: 100%|██████████████████████████████████████████████████████████████████████| 72/72 [04:26<00:00,  3.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 Validation Scores: ROUGE: {'rouge1': 0.4536722121931151, 'rouge2': 0.2183332167168207, 'rougeL': 0.3453298134346218, 'rougeLsum': 0.3453298134346218}, BERTScore: {'precision': 0.33897300540573067, 'recall': 0.27396811203410226, 'f1': 0.3065092709536354}\n",
      "9\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9:   0%|                                                                              | 0/334 [00:00<?, ?batch/s]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Epoch 9: 100%|████████████████████████████████████████████████████████| 334/334 [01:41<00:00,  3.30batch/s, loss=0.579]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5790, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████████████████████████████████████████████████████| 72/72 [00:09<00:00,  7.52batch/s, loss=0.953]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9528, device='cuda:0')\n",
      "train_loss = 0.7868955287033926\n",
      " test_loss = 1.2095284805529647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:   0%|                                                                               | 0/72 [00:00<?, ?it/s]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:   1%|▉                                                                      | 1/72 [00:04<05:09,  4.36s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:   3%|█▉                                                                     | 2/72 [00:08<05:05,  4.36s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:   4%|██▉                                                                    | 3/72 [00:12<04:41,  4.07s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:   6%|███▉                                                                   | 4/72 [00:16<04:45,  4.20s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:   7%|████▉                                                                  | 5/72 [00:20<04:29,  4.03s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:   8%|█████▉                                                                 | 6/72 [00:24<04:31,  4.12s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  10%|██████▉                                                                | 7/72 [00:28<04:12,  3.89s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  11%|███████▉                                                               | 8/72 [00:31<03:58,  3.73s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  12%|████████▉                                                              | 9/72 [00:34<03:43,  3.54s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  14%|█████████▋                                                            | 10/72 [00:39<03:58,  3.84s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  15%|██████████▋                                                           | 11/72 [00:42<03:45,  3.70s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  17%|███████████▋                                                          | 12/72 [00:46<03:45,  3.76s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  18%|████████████▋                                                         | 13/72 [00:50<03:38,  3.70s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  19%|█████████████▌                                                        | 14/72 [00:53<03:23,  3.51s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  21%|██████████████▌                                                       | 15/72 [00:56<03:21,  3.54s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  22%|███████████████▌                                                      | 16/72 [01:00<03:22,  3.62s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  24%|████████████████▌                                                     | 17/72 [01:04<03:21,  3.66s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  25%|█████████████████▌                                                    | 18/72 [01:08<03:30,  3.90s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  26%|██████████████████▍                                                   | 19/72 [01:13<03:37,  4.10s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  28%|███████████████████▍                                                  | 20/72 [01:17<03:29,  4.03s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  29%|████████████████████▍                                                 | 21/72 [01:21<03:22,  3.98s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  31%|█████████████████████▍                                                | 22/72 [01:25<03:19,  4.00s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  32%|██████████████████████▎                                               | 23/72 [01:29<03:21,  4.11s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  33%|███████████████████████▎                                              | 24/72 [01:32<03:04,  3.85s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  35%|████████████████████████▎                                             | 25/72 [01:36<03:02,  3.88s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  36%|█████████████████████████▎                                            | 26/72 [01:39<02:49,  3.69s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  38%|██████████████████████████▎                                           | 27/72 [01:43<02:49,  3.77s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  39%|███████████████████████████▏                                          | 28/72 [01:47<02:41,  3.66s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  40%|████████████████████████████▏                                         | 29/72 [01:50<02:36,  3.64s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  42%|█████████████████████████████▏                                        | 30/72 [01:54<02:27,  3.51s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  43%|██████████████████████████████▏                                       | 31/72 [01:57<02:25,  3.56s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  44%|███████████████████████████████                                       | 32/72 [02:01<02:18,  3.46s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  46%|████████████████████████████████                                      | 33/72 [02:05<02:32,  3.91s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  47%|█████████████████████████████████                                     | 34/72 [02:09<02:19,  3.68s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  49%|██████████████████████████████████                                    | 35/72 [02:12<02:16,  3.69s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  50%|███████████████████████████████████                                   | 36/72 [02:17<02:25,  4.03s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  51%|███████████████████████████████████▉                                  | 37/72 [02:21<02:22,  4.07s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  53%|████████████████████████████████████▉                                 | 38/72 [02:26<02:19,  4.10s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  54%|█████████████████████████████████████▉                                | 39/72 [02:29<02:12,  4.02s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  56%|██████████████████████████████████████▉                               | 40/72 [02:33<02:04,  3.89s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  57%|███████████████████████████████████████▊                              | 41/72 [02:37<02:01,  3.92s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  58%|████████████████████████████████████████▊                             | 42/72 [02:41<02:00,  4.03s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  60%|█████████████████████████████████████████▊                            | 43/72 [02:45<01:50,  3.81s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  61%|██████████████████████████████████████████▊                           | 44/72 [02:48<01:43,  3.71s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  62%|███████████████████████████████████████████▊                          | 45/72 [02:51<01:37,  3.60s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  64%|████████████████████████████████████████████▋                         | 46/72 [02:55<01:35,  3.69s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  65%|█████████████████████████████████████████████▋                        | 47/72 [02:58<01:27,  3.50s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  67%|██████████████████████████████████████████████▋                       | 48/72 [03:02<01:25,  3.57s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  68%|███████████████████████████████████████████████▋                      | 49/72 [03:06<01:26,  3.74s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  69%|████████████████████████████████████████████████▌                     | 50/72 [03:10<01:20,  3.64s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  71%|█████████████████████████████████████████████████▌                    | 51/72 [03:14<01:18,  3.75s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  72%|██████████████████████████████████████████████████▌                   | 52/72 [03:17<01:14,  3.75s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  74%|███████████████████████████████████████████████████▌                  | 53/72 [03:21<01:11,  3.76s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  75%|████████████████████████████████████████████████████▌                 | 54/72 [03:24<01:03,  3.55s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  76%|█████████████████████████████████████████████████████▍                | 55/72 [03:27<00:59,  3.48s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  78%|██████████████████████████████████████████████████████▍               | 56/72 [03:32<00:59,  3.70s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  79%|███████████████████████████████████████████████████████▍              | 57/72 [03:36<00:56,  3.78s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  81%|████████████████████████████████████████████████████████▍             | 58/72 [03:40<00:54,  3.86s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  82%|█████████████████████████████████████████████████████████▎            | 59/72 [03:44<00:51,  3.98s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  83%|██████████████████████████████████████████████████████████▎           | 60/72 [03:48<00:46,  3.89s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  85%|███████████████████████████████████████████████████████████▎          | 61/72 [03:51<00:41,  3.76s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  86%|████████████████████████████████████████████████████████████▎         | 62/72 [03:54<00:36,  3.62s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  88%|█████████████████████████████████████████████████████████████▎        | 63/72 [03:58<00:32,  3.66s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  89%|██████████████████████████████████████████████████████████████▏       | 64/72 [04:02<00:29,  3.71s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  90%|███████████████████████████████████████████████████████████████▏      | 65/72 [04:07<00:27,  3.97s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  92%|████████████████████████████████████████████████████████████████▏     | 66/72 [04:10<00:23,  3.91s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  93%|█████████████████████████████████████████████████████████████████▏    | 67/72 [04:14<00:19,  3.87s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  94%|██████████████████████████████████████████████████████████████████    | 68/72 [04:18<00:16,  4.02s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  96%|███████████████████████████████████████████████████████████████████   | 69/72 [04:23<00:12,  4.10s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  97%|████████████████████████████████████████████████████████████████████  | 70/72 [04:26<00:07,  3.90s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  99%|█████████████████████████████████████████████████████████████████████ | 71/72 [04:30<00:03,  3.75s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating: 100%|██████████████████████████████████████████████████████████████████████| 72/72 [04:33<00:00,  3.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 Validation Scores: ROUGE: {'rouge1': 0.45588201098674286, 'rouge2': 0.2193281390539879, 'rougeL': 0.34397407742925346, 'rougeLsum': 0.34397407742925346}, BERTScore: {'precision': 0.3314268173029025, 'recall': 0.2821064311493602, 'f1': 0.3066777867368526}\n",
      "10\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10:   0%|                                                                             | 0/334 [00:00<?, ?batch/s]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Epoch 10: 100%|█████████████████████████████████████████████████████████| 334/334 [01:41<00:00,  3.30batch/s, loss=1.2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.2030, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|█████████████████████████████████████████████████████████| 72/72 [00:09<00:00,  7.26batch/s, loss=0.917]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9168, device='cuda:0')\n",
      "train_loss = 0.7234189599127827\n",
      " test_loss = 1.2443839609622955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:   0%|                                                                               | 0/72 [00:00<?, ?it/s]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:   1%|▉                                                                      | 1/72 [00:03<04:34,  3.86s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:   3%|█▉                                                                     | 2/72 [00:07<04:15,  3.65s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:   4%|██▉                                                                    | 3/72 [00:10<04:04,  3.55s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:   6%|███▉                                                                   | 4/72 [00:14<03:55,  3.46s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:   7%|████▉                                                                  | 5/72 [00:17<03:50,  3.44s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:   8%|█████▉                                                                 | 6/72 [00:21<03:52,  3.53s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  10%|██████▉                                                                | 7/72 [00:24<03:53,  3.60s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  11%|███████▉                                                               | 8/72 [00:28<03:40,  3.44s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  12%|████████▉                                                              | 9/72 [00:31<03:33,  3.39s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  14%|█████████▋                                                            | 10/72 [00:35<03:42,  3.59s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  15%|██████████▋                                                           | 11/72 [00:39<03:42,  3.65s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  17%|███████████▋                                                          | 12/72 [00:43<03:49,  3.82s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  18%|████████████▋                                                         | 13/72 [00:47<03:48,  3.88s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  19%|█████████████▌                                                        | 14/72 [00:50<03:34,  3.70s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  21%|██████████████▌                                                       | 15/72 [00:54<03:25,  3.61s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  22%|███████████████▌                                                      | 16/72 [00:57<03:22,  3.62s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  24%|████████████████▌                                                     | 17/72 [01:01<03:19,  3.63s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  25%|█████████████████▌                                                    | 18/72 [01:05<03:20,  3.72s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  26%|██████████████████▍                                                   | 19/72 [01:08<03:11,  3.61s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  28%|███████████████████▍                                                  | 20/72 [01:11<03:00,  3.46s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  29%|████████████████████▍                                                 | 21/72 [01:15<02:58,  3.49s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  31%|█████████████████████▍                                                | 22/72 [01:19<02:57,  3.56s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  32%|██████████████████████▎                                               | 23/72 [01:22<02:52,  3.52s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  33%|███████████████████████▎                                              | 24/72 [01:25<02:44,  3.43s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  35%|████████████████████████▎                                             | 25/72 [01:29<02:47,  3.57s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  36%|█████████████████████████▎                                            | 26/72 [01:32<02:39,  3.47s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  38%|██████████████████████████▎                                           | 27/72 [01:36<02:33,  3.41s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  39%|███████████████████████████▏                                          | 28/72 [01:40<02:40,  3.64s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  40%|████████████████████████████▏                                         | 29/72 [01:44<02:38,  3.70s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  42%|█████████████████████████████▏                                        | 30/72 [01:47<02:37,  3.75s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  43%|██████████████████████████████▏                                       | 31/72 [01:51<02:32,  3.72s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  44%|███████████████████████████████                                       | 32/72 [01:55<02:36,  3.92s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  46%|████████████████████████████████                                      | 33/72 [02:00<02:36,  4.01s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  47%|█████████████████████████████████                                     | 34/72 [02:03<02:27,  3.87s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  49%|██████████████████████████████████                                    | 35/72 [02:07<02:16,  3.69s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  50%|███████████████████████████████████                                   | 36/72 [02:10<02:09,  3.61s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  51%|███████████████████████████████████▉                                  | 37/72 [02:14<02:06,  3.61s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  53%|████████████████████████████████████▉                                 | 38/72 [02:17<02:02,  3.62s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  54%|█████████████████████████████████████▉                                | 39/72 [02:21<01:56,  3.54s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  56%|██████████████████████████████████████▉                               | 40/72 [02:24<01:55,  3.60s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  57%|███████████████████████████████████████▊                              | 41/72 [02:28<01:48,  3.49s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  58%|████████████████████████████████████████▊                             | 42/72 [02:31<01:47,  3.57s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  60%|█████████████████████████████████████████▊                            | 43/72 [02:35<01:43,  3.57s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  61%|██████████████████████████████████████████▊                           | 44/72 [02:38<01:38,  3.53s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  62%|███████████████████████████████████████████▊                          | 45/72 [02:42<01:33,  3.46s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  64%|████████████████████████████████████████████▋                         | 46/72 [02:45<01:31,  3.51s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  65%|█████████████████████████████████████████████▋                        | 47/72 [02:48<01:25,  3.40s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  67%|██████████████████████████████████████████████▋                       | 48/72 [02:52<01:23,  3.49s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  68%|███████████████████████████████████████████████▋                      | 49/72 [02:56<01:23,  3.65s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  69%|████████████████████████████████████████████████▌                     | 50/72 [02:59<01:17,  3.51s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  71%|█████████████████████████████████████████████████▌                    | 51/72 [03:03<01:14,  3.55s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  72%|██████████████████████████████████████████████████▌                   | 52/72 [03:06<01:10,  3.54s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  74%|███████████████████████████████████████████████████▌                  | 53/72 [03:10<01:05,  3.47s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  75%|████████████████████████████████████████████████████▌                 | 54/72 [03:13<01:00,  3.34s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  76%|█████████████████████████████████████████████████████▍                | 55/72 [03:16<00:56,  3.33s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  78%|██████████████████████████████████████████████████████▍               | 56/72 [03:19<00:53,  3.33s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  79%|███████████████████████████████████████████████████████▍              | 57/72 [03:23<00:53,  3.54s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  81%|████████████████████████████████████████████████████████▍             | 58/72 [03:27<00:49,  3.52s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  82%|█████████████████████████████████████████████████████████▎            | 59/72 [03:31<00:46,  3.54s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  83%|██████████████████████████████████████████████████████████▎           | 60/72 [03:35<00:44,  3.74s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  85%|███████████████████████████████████████████████████████████▎          | 61/72 [03:39<00:42,  3.86s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  86%|████████████████████████████████████████████████████████████▎         | 62/72 [03:42<00:36,  3.68s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  88%|█████████████████████████████████████████████████████████████▎        | 63/72 [03:46<00:34,  3.87s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  89%|██████████████████████████████████████████████████████████████▏       | 64/72 [03:50<00:30,  3.82s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  90%|███████████████████████████████████████████████████████████████▏      | 65/72 [03:55<00:28,  4.12s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  92%|████████████████████████████████████████████████████████████████▏     | 66/72 [04:00<00:25,  4.25s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  93%|█████████████████████████████████████████████████████████████████▏    | 67/72 [04:03<00:20,  4.09s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  94%|██████████████████████████████████████████████████████████████████    | 68/72 [04:07<00:16,  4.09s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  96%|███████████████████████████████████████████████████████████████████   | 69/72 [04:11<00:12,  4.01s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  97%|████████████████████████████████████████████████████████████████████  | 70/72 [04:14<00:07,  3.69s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  99%|█████████████████████████████████████████████████████████████████████ | 71/72 [04:17<00:03,  3.55s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating: 100%|██████████████████████████████████████████████████████████████████████| 72/72 [04:20<00:00,  3.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 Validation Scores: ROUGE: {'rouge1': 0.4636192739399708, 'rouge2': 0.22471876410679206, 'rougeL': 0.35089156956351014, 'rougeLsum': 0.35089156956351014}, BERTScore: {'precision': 0.3500059539866116, 'recall': 0.28499961235663956, 'f1': 0.3175645141551892}\n",
      "11\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11:   0%|                                                                             | 0/334 [00:00<?, ?batch/s]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Epoch 11: 100%|████████████████████████████████████████████████████████| 334/334 [01:41<00:00,  3.29batch/s, loss=0.52]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5195, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|█████████████████████████████████████████████████████████| 72/72 [00:09<00:00,  7.35batch/s, loss=0.937]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9374, device='cuda:0')\n",
      "train_loss = 0.660680072214789\n",
      " test_loss = 1.2677430742316775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:   0%|                                                                               | 0/72 [00:00<?, ?it/s]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:   1%|▉                                                                      | 1/72 [00:03<03:46,  3.19s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:   3%|█▉                                                                     | 2/72 [00:06<03:54,  3.36s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:   4%|██▉                                                                    | 3/72 [00:10<03:58,  3.46s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:   6%|███▉                                                                   | 4/72 [00:14<04:10,  3.69s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:   7%|████▉                                                                  | 5/72 [00:17<03:53,  3.49s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:   8%|█████▉                                                                 | 6/72 [00:21<04:00,  3.64s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  10%|██████▉                                                                | 7/72 [00:24<03:53,  3.59s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  11%|███████▉                                                               | 8/72 [00:28<03:55,  3.69s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  12%|████████▉                                                              | 9/72 [00:32<03:54,  3.72s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  14%|█████████▋                                                            | 10/72 [00:36<04:02,  3.90s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  15%|██████████▋                                                           | 11/72 [00:40<03:48,  3.75s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  17%|███████████▋                                                          | 12/72 [00:44<03:47,  3.80s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  18%|████████████▋                                                         | 13/72 [00:47<03:38,  3.71s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  19%|█████████████▌                                                        | 14/72 [00:50<03:27,  3.58s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  21%|██████████████▌                                                       | 15/72 [00:54<03:23,  3.56s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  22%|███████████████▌                                                      | 16/72 [00:57<03:14,  3.47s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  24%|████████████████▌                                                     | 17/72 [01:01<03:09,  3.44s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  25%|█████████████████▌                                                    | 18/72 [01:05<03:15,  3.63s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  26%|██████████████████▍                                                   | 19/72 [01:09<03:16,  3.71s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  28%|███████████████████▍                                                  | 20/72 [01:12<03:08,  3.63s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  29%|████████████████████▍                                                 | 21/72 [01:16<03:09,  3.71s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  31%|█████████████████████▍                                                | 22/72 [01:20<03:13,  3.87s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  32%|██████████████████████▎                                               | 23/72 [01:24<03:04,  3.77s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  33%|███████████████████████▎                                              | 24/72 [01:27<02:54,  3.63s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  35%|████████████████████████▎                                             | 25/72 [01:32<03:10,  4.06s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  36%|█████████████████████████▎                                            | 26/72 [01:36<03:01,  3.94s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  38%|██████████████████████████▎                                           | 27/72 [01:40<02:56,  3.93s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  39%|███████████████████████████▏                                          | 28/72 [01:44<02:54,  3.97s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  40%|████████████████████████████▏                                         | 29/72 [01:47<02:45,  3.85s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  42%|█████████████████████████████▏                                        | 30/72 [01:51<02:38,  3.77s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  43%|██████████████████████████████▏                                       | 31/72 [01:54<02:33,  3.75s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  44%|███████████████████████████████                                       | 32/72 [01:58<02:25,  3.64s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  46%|████████████████████████████████                                      | 33/72 [02:02<02:28,  3.81s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  47%|█████████████████████████████████                                     | 34/72 [02:05<02:16,  3.60s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  49%|██████████████████████████████████                                    | 35/72 [02:09<02:09,  3.51s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  50%|███████████████████████████████████                                   | 36/72 [02:12<02:06,  3.53s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  51%|███████████████████████████████████▉                                  | 37/72 [02:16<02:03,  3.53s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  53%|████████████████████████████████████▉                                 | 38/72 [02:20<02:05,  3.69s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  54%|█████████████████████████████████████▉                                | 39/72 [02:23<01:57,  3.56s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  56%|██████████████████████████████████████▉                               | 40/72 [02:27<01:57,  3.66s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  57%|███████████████████████████████████████▊                              | 41/72 [02:30<01:53,  3.66s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  58%|████████████████████████████████████████▊                             | 42/72 [02:34<01:52,  3.76s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  60%|█████████████████████████████████████████▊                            | 43/72 [02:38<01:43,  3.56s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  61%|██████████████████████████████████████████▊                           | 44/72 [02:41<01:40,  3.60s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  62%|███████████████████████████████████████████▊                          | 45/72 [02:45<01:36,  3.58s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  64%|████████████████████████████████████████████▋                         | 46/72 [02:49<01:36,  3.72s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  65%|█████████████████████████████████████████████▋                        | 47/72 [02:52<01:29,  3.60s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  67%|██████████████████████████████████████████████▋                       | 48/72 [02:57<01:35,  3.98s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  68%|███████████████████████████████████████████████▋                      | 49/72 [03:01<01:32,  4.02s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  69%|████████████████████████████████████████████████▌                     | 50/72 [03:04<01:22,  3.77s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  71%|█████████████████████████████████████████████████▌                    | 51/72 [03:08<01:17,  3.69s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  72%|██████████████████████████████████████████████████▌                   | 52/72 [03:11<01:11,  3.58s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  74%|███████████████████████████████████████████████████▌                  | 53/72 [03:15<01:08,  3.59s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  75%|████████████████████████████████████████████████████▌                 | 54/72 [03:18<01:01,  3.42s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  76%|█████████████████████████████████████████████████████▍                | 55/72 [03:21<00:58,  3.42s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  78%|██████████████████████████████████████████████████████▍               | 56/72 [03:25<00:55,  3.49s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  79%|███████████████████████████████████████████████████████▍              | 57/72 [03:29<00:55,  3.71s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  81%|████████████████████████████████████████████████████████▍             | 58/72 [03:33<00:54,  3.91s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  82%|█████████████████████████████████████████████████████████▎            | 59/72 [03:38<00:52,  4.04s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  83%|██████████████████████████████████████████████████████████▎           | 60/72 [03:42<00:49,  4.09s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  85%|███████████████████████████████████████████████████████████▎          | 61/72 [03:46<00:44,  4.06s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  86%|████████████████████████████████████████████████████████████▎         | 62/72 [03:49<00:38,  3.83s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  88%|█████████████████████████████████████████████████████████████▎        | 63/72 [03:53<00:34,  3.81s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  89%|██████████████████████████████████████████████████████████████▏       | 64/72 [03:56<00:29,  3.67s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  90%|███████████████████████████████████████████████████████████████▏      | 65/72 [04:00<00:26,  3.74s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  92%|████████████████████████████████████████████████████████████████▏     | 66/72 [04:03<00:21,  3.55s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  93%|█████████████████████████████████████████████████████████████████▏    | 67/72 [04:07<00:17,  3.54s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  94%|██████████████████████████████████████████████████████████████████    | 68/72 [04:11<00:14,  3.68s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  96%|███████████████████████████████████████████████████████████████████   | 69/72 [04:15<00:11,  3.76s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  97%|████████████████████████████████████████████████████████████████████  | 70/72 [04:18<00:07,  3.61s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  99%|█████████████████████████████████████████████████████████████████████ | 71/72 [04:21<00:03,  3.47s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating: 100%|██████████████████████████████████████████████████████████████████████| 72/72 [04:24<00:00,  3.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 Validation Scores: ROUGE: {'rouge1': 0.4566569637452758, 'rouge2': 0.2192315941544441, 'rougeL': 0.34697098711108926, 'rougeLsum': 0.34697098711108926}, BERTScore: {'precision': 0.35108539793226456, 'recall': 0.2845852891397145, 'f1': 0.31758918188926244}\n",
      "12\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12:   0%|                                                                             | 0/334 [00:00<?, ?batch/s]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Epoch 12: 100%|███████████████████████████████████████████████████████| 334/334 [01:41<00:00,  3.29batch/s, loss=0.722]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7220, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|█████████████████████████████████████████████████████████| 72/72 [00:09<00:00,  7.40batch/s, loss=0.965]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9646, device='cuda:0')\n",
      "train_loss = 0.6078674363161989\n",
      " test_loss = 1.3225918722649415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:   0%|                                                                               | 0/72 [00:00<?, ?it/s]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:   1%|▉                                                                      | 1/72 [00:04<05:04,  4.29s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:   3%|█▉                                                                     | 2/72 [00:07<04:27,  3.82s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:   4%|██▉                                                                    | 3/72 [00:12<04:36,  4.01s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:   6%|███▉                                                                   | 4/72 [00:15<04:12,  3.72s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:   7%|████▉                                                                  | 5/72 [00:19<04:11,  3.75s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:   8%|█████▉                                                                 | 6/72 [00:23<04:27,  4.06s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  10%|██████▉                                                                | 7/72 [00:27<04:07,  3.80s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  11%|███████▉                                                               | 8/72 [00:31<04:14,  3.97s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  12%|████████▉                                                              | 9/72 [00:34<03:57,  3.77s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  14%|█████████▋                                                            | 10/72 [00:38<04:03,  3.92s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  15%|██████████▋                                                           | 11/72 [00:42<03:46,  3.71s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  17%|███████████▋                                                          | 12/72 [00:46<03:45,  3.76s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  18%|████████████▋                                                         | 13/72 [00:49<03:29,  3.56s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  19%|█████████████▌                                                        | 14/72 [00:52<03:18,  3.43s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  21%|██████████████▌                                                       | 15/72 [00:55<03:14,  3.42s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  22%|███████████████▌                                                      | 16/72 [00:58<03:09,  3.38s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  24%|████████████████▌                                                     | 17/72 [01:02<03:04,  3.35s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  25%|█████████████████▌                                                    | 18/72 [01:06<03:08,  3.49s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  26%|██████████████████▍                                                   | 19/72 [01:09<03:03,  3.46s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  28%|███████████████████▍                                                  | 20/72 [01:12<02:58,  3.44s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  29%|████████████████████▍                                                 | 21/72 [01:17<03:08,  3.70s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  31%|█████████████████████▍                                                | 22/72 [01:21<03:18,  3.96s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  32%|██████████████████████▎                                               | 23/72 [01:25<03:06,  3.81s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  33%|███████████████████████▎                                              | 24/72 [01:28<03:00,  3.77s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  35%|████████████████████████▎                                             | 25/72 [01:33<03:14,  4.14s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  36%|█████████████████████████▎                                            | 26/72 [01:37<02:58,  3.89s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  38%|██████████████████████████▎                                           | 27/72 [01:41<02:55,  3.90s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  39%|███████████████████████████▏                                          | 28/72 [01:44<02:42,  3.69s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  40%|████████████████████████████▏                                         | 29/72 [01:47<02:35,  3.61s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  42%|█████████████████████████████▏                                        | 30/72 [01:51<02:29,  3.56s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  43%|██████████████████████████████▏                                       | 31/72 [01:55<02:31,  3.71s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  44%|███████████████████████████████                                       | 32/72 [01:58<02:26,  3.66s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  46%|████████████████████████████████                                      | 33/72 [02:02<02:26,  3.75s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  47%|█████████████████████████████████                                     | 34/72 [02:05<02:15,  3.56s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  49%|██████████████████████████████████                                    | 35/72 [02:09<02:11,  3.57s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  50%|███████████████████████████████████                                   | 36/72 [02:13<02:15,  3.76s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  51%|███████████████████████████████████▉                                  | 37/72 [02:17<02:08,  3.68s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  53%|████████████████████████████████████▉                                 | 38/72 [02:21<02:07,  3.76s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  54%|█████████████████████████████████████▉                                | 39/72 [02:24<01:59,  3.61s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  56%|██████████████████████████████████████▉                               | 40/72 [02:28<01:58,  3.69s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  57%|███████████████████████████████████████▊                              | 41/72 [02:31<01:51,  3.58s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  58%|████████████████████████████████████████▊                             | 42/72 [02:35<01:51,  3.70s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  60%|█████████████████████████████████████████▊                            | 43/72 [02:38<01:42,  3.54s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  61%|██████████████████████████████████████████▊                           | 44/72 [02:42<01:41,  3.63s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  62%|███████████████████████████████████████████▊                          | 45/72 [02:45<01:35,  3.52s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  64%|████████████████████████████████████████████▋                         | 46/72 [02:49<01:31,  3.54s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  65%|█████████████████████████████████████████████▋                        | 47/72 [02:52<01:24,  3.39s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  67%|██████████████████████████████████████████████▋                       | 48/72 [02:56<01:26,  3.59s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  68%|███████████████████████████████████████████████▋                      | 49/72 [03:00<01:23,  3.63s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  69%|████████████████████████████████████████████████▌                     | 50/72 [03:03<01:17,  3.51s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  71%|█████████████████████████████████████████████████▌                    | 51/72 [03:07<01:17,  3.69s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  72%|██████████████████████████████████████████████████▌                   | 52/72 [03:11<01:17,  3.85s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  74%|███████████████████████████████████████████████████▌                  | 53/72 [03:15<01:13,  3.84s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  75%|████████████████████████████████████████████████████▌                 | 54/72 [03:18<01:06,  3.68s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  76%|█████████████████████████████████████████████████████▍                | 55/72 [03:22<01:03,  3.71s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  78%|██████████████████████████████████████████████████████▍               | 56/72 [03:26<00:59,  3.72s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  79%|███████████████████████████████████████████████████████▍              | 57/72 [03:30<00:55,  3.72s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  81%|████████████████████████████████████████████████████████▍             | 58/72 [03:34<00:54,  3.92s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  82%|█████████████████████████████████████████████████████████▎            | 59/72 [03:38<00:50,  3.88s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  83%|██████████████████████████████████████████████████████████▎           | 60/72 [03:42<00:47,  3.97s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  85%|███████████████████████████████████████████████████████████▎          | 61/72 [03:46<00:42,  3.84s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  86%|████████████████████████████████████████████████████████████▎         | 62/72 [03:49<00:36,  3.63s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  88%|█████████████████████████████████████████████████████████████▎        | 63/72 [03:52<00:32,  3.63s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  89%|██████████████████████████████████████████████████████████████▏       | 64/72 [03:56<00:29,  3.66s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  90%|███████████████████████████████████████████████████████████████▏      | 65/72 [04:00<00:26,  3.73s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  92%|████████████████████████████████████████████████████████████████▏     | 66/72 [04:03<00:21,  3.59s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  93%|█████████████████████████████████████████████████████████████████▏    | 67/72 [04:07<00:17,  3.53s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  94%|██████████████████████████████████████████████████████████████████    | 68/72 [04:11<00:14,  3.74s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  96%|███████████████████████████████████████████████████████████████████   | 69/72 [04:15<00:11,  3.76s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  97%|████████████████████████████████████████████████████████████████████  | 70/72 [04:18<00:07,  3.52s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  99%|█████████████████████████████████████████████████████████████████████ | 71/72 [04:21<00:03,  3.49s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating: 100%|██████████████████████████████████████████████████████████████████████| 72/72 [04:24<00:00,  3.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 Validation Scores: ROUGE: {'rouge1': 0.4520689755334895, 'rouge2': 0.2183389265891289, 'rougeL': 0.3387468123983579, 'rougeLsum': 0.3387468123983579}, BERTScore: {'precision': 0.3374437134609454, 'recall': 0.2806173694940905, 'f1': 0.3090001365247493}\n",
      "13\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13:   0%|                                                                             | 0/334 [00:00<?, ?batch/s]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Epoch 13: 100%|████████████████████████████████████████████████████████| 334/334 [01:41<00:00,  3.28batch/s, loss=0.63]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6303, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|██████████████████████████████████████████████████████████| 72/72 [00:08<00:00,  8.95batch/s, loss=1.01]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0073, device='cuda:0')\n",
      "train_loss = 0.5679932041707153\n",
      " test_loss = 1.3418765275014772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:   0%|                                                                               | 0/72 [00:00<?, ?it/s]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:   1%|▉                                                                      | 1/72 [00:03<04:14,  3.59s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:   3%|█▉                                                                     | 2/72 [00:07<04:08,  3.55s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:   4%|██▉                                                                    | 3/72 [00:11<04:18,  3.74s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:   6%|███▉                                                                   | 4/72 [00:15<04:25,  3.90s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:   7%|████▉                                                                  | 5/72 [00:18<04:12,  3.76s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:   8%|█████▉                                                                 | 6/72 [00:23<04:33,  4.15s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  10%|██████▉                                                                | 7/72 [00:26<04:08,  3.82s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  11%|███████▉                                                               | 8/72 [00:30<03:54,  3.66s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  12%|████████▉                                                              | 9/72 [00:33<03:47,  3.62s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  14%|█████████▋                                                            | 10/72 [00:36<03:35,  3.48s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  15%|██████████▋                                                           | 11/72 [00:40<03:32,  3.49s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  17%|███████████▋                                                          | 12/72 [00:44<03:33,  3.55s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  18%|████████████▋                                                         | 13/72 [00:47<03:27,  3.52s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  19%|█████████████▌                                                        | 14/72 [00:50<03:18,  3.43s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  21%|██████████████▌                                                       | 15/72 [00:53<03:12,  3.38s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  22%|███████████████▌                                                      | 16/72 [00:57<03:08,  3.37s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  24%|████████████████▌                                                     | 17/72 [01:00<03:05,  3.38s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  25%|█████████████████▌                                                    | 18/72 [01:04<03:09,  3.51s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  26%|██████████████████▍                                                   | 19/72 [01:08<03:06,  3.51s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  28%|███████████████████▍                                                  | 20/72 [01:11<03:03,  3.53s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  29%|████████████████████▍                                                 | 21/72 [01:15<02:59,  3.52s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  31%|█████████████████████▍                                                | 22/72 [01:19<03:03,  3.68s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  32%|██████████████████████▎                                               | 23/72 [01:22<02:57,  3.63s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  33%|███████████████████████▎                                              | 24/72 [01:26<02:50,  3.55s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  35%|████████████████████████▎                                             | 25/72 [01:29<02:51,  3.64s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  36%|█████████████████████████▎                                            | 26/72 [01:32<02:40,  3.48s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  38%|██████████████████████████▎                                           | 27/72 [01:36<02:41,  3.60s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  39%|███████████████████████████▏                                          | 28/72 [01:40<02:45,  3.76s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  40%|████████████████████████████▏                                         | 29/72 [01:44<02:34,  3.59s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  42%|█████████████████████████████▏                                        | 30/72 [01:47<02:29,  3.55s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  43%|██████████████████████████████▏                                       | 31/72 [01:51<02:24,  3.52s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  44%|███████████████████████████████                                       | 32/72 [01:54<02:21,  3.54s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  46%|████████████████████████████████                                      | 33/72 [01:58<02:23,  3.69s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  47%|█████████████████████████████████                                     | 34/72 [02:01<02:14,  3.55s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  49%|██████████████████████████████████                                    | 35/72 [02:05<02:06,  3.42s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  50%|███████████████████████████████████                                   | 36/72 [02:08<02:08,  3.56s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  51%|███████████████████████████████████▉                                  | 37/72 [02:12<02:04,  3.55s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  53%|████████████████████████████████████▉                                 | 38/72 [02:16<02:03,  3.64s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  54%|█████████████████████████████████████▉                                | 39/72 [02:19<01:57,  3.56s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  56%|██████████████████████████████████████▉                               | 40/72 [02:23<01:55,  3.62s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  57%|███████████████████████████████████████▊                              | 41/72 [02:26<01:47,  3.47s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  58%|████████████████████████████████████████▊                             | 42/72 [02:30<01:45,  3.51s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  60%|█████████████████████████████████████████▊                            | 43/72 [02:33<01:42,  3.52s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  61%|██████████████████████████████████████████▊                           | 44/72 [02:37<01:38,  3.53s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  62%|███████████████████████████████████████████▊                          | 45/72 [02:40<01:34,  3.50s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  64%|████████████████████████████████████████████▋                         | 46/72 [02:44<01:31,  3.52s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  65%|█████████████████████████████████████████████▋                        | 47/72 [02:47<01:26,  3.45s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  67%|██████████████████████████████████████████████▋                       | 48/72 [02:51<01:22,  3.45s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  68%|███████████████████████████████████████████████▋                      | 49/72 [02:54<01:21,  3.55s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  69%|████████████████████████████████████████████████▌                     | 50/72 [02:58<01:16,  3.48s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  71%|█████████████████████████████████████████████████▌                    | 51/72 [03:01<01:15,  3.58s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  72%|██████████████████████████████████████████████████▌                   | 52/72 [03:06<01:14,  3.74s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  74%|███████████████████████████████████████████████████▌                  | 53/72 [03:10<01:13,  3.89s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  75%|████████████████████████████████████████████████████▌                 | 54/72 [03:13<01:06,  3.69s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  76%|█████████████████████████████████████████████████████▍                | 55/72 [03:17<01:02,  3.65s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  78%|██████████████████████████████████████████████████████▍               | 56/72 [03:20<00:58,  3.68s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  79%|███████████████████████████████████████████████████████▍              | 57/72 [03:24<00:55,  3.68s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  81%|████████████████████████████████████████████████████████▍             | 58/72 [03:28<00:52,  3.72s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  82%|█████████████████████████████████████████████████████████▎            | 59/72 [03:32<00:48,  3.73s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  83%|██████████████████████████████████████████████████████████▎           | 60/72 [03:35<00:44,  3.69s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  85%|███████████████████████████████████████████████████████████▎          | 61/72 [03:39<00:40,  3.66s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  86%|████████████████████████████████████████████████████████████▎         | 62/72 [03:42<00:35,  3.57s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  88%|█████████████████████████████████████████████████████████████▎        | 63/72 [03:45<00:31,  3.49s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  89%|██████████████████████████████████████████████████████████████▏       | 64/72 [03:49<00:28,  3.53s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  90%|███████████████████████████████████████████████████████████████▏      | 65/72 [03:53<00:25,  3.64s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  92%|████████████████████████████████████████████████████████████████▏     | 66/72 [03:56<00:21,  3.53s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  93%|█████████████████████████████████████████████████████████████████▏    | 67/72 [03:59<00:17,  3.44s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  94%|██████████████████████████████████████████████████████████████████    | 68/72 [04:03<00:14,  3.54s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  96%|███████████████████████████████████████████████████████████████████   | 69/72 [04:07<00:10,  3.66s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  97%|████████████████████████████████████████████████████████████████████  | 70/72 [04:10<00:07,  3.52s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  99%|█████████████████████████████████████████████████████████████████████ | 71/72 [04:14<00:03,  3.48s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating: 100%|██████████████████████████████████████████████████████████████████████| 72/72 [04:17<00:00,  3.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 Validation Scores: ROUGE: {'rouge1': 0.45757021949622956, 'rouge2': 0.21986755816135567, 'rougeL': 0.34507169250986913, 'rougeLsum': 0.34507169250986913}, BERTScore: {'precision': 0.3412000018482407, 'recall': 0.284013704293304, 'f1': 0.3125927331857383}\n",
      "14\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14:   0%|                                                                             | 0/334 [00:00<?, ?batch/s]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Epoch 14: 100%|███████████████████████████████████████████████████████| 334/334 [01:42<00:00,  3.27batch/s, loss=0.349]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3494, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████████████████████████████████████████████████████| 72/72 [00:08<00:00,  8.92batch/s, loss=1.01]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0099, device='cuda:0')\n",
      "train_loss = 0.5236461202273825\n",
      " test_loss = 1.3832195388774078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:   0%|                                                                               | 0/72 [00:00<?, ?it/s]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:   1%|▉                                                                      | 1/72 [00:03<04:15,  3.60s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:   3%|█▉                                                                     | 2/72 [00:07<04:16,  3.66s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:   4%|██▉                                                                    | 3/72 [00:11<04:22,  3.81s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:   6%|███▉                                                                   | 4/72 [00:14<04:09,  3.67s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:   7%|████▉                                                                  | 5/72 [00:17<03:52,  3.47s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:   8%|█████▉                                                                 | 6/72 [00:21<03:50,  3.49s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  10%|██████▉                                                                | 7/72 [00:24<03:37,  3.34s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  11%|███████▉                                                               | 8/72 [00:27<03:30,  3.29s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  12%|████████▉                                                              | 9/72 [00:30<03:24,  3.25s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  14%|█████████▋                                                            | 10/72 [00:34<03:32,  3.42s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  15%|██████████▋                                                           | 11/72 [00:37<03:24,  3.35s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  17%|███████████▋                                                          | 12/72 [00:41<03:30,  3.51s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  18%|████████████▋                                                         | 13/72 [00:44<03:21,  3.41s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  19%|█████████████▌                                                        | 14/72 [00:47<03:13,  3.33s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  21%|██████████████▌                                                       | 15/72 [00:52<03:23,  3.57s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  22%|███████████████▌                                                      | 16/72 [00:55<03:16,  3.50s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  24%|████████████████▌                                                     | 17/72 [00:59<03:21,  3.66s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  25%|█████████████████▌                                                    | 18/72 [01:04<03:33,  3.96s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  26%|██████████████████▍                                                   | 19/72 [01:09<03:52,  4.39s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  28%|███████████████████▍                                                  | 20/72 [01:13<03:37,  4.18s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  29%|████████████████████▍                                                 | 21/72 [01:16<03:21,  3.94s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  31%|█████████████████████▍                                                | 22/72 [01:20<03:14,  3.88s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  32%|██████████████████████▎                                               | 23/72 [01:23<03:05,  3.78s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  33%|███████████████████████▎                                              | 24/72 [01:27<02:57,  3.70s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  35%|████████████████████████▎                                             | 25/72 [01:31<02:57,  3.77s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  36%|█████████████████████████▎                                            | 26/72 [01:34<02:45,  3.59s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  38%|██████████████████████████▎                                           | 27/72 [01:38<02:46,  3.70s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  39%|███████████████████████████▏                                          | 28/72 [01:41<02:40,  3.64s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  40%|████████████████████████████▏                                         | 29/72 [01:45<02:35,  3.61s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  42%|█████████████████████████████▏                                        | 30/72 [01:49<02:38,  3.78s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  43%|██████████████████████████████▏                                       | 31/72 [01:53<02:37,  3.83s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  44%|███████████████████████████████                                       | 32/72 [01:57<02:28,  3.71s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  46%|████████████████████████████████                                      | 33/72 [02:01<02:30,  3.86s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  47%|█████████████████████████████████                                     | 34/72 [02:04<02:18,  3.66s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  49%|██████████████████████████████████                                    | 35/72 [02:08<02:14,  3.64s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  50%|███████████████████████████████████                                   | 36/72 [02:11<02:13,  3.71s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  51%|███████████████████████████████████▉                                  | 37/72 [02:15<02:10,  3.73s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  53%|████████████████████████████████████▉                                 | 38/72 [02:19<02:08,  3.78s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  54%|█████████████████████████████████████▉                                | 39/72 [02:22<01:58,  3.59s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  56%|██████████████████████████████████████▉                               | 40/72 [02:26<01:53,  3.54s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  57%|███████████████████████████████████████▊                              | 41/72 [02:29<01:46,  3.44s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  58%|████████████████████████████████████████▊                             | 42/72 [02:33<01:45,  3.51s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  60%|█████████████████████████████████████████▊                            | 43/72 [02:36<01:41,  3.49s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  61%|██████████████████████████████████████████▊                           | 44/72 [02:39<01:37,  3.48s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  62%|███████████████████████████████████████████▊                          | 45/72 [02:43<01:31,  3.39s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  64%|████████████████████████████████████████████▋                         | 46/72 [02:47<01:37,  3.73s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  65%|█████████████████████████████████████████████▋                        | 47/72 [02:51<01:31,  3.65s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  67%|██████████████████████████████████████████████▋                       | 48/72 [02:54<01:26,  3.62s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  68%|███████████████████████████████████████████████▋                      | 49/72 [02:58<01:23,  3.64s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  69%|████████████████████████████████████████████████▌                     | 50/72 [03:01<01:18,  3.57s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  71%|█████████████████████████████████████████████████▌                    | 51/72 [03:05<01:15,  3.58s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  72%|██████████████████████████████████████████████████▌                   | 52/72 [03:09<01:12,  3.63s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  74%|███████████████████████████████████████████████████▌                  | 53/72 [03:12<01:09,  3.66s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  75%|████████████████████████████████████████████████████▌                 | 54/72 [03:16<01:03,  3.55s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  76%|█████████████████████████████████████████████████████▍                | 55/72 [03:19<00:59,  3.49s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  78%|██████████████████████████████████████████████████████▍               | 56/72 [03:22<00:55,  3.49s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  79%|███████████████████████████████████████████████████████▍              | 57/72 [03:26<00:52,  3.47s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  81%|████████████████████████████████████████████████████████▍             | 58/72 [03:30<00:50,  3.60s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  82%|█████████████████████████████████████████████████████████▎            | 59/72 [03:34<00:47,  3.64s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  83%|██████████████████████████████████████████████████████████▎           | 60/72 [03:37<00:43,  3.63s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  85%|███████████████████████████████████████████████████████████▎          | 61/72 [03:41<00:40,  3.68s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  86%|████████████████████████████████████████████████████████████▎         | 62/72 [03:44<00:35,  3.60s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  88%|█████████████████████████████████████████████████████████████▎        | 63/72 [03:48<00:32,  3.64s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  89%|██████████████████████████████████████████████████████████████▏       | 64/72 [03:52<00:28,  3.60s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  90%|███████████████████████████████████████████████████████████████▏      | 65/72 [03:56<00:26,  3.74s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  92%|████████████████████████████████████████████████████████████████▏     | 66/72 [03:59<00:21,  3.59s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  93%|█████████████████████████████████████████████████████████████████▏    | 67/72 [04:02<00:17,  3.50s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  94%|██████████████████████████████████████████████████████████████████    | 68/72 [04:06<00:14,  3.62s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  96%|███████████████████████████████████████████████████████████████████   | 69/72 [04:10<00:10,  3.66s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  97%|████████████████████████████████████████████████████████████████████  | 70/72 [04:13<00:06,  3.50s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  99%|█████████████████████████████████████████████████████████████████████ | 71/72 [04:16<00:03,  3.50s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating: 100%|██████████████████████████████████████████████████████████████████████| 72/72 [04:20<00:00,  3.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 Validation Scores: ROUGE: {'rouge1': 0.45848486994699855, 'rouge2': 0.21857534642160154, 'rougeL': 0.3416526481673319, 'rougeLsum': 0.3416526481673319}, BERTScore: {'precision': 0.3458857624274161, 'recall': 0.2924213786092069, 'f1': 0.3192035724253704}\n",
      "15\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15:   0%|                                                                             | 0/334 [00:00<?, ?batch/s]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Epoch 15: 100%|███████████████████████████████████████████████████████| 334/334 [01:41<00:00,  3.28batch/s, loss=0.448]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4485, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|██████████████████████████████████████████████████████████| 72/72 [00:07<00:00,  9.09batch/s, loss=1.04]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0433, device='cuda:0')\n",
      "train_loss = 0.47726772551586527\n",
      " test_loss = 1.42598906904459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:   0%|                                                                               | 0/72 [00:00<?, ?it/s]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:   1%|▉                                                                      | 1/72 [00:03<04:36,  3.90s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:   3%|█▉                                                                     | 2/72 [00:07<04:11,  3.60s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:   4%|██▉                                                                    | 3/72 [00:10<04:08,  3.61s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:   6%|███▉                                                                   | 4/72 [00:14<04:15,  3.75s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:   7%|████▉                                                                  | 5/72 [00:17<03:54,  3.50s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:   8%|█████▉                                                                 | 6/72 [00:21<03:44,  3.40s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  10%|██████▉                                                                | 7/72 [00:24<03:38,  3.37s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  11%|███████▉                                                               | 8/72 [00:27<03:34,  3.36s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  12%|████████▉                                                              | 9/72 [00:31<03:29,  3.33s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  14%|█████████▋                                                            | 10/72 [00:34<03:32,  3.42s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  15%|██████████▋                                                           | 11/72 [00:38<03:27,  3.40s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  17%|███████████▋                                                          | 12/72 [00:42<03:35,  3.59s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  18%|████████████▋                                                         | 13/72 [00:45<03:34,  3.64s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  19%|█████████████▌                                                        | 14/72 [00:49<03:28,  3.59s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  21%|██████████████▌                                                       | 15/72 [00:53<03:40,  3.87s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  22%|███████████████▌                                                      | 16/72 [00:57<03:31,  3.79s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  24%|████████████████▌                                                     | 17/72 [01:02<03:46,  4.12s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  25%|█████████████████▌                                                    | 18/72 [01:06<03:38,  4.05s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  26%|██████████████████▍                                                   | 19/72 [01:10<03:32,  4.01s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  28%|███████████████████▍                                                  | 20/72 [01:13<03:17,  3.81s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  29%|████████████████████▍                                                 | 21/72 [01:17<03:13,  3.79s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  31%|█████████████████████▍                                                | 22/72 [01:20<03:09,  3.79s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  32%|██████████████████████▎                                               | 23/72 [01:24<02:56,  3.60s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  33%|███████████████████████▎                                              | 24/72 [01:27<02:51,  3.58s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  35%|████████████████████████▎                                             | 25/72 [01:31<02:56,  3.75s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  36%|█████████████████████████▎                                            | 26/72 [01:34<02:44,  3.58s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  38%|██████████████████████████▎                                           | 27/72 [01:38<02:46,  3.70s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  39%|███████████████████████████▏                                          | 28/72 [01:42<02:39,  3.61s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  40%|████████████████████████████▏                                         | 29/72 [01:45<02:31,  3.52s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  42%|█████████████████████████████▏                                        | 30/72 [01:49<02:32,  3.62s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  43%|██████████████████████████████▏                                       | 31/72 [01:53<02:34,  3.77s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  44%|███████████████████████████████                                       | 32/72 [01:57<02:25,  3.64s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  46%|████████████████████████████████                                      | 33/72 [02:00<02:22,  3.66s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  47%|█████████████████████████████████                                     | 34/72 [02:04<02:15,  3.57s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  49%|██████████████████████████████████                                    | 35/72 [02:07<02:07,  3.46s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  50%|███████████████████████████████████                                   | 36/72 [02:11<02:10,  3.63s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  51%|███████████████████████████████████▉                                  | 37/72 [02:14<02:05,  3.58s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  53%|████████████████████████████████████▉                                 | 38/72 [02:18<02:03,  3.65s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  54%|█████████████████████████████████████▉                                | 39/72 [02:21<01:56,  3.54s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  56%|██████████████████████████████████████▉                               | 40/72 [02:25<01:56,  3.64s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  57%|███████████████████████████████████████▊                              | 41/72 [02:29<01:57,  3.78s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  58%|████████████████████████████████████████▊                             | 42/72 [02:33<01:49,  3.65s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  60%|█████████████████████████████████████████▊                            | 43/72 [02:36<01:44,  3.60s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  61%|██████████████████████████████████████████▊                           | 44/72 [02:40<01:40,  3.60s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  62%|███████████████████████████████████████████▊                          | 45/72 [02:44<01:40,  3.72s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  64%|████████████████████████████████████████████▋                         | 46/72 [02:48<01:38,  3.78s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  65%|█████████████████████████████████████████████▋                        | 47/72 [02:51<01:33,  3.75s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  67%|██████████████████████████████████████████████▋                       | 48/72 [02:55<01:29,  3.74s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  68%|███████████████████████████████████████████████▋                      | 49/72 [02:59<01:28,  3.86s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  69%|████████████████████████████████████████████████▌                     | 50/72 [03:03<01:21,  3.70s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  71%|█████████████████████████████████████████████████▌                    | 51/72 [03:06<01:16,  3.63s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  72%|██████████████████████████████████████████████████▌                   | 52/72 [03:09<01:10,  3.55s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  74%|███████████████████████████████████████████████████▌                  | 53/72 [03:13<01:07,  3.56s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  75%|████████████████████████████████████████████████████▌                 | 54/72 [03:16<01:01,  3.41s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  76%|█████████████████████████████████████████████████████▍                | 55/72 [03:19<00:57,  3.40s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  78%|██████████████████████████████████████████████████████▍               | 56/72 [03:23<00:55,  3.47s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  79%|███████████████████████████████████████████████████████▍              | 57/72 [03:26<00:51,  3.45s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  81%|████████████████████████████████████████████████████████▍             | 58/72 [03:30<00:50,  3.63s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  82%|█████████████████████████████████████████████████████████▎            | 59/72 [03:34<00:46,  3.59s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  83%|██████████████████████████████████████████████████████████▎           | 60/72 [03:38<00:43,  3.63s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  85%|███████████████████████████████████████████████████████████▎          | 61/72 [03:41<00:39,  3.59s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  86%|████████████████████████████████████████████████████████████▎         | 62/72 [03:44<00:34,  3.49s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  88%|█████████████████████████████████████████████████████████████▎        | 63/72 [03:49<00:33,  3.77s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  89%|██████████████████████████████████████████████████████████████▏       | 64/72 [03:52<00:28,  3.62s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  90%|███████████████████████████████████████████████████████████████▏      | 65/72 [03:56<00:26,  3.77s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  92%|████████████████████████████████████████████████████████████████▏     | 66/72 [04:00<00:23,  3.89s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  93%|█████████████████████████████████████████████████████████████████▏    | 67/72 [04:04<00:18,  3.70s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  94%|██████████████████████████████████████████████████████████████████    | 68/72 [04:08<00:14,  3.74s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  96%|███████████████████████████████████████████████████████████████████   | 69/72 [04:11<00:11,  3.78s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  97%|████████████████████████████████████████████████████████████████████  | 70/72 [04:14<00:07,  3.54s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  99%|█████████████████████████████████████████████████████████████████████ | 71/72 [04:18<00:03,  3.52s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating: 100%|██████████████████████████████████████████████████████████████████████| 72/72 [04:21<00:00,  3.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 Validation Scores: ROUGE: {'rouge1': 0.4645681134966335, 'rouge2': 0.22390592996146783, 'rougeL': 0.34688921785318216, 'rougeLsum': 0.34688921785318216}, BERTScore: {'precision': 0.3488570442940626, 'recall': 0.3097191255332695, 'f1': 0.32946117636230254}\n",
      "16\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16:   0%|                                                                             | 0/334 [00:00<?, ?batch/s]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Epoch 16: 100%|███████████████████████████████████████████████████████| 334/334 [01:41<00:00,  3.28batch/s, loss=0.557]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5567, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|██████████████████████████████████████████████████████████| 72/72 [00:08<00:00,  8.81batch/s, loss=1.11]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.1090, device='cuda:0')\n",
      "train_loss = 0.43711181518114256\n",
      " test_loss = 1.4816977547274695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:   0%|                                                                               | 0/72 [00:00<?, ?it/s]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:   1%|▉                                                                      | 1/72 [00:03<04:00,  3.39s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:   3%|█▉                                                                     | 2/72 [00:07<04:10,  3.57s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:   4%|██▉                                                                    | 3/72 [00:10<04:07,  3.58s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:   6%|███▉                                                                   | 4/72 [00:14<04:11,  3.70s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:   7%|████▉                                                                  | 5/72 [00:17<03:52,  3.46s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:   8%|█████▉                                                                 | 6/72 [00:20<03:42,  3.36s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  10%|██████▉                                                                | 7/72 [00:24<03:40,  3.39s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  11%|███████▉                                                               | 8/72 [00:27<03:43,  3.50s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  12%|████████▉                                                              | 9/72 [00:31<03:33,  3.39s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  14%|█████████▋                                                            | 10/72 [00:34<03:39,  3.54s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  15%|██████████▋                                                           | 11/72 [00:38<03:37,  3.56s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  17%|███████████▋                                                          | 12/72 [00:42<03:43,  3.73s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  18%|████████████▋                                                         | 13/72 [00:46<03:39,  3.72s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  19%|█████████████▌                                                        | 14/72 [00:49<03:32,  3.66s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  21%|██████████████▌                                                       | 15/72 [00:53<03:25,  3.61s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  22%|███████████████▌                                                      | 16/72 [00:57<03:21,  3.60s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  24%|████████████████▌                                                     | 17/72 [01:00<03:19,  3.62s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  25%|█████████████████▌                                                    | 18/72 [01:04<03:25,  3.81s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  26%|██████████████████▍                                                   | 19/72 [01:08<03:14,  3.67s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  28%|███████████████████▍                                                  | 20/72 [01:11<03:07,  3.60s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  29%|████████████████████▍                                                 | 21/72 [01:15<03:04,  3.61s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  31%|█████████████████████▍                                                | 22/72 [01:19<03:03,  3.67s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  32%|██████████████████████▎                                               | 23/72 [01:22<02:52,  3.53s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  33%|███████████████████████▎                                              | 24/72 [01:25<02:49,  3.53s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  35%|████████████████████████▎                                             | 25/72 [01:30<02:55,  3.73s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  36%|█████████████████████████▎                                            | 26/72 [01:33<02:47,  3.63s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  38%|██████████████████████████▎                                           | 27/72 [01:37<02:48,  3.74s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  39%|███████████████████████████▏                                          | 28/72 [01:41<02:44,  3.74s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  40%|████████████████████████████▏                                         | 29/72 [01:44<02:38,  3.68s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  42%|█████████████████████████████▏                                        | 30/72 [01:48<02:32,  3.63s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  43%|██████████████████████████████▏                                       | 31/72 [01:52<02:31,  3.70s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  44%|███████████████████████████████                                       | 32/72 [01:55<02:20,  3.52s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  46%|████████████████████████████████                                      | 33/72 [01:59<02:24,  3.71s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  47%|█████████████████████████████████                                     | 34/72 [02:02<02:13,  3.51s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  49%|██████████████████████████████████                                    | 35/72 [02:05<02:07,  3.46s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  50%|███████████████████████████████████                                   | 36/72 [02:09<02:06,  3.52s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  51%|███████████████████████████████████▉                                  | 37/72 [02:13<02:04,  3.57s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  53%|████████████████████████████████████▉                                 | 38/72 [02:17<02:07,  3.74s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  54%|█████████████████████████████████████▉                                | 39/72 [02:20<01:58,  3.58s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  56%|██████████████████████████████████████▉                               | 40/72 [02:23<01:52,  3.53s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  57%|███████████████████████████████████████▊                              | 41/72 [02:27<01:52,  3.63s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  58%|████████████████████████████████████████▊                             | 42/72 [02:31<01:46,  3.55s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  60%|█████████████████████████████████████████▊                            | 43/72 [02:35<01:50,  3.80s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  61%|██████████████████████████████████████████▊                           | 44/72 [02:39<01:46,  3.79s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  62%|███████████████████████████████████████████▊                          | 45/72 [02:42<01:38,  3.65s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  64%|████████████████████████████████████████████▋                         | 46/72 [02:46<01:38,  3.78s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  65%|█████████████████████████████████████████████▋                        | 47/72 [02:50<01:32,  3.70s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  67%|██████████████████████████████████████████████▋                       | 48/72 [02:53<01:28,  3.69s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  68%|███████████████████████████████████████████████▋                      | 49/72 [02:57<01:26,  3.75s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  69%|████████████████████████████████████████████████▌                     | 50/72 [03:01<01:24,  3.84s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  71%|█████████████████████████████████████████████████▌                    | 51/72 [03:05<01:19,  3.80s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  72%|██████████████████████████████████████████████████▌                   | 52/72 [03:09<01:15,  3.78s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  74%|███████████████████████████████████████████████████▌                  | 53/72 [03:12<01:09,  3.68s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  75%|████████████████████████████████████████████████████▌                 | 54/72 [03:15<01:02,  3.45s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  76%|█████████████████████████████████████████████████████▍                | 55/72 [03:18<00:58,  3.42s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  78%|██████████████████████████████████████████████████████▍               | 56/72 [03:22<00:54,  3.41s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  79%|███████████████████████████████████████████████████████▍              | 57/72 [03:25<00:51,  3.46s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  81%|████████████████████████████████████████████████████████▍             | 58/72 [03:29<00:49,  3.54s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  82%|█████████████████████████████████████████████████████████▎            | 59/72 [03:33<00:47,  3.63s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  83%|██████████████████████████████████████████████████████████▎           | 60/72 [03:36<00:42,  3.53s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  85%|███████████████████████████████████████████████████████████▎          | 61/72 [03:40<00:38,  3.53s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  86%|████████████████████████████████████████████████████████████▎         | 62/72 [03:43<00:35,  3.51s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  88%|█████████████████████████████████████████████████████████████▎        | 63/72 [03:47<00:31,  3.55s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  89%|██████████████████████████████████████████████████████████████▏       | 64/72 [03:50<00:27,  3.49s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  90%|███████████████████████████████████████████████████████████████▏      | 65/72 [03:54<00:25,  3.61s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  92%|████████████████████████████████████████████████████████████████▏     | 66/72 [03:58<00:22,  3.73s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  93%|█████████████████████████████████████████████████████████████████▏    | 67/72 [04:01<00:17,  3.57s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  94%|██████████████████████████████████████████████████████████████████    | 68/72 [04:05<00:14,  3.70s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  96%|███████████████████████████████████████████████████████████████████   | 69/72 [04:09<00:11,  3.68s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  97%|████████████████████████████████████████████████████████████████████  | 70/72 [04:12<00:06,  3.44s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  99%|█████████████████████████████████████████████████████████████████████ | 71/72 [04:15<00:03,  3.34s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating: 100%|██████████████████████████████████████████████████████████████████████| 72/72 [04:18<00:00,  3.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 Validation Scores: ROUGE: {'rouge1': 0.4531546066029498, 'rouge2': 0.21790520929411447, 'rougeL': 0.3431293928432541, 'rougeLsum': 0.3431293928432541}, BERTScore: {'precision': 0.34619525261223316, 'recall': 0.2823543107757966, 'f1': 0.3140304561497437}\n",
      "17\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17:   0%|                                                                             | 0/334 [00:00<?, ?batch/s]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Epoch 17: 100%|███████████████████████████████████████████████████████| 334/334 [01:42<00:00,  3.27batch/s, loss=0.488]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4882, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|██████████████████████████████████████████████████████████| 72/72 [00:07<00:00,  9.30batch/s, loss=1.14]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.1362, device='cuda:0')\n",
      "train_loss = 0.41241368727234307\n",
      " test_loss = 1.499661395119296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:   0%|                                                                               | 0/72 [00:00<?, ?it/s]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:   1%|▉                                                                      | 1/72 [00:03<03:49,  3.23s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:   3%|█▉                                                                     | 2/72 [00:06<03:43,  3.19s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:   4%|██▉                                                                    | 3/72 [00:10<04:01,  3.50s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:   6%|███▉                                                                   | 4/72 [00:13<04:04,  3.59s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:   7%|████▉                                                                  | 5/72 [00:17<03:58,  3.56s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:   8%|█████▉                                                                 | 6/72 [00:20<03:53,  3.54s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  10%|██████▉                                                                | 7/72 [00:24<03:44,  3.45s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  11%|███████▉                                                               | 8/72 [00:28<03:49,  3.58s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  12%|████████▉                                                              | 9/72 [00:31<03:48,  3.63s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  14%|█████████▋                                                            | 10/72 [00:36<03:55,  3.80s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  15%|██████████▋                                                           | 11/72 [00:39<03:48,  3.75s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  17%|███████████▋                                                          | 12/72 [00:43<03:48,  3.81s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  18%|████████████▋                                                         | 13/72 [00:46<03:32,  3.60s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  19%|█████████████▌                                                        | 14/72 [00:49<03:19,  3.45s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  21%|██████████████▌                                                       | 15/72 [00:53<03:17,  3.46s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  22%|███████████████▌                                                      | 16/72 [00:56<03:15,  3.50s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  24%|████████████████▌                                                     | 17/72 [01:00<03:14,  3.54s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  25%|█████████████████▌                                                    | 18/72 [01:04<03:19,  3.70s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  26%|██████████████████▍                                                   | 19/72 [01:07<03:09,  3.58s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  28%|███████████████████▍                                                  | 20/72 [01:11<03:02,  3.51s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  29%|████████████████████▍                                                 | 21/72 [01:14<02:58,  3.50s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  31%|█████████████████████▍                                                | 22/72 [01:18<03:03,  3.67s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  32%|██████████████████████▎                                               | 23/72 [01:23<03:09,  3.86s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  33%|███████████████████████▎                                              | 24/72 [01:26<02:58,  3.73s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  35%|████████████████████████▎                                             | 25/72 [01:30<03:04,  3.93s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  36%|█████████████████████████▎                                            | 26/72 [01:34<02:50,  3.70s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  38%|██████████████████████████▎                                           | 27/72 [01:37<02:43,  3.62s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  39%|███████████████████████████▏                                          | 28/72 [01:41<02:44,  3.73s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  40%|████████████████████████████▏                                         | 29/72 [01:44<02:36,  3.64s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  42%|█████████████████████████████▏                                        | 30/72 [01:48<02:33,  3.65s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  43%|██████████████████████████████▏                                       | 31/72 [01:51<02:24,  3.51s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  44%|███████████████████████████████                                       | 32/72 [01:54<02:16,  3.41s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  46%|████████████████████████████████                                      | 33/72 [01:58<02:16,  3.50s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  47%|█████████████████████████████████                                     | 34/72 [02:01<02:09,  3.40s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  49%|██████████████████████████████████                                    | 35/72 [02:04<02:02,  3.31s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  50%|███████████████████████████████████                                   | 36/72 [02:08<02:02,  3.41s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  51%|███████████████████████████████████▉                                  | 37/72 [02:12<02:01,  3.46s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  53%|████████████████████████████████████▉                                 | 38/72 [02:16<02:02,  3.61s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  54%|█████████████████████████████████████▉                                | 39/72 [02:19<01:57,  3.56s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  56%|██████████████████████████████████████▉                               | 40/72 [02:23<01:59,  3.72s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  57%|███████████████████████████████████████▊                              | 41/72 [02:27<01:55,  3.72s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  58%|████████████████████████████████████████▊                             | 42/72 [02:31<01:50,  3.69s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  60%|█████████████████████████████████████████▊                            | 43/72 [02:35<01:53,  3.92s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  61%|██████████████████████████████████████████▊                           | 44/72 [02:39<01:48,  3.88s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  62%|███████████████████████████████████████████▊                          | 45/72 [02:42<01:41,  3.75s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  64%|████████████████████████████████████████████▋                         | 46/72 [02:46<01:39,  3.82s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  65%|█████████████████████████████████████████████▋                        | 47/72 [02:50<01:32,  3.71s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  67%|██████████████████████████████████████████████▋                       | 48/72 [02:53<01:28,  3.70s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  68%|███████████████████████████████████████████████▋                      | 49/72 [02:57<01:25,  3.73s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  69%|████████████████████████████████████████████████▌                     | 50/72 [03:00<01:17,  3.52s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  71%|█████████████████████████████████████████████████▌                    | 51/72 [03:04<01:16,  3.62s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  72%|██████████████████████████████████████████████████▌                   | 52/72 [03:07<01:10,  3.54s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  74%|███████████████████████████████████████████████████▌                  | 53/72 [03:11<01:06,  3.53s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  75%|████████████████████████████████████████████████████▌                 | 54/72 [03:14<01:00,  3.38s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  76%|█████████████████████████████████████████████████████▍                | 55/72 [03:17<00:57,  3.36s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  78%|██████████████████████████████████████████████████████▍               | 56/72 [03:21<00:53,  3.36s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  79%|███████████████████████████████████████████████████████▍              | 57/72 [03:24<00:53,  3.54s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  81%|████████████████████████████████████████████████████████▍             | 58/72 [03:28<00:49,  3.53s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  82%|█████████████████████████████████████████████████████████▎            | 59/72 [03:32<00:47,  3.64s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  83%|██████████████████████████████████████████████████████████▎           | 60/72 [03:36<00:43,  3.63s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  85%|███████████████████████████████████████████████████████████▎          | 61/72 [03:39<00:40,  3.67s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  86%|████████████████████████████████████████████████████████████▎         | 62/72 [03:43<00:36,  3.62s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  88%|█████████████████████████████████████████████████████████████▎        | 63/72 [03:46<00:32,  3.56s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  89%|██████████████████████████████████████████████████████████████▏       | 64/72 [03:49<00:27,  3.41s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  90%|███████████████████████████████████████████████████████████████▏      | 65/72 [03:53<00:25,  3.64s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  92%|████████████████████████████████████████████████████████████████▏     | 66/72 [03:57<00:21,  3.65s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  93%|█████████████████████████████████████████████████████████████████▏    | 67/72 [04:00<00:17,  3.53s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  94%|██████████████████████████████████████████████████████████████████    | 68/72 [04:04<00:14,  3.54s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  96%|███████████████████████████████████████████████████████████████████   | 69/72 [04:07<00:10,  3.53s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  97%|████████████████████████████████████████████████████████████████████  | 70/72 [04:10<00:06,  3.34s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  99%|█████████████████████████████████████████████████████████████████████ | 71/72 [04:14<00:03,  3.48s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating: 100%|██████████████████████████████████████████████████████████████████████| 72/72 [04:17<00:00,  3.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 Validation Scores: ROUGE: {'rouge1': 0.4499719978403187, 'rouge2': 0.21200027529468596, 'rougeL': 0.3363115465298012, 'rougeLsum': 0.3363115465298012}, BERTScore: {'precision': 0.3393404788027207, 'recall': 0.29486952535808086, 'f1': 0.3173031661038597}\n",
      "18\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18:   0%|                                                                             | 0/334 [00:00<?, ?batch/s]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Epoch 18: 100%|███████████████████████████████████████████████████████| 334/334 [01:41<00:00,  3.29batch/s, loss=0.292]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2922, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|██████████████████████████████████████████████████████████| 72/72 [00:07<00:00,  9.26batch/s, loss=1.17]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.1711, device='cuda:0')\n",
      "train_loss = 0.37305691010670033\n",
      " test_loss = 1.5622399929496977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:   0%|                                                                               | 0/72 [00:00<?, ?it/s]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:   1%|▉                                                                      | 1/72 [00:03<03:59,  3.37s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:   3%|█▉                                                                     | 2/72 [00:06<03:42,  3.17s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:   4%|██▉                                                                    | 3/72 [00:10<04:01,  3.50s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:   6%|███▉                                                                   | 4/72 [00:14<04:21,  3.85s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:   7%|████▉                                                                  | 5/72 [00:18<04:17,  3.84s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:   8%|█████▉                                                                 | 6/72 [00:21<04:03,  3.68s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  10%|██████▉                                                                | 7/72 [00:25<03:53,  3.59s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  11%|███████▉                                                               | 8/72 [00:28<03:49,  3.59s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  12%|████████▉                                                              | 9/72 [00:32<03:56,  3.75s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  14%|█████████▋                                                            | 10/72 [00:36<03:56,  3.82s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  15%|██████████▋                                                           | 11/72 [00:40<03:44,  3.69s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  17%|███████████▋                                                          | 12/72 [00:44<03:45,  3.76s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  18%|████████████▋                                                         | 13/72 [00:47<03:40,  3.74s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  19%|█████████████▌                                                        | 14/72 [00:51<03:35,  3.71s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  21%|██████████████▌                                                       | 15/72 [00:55<03:32,  3.73s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  22%|███████████████▌                                                      | 16/72 [00:58<03:15,  3.50s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  24%|████████████████▌                                                     | 17/72 [01:01<03:08,  3.43s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  25%|█████████████████▌                                                    | 18/72 [01:05<03:16,  3.64s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  26%|██████████████████▍                                                   | 19/72 [01:08<03:06,  3.52s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  28%|███████████████████▍                                                  | 20/72 [01:12<03:04,  3.55s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  29%|████████████████████▍                                                 | 21/72 [01:16<03:00,  3.54s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  31%|█████████████████████▍                                                | 22/72 [01:20<03:03,  3.66s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  32%|██████████████████████▎                                               | 23/72 [01:23<02:53,  3.53s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  33%|███████████████████████▎                                              | 24/72 [01:27<03:00,  3.75s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  35%|████████████████████████▎                                             | 25/72 [01:31<02:59,  3.81s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  36%|█████████████████████████▎                                            | 26/72 [01:34<02:45,  3.60s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  38%|██████████████████████████▎                                           | 27/72 [01:38<02:40,  3.57s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  39%|███████████████████████████▏                                          | 28/72 [01:41<02:33,  3.49s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  40%|████████████████████████████▏                                         | 29/72 [01:44<02:29,  3.47s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  42%|█████████████████████████████▏                                        | 30/72 [01:48<02:31,  3.60s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  43%|██████████████████████████████▏                                       | 31/72 [01:52<02:26,  3.58s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  44%|███████████████████████████████                                       | 32/72 [01:55<02:18,  3.46s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  46%|████████████████████████████████                                      | 33/72 [01:59<02:17,  3.52s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  47%|█████████████████████████████████                                     | 34/72 [02:01<02:06,  3.32s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  49%|██████████████████████████████████                                    | 35/72 [02:05<02:00,  3.25s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  50%|███████████████████████████████████                                   | 36/72 [02:08<02:03,  3.43s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  51%|███████████████████████████████████▉                                  | 37/72 [02:12<02:03,  3.53s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  53%|████████████████████████████████████▉                                 | 38/72 [02:16<02:04,  3.66s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  54%|█████████████████████████████████████▉                                | 39/72 [02:20<02:02,  3.70s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  56%|██████████████████████████████████████▉                               | 40/72 [02:24<01:59,  3.75s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  57%|███████████████████████████████████████▊                              | 41/72 [02:28<01:56,  3.76s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  58%|████████████████████████████████████████▊                             | 42/72 [02:32<01:54,  3.83s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  60%|█████████████████████████████████████████▊                            | 43/72 [02:35<01:48,  3.73s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  61%|██████████████████████████████████████████▊                           | 44/72 [02:39<01:43,  3.69s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  62%|███████████████████████████████████████████▊                          | 45/72 [02:42<01:37,  3.61s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  64%|████████████████████████████████████████████▋                         | 46/72 [02:46<01:36,  3.71s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  65%|█████████████████████████████████████████████▋                        | 47/72 [02:49<01:26,  3.48s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  67%|██████████████████████████████████████████████▋                       | 48/72 [02:52<01:23,  3.48s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  68%|███████████████████████████████████████████████▋                      | 49/72 [02:56<01:22,  3.60s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  69%|████████████████████████████████████████████████▌                     | 50/72 [02:59<01:16,  3.46s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  71%|█████████████████████████████████████████████████▌                    | 51/72 [03:03<01:15,  3.58s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  72%|██████████████████████████████████████████████████▌                   | 52/72 [03:07<01:10,  3.55s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  74%|███████████████████████████████████████████████████▌                  | 53/72 [03:11<01:09,  3.66s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  75%|████████████████████████████████████████████████████▌                 | 54/72 [03:14<01:02,  3.47s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  76%|█████████████████████████████████████████████████████▍                | 55/72 [03:17<00:58,  3.41s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  78%|██████████████████████████████████████████████████████▍               | 56/72 [03:21<00:55,  3.46s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  79%|███████████████████████████████████████████████████████▍              | 57/72 [03:24<00:52,  3.49s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  81%|████████████████████████████████████████████████████████▍             | 58/72 [03:28<00:52,  3.74s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  82%|█████████████████████████████████████████████████████████▎            | 59/72 [03:32<00:47,  3.68s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  83%|██████████████████████████████████████████████████████████▎           | 60/72 [03:35<00:43,  3.63s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  85%|███████████████████████████████████████████████████████████▎          | 61/72 [03:40<00:43,  3.92s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  86%|████████████████████████████████████████████████████████████▎         | 62/72 [03:43<00:36,  3.69s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  88%|█████████████████████████████████████████████████████████████▎        | 63/72 [03:47<00:32,  3.61s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  89%|██████████████████████████████████████████████████████████████▏       | 64/72 [03:50<00:28,  3.58s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  90%|███████████████████████████████████████████████████████████████▏      | 65/72 [03:54<00:24,  3.55s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  92%|████████████████████████████████████████████████████████████████▏     | 66/72 [03:57<00:21,  3.62s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  93%|█████████████████████████████████████████████████████████████████▏    | 67/72 [04:01<00:17,  3.48s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  94%|██████████████████████████████████████████████████████████████████    | 68/72 [04:04<00:13,  3.48s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  96%|███████████████████████████████████████████████████████████████████   | 69/72 [04:08<00:10,  3.49s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  97%|████████████████████████████████████████████████████████████████████  | 70/72 [04:11<00:06,  3.36s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  99%|█████████████████████████████████████████████████████████████████████ | 71/72 [04:15<00:03,  3.62s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating: 100%|██████████████████████████████████████████████████████████████████████| 72/72 [04:18<00:00,  3.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 Validation Scores: ROUGE: {'rouge1': 0.46472538456072005, 'rouge2': 0.22499673115588814, 'rougeL': 0.3514484408757969, 'rougeLsum': 0.3514484408757969}, BERTScore: {'precision': 0.3474258203059435, 'recall': 0.3037554311255614, 'f1': 0.32578397588804364}\n",
      "19\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19:   0%|                                                                             | 0/334 [00:00<?, ?batch/s]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Epoch 19: 100%|███████████████████████████████████████████████████████| 334/334 [01:41<00:00,  3.29batch/s, loss=0.344]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3444, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████████████████████████████████████████████████████| 72/72 [00:07<00:00,  9.24batch/s, loss=1.17]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.1672, device='cuda:0')\n",
      "train_loss = 0.3487208857240077\n",
      " test_loss = 1.5839224437044725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:   0%|                                                                               | 0/72 [00:00<?, ?it/s]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:   1%|▉                                                                      | 1/72 [00:03<04:32,  3.84s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:   3%|█▉                                                                     | 2/72 [00:07<04:03,  3.48s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:   4%|██▉                                                                    | 3/72 [00:11<04:25,  3.85s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:   6%|███▉                                                                   | 4/72 [00:15<04:22,  3.86s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:   7%|████▉                                                                  | 5/72 [00:19<04:21,  3.91s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:   8%|█████▉                                                                 | 6/72 [00:23<04:24,  4.01s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  10%|██████▉                                                                | 7/72 [00:27<04:21,  4.02s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  11%|███████▉                                                               | 8/72 [00:31<04:11,  3.93s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  12%|████████▉                                                              | 9/72 [00:34<04:03,  3.87s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  14%|█████████▋                                                            | 10/72 [00:38<04:03,  3.92s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  15%|██████████▋                                                           | 11/72 [00:42<03:56,  3.88s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  17%|███████████▋                                                          | 12/72 [00:46<03:57,  3.96s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  18%|████████████▋                                                         | 13/72 [00:50<03:44,  3.80s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  19%|█████████████▌                                                        | 14/72 [00:53<03:29,  3.62s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  21%|██████████████▌                                                       | 15/72 [00:56<03:21,  3.54s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  22%|███████████████▌                                                      | 16/72 [01:00<03:21,  3.61s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  24%|████████████████▌                                                     | 17/72 [01:04<03:14,  3.54s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  25%|█████████████████▌                                                    | 18/72 [01:07<03:17,  3.65s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  26%|██████████████████▍                                                   | 19/72 [01:11<03:12,  3.64s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  28%|███████████████████▍                                                  | 20/72 [01:15<03:07,  3.61s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  29%|████████████████████▍                                                 | 21/72 [01:18<03:07,  3.67s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  31%|█████████████████████▍                                                | 22/72 [01:23<03:11,  3.82s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  32%|██████████████████████▎                                               | 23/72 [01:26<03:03,  3.75s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  33%|███████████████████████▎                                              | 24/72 [01:30<02:56,  3.67s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  35%|████████████████████████▎                                             | 25/72 [01:34<02:57,  3.78s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  36%|█████████████████████████▎                                            | 26/72 [01:37<02:48,  3.67s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  38%|██████████████████████████▎                                           | 27/72 [01:41<02:47,  3.73s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  39%|███████████████████████████▏                                          | 28/72 [01:45<02:46,  3.78s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  40%|████████████████████████████▏                                         | 29/72 [01:49<02:40,  3.74s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  42%|█████████████████████████████▏                                        | 30/72 [01:52<02:32,  3.62s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  43%|██████████████████████████████▏                                       | 31/72 [01:56<02:32,  3.73s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  44%|███████████████████████████████                                       | 32/72 [01:59<02:23,  3.58s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  46%|████████████████████████████████                                      | 33/72 [02:03<02:29,  3.82s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  47%|█████████████████████████████████                                     | 34/72 [02:07<02:22,  3.75s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  49%|██████████████████████████████████                                    | 35/72 [02:11<02:19,  3.77s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  50%|███████████████████████████████████                                   | 36/72 [02:15<02:17,  3.81s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  51%|███████████████████████████████████▉                                  | 37/72 [02:19<02:14,  3.84s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  53%|████████████████████████████████████▉                                 | 38/72 [02:23<02:10,  3.85s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  54%|█████████████████████████████████████▉                                | 39/72 [02:26<02:03,  3.74s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  56%|██████████████████████████████████████▉                               | 40/72 [02:30<02:03,  3.86s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  57%|███████████████████████████████████████▊                              | 41/72 [02:34<01:55,  3.72s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  58%|████████████████████████████████████████▊                             | 42/72 [02:37<01:52,  3.76s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  60%|█████████████████████████████████████████▊                            | 43/72 [02:41<01:48,  3.76s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  61%|██████████████████████████████████████████▊                           | 44/72 [02:44<01:39,  3.57s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  62%|███████████████████████████████████████████▊                          | 45/72 [02:48<01:34,  3.52s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  64%|████████████████████████████████████████████▋                         | 46/72 [02:52<01:35,  3.66s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  65%|█████████████████████████████████████████████▋                        | 47/72 [02:55<01:27,  3.49s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  67%|██████████████████████████████████████████████▋                       | 48/72 [02:58<01:24,  3.51s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  68%|███████████████████████████████████████████████▋                      | 49/72 [03:02<01:21,  3.56s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  69%|████████████████████████████████████████████████▌                     | 50/72 [03:06<01:18,  3.57s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  71%|█████████████████████████████████████████████████▌                    | 51/72 [03:09<01:16,  3.63s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  72%|██████████████████████████████████████████████████▌                   | 52/72 [03:13<01:11,  3.56s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  74%|███████████████████████████████████████████████████▌                  | 53/72 [03:17<01:13,  3.85s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  75%|████████████████████████████████████████████████████▌                 | 54/72 [03:20<01:04,  3.60s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  76%|█████████████████████████████████████████████████████▍                | 55/72 [03:24<01:01,  3.60s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  78%|██████████████████████████████████████████████████████▍               | 56/72 [03:27<00:56,  3.56s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  79%|███████████████████████████████████████████████████████▍              | 57/72 [03:31<00:53,  3.59s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  81%|████████████████████████████████████████████████████████▍             | 58/72 [03:35<00:49,  3.55s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  82%|█████████████████████████████████████████████████████████▎            | 59/72 [03:38<00:45,  3.50s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  83%|██████████████████████████████████████████████████████████▎           | 60/72 [03:41<00:41,  3.48s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  85%|███████████████████████████████████████████████████████████▎          | 61/72 [03:45<00:39,  3.55s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  86%|████████████████████████████████████████████████████████████▎         | 62/72 [03:48<00:34,  3.45s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  88%|█████████████████████████████████████████████████████████████▎        | 63/72 [03:52<00:31,  3.47s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  89%|██████████████████████████████████████████████████████████████▏       | 64/72 [03:55<00:27,  3.44s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  90%|███████████████████████████████████████████████████████████████▏      | 65/72 [03:59<00:25,  3.65s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  92%|████████████████████████████████████████████████████████████████▏     | 66/72 [04:03<00:22,  3.70s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  93%|█████████████████████████████████████████████████████████████████▏    | 67/72 [04:07<00:18,  3.69s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  94%|██████████████████████████████████████████████████████████████████    | 68/72 [04:11<00:15,  3.81s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  96%|███████████████████████████████████████████████████████████████████   | 69/72 [04:15<00:12,  4.01s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  97%|████████████████████████████████████████████████████████████████████  | 70/72 [04:19<00:07,  3.79s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating:  99%|█████████████████████████████████████████████████████████████████████ | 71/72 [04:22<00:03,  3.69s/it]C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Validating: 100%|██████████████████████████████████████████████████████████████████████| 72/72 [04:25<00:00,  3.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 Validation Scores: ROUGE: {'rouge1': 0.4598373500442183, 'rouge2': 0.2225923150569902, 'rougeL': 0.34244664243253925, 'rougeLsum': 0.34244664243253925}, BERTScore: {'precision': 0.34000904206186533, 'recall': 0.3003429068873326, 'f1': 0.32034363018141854}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = LongT5ForConditionalGeneration.from_pretrained(\"google/long-t5-tglobal-base\")\n",
    "\n",
    "epochs = 20\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = 1e-4)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(str(epoch)+\"\\n\")\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    with tqdm(train_dataloader, unit=\"batch\") as tepoch:    \n",
    "        for batch in tepoch:\n",
    "            tepoch.set_description(f\"Epoch {epoch}\")\n",
    "            input_ids, attention_masks, labels = tuple(t.to(device) for t in batch)\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_masks, labels=labels)\n",
    "\n",
    "            loss = outputs.loss\n",
    "            train_loss += loss.item()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            tepoch.set_postfix(loss= loss.item())\n",
    "        train_loss /= len(train_dataloader)\n",
    "        sleep(0.1)\n",
    "    print(loss)\n",
    "\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        with tqdm(test_dataloader, unit=\"batch\") as tepoch:\n",
    "            for batch in tepoch:\n",
    "                tepoch.set_description(f\"Epoch {epoch}\")\n",
    "                input_ids, attention_masks, labels = tuple(t.to(device) for t in batch)\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_masks, labels=labels)\n",
    "                loss = outputs.loss\n",
    "                test_loss += loss.item()\n",
    "                tepoch.set_postfix(loss= loss.item())\n",
    "\n",
    "            # Compute the average testing loss for the epoch\n",
    "            test_loss /= len(test_dataloader)\n",
    "    print(loss)\n",
    "    print(f'train_loss = {train_loss}\\n test_loss = {test_loss}')\n",
    "    # Validation and Metric Calculation\n",
    "    rouge_scores, bert_scores = validate_and_calculate_metrics(model, val_dataloader, tokenizer, device)\n",
    "    print(f\"Epoch {epoch} Validation Scores: ROUGE: {rouge_scores}, BERTScore: {bert_scores}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89e3ec1",
   "metadata": {},
   "source": [
    "## It is giving a good rouge and bert score now to address the issue with overfitting, we are adding early stopping, warmup steps and eight decay and regularisation techniques,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5b8cc1",
   "metadata": {},
   "source": [
    "## Training longt5 with adding neccessary steps for avoiding overfitiing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a80b2416",
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_json('sum_anno_human_proper.json', lines= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35d0215f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>from</th>\n",
       "      <th>to</th>\n",
       "      <th>date</th>\n",
       "      <th>body</th>\n",
       "      <th>Body_Length</th>\n",
       "      <th>Subject_Length</th>\n",
       "      <th>Cleaned_Body</th>\n",
       "      <th>Cleaned_Subject</th>\n",
       "      <th>BERT_Embeddings</th>\n",
       "      <th>...</th>\n",
       "      <th>Category</th>\n",
       "      <th>Cleaned_mails</th>\n",
       "      <th>summary_TXTRNK_1</th>\n",
       "      <th>Summary</th>\n",
       "      <th>summary_BART</th>\n",
       "      <th>index_number</th>\n",
       "      <th>Tokenized_Email</th>\n",
       "      <th>Entities</th>\n",
       "      <th>Cluster_retrieved</th>\n",
       "      <th>Summary_human</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Thank you for attending Microsoft Ignite</td>\n",
       "      <td>Microsoft Team &lt;microsoft@email.microsoft.com&gt;</td>\n",
       "      <td>richard.potter@raddsolutions.co.uk</td>\n",
       "      <td>Wed, 19 Oct 2022 20:31:34 +0100</td>\n",
       "      <td>...</td>\n",
       "      <td>6232</td>\n",
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td>Thank you for attending Microsoft Ignite</td>\n",
       "      <td>[-0.059376951307058334, 0.17135855555534363, 0...</td>\n",
       "      <td>...</td>\n",
       "      <td>Cyber Alerts</td>\n",
       "      <td>microsoft ignite may be over but heres your ch...</td>\n",
       "      <td>microsoft ignite may be over but heres your ch...</td>\n",
       "      <td>microsoft ignite may be over but heres your ch...</td>\n",
       "      <td>Learn how microsoft empowers organisations to ...</td>\n",
       "      <td>1543</td>\n",
       "      <td>{'input_ids': tensor([[  101,  7513, 16270,  4...</td>\n",
       "      <td>[('microsoft', 'ORG'), ('2022', 'DATE'), ('mic...</td>\n",
       "      <td>0</td>\n",
       "      <td>The email discusses post-Microsoft Ignite 2022...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Invitation to learn how Windows 365 Cloud PCs ...</td>\n",
       "      <td>Microsoft &lt;replyto@email.microsoft.com&gt;</td>\n",
       "      <td>richard.potter@raddsolutions.co.uk</td>\n",
       "      <td>Tue, 01 Nov 2022 11:01:50 +0000</td>\n",
       "      <td>Webinar with demos of Windows 365 and vision f...</td>\n",
       "      <td>2943</td>\n",
       "      <td>90</td>\n",
       "      <td>webinar with demos of windows 365 and vision f...</td>\n",
       "      <td>Invitation to learn how Windows 365 Cloud PCs ...</td>\n",
       "      <td>[-0.1439182013273239, 0.22149936854839325, 0.6...</td>\n",
       "      <td>...</td>\n",
       "      <td>Social Media Alerts</td>\n",
       "      <td>webinar with demos of windows 365 and vision f...</td>\n",
       "      <td>No Summary</td>\n",
       "      <td></td>\n",
       "      <td>webinar with demos of windows 365 and vision f...</td>\n",
       "      <td>765</td>\n",
       "      <td>{'input_ids': tensor([[  101,  4773,  3981,  2...</td>\n",
       "      <td>[('thursday 17th', 'DATE'), ('2022 1400  1500'...</td>\n",
       "      <td>0</td>\n",
       "      <td>Webinar Announcement: \"Windows 365 for Your Hy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Don’t fall behind – embrace AI with Dell Techn...</td>\n",
       "      <td>Dell Technologies Partner Program &lt;DellTechnol...</td>\n",
       "      <td>richard.potter@raddsolutions.co.uk</td>\n",
       "      <td>Tue, 15 Nov 2022 06:01:17 +0000</td>\n",
       "      <td>&lt;https://click.comm.delltechnologies.com/open...</td>\n",
       "      <td>4498</td>\n",
       "      <td>64</td>\n",
       "      <td>\\n   \\t\\r\\nview online \\n\\n  \\t\\r\\n \\t\\r\\nwhy...</td>\n",
       "      <td>Dont fall behind  embrace AI with Dell Technol...</td>\n",
       "      <td>[-0.15142026543617249, 0.11411778628826141, 0....</td>\n",
       "      <td>...</td>\n",
       "      <td>Social Media Alerts</td>\n",
       "      <td>why ai and why now we are witnessing and livin...</td>\n",
       "      <td>we are witnessing and living through the rise ...</td>\n",
       "      <td>why ai and why now\\nwe are witnessing and livi...</td>\n",
       "      <td>Artificial intelligence ai market is forecast ...</td>\n",
       "      <td>369</td>\n",
       "      <td>{'input_ids': tensor([[  101,  3193,  3784,  2...</td>\n",
       "      <td>[('500 billion', 'MONEY'), ('20231', 'DATE'), ...</td>\n",
       "      <td>0</td>\n",
       "      <td>The email discusses the rapid growth of artifi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Microsoft Envision Season 3 begins December 13</td>\n",
       "      <td>Microsoft Team &lt;microsoft@email.microsoft.com&gt;</td>\n",
       "      <td>richard.potter@raddsolutions.co.uk</td>\n",
       "      <td>Wed, 09 Nov 2022 17:05:09 +0000</td>\n",
       "      <td>Episode 1 airs December 13, 2022 \\r\\nHaving tr...</td>\n",
       "      <td>3476</td>\n",
       "      <td>46</td>\n",
       "      <td>episode 1 airs december 13 2022 \\r\\nhaving tro...</td>\n",
       "      <td>Microsoft Envision Season 3 begins December 13</td>\n",
       "      <td>[-0.36103835701942444, 0.06514844298362732, 0....</td>\n",
       "      <td>...</td>\n",
       "      <td>Social Media Alerts</td>\n",
       "      <td>episode 1 airs december 13 2022 having trouble...</td>\n",
       "      <td>episode 1 airs december 13 2022\\nregister now ...</td>\n",
       "      <td>episode 1 airs december 13 2022\\nregister now ...</td>\n",
       "      <td>register now for microsoft envision season 3. ...</td>\n",
       "      <td>895</td>\n",
       "      <td>{'input_ids': tensor([[  101,  2792,  1015, 14...</td>\n",
       "      <td>[('1', 'CARDINAL'), ('13 2022', 'DATE'), ('mic...</td>\n",
       "      <td>0</td>\n",
       "      <td>The email announces the premiere of Microsoft ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In September, you had 71 users visit your webs...</td>\n",
       "      <td>Google Analytics &lt;analytics-noreply@google.com&gt;</td>\n",
       "      <td>richard.potter@raddsolutions.co.uk</td>\n",
       "      <td>Tue, 11 Oct 2022 06:02:01 +0100</td>\n",
       "      <td>&lt;https://www.google.com/images/branding/googl...</td>\n",
       "      <td>5559</td>\n",
       "      <td>68</td>\n",
       "      <td>\\n \\r\\nuniversal analytics will no longer pr...</td>\n",
       "      <td>In September you had 71 users visit your websi...</td>\n",
       "      <td>[-0.10645909607410431, 0.17022578418254852, 0....</td>\n",
       "      <td>...</td>\n",
       "      <td>Social Media Alerts</td>\n",
       "      <td>universal analytics will no longer process new...</td>\n",
       "      <td>81 35 bounce rate\\nbreakdown of visitors acqui...</td>\n",
       "      <td>81 35 bounce rate\\nbreakdown of visitors acqui...</td>\n",
       "      <td>universal analytics will no longer process new...</td>\n",
       "      <td>740</td>\n",
       "      <td>{'input_ids': tensor([[  101,  5415, 25095,  2...</td>\n",
       "      <td>[('2023', 'DATE'), ('4', 'CARDINAL'), ('septem...</td>\n",
       "      <td>0</td>\n",
       "      <td>Starting in 2023, Universal Analytics will no ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>949</th>\n",
       "      <td>Our Black Friday offers have landed!</td>\n",
       "      <td>IT Governance &lt;emailsupport@itgovernance.co.uk&gt;</td>\n",
       "      <td>richie.wynne@raddsolutions.co.uk</td>\n",
       "      <td>Mon, 21 Nov 2022 11:05:15 +0000</td>\n",
       "      <td>You’re not going to want to miss these savings...</td>\n",
       "      <td>3228</td>\n",
       "      <td>36</td>\n",
       "      <td>youre not going to want to miss these savings ...</td>\n",
       "      <td>Our Black Friday offers have landed</td>\n",
       "      <td>[0.09008561074733734, 0.16759826242923737, 0.6...</td>\n",
       "      <td>...</td>\n",
       "      <td>Service Alerts</td>\n",
       "      <td>youre not going to want to miss these savings ...</td>\n",
       "      <td>use promo code bf25\\nuse promo code bf25\\nunit...</td>\n",
       "      <td>were starting our black friday offers early wi...</td>\n",
       "      <td>were starting our black friday offers early wi...</td>\n",
       "      <td>1130</td>\n",
       "      <td>{'input_ids': tensor([[  101,  2115,  2063,  2...</td>\n",
       "      <td>[('friday', 'DATE'), ('25', 'CARDINAL'), ('25'...</td>\n",
       "      <td>7</td>\n",
       "      <td>The email advertises an early Black Friday off...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>950</th>\n",
       "      <td>Jeff Fritz reviews IronPDF vs SyncFusion, iTex...</td>\n",
       "      <td>\"Jacob, Head of Engineering\" &lt;developers@irons...</td>\n",
       "      <td>Richard Potter &lt;richard.potter@raddsolutions.c...</td>\n",
       "      <td>Tue, 13 Dec 2022 15:31:28 +0000</td>\n",
       "      <td>&lt;https://ironsoftware.lt.acemlnb.com/Prod/lin...</td>\n",
       "      <td>8587</td>\n",
       "      <td>55</td>\n",
       "      <td>\\t \\r\\n \\t \\r\\nhi richard\\r\\nimagine spendin...</td>\n",
       "      <td>Jeff Fritz reviews IronPDF vs SyncFusion iText...</td>\n",
       "      <td>[-0.20470425486564636, 0.20281416177749634, 0....</td>\n",
       "      <td>...</td>\n",
       "      <td>Service Alerts</td>\n",
       "      <td>imagine spending a lot of money on software li...</td>\n",
       "      <td>would your project fail hopefully not but it n...</td>\n",
       "      <td>imagine spending a lot of money on software li...</td>\n",
       "      <td>iron software is a free open source solution t...</td>\n",
       "      <td>783</td>\n",
       "      <td>{'input_ids': tensor([[  101,  7632,  2957,  5...</td>\n",
       "      <td>[('jeff fritz', 'PERSON'), ('net conf', 'ORG')...</td>\n",
       "      <td>7</td>\n",
       "      <td>Jeff Fritz from .NET Conf reviewed IronPDF aga...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>951</th>\n",
       "      <td>Don't Miss Out: Help to Grow: Digital Ends in ...</td>\n",
       "      <td>Zym &lt;rebecca@zymplify.com&gt;</td>\n",
       "      <td>Richard Potter &lt;richard.potter@raddsolutions.c...</td>\n",
       "      <td>Tue, 17 Jan 2023 12:01:49 +0000</td>\n",
       "      <td>ZYM helps business owners understand their mar...</td>\n",
       "      <td>6966</td>\n",
       "      <td>56</td>\n",
       "      <td>zym helps business owners understand their mar...</td>\n",
       "      <td>Dont Miss Out Help to Grow Digital Ends in 16 ...</td>\n",
       "      <td>[0.0035861318465322256, 0.07786554843187332, 0...</td>\n",
       "      <td>...</td>\n",
       "      <td>Service Alerts</td>\n",
       "      <td>zym helps business owners understand their mar...</td>\n",
       "      <td>you may qualify for the grant meaning you coul...</td>\n",
       "      <td>even better you can try zym for free for 14 da...</td>\n",
       "      <td>zym helps business owners understand their mar...</td>\n",
       "      <td>345</td>\n",
       "      <td>{'input_ids': tensor([[  101,  1062, 24335,  7...</td>\n",
       "      <td>[('uk', 'GPE'), ('up to 5000', 'CARDINAL'), ('...</td>\n",
       "      <td>7</td>\n",
       "      <td>Zym aids business owners in comprehending mark...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>952</th>\n",
       "      <td>New videos coming in 2023</td>\n",
       "      <td>Clear Measure &lt;clearmeasure@clear-measure.com&gt;</td>\n",
       "      <td>richard.potter@raddsolutions.co.uk</td>\n",
       "      <td>Thu, 29 Dec 2022 10:00:02 +0000</td>\n",
       "      <td>New videos coming in 2023 … made to empower yo...</td>\n",
       "      <td>3404</td>\n",
       "      <td>25</td>\n",
       "      <td>new videos coming in 2023  made to empower you...</td>\n",
       "      <td>New videos coming in 2023</td>\n",
       "      <td>[-0.08648061007261276, 0.11852650344371796, 0....</td>\n",
       "      <td>...</td>\n",
       "      <td>Service Alerts</td>\n",
       "      <td>new videos coming in 2023 made to empower your...</td>\n",
       "      <td>our new videos coming in 2023 are made to empo...</td>\n",
       "      <td>our new videos coming in 2023 are made to empo...</td>\n",
       "      <td>new videos coming in 2023  made to empower you...</td>\n",
       "      <td>958</td>\n",
       "      <td>{'input_ids': tensor([[  101,  2047,  6876,  2...</td>\n",
       "      <td>[('2023', 'DATE'), ('2023', 'DATE'), ('10815',...</td>\n",
       "      <td>7</td>\n",
       "      <td>Summary:\\n\\nClear Measure has announced an upc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>953</th>\n",
       "      <td>Business PCs up to 40% off</td>\n",
       "      <td>Lenovo New beginnings! &lt;lenovo@ecomm.lenovo.com&gt;</td>\n",
       "      <td>richard.potter@raddsolutions.co.uk</td>\n",
       "      <td>Wed, 01 Feb 2023 09:04:42 +0000</td>\n",
       "      <td>&lt;https://trk.ecomm.lenovo.com/ss/o/k8leYzsS8M...</td>\n",
       "      <td>8108</td>\n",
       "      <td>26</td>\n",
       "      <td>\\n \\t\\r\\n\\tview it in browser instead \\n  fre...</td>\n",
       "      <td>Business PCs up to 40 off</td>\n",
       "      <td>[-0.09954569488763809, 0.13386352360248566, 0....</td>\n",
       "      <td>...</td>\n",
       "      <td>Service Alerts</td>\n",
       "      <td>view it in browser instead free shipping on al...</td>\n",
       "      <td>workspace refresh with up to 40 discount until...</td>\n",
       "      <td>thinkpad p1 gen 4\\nthinkpad z13 gen 1\\nideapad...</td>\n",
       "      <td>Free shipping on all orders with up to 40% dis...</td>\n",
       "      <td>132</td>\n",
       "      <td>{'input_ids': tensor([[  101,  3193,  2009,  1...</td>\n",
       "      <td>[('up to 40', 'CARDINAL'), ('up to 40', 'CARDI...</td>\n",
       "      <td>7</td>\n",
       "      <td>Email from Lenovo announces a workspace refres...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>954 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               subject  \\\n",
       "0             Thank you for attending Microsoft Ignite   \n",
       "1    Invitation to learn how Windows 365 Cloud PCs ...   \n",
       "2    Don’t fall behind – embrace AI with Dell Techn...   \n",
       "3       Microsoft Envision Season 3 begins December 13   \n",
       "4    In September, you had 71 users visit your webs...   \n",
       "..                                                 ...   \n",
       "949               Our Black Friday offers have landed!   \n",
       "950  Jeff Fritz reviews IronPDF vs SyncFusion, iTex...   \n",
       "951  Don't Miss Out: Help to Grow: Digital Ends in ...   \n",
       "952                          New videos coming in 2023   \n",
       "953                         Business PCs up to 40% off   \n",
       "\n",
       "                                                  from  \\\n",
       "0       Microsoft Team <microsoft@email.microsoft.com>   \n",
       "1              Microsoft <replyto@email.microsoft.com>   \n",
       "2    Dell Technologies Partner Program <DellTechnol...   \n",
       "3       Microsoft Team <microsoft@email.microsoft.com>   \n",
       "4      Google Analytics <analytics-noreply@google.com>   \n",
       "..                                                 ...   \n",
       "949    IT Governance <emailsupport@itgovernance.co.uk>   \n",
       "950  \"Jacob, Head of Engineering\" <developers@irons...   \n",
       "951                         Zym <rebecca@zymplify.com>   \n",
       "952     Clear Measure <clearmeasure@clear-measure.com>   \n",
       "953   Lenovo New beginnings! <lenovo@ecomm.lenovo.com>   \n",
       "\n",
       "                                                    to  \\\n",
       "0                   richard.potter@raddsolutions.co.uk   \n",
       "1                   richard.potter@raddsolutions.co.uk   \n",
       "2                   richard.potter@raddsolutions.co.uk   \n",
       "3                   richard.potter@raddsolutions.co.uk   \n",
       "4                   richard.potter@raddsolutions.co.uk   \n",
       "..                                                 ...   \n",
       "949                   richie.wynne@raddsolutions.co.uk   \n",
       "950  Richard Potter <richard.potter@raddsolutions.c...   \n",
       "951  Richard Potter <richard.potter@raddsolutions.c...   \n",
       "952                 richard.potter@raddsolutions.co.uk   \n",
       "953                 richard.potter@raddsolutions.co.uk   \n",
       "\n",
       "                                date  \\\n",
       "0    Wed, 19 Oct 2022 20:31:34 +0100   \n",
       "1    Tue, 01 Nov 2022 11:01:50 +0000   \n",
       "2    Tue, 15 Nov 2022 06:01:17 +0000   \n",
       "3    Wed, 09 Nov 2022 17:05:09 +0000   \n",
       "4    Tue, 11 Oct 2022 06:02:01 +0100   \n",
       "..                               ...   \n",
       "949  Mon, 21 Nov 2022 11:05:15 +0000   \n",
       "950  Tue, 13 Dec 2022 15:31:28 +0000   \n",
       "951  Tue, 17 Jan 2023 12:01:49 +0000   \n",
       "952  Thu, 29 Dec 2022 10:00:02 +0000   \n",
       "953  Wed, 01 Feb 2023 09:04:42 +0000   \n",
       "\n",
       "                                                  body  Body_Length  \\\n",
       "0                                                  ...         6232   \n",
       "1    Webinar with demos of Windows 365 and vision f...         2943   \n",
       "2     <https://click.comm.delltechnologies.com/open...         4498   \n",
       "3    Episode 1 airs December 13, 2022 \\r\\nHaving tr...         3476   \n",
       "4     <https://www.google.com/images/branding/googl...         5559   \n",
       "..                                                 ...          ...   \n",
       "949  You’re not going to want to miss these savings...         3228   \n",
       "950   <https://ironsoftware.lt.acemlnb.com/Prod/lin...         8587   \n",
       "951  ZYM helps business owners understand their mar...         6966   \n",
       "952  New videos coming in 2023 … made to empower yo...         3404   \n",
       "953   <https://trk.ecomm.lenovo.com/ss/o/k8leYzsS8M...         8108   \n",
       "\n",
       "     Subject_Length                                       Cleaned_Body  \\\n",
       "0                40                                                ...   \n",
       "1                90  webinar with demos of windows 365 and vision f...   \n",
       "2                64   \\n   \\t\\r\\nview online \\n\\n  \\t\\r\\n \\t\\r\\nwhy...   \n",
       "3                46  episode 1 airs december 13 2022 \\r\\nhaving tro...   \n",
       "4                68    \\n \\r\\nuniversal analytics will no longer pr...   \n",
       "..              ...                                                ...   \n",
       "949              36  youre not going to want to miss these savings ...   \n",
       "950              55    \\t \\r\\n \\t \\r\\nhi richard\\r\\nimagine spendin...   \n",
       "951              56  zym helps business owners understand their mar...   \n",
       "952              25  new videos coming in 2023  made to empower you...   \n",
       "953              26   \\n \\t\\r\\n\\tview it in browser instead \\n  fre...   \n",
       "\n",
       "                                       Cleaned_Subject  \\\n",
       "0             Thank you for attending Microsoft Ignite   \n",
       "1    Invitation to learn how Windows 365 Cloud PCs ...   \n",
       "2    Dont fall behind  embrace AI with Dell Technol...   \n",
       "3       Microsoft Envision Season 3 begins December 13   \n",
       "4    In September you had 71 users visit your websi...   \n",
       "..                                                 ...   \n",
       "949                Our Black Friday offers have landed   \n",
       "950  Jeff Fritz reviews IronPDF vs SyncFusion iText...   \n",
       "951  Dont Miss Out Help to Grow Digital Ends in 16 ...   \n",
       "952                          New videos coming in 2023   \n",
       "953                          Business PCs up to 40 off   \n",
       "\n",
       "                                       BERT_Embeddings  ...  \\\n",
       "0    [-0.059376951307058334, 0.17135855555534363, 0...  ...   \n",
       "1    [-0.1439182013273239, 0.22149936854839325, 0.6...  ...   \n",
       "2    [-0.15142026543617249, 0.11411778628826141, 0....  ...   \n",
       "3    [-0.36103835701942444, 0.06514844298362732, 0....  ...   \n",
       "4    [-0.10645909607410431, 0.17022578418254852, 0....  ...   \n",
       "..                                                 ...  ...   \n",
       "949  [0.09008561074733734, 0.16759826242923737, 0.6...  ...   \n",
       "950  [-0.20470425486564636, 0.20281416177749634, 0....  ...   \n",
       "951  [0.0035861318465322256, 0.07786554843187332, 0...  ...   \n",
       "952  [-0.08648061007261276, 0.11852650344371796, 0....  ...   \n",
       "953  [-0.09954569488763809, 0.13386352360248566, 0....  ...   \n",
       "\n",
       "                Category                                      Cleaned_mails  \\\n",
       "0           Cyber Alerts  microsoft ignite may be over but heres your ch...   \n",
       "1    Social Media Alerts  webinar with demos of windows 365 and vision f...   \n",
       "2    Social Media Alerts  why ai and why now we are witnessing and livin...   \n",
       "3    Social Media Alerts  episode 1 airs december 13 2022 having trouble...   \n",
       "4    Social Media Alerts  universal analytics will no longer process new...   \n",
       "..                   ...                                                ...   \n",
       "949       Service Alerts  youre not going to want to miss these savings ...   \n",
       "950       Service Alerts  imagine spending a lot of money on software li...   \n",
       "951       Service Alerts  zym helps business owners understand their mar...   \n",
       "952       Service Alerts  new videos coming in 2023 made to empower your...   \n",
       "953       Service Alerts  view it in browser instead free shipping on al...   \n",
       "\n",
       "                                      summary_TXTRNK_1  \\\n",
       "0    microsoft ignite may be over but heres your ch...   \n",
       "1                                           No Summary   \n",
       "2    we are witnessing and living through the rise ...   \n",
       "3    episode 1 airs december 13 2022\\nregister now ...   \n",
       "4    81 35 bounce rate\\nbreakdown of visitors acqui...   \n",
       "..                                                 ...   \n",
       "949  use promo code bf25\\nuse promo code bf25\\nunit...   \n",
       "950  would your project fail hopefully not but it n...   \n",
       "951  you may qualify for the grant meaning you coul...   \n",
       "952  our new videos coming in 2023 are made to empo...   \n",
       "953  workspace refresh with up to 40 discount until...   \n",
       "\n",
       "                                               Summary  \\\n",
       "0    microsoft ignite may be over but heres your ch...   \n",
       "1                                                        \n",
       "2    why ai and why now\\nwe are witnessing and livi...   \n",
       "3    episode 1 airs december 13 2022\\nregister now ...   \n",
       "4    81 35 bounce rate\\nbreakdown of visitors acqui...   \n",
       "..                                                 ...   \n",
       "949  were starting our black friday offers early wi...   \n",
       "950  imagine spending a lot of money on software li...   \n",
       "951  even better you can try zym for free for 14 da...   \n",
       "952  our new videos coming in 2023 are made to empo...   \n",
       "953  thinkpad p1 gen 4\\nthinkpad z13 gen 1\\nideapad...   \n",
       "\n",
       "                                          summary_BART index_number  \\\n",
       "0    Learn how microsoft empowers organisations to ...         1543   \n",
       "1    webinar with demos of windows 365 and vision f...          765   \n",
       "2    Artificial intelligence ai market is forecast ...          369   \n",
       "3    register now for microsoft envision season 3. ...          895   \n",
       "4    universal analytics will no longer process new...          740   \n",
       "..                                                 ...          ...   \n",
       "949  were starting our black friday offers early wi...         1130   \n",
       "950  iron software is a free open source solution t...          783   \n",
       "951  zym helps business owners understand their mar...          345   \n",
       "952  new videos coming in 2023  made to empower you...          958   \n",
       "953  Free shipping on all orders with up to 40% dis...          132   \n",
       "\n",
       "                                       Tokenized_Email  \\\n",
       "0    {'input_ids': tensor([[  101,  7513, 16270,  4...   \n",
       "1    {'input_ids': tensor([[  101,  4773,  3981,  2...   \n",
       "2    {'input_ids': tensor([[  101,  3193,  3784,  2...   \n",
       "3    {'input_ids': tensor([[  101,  2792,  1015, 14...   \n",
       "4    {'input_ids': tensor([[  101,  5415, 25095,  2...   \n",
       "..                                                 ...   \n",
       "949  {'input_ids': tensor([[  101,  2115,  2063,  2...   \n",
       "950  {'input_ids': tensor([[  101,  7632,  2957,  5...   \n",
       "951  {'input_ids': tensor([[  101,  1062, 24335,  7...   \n",
       "952  {'input_ids': tensor([[  101,  2047,  6876,  2...   \n",
       "953  {'input_ids': tensor([[  101,  3193,  2009,  1...   \n",
       "\n",
       "                                              Entities Cluster_retrieved  \\\n",
       "0    [('microsoft', 'ORG'), ('2022', 'DATE'), ('mic...                 0   \n",
       "1    [('thursday 17th', 'DATE'), ('2022 1400  1500'...                 0   \n",
       "2    [('500 billion', 'MONEY'), ('20231', 'DATE'), ...                 0   \n",
       "3    [('1', 'CARDINAL'), ('13 2022', 'DATE'), ('mic...                 0   \n",
       "4    [('2023', 'DATE'), ('4', 'CARDINAL'), ('septem...                 0   \n",
       "..                                                 ...               ...   \n",
       "949  [('friday', 'DATE'), ('25', 'CARDINAL'), ('25'...                 7   \n",
       "950  [('jeff fritz', 'PERSON'), ('net conf', 'ORG')...                 7   \n",
       "951  [('uk', 'GPE'), ('up to 5000', 'CARDINAL'), ('...                 7   \n",
       "952  [('2023', 'DATE'), ('2023', 'DATE'), ('10815',...                 7   \n",
       "953  [('up to 40', 'CARDINAL'), ('up to 40', 'CARDI...                 7   \n",
       "\n",
       "                                         Summary_human  \n",
       "0    The email discusses post-Microsoft Ignite 2022...  \n",
       "1    Webinar Announcement: \"Windows 365 for Your Hy...  \n",
       "2    The email discusses the rapid growth of artifi...  \n",
       "3    The email announces the premiere of Microsoft ...  \n",
       "4    Starting in 2023, Universal Analytics will no ...  \n",
       "..                                                 ...  \n",
       "949  The email advertises an early Black Friday off...  \n",
       "950  Jeff Fritz from .NET Conf reviewed IronPDF aga...  \n",
       "951  Zym aids business owners in comprehending mark...  \n",
       "952  Summary:\\n\\nClear Measure has announced an upc...  \n",
       "953  Email from Lenovo announces a workspace refres...  \n",
       "\n",
       "[954 rows x 21 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9155dfec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from transformers import T5Tokenizer\n",
    "from dateutil.parser import parse\n",
    "\n",
    "\n",
    "def convert_html_to_text(html):\n",
    "    # Use BeautifulSoup to parse HTML and extract text\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    text = soup.get_text(separator=\" \")\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "def standardize_date_time(text):\n",
    "\n",
    "    # Function to standardize dates\n",
    "    def replace_date(match):\n",
    "        try:\n",
    "            date = parse(match.group(), fuzzy=True)\n",
    "            return date.strftime(\"%Y-%m-%d\")\n",
    "        except ValueError:\n",
    "            # Return the original string if parsing fails\n",
    "            return match.group()\n",
    "\n",
    "    # Function to standardize times\n",
    "    def replace_time(match):\n",
    "        try:\n",
    "            time = parse(match.group(), fuzzy=True)\n",
    "            return time.strftime(\"%H:%M\")\n",
    "        except ValueError:\n",
    "            # Return the original string if parsing fails\n",
    "            return match.group()\n",
    "\n",
    "    # Standardize dates\n",
    "    text = re.sub(r'\\b\\d{1,2}[-/]\\d{1,2}[-/]\\d{2,4}\\b', replace_date, text)\n",
    "    # Standardize times\n",
    "    text = re.sub(r'\\b\\d{1,2}:\\d{2}\\b', replace_time, text)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def preprocess_email_for_t5(email_body):\n",
    "    # Convert HTML to text\n",
    "    email_body = convert_html_to_text(email_body)\n",
    "\n",
    "    # Remove extra whitespaces\n",
    "    email_body = ' '.join(email_body.split())\n",
    "\n",
    "    # Remove common greeting texts\n",
    "    greetings_pattern = (\n",
    "    r\"Hi\\s\\w+|\"\n",
    "    r\"Hello\\s\\w+|\"\n",
    "    r\"Dear\\s\\w+|\"\n",
    "    r\"Dear\\s[Mm]r\\.\\s\\w+|\"\n",
    "    r\"Dear\\s[Mm]rs\\.\\s\\w+|\"\n",
    "    r\"Dear\\s[Mm]s\\.\\s\\w+|\"\n",
    "    r\"Dear\\s[MD]r(s)?\\.\\s\\w+|\"\n",
    "    r\"Dear\\sProf\\.\\s\\w+|\"\n",
    "    r\"Dear\\sDoctor\\.\\s\\w+|\"\n",
    "    r\"Greetings|\"\n",
    "    r\"Good\\s[Mm]orning|\"\n",
    "    r\"Good\\s[Aa]fternoon|\"\n",
    "    r\"Good\\s[Ee]vening|\"\n",
    "    r\"Hey\\s\\w+|\"\n",
    "    r\"Hey\\sthere|\"\n",
    "    r\"Hello\\severyone|\"\n",
    "    r\"Hope\\syou\\sare\\sdoing\\swell|\"\n",
    "    r\"Hope\\syou\\sare\\sdoing\\swell|\"\n",
    "    r\"Hope\\sthis\\semail\\sfinds\\syou\\swell|\"\n",
    "    r\"Hope\\sthis\\semail\\sfinds\\syou\\swell|\"\n",
    "    r\"Hope\\sthis\\semail\\sfinds\\syou\\sin\\sgood\\shealth|\"\n",
    "    r\"How\\sare\\syou\\sdoing|\"\n",
    "    r\"How\\sis\\sit\\sgoing|\"\n",
    "    r\"To\\swhom\\sit\\smay\\sconcern|\"\n",
    "    r\"To\\swhom\\sit\\smay\\sconcern|\"\n",
    "    r\"I\\sam\\swriting\\sto\\syou\\sbecause|\"\n",
    "    r\"I\\sam\\swriting\\sto\\syou\\sto|\"\n",
    "    r\"I\\shope\\sthat\\syou|\"\n",
    "    r\"I\\swanted\\sto\\sreach\\sout\\sto|\"\n",
    "    r\"I\\swanted\\sto\\slet\\syou\\sknow|\"\n",
    "    r\"I\\swould\\slike\\sto\\sinform\\syou|\"\n",
    "    r\"It\\spleases\\sme\\sto\\scontact\\syou|\"\n",
    "    r\"It\\shas\\scome\\sto\\smy\\sattention|\"\n",
    "    r\"I\\swas\\sjust\\sthinking\\sabout\\syou\\sand\\s|\"\n",
    "    r\"Allow\\sme\\sto\\sintroduce\\smyself|\"\n",
    "    r\"I\\shope\\sthat\\severything\\sis\\sgoing\\swell|\"\n",
    "    r\"Thank\\syou\\sfor\\syour\\semail|\"\n",
    "    r\"Thank\\syou\\sfor\\sreaching\\sout\")\n",
    "\n",
    "    email_body = re.sub(greetings_pattern, \"\", email_body, flags=re.IGNORECASE)\n",
    "\n",
    "    # Remove common sign-offs\n",
    "    signoffs_pattern = (\n",
    "    r\"Best\\sregards|\"\n",
    "    r\"Best\\s\\w+|\"\n",
    "    r\"Sincerely|\"\n",
    "    r\"Yours\\sfaithfully|\"\n",
    "    r\"Yours\\ssincerely|\"\n",
    "    r\"Regards|\"\n",
    "    r\"Warm\\sregards|\"\n",
    "    r\"Kind\\sregards|\"\n",
    "    r\"Cheers|\"\n",
    "    r\"Thanks\\sand\\sregards|\"\n",
    "    r\"Thank\\syou|\"\n",
    "    r\"Take\\scare|\"\n",
    "    r\"Looking\\sforward|\"\n",
    "    r\"All\\sbest|\"\n",
    "    r\"Best\\swishes|\"\n",
    "    r\"Best|\"\n",
    "    r\"Yours\\struly|\"\n",
    "    r\"Cordially|\"\n",
    "    r\"With\\sappreciation|\"\n",
    "    r\"Respectfully|\"\n",
    "    r\"With\\sregards|\"\n",
    "    r\"Many\\sthanks|\"\n",
    "    r\"Hope\\sto\\shear\\sfrom\\syou\\ssoon|\"\n",
    "    r\"Until\\snext\\stime|\"\n",
    "    r\"Yours\\svery\\struly|\"\n",
    "    r\"Yours|\"\n",
    "    r\"In\\sgratitude|\"\n",
    "    r\"In\\ssympathy|\"\n",
    "    r\"Thoughtfully|\"\n",
    "    r\"With\\saffection|\"\n",
    "    r\"Fond\\sregards|\"\n",
    "    r\"With\\santicipation|\"\n",
    "    r\"Stay\\swell|\"\n",
    "    r\"Stay\\ssafe|\"\n",
    "    r\"Peace|\"\n",
    "    r\"God\\sbless|\"\n",
    "    r\"Love|\"\n",
    "    r\"Sent\\sfrom\\smy\\s\\w+|\"\n",
    "    r\"Sent\\sfrom\\smy\\s\\w+\\s\\w+|\"\n",
    "    r\"Talk\\sto\\syou\\ssoon|\"\n",
    "    r\"See\\syou\\ssoon|\"\n",
    "    r\"See\\sya|\"\n",
    "    r\"Ciao|\"\n",
    "    r\"Adieu|\"\n",
    "    r\"Farewell|\"\n",
    "    r\"Good\\sbye|\"\n",
    "    r\"Bye\\sfor\\snow|\"\n",
    "    r\"Signing\\soff|\"\n",
    "    r\"Out|\"\n",
    "    r\"Yours\\s[in]\\s\\w+|\"\n",
    "    r\"Your\\sfriend|\"\n",
    "    r\"Your\\s\\w+\\s\\w+|\"\n",
    "    r\"Keep\\sin\\stouch|\"\n",
    "    r\"Yours\\sfaithfully|\"\n",
    "    r\"Yours\\ssincerely|\"\n",
    "    r\"Yours\\sobediently|\"\n",
    "    r\"Yours\\saffectionately|\"\n",
    "    r\"Yours\\scordially|\"\n",
    "    r\"Yours\\srespectfully|\"\n",
    "    r\"Yours\\struly|\"\n",
    "    r\"Yours\\sever\")\n",
    "\n",
    "    email_body = re.sub(signoffs_pattern, \"\", email_body, flags=re.IGNORECASE)\n",
    "\n",
    "    # Pattern to remove repetitive characters like dashes or underscores\n",
    "    repetitive_pattern = r\"[-_]{3,}\"  \n",
    "    email_body = re.sub(repetitive_pattern, \"\", email_body)\n",
    "\n",
    "    # Standardize dates and times\n",
    "    email_body = standardize_date_time(email_body)\n",
    "\n",
    "    # Remove email signatures and disclaimers\n",
    "    email_body = re.sub(r\"--\\s*[\\s\\S]*$\", \"\", email_body)\n",
    "\n",
    "    # Standardize email addresses and URLs\n",
    "    email_body = re.sub(r\"\\S+@\\S+\\.\\S+\", \"<email>\", email_body)\n",
    "    email_body = re.sub(r\"http\\S+\", \"<url>\", email_body)\n",
    "\n",
    "    # Remove phrases indicating difficulty in viewing images or links\n",
    "    irrelevant_phrases_pattern = (\n",
    "    r\"difficulty in viewing this image|\"\n",
    "    r\"click here|\"\n",
    "    r\"having trouble viewing this|\"\n",
    "    r\"view this email in your browser|\"\n",
    "    r\"to ensure delivery to your inbox|\"\n",
    "    r\"if you cannot see this message|\"\n",
    "    r\"message not displaying correctly|\"\n",
    "    r\"trouble seeing this email|\"\n",
    "    r\"can't see the images below|\"\n",
    "    r\"email not looking quite right|\"\n",
    "    r\"viewing this email on a mobile device|\"\n",
    "    r\"can't read this email|\"\n",
    "    r\"images not showing up|\"\n",
    "    r\"to view the online version of this email|\"\n",
    "    r\"email doesn't display correctly|\"\n",
    "    r\"problems seeing this email|\"\n",
    "    r\"to unsubscribe or change preferences|\"\n",
    "    r\"this message was sent to <email>|\"\n",
    "    r\"not interested in these emails|\"\n",
    "    r\"you're receiving this email because|\"\n",
    "    r\"to stop receiving these emails|\"\n",
    "    r\"unsubscribe from this list|\"\n",
    "    r\"manage your email preferences\")\n",
    "    email_body = re.sub(irrelevant_phrases_pattern, \"\", email_body, flags=re.IGNORECASE)\n",
    "\n",
    "    # Add task-specific prefix (if you are going to use this text directly for T5 summarization later)\n",
    "    email_body = \"summarize: \" + email_body\n",
    "\n",
    "    return email_body\n",
    "\n",
    "\n",
    "# Apply the preprocessing function to the 'body' column\n",
    "df['Preprocessed_Body'] = df['body'].apply(lambda x: preprocess_email_for_t5(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3de5f1da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\764883\\AppData\\Local\\Temp\\ipykernel_28544\\4234254842.py:8: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(html, \"html.parser\")\n"
     ]
    }
   ],
   "source": [
    "def preprocess_summary_for_t5(summary):\n",
    "    summary = convert_html_to_text(summary)\n",
    "\n",
    "    # Remove extra whitespaces\n",
    "    summary = ' '.join(summary.split())\n",
    "\n",
    "    # Remove common greeting texts\n",
    "    greetings_pattern = (\n",
    "    r\"Hi\\s\\w+|\"\n",
    "    r\"Hello\\s\\w+|\"\n",
    "    r\"Dear\\s\\w+|\"\n",
    "    r\"Dear\\s[Mm]r\\.\\s\\w+|\"\n",
    "    r\"Dear\\s[Mm]rs\\.\\s\\w+|\"\n",
    "    r\"Dear\\s[Mm]s\\.\\s\\w+|\"\n",
    "    r\"Dear\\s[MD]r(s)?\\.\\s\\w+|\"\n",
    "    r\"Dear\\sProf\\.\\s\\w+|\"\n",
    "    r\"Dear\\sDoctor\\.\\s\\w+|\"\n",
    "    r\"Greetings|\"\n",
    "    r\"Good\\s[Mm]orning|\"\n",
    "    r\"Good\\s[Aa]fternoon|\"\n",
    "    r\"Good\\s[Ee]vening|\"\n",
    "    r\"Hey\\s\\w+|\"\n",
    "    r\"Hey\\sthere|\"\n",
    "    r\"Hello\\severyone|\"\n",
    "    r\"Hope\\syou\\sare\\sdoing\\swell|\"\n",
    "    r\"Hope\\syou\\sare\\sdoing\\swell|\"\n",
    "    r\"Hope\\sthis\\semail\\sfinds\\syou\\swell|\"\n",
    "    r\"Hope\\sthis\\semail\\sfinds\\syou\\swell|\"\n",
    "    r\"Hope\\sthis\\semail\\sfinds\\syou\\sin\\sgood\\shealth|\"\n",
    "    r\"How\\sare\\syou\\sdoing|\"\n",
    "    r\"How\\sis\\sit\\sgoing|\"\n",
    "    r\"To\\swhom\\sit\\smay\\sconcern|\"\n",
    "    r\"To\\swhom\\sit\\smay\\sconcern|\"\n",
    "    r\"I\\sam\\swriting\\sto\\syou\\sbecause|\"\n",
    "    r\"I\\sam\\swriting\\sto\\syou\\sto|\"\n",
    "    r\"I\\shope\\sthat\\syou|\"\n",
    "    r\"I\\swanted\\sto\\sreach\\sout\\sto|\"\n",
    "    r\"I\\swanted\\sto\\slet\\syou\\sknow|\"\n",
    "    r\"I\\swould\\slike\\sto\\sinform\\syou|\"\n",
    "    r\"It\\spleases\\sme\\sto\\scontact\\syou|\"\n",
    "    r\"It\\shas\\scome\\sto\\smy\\sattention|\"\n",
    "    r\"I\\swas\\sjust\\sthinking\\sabout\\syou\\sand\\s|\"\n",
    "    r\"Allow\\sme\\sto\\sintroduce\\smyself|\"\n",
    "    r\"I\\shope\\sthat\\severything\\sis\\sgoing\\swell|\"\n",
    "    r\"Thank\\syou\\sfor\\syour\\semail|\"\n",
    "    r\"Thank\\syou\\sfor\\sreaching\\sout\")\n",
    "\n",
    "    summary = re.sub(greetings_pattern, \"\", summary, flags=re.IGNORECASE)\n",
    "\n",
    "    # Remove common sign-offs\n",
    "    signoffs_pattern = (\n",
    "    r\"Best\\sregards|\"\n",
    "    r\"Best\\s\\w+|\"\n",
    "    r\"Sincerely|\"\n",
    "    r\"Yours\\sfaithfully|\"\n",
    "    r\"Yours\\ssincerely|\"\n",
    "    r\"Regards|\"\n",
    "    r\"Warm\\sregards|\"\n",
    "    r\"Kind\\sregards|\"\n",
    "    r\"Cheers|\"\n",
    "    r\"Thanks\\sand\\sregards|\"\n",
    "    r\"Thank\\syou|\"\n",
    "    r\"Take\\scare|\"\n",
    "    r\"Looking\\sforward|\"\n",
    "    r\"All\\sbest|\"\n",
    "    r\"Best\\swishes|\"\n",
    "    r\"Best|\"\n",
    "    r\"Yours\\struly|\"\n",
    "    r\"Cordially|\"\n",
    "    r\"With\\sappreciation|\"\n",
    "    r\"Respectfully|\"\n",
    "    r\"With\\sregards|\"\n",
    "    r\"Many\\sthanks|\"\n",
    "    r\"Hope\\sto\\shear\\sfrom\\syou\\ssoon|\"\n",
    "    r\"Until\\snext\\stime|\"\n",
    "    r\"Yours\\svery\\struly|\"\n",
    "    r\"Yours|\"\n",
    "    r\"In\\sgratitude|\"\n",
    "    r\"In\\ssympathy|\"\n",
    "    r\"Thoughtfully|\"\n",
    "    r\"With\\saffection|\"\n",
    "    r\"Fond\\sregards|\"\n",
    "    r\"With\\santicipation|\"\n",
    "    r\"Stay\\swell|\"\n",
    "    r\"Stay\\ssafe|\"\n",
    "    r\"Peace|\"\n",
    "    r\"God\\sbless|\"\n",
    "    r\"Love|\"\n",
    "    r\"Sent\\sfrom\\smy\\s\\w+|\"\n",
    "    r\"Sent\\sfrom\\smy\\s\\w+\\s\\w+|\"\n",
    "    r\"Talk\\sto\\syou\\ssoon|\"\n",
    "    r\"See\\syou\\ssoon|\"\n",
    "    r\"See\\sya|\"\n",
    "    r\"Ciao|\"\n",
    "    r\"Adieu|\"\n",
    "    r\"Farewell|\"\n",
    "    r\"Good\\sbye|\"\n",
    "    r\"Bye\\sfor\\snow|\"\n",
    "    r\"Signing\\soff|\"\n",
    "    r\"Out|\"\n",
    "    r\"Yours\\s[in]\\s\\w+|\"\n",
    "    r\"Your\\sfriend|\"\n",
    "    r\"Your\\s\\w+\\s\\w+|\"\n",
    "    r\"Keep\\sin\\stouch|\"\n",
    "    r\"Yours\\sfaithfully|\"\n",
    "    r\"Yours\\ssincerely|\"\n",
    "    r\"Yours\\sobediently|\"\n",
    "    r\"Yours\\saffectionately|\"\n",
    "    r\"Yours\\scordially|\"\n",
    "    r\"Yours\\srespectfully|\"\n",
    "    r\"Yours\\struly|\"\n",
    "    r\"Yours\\sever\")\n",
    "\n",
    "    summary = re.sub(signoffs_pattern, \"\", summary, flags=re.IGNORECASE)\n",
    "\n",
    "    # Pattern to remove repetitive characters like dashes or underscores\n",
    "    repetitive_pattern = r\"[-_]{3,}\"  # Adjust the number based on your observation, here it's set to match 3 or more\n",
    "    summary = re.sub(repetitive_pattern, \"\", summary)\n",
    "\n",
    "    # Standardize dates and times\n",
    "    summary = standardize_date_time(summary)\n",
    "\n",
    "    # Remove email signatures and disclaimers\n",
    "    summary = re.sub(r\"--\\s*[\\s\\S]*$\", \"\", summary)\n",
    "\n",
    "    # Standardize email addresses and URLs\n",
    "    summary = re.sub(r\"\\S+@\\S+\\.\\S+\", \"<email>\", summary)\n",
    "    summary = re.sub(r\"http\\S+\", \"<url>\", summary)\n",
    "\n",
    "    # Remove phrases indicating difficulty in viewing images or links\n",
    "    irrelevant_phrases_pattern = (\n",
    "    r\"difficulty in viewing this image|\"\n",
    "    r\"click here|\"\n",
    "    r\"having trouble viewing this|\"\n",
    "    r\"view this email in your browser|\"\n",
    "    r\"to ensure delivery to your inbox|\"\n",
    "    r\"if you cannot see this message|\"\n",
    "    r\"message not displaying correctly|\"\n",
    "    r\"trouble seeing this email|\"\n",
    "    r\"can't see the images below|\"\n",
    "    r\"email not looking quite right|\"\n",
    "    r\"viewing this email on a mobile device|\"\n",
    "    r\"can't read this email|\"\n",
    "    r\"images not showing up|\"\n",
    "    r\"to view the online version of this email|\"\n",
    "    r\"email doesn't display correctly|\"\n",
    "    r\"problems seeing this email|\"\n",
    "    r\"to unsubscribe or change preferences|\"\n",
    "    r\"this message was sent to <email>|\"\n",
    "    r\"not interested in these emails|\"\n",
    "    r\"you're receiving this email because|\"\n",
    "    r\"to stop receiving these emails|\"\n",
    "    r\"unsubscribe from this list|\"\n",
    "    r\"manage your email preferences\")\n",
    "    summary = re.sub(irrelevant_phrases_pattern, \"\", summary, flags=re.IGNORECASE)\n",
    "\n",
    "    # Add task-specific prefix (if you are going to use this text directly for T5 summarization later)\n",
    "    email_body = \"summarize: \" + email_body\n",
    "\n",
    "    return summary\n",
    "df['Preprocessed_Summary'] = df['Summary_human'].apply(lambda x: preprocess_summary_for_t5(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3dc0a2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      Microsoft Ignite may be over, but here’s  cont...\n",
       "1      Webinar with demos of Windows 365 and vision f...\n",
       "2      View Online Why AI and why now ? Why AI and wh...\n",
       "3      Episode 1 airs December 13, 2022  email? | Vie...\n",
       "4      Universal Analytics will no longer process new...\n",
       "                             ...                        \n",
       "949    You’re not going to want to miss these savings...\n",
       "950    , Imagine spending a lot of money on software ...\n",
       "951    ZYM helps business owners understand their mar...\n",
       "952    New videos coming in 2023 … made to empower . ...\n",
       "953    View it in browser instead Free shipping on al...\n",
       "Name: Preprocessed_Body, Length: 954, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Preprocessed_Body']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6773710",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      The email discusses post-Microsoft Ignite 2022...\n",
       "1      Webinar Announcement: \"Windows 365 for \" event...\n",
       "2      The email discusses the rapid growth of artifi...\n",
       "3      The email announces the premiere of Microsoft ...\n",
       "4      Starting in 2023, Universal Analytics will no ...\n",
       "                             ...                        \n",
       "949    The email advertises an early Black Friday off...\n",
       "950    Jeff Fritz from .NET Conf reviewed IronPDF aga...\n",
       "951    Zym aids business owners in comprehending mark...\n",
       "952    Summary: Clear Measure has announced an upcomi...\n",
       "953    Email from Lenovo announces a workspace refres...\n",
       "Name: Preprocessed_Summary, Length: 954, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Preprocessed_Summary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "58128b0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for empty or NaN entries in 'Preprocessed_Body'\n",
    "empty_data_count = df['Preprocessed_Body'].isna().sum() + (df['Preprocessed_Body'] == '').sum()\n",
    "\n",
    "empty_data_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f15f9f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for empty or NaN entries in 'Preprocessed_Body'\n",
    "empty_data_count = df['Preprocessed_Summary'].isna().sum() + (df['Preprocessed_Summary'] == '').sum()\n",
    "\n",
    "empty_data_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "333369d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import EvalPrediction\n",
    "from datasets import load_metric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1b32e176",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import LongT5ForConditionalGeneration, T5Tokenizer, TrainingArguments, Trainer, AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "from time import sleep\n",
    "\n",
    "\n",
    "\n",
    "# Define the dataset class\n",
    "class EmailSummarizationDataset(Dataset):\n",
    "    def __init__(self, tokenizer, dataframe, max_input_length=1500, max_target_length=150):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = df\n",
    "        self.max_input_length = max_input_length\n",
    "        self.max_target_length = max_target_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data_row = self.data.iloc[index]\n",
    "\n",
    "        source_encoding = tokenizer(\n",
    "            data_row['Preprocessed_Body'], \n",
    "            max_length=self.max_input_length, \n",
    "            padding='max_length', \n",
    "            truncation=True, \n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        target_encoding = tokenizer(\n",
    "            data_row['Preprocessed_Summary'], \n",
    "            max_length=self.max_target_length, \n",
    "            padding='max_length', \n",
    "            truncation=True, \n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        labels = target_encoding.input_ids\n",
    "        labels[labels == 0] = -100\n",
    "\n",
    "        return dict(\n",
    "            input_ids=source_encoding.input_ids.flatten(),\n",
    "            attention_mask=source_encoding.attention_mask.flatten(),\n",
    "            labels=labels.flatten()\n",
    "        )\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/long-t5-local-base\")\n",
    "model = LongT5ForConditionalGeneration.from_pretrained(\"google/long-t5-tglobal-base\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "41a634be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/long-t5-local-base\")\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_df, test_df = train_test_split(df, test_size=0.2)\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "# Prepare the training and testing datasets\n",
    "train_dataset = EmailSummarizationDataset(tokenizer, train_df)\n",
    "test_dataset = EmailSummarizationDataset(tokenizer, test_df)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 2  # You can adjust this based on your GPU's memory\n",
    "\n",
    "# Create DataLoaders for training and testing datasets\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1d7472ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback\n",
    "import numpy as np\n",
    "from rouge_score import rouge_scorer, scoring\n",
    "from bert_score import score\n",
    "import torch\n",
    "\n",
    "class CustomEvaluationCallback(TrainerCallback):\n",
    "    def __init__(self, eval_dataset, tokenizer, device):\n",
    "        self.eval_dataset = test_dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "\n",
    "    def on_evaluate(self, args, state, control, **kwargs):\n",
    "        model = kwargs[\"model\"]\n",
    "        predictions, references = [], []\n",
    "\n",
    "        for batch in self.eval_dataset:\n",
    "            inputs = batch['input_ids'].to(self.device)\n",
    "#             attention_mask = batch.get('attention_mask', None).to(self.device) if 'attention_mask' in batch else None\n",
    "            if 'attention_mask' in batch:\n",
    "                attention_mask = batch['attention_mask'].to(self.device)\n",
    "            else:\n",
    "                attention_mask = None\n",
    "\n",
    "            if len(inputs.shape) == 1:\n",
    "                inputs = inputs.unsqueeze(0)\n",
    "            if attention_mask is not None and len(attention_mask.shape) == 1:\n",
    "                attention_mask = attention_mask.unsqueeze(0)\n",
    "\n",
    "            outputs = model.generate(inputs, attention_mask=attention_mask)\n",
    "            batch_predictions = [self.tokenizer.decode(o, skip_special_tokens=True) for o in outputs]\n",
    "\n",
    "            for i in range(len(batch_predictions)):\n",
    "                predictions.append(batch_predictions[i])\n",
    "                references.append(self.tokenizer.decode(batch['labels'][i], skip_special_tokens=True))\n",
    "\n",
    "        # Compute ROUGE and BERTScore\n",
    "        rouge_scores = self.compute_rouge(references, predictions)\n",
    "        bert_scores = self.compute_bertScore(references, predictions)\n",
    "        \n",
    "\n",
    "        # Print formatted scores\n",
    "        self.print_formatted_scores(rouge_scores, bert_scores, state.epoch)\n",
    "\n",
    "        return control\n",
    "\n",
    "    def compute_rouge(self, targets, predictions, score_keys=None, use_stemmer=True):\n",
    "        if score_keys is None:\n",
    "            score_keys = [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]\n",
    "        scorer = rouge_scorer.RougeScorer(score_keys, use_stemmer=use_stemmer)\n",
    "        aggregator = scoring.BootstrapAggregator()\n",
    "\n",
    "        for target, prediction in zip(targets, predictions):\n",
    "            aggregator.add_scores(scorer.score(target=target, prediction=prediction))\n",
    "        result = aggregator.aggregate()\n",
    "\n",
    "        return {key: result[key].mid.fmeasure  for key in score_keys}\n",
    "\n",
    "    def compute_bertScore(self, refs, cands, rescale_with_baseline=True):\n",
    "        P, R, F1 = score(cands, refs, lang=\"en\", rescale_with_baseline=rescale_with_baseline)\n",
    "        return {\"bertScore\": F1.mean().item() }\n",
    "\n",
    "    def print_formatted_scores(self, rouge_scores, bert_scores, epoch):\n",
    "    # Headers for the scores\n",
    "        headers = [\"Epoch\"] + list(rouge_scores.keys()) + [\"bertScore\"]\n",
    "    \n",
    "    # Values for the scores\n",
    "        values = [f\"{epoch}\"] + [f\"{score:.2f}\" for score in rouge_scores.values()] + [f\"{bert_scores['bertScore']:.2f}\"]\n",
    "\n",
    "    # Calculate the max width for each column\n",
    "        column_widths = [max(len(header), len(value)) + 2 for header, value in zip(headers, values)]\n",
    "\n",
    "    # Print the header\n",
    "        header_row = \" | \".join(f\"{header:<{column_widths[idx]}}\" for idx, header in enumerate(headers))\n",
    "        print(header_row)\n",
    "        print(\"-\" * len(header_row))\n",
    "\n",
    "    # Print the scores\n",
    "        score_row = \" | \".join(f\"{value:<{column_widths[idx]}}\" for idx, value in enumerate(values))\n",
    "        print(score_row)\n",
    "        print(\"\\n\" + \"-\" * len(header_row))\n",
    "\n",
    "# Usage\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "custom_eval_callback = CustomEvaluationCallback(test_dataset, tokenizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e9142ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LongT5ForConditionalGeneration were not initialized from the model checkpoint at google/long-t5-local-base and are newly initialized: ['lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LongT5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 768)\n",
       "  (encoder): LongT5Stack(\n",
       "    (embed_tokens): Embedding(32128, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): LongT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): LongT5LayerLocalSelfAttention(\n",
       "            (LocalSelfAttention): LongT5LocalAttention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): LongT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): LongT5LayerFF(\n",
       "            (DenseReluDense): LongT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): LongT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x LongT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): LongT5LayerLocalSelfAttention(\n",
       "            (LocalSelfAttention): LongT5LocalAttention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): LongT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): LongT5LayerFF(\n",
       "            (DenseReluDense): LongT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): LongT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LongT5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): LongT5Stack(\n",
       "    (embed_tokens): Embedding(32128, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): LongT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): LongT5LayerSelfAttention(\n",
       "            (SelfAttention): LongT5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): LongT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): LongT5LayerCrossAttention(\n",
       "            (EncDecAttention): LongT5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): LongT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): LongT5LayerFF(\n",
       "            (DenseReluDense): LongT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): LongT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x LongT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): LongT5LayerSelfAttention(\n",
       "            (SelfAttention): LongT5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): LongT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): LongT5LayerCrossAttention(\n",
       "            (EncDecAttention): LongT5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): LongT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): LongT5LayerFF(\n",
       "            (DenseReluDense): LongT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): LongT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LongT5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the LongT5 model\n",
    "model = LongT5ForConditionalGeneration.from_pretrained(\"google/long-t5-local-base\")\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "019d2cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load metrics\n",
    "rouge_metric = load_metric(\"rouge\")\n",
    "bert_metric = load_metric(\"bertscore\")\n",
    "\n",
    "\n",
    "\n",
    "from transformers import EvalPrediction\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    # Extract predictions\n",
    "    predictions = p.predictions\n",
    "    if isinstance(predictions, tuple):\n",
    "        predictions = predictions[0]  # Assuming the first element of the tuple contains the logits\n",
    "\n",
    "    # Check if the predictions are logits (3D tensor) or token IDs (2D tensor)\n",
    "    if len(predictions.shape) == 3:\n",
    "        # Convert logits to token IDs\n",
    "        preds = np.argmax(predictions, axis=-1)\n",
    "    else:\n",
    "        # Predictions are already token IDs\n",
    "        preds = predictions\n",
    "\n",
    "    # Flatten the predictions and labels if they are in nested lists\n",
    "    flat_preds = [item for sublist in preds for item in sublist] if isinstance(preds[0], list) else preds\n",
    "    flat_labels = [item for sublist in p.label_ids for item in sublist] if isinstance(p.label_ids[0], list) else p.label_ids\n",
    "\n",
    "    # Decode the predictions and labels\n",
    "    decoded_preds = tokenizer.batch_decode(flat_preds, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(flat_labels, skip_special_tokens=True)\n",
    "\n",
    "    # Compute ROUGE and BERTScore\n",
    "    rouge_result = rouge_metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    bert_result = bert_metric.compute(predictions=decoded_preds, references=decoded_labels, lang=\"en\")\n",
    "\n",
    "    # Format the results\n",
    "    rouge_result = {key: value.mid.fmeasure for key, value in rouge_result.items()}\n",
    "    bert_result = {key: sum([v for v in value if isinstance(v, (int, float))]) / len(value)  for key, value in bert_result.items()}\n",
    "\n",
    "\n",
    "    return {**rouge_result, **bert_result}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "704be14c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='45' max='954' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 45/954 04:16 < 1:30:20, 0.17 it/s, Epoch 0.09/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 81\u001b[0m\n\u001b[0;32m     70\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m     71\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     72\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;66;03m#     data_collator=custom_data_collator\u001b[39;00m\n\u001b[0;32m     78\u001b[0m )\n\u001b[0;32m     80\u001b[0m \u001b[38;5;66;03m# Start training\u001b[39;00m\n\u001b[1;32m---> 81\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;66;03m# Plotting\u001b[39;00m\n\u001b[0;32m     83\u001b[0m training_logs \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mlog_history\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\trainer.py:1555\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1553\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   1554\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1555\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1556\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1557\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1558\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1559\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1560\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\trainer.py:1860\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1857\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m   1859\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[1;32m-> 1860\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1862\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   1863\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   1864\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[0;32m   1865\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   1866\u001b[0m ):\n\u001b[0;32m   1867\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   1868\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\trainer.py:2725\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   2722\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m   2724\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[1;32m-> 2725\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2727\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mn_gpu \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   2728\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()  \u001b[38;5;66;03m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\trainer.py:2748\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[1;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[0;32m   2746\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2747\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 2748\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2749\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[0;32m   2750\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[0;32m   2751\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\models\\longt5\\modeling_longt5.py:2021\u001b[0m, in \u001b[0;36mLongT5ForConditionalGeneration.forward\u001b[1;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   2018\u001b[0m \u001b[38;5;66;03m# Encode if needed (training, first prediction pass)\u001b[39;00m\n\u001b[0;32m   2019\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m encoder_outputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2020\u001b[0m     \u001b[38;5;66;03m# Convert encoder inputs in embeddings if needed\u001b[39;00m\n\u001b[1;32m-> 2021\u001b[0m     encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2022\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2023\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2024\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2025\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2026\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2027\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2028\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2029\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2030\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(encoder_outputs, BaseModelOutput):\n\u001b[0;32m   2031\u001b[0m     encoder_outputs \u001b[38;5;241m=\u001b[39m BaseModelOutput(\n\u001b[0;32m   2032\u001b[0m         last_hidden_state\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m   2033\u001b[0m         hidden_states\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(encoder_outputs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   2034\u001b[0m         attentions\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(encoder_outputs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   2035\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\models\\longt5\\modeling_longt5.py:1521\u001b[0m, in \u001b[0;36mLongT5Stack.forward\u001b[1;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1506\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m   1507\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39mforward,\n\u001b[0;32m   1508\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1518\u001b[0m         output_attentions,\n\u001b[0;32m   1519\u001b[0m     )\n\u001b[0;32m   1520\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1521\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1523\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1524\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1525\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1526\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1527\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1528\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1530\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1531\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1532\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1533\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1535\u001b[0m \u001b[38;5;66;03m# layer_outputs is a tuple with:\u001b[39;00m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\models\\longt5\\modeling_longt5.py:1203\u001b[0m, in \u001b[0;36mLongT5Block.forward\u001b[1;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict)\u001b[0m\n\u001b[0;32m   1200\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1201\u001b[0m     self_attn_past_key_value, cross_attn_past_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1203\u001b[0m self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1204\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1205\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1206\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1207\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1208\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1209\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1210\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1211\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1212\u001b[0m hidden_states, present_key_value_state \u001b[38;5;241m=\u001b[39m self_attention_outputs[:\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m   1213\u001b[0m attention_outputs \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m2\u001b[39m:]  \u001b[38;5;66;03m# Keep self-attention outputs and relative position weights\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\models\\longt5\\modeling_longt5.py:1067\u001b[0m, in \u001b[0;36mLongT5LayerLocalSelfAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, position_bias, layer_head_mask, output_attentions, **kwargs)\u001b[0m\n\u001b[0;32m   1057\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m   1058\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1059\u001b[0m     hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1064\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,  \u001b[38;5;66;03m# to accept past_key_value and use_cache kwargs\u001b[39;00m\n\u001b[0;32m   1065\u001b[0m ):\n\u001b[0;32m   1066\u001b[0m     normed_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm(hidden_states)\n\u001b[1;32m-> 1067\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLocalSelfAttention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1068\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnormed_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1069\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1070\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1071\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1072\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1073\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1074\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(attention_output[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m   1075\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (hidden_states,) \u001b[38;5;241m+\u001b[39m attention_output[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\models\\longt5\\modeling_longt5.py:732\u001b[0m, in \u001b[0;36mLongT5LocalAttention.forward\u001b[1;34m(self, hidden_states, mask, position_bias, layer_head_mask, output_attentions)\u001b[0m\n\u001b[0;32m    730\u001b[0m scores \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m position_bias\n\u001b[0;32m    731\u001b[0m \u001b[38;5;66;03m# (batch_size, num_blocks, n_heads, block_len, 3 * block_len)\u001b[39;00m\n\u001b[1;32m--> 732\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39msoftmax(\u001b[43mscores\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mtype_as(scores)\n\u001b[0;32m    733\u001b[0m \u001b[38;5;66;03m# (batch_size, num_blocks, n_heads, block_len, 3 * block_len)\u001b[39;00m\n\u001b[0;32m    734\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mdropout(attn_weights, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Calculate the size of the training dataset\n",
    "training_dataset_size = len(train_dataset)\n",
    "\n",
    "N = training_dataset_size\n",
    "batch_size = 2  \n",
    "\n",
    "# Calculate steps per epoch\n",
    "steps_per_epoch = N // batch_size\n",
    "\n",
    "\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",                      # Directory for storing outputs\n",
    "    num_train_epochs= 2,                         # Set number of epochs to 10\n",
    "    per_device_train_batch_size=2,               # Batch size for training\n",
    "    per_device_eval_batch_size=2,                # Batch size for evaluation\n",
    "    warmup_steps=500,                            # Number of warmup steps\n",
    "    weight_decay=0.01,                           # Weight decay for regularization\n",
    "    logging_dir=\"./logs\",                        # Directory for storing logs\n",
    "    logging_steps=steps_per_epoch,                            # Log every steps in epoch\n",
    "    do_train=True,                               # Enable training\n",
    "    do_eval=True,\n",
    "    #gradient_accumulation_steps=4,# Enable evaluation\n",
    "    evaluation_strategy=\"epoch\",                 # Evaluate at the end of each epoch\n",
    "    save_strategy=\"no\",                    # Disable saving the model\n",
    "    save_total_limit=0,                 # Limit on the total amount of checkpoints. Setting to 0 disables it\n",
    "    \n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,  # Use the dataset instance for training data\n",
    "    eval_dataset=test_dataset,     # Use the dataset instance for validation data\n",
    "    compute_metrics=compute_metrics,  # Custom callback function for metrics\n",
    "    optimizers=(AdamW(model.parameters(), lr=1e-4), None),  # AdamW optimizer with a specified learning rate\n",
    "#     data_collator=custom_data_collator\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n",
    "# Plotting\n",
    "training_logs = trainer.state.log_history\n",
    "\n",
    "training_loss = [log['loss'] for log in training_logs if 'loss' in log]\n",
    "validation_loss = [log['eval_loss'] for log in training_logs if 'eval_loss' in log]\n",
    "epochs = range(1, len(training_loss) + 1)\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(epochs, training_loss, 'bo-', label='Training loss')\n",
    "plt.plot(epochs, validation_loss, 'ro-', label='Validation loss')\n",
    "plt.title('Training and Validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc6fc636",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Since DAIM pc's are i9 12th gen with more cores and threads, num_workers can be kept ot upto 16 based on the number of cores could speedup the process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e19e757",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained('google/long-t5-local-base')\n",
    "\n",
    "\n",
    "# Split the data into training, validation, and testing sets\n",
    "train_df, temp_df = train_test_split(df, test_size=0.3, random_state=42)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "# Prepare the datasets\n",
    "train_dataset = EmailSummarizationDataset(tokenizer, train_df)\n",
    "val_dataset = EmailSummarizationDataset(tokenizer, val_df)\n",
    "test_dataset = EmailSummarizationDataset(tokenizer, test_df)\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 2  # Adjust based on  GPU's memory constantly has to be monitorwed. \n",
    "\n",
    "# Create DataLoaders for datasets\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,num_workers=4)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size,num_workers=4)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size,num_workers=4)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8176a3c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.EmailSummarizationDataset at 0x210a7611610>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3c86a91d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.EmailSummarizationDataset at 0x1dc02ccd390>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "853ec270",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LongT5ForConditionalGeneration were not initialized from the model checkpoint at google/long-t5-local-base and are newly initialized: ['lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LongT5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 768)\n",
       "  (encoder): LongT5Stack(\n",
       "    (embed_tokens): Embedding(32128, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): LongT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): LongT5LayerLocalSelfAttention(\n",
       "            (LocalSelfAttention): LongT5LocalAttention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): LongT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): LongT5LayerFF(\n",
       "            (DenseReluDense): LongT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): LongT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x LongT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): LongT5LayerLocalSelfAttention(\n",
       "            (LocalSelfAttention): LongT5LocalAttention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): LongT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): LongT5LayerFF(\n",
       "            (DenseReluDense): LongT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): LongT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LongT5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): LongT5Stack(\n",
       "    (embed_tokens): Embedding(32128, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): LongT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): LongT5LayerSelfAttention(\n",
       "            (SelfAttention): LongT5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): LongT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): LongT5LayerCrossAttention(\n",
       "            (EncDecAttention): LongT5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): LongT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): LongT5LayerFF(\n",
       "            (DenseReluDense): LongT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): LongT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x LongT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): LongT5LayerSelfAttention(\n",
       "            (SelfAttention): LongT5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): LongT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): LongT5LayerCrossAttention(\n",
       "            (EncDecAttention): LongT5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): LongT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): LongT5LayerFF(\n",
       "            (DenseReluDense): LongT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): LongT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LongT5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the LongT5 model\n",
    "model = LongT5ForConditionalGeneration.from_pretrained(\"google/long-t5-local-base\")\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b59d95e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\764883\\AppData\\Local\\Temp\\ipykernel_17608\\321762531.py:2: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  rouge_metric = load_metric(\"rouge\")\n"
     ]
    }
   ],
   "source": [
    "# Load metrics\n",
    "rouge_metric = load_metric(\"rouge\")\n",
    "bert_metric = load_metric(\"bertscore\")\n",
    "\n",
    "\n",
    "\n",
    "from transformers import EvalPrediction\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    # Extract predictions\n",
    "    predictions = p.predictions\n",
    "    if isinstance(predictions, tuple):\n",
    "        predictions = predictions[0]  \n",
    "    # Check if the predictions are logits (3D tensor) or token IDs (2D tensor)\n",
    "    if len(predictions.shape) == 3:\n",
    "        # Convert logits to token IDs\n",
    "        preds = np.argmax(predictions, axis=-1)\n",
    "    else:\n",
    "        # Predictions are already token IDs\n",
    "        preds = predictions\n",
    "\n",
    "    # Flatten the predictions and labels if they are in nested lists\n",
    "    flat_preds = [item for sublist in preds for item in sublist] if isinstance(preds[0], list) else preds\n",
    "    flat_labels = [item for sublist in p.label_ids for item in sublist] if isinstance(p.label_ids[0], list) else p.label_ids\n",
    "\n",
    "    # Decode the predictions and labels\n",
    "    decoded_preds = tokenizer.batch_decode(flat_preds, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(flat_labels, skip_special_tokens=True)\n",
    "\n",
    "    # Compute ROUGE and BERTScore\n",
    "    rouge_result = rouge_metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    bert_result = bert_metric.compute(predictions=decoded_preds, references=decoded_labels, lang=\"en\")\n",
    "\n",
    "    # Format the results\n",
    "    rouge_result = {key: value.mid.fmeasure for key, value in rouge_result.items()}\n",
    "    bert_result = {key: sum([v for v in value if isinstance(v, (int, float))]) / len(value)  for key, value in bert_result.items()}\n",
    "\n",
    "\n",
    "    return {**rouge_result, **bert_result}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac74209c",
   "metadata": {},
   "source": [
    "## Notes for reference, Addresing the overfitting"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3394ffe0",
   "metadata": {},
   "source": [
    "Set the Gradient Accumulation Steps: This is the number of steps over which gradients will be accumulated. For example, if your desired effective batch size is 32 and your per_device_train_batch_size is 8, you should set gradient_accumulation_steps to 4.\n",
    "\n",
    "Modify the TrainingArguments: In the TrainingArguments, set the gradient_accumulation_steps parameter."
   ]
  },
  {
   "cell_type": "raw",
   "id": "1b0f830a",
   "metadata": {},
   "source": [
    "With this more batch sizes has to be experimented. Compuation constraints are seriously concerning at this point, to run one model I have to \n",
    "clear the gpu memmory each time. "
   ]
  },
  {
   "cell_type": "raw",
   "id": "3f9f2263",
   "metadata": {},
   "source": [
    "I am not using early stopping since it requires to save themodel at each epoch or in intervals, due to memmory Concersn I am not in a position to do that, because this high computation required model will not run anymore then. "
   ]
  },
  {
   "cell_type": "raw",
   "id": "df682f7e",
   "metadata": {},
   "source": [
    "I am going to stick to the small learning rate 5e-5 to avoid overfitting, which really concerns me at this point. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05cd38e5",
   "metadata": {},
   "source": [
    "Also I am going to stick with the huggingface transfromer trainer class for the training, since the conventional training loops are so slow, I have experimented, and felt the difference so well. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27cbcd9e",
   "metadata": {},
   "source": [
    "For standard model like LongT5, the dropout rates are predefined and typically don't need to be adjusted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd4fa63",
   "metadata": {},
   "source": [
    "L2 regularisation technique like weigt decay is used in the fine tuning "
   ]
  },
  {
   "cell_type": "raw",
   "id": "5d44b109",
   "metadata": {},
   "source": [
    "Pure FP16 training can lead to issues with numerical stability and model accuracy. To overcome this, mixed-precision training uses FP16 for most computations but keeps certain parts of the model (like weight updates) in FP32. This approach helps in maintaining the accuracy of the model while still benefiting from the reduced memory usage and increased speed of FP16 calculations.\n",
    "\n",
    "Automatic Handling by the Library: When you set fp16=True in TrainingArguments, the Hugging Face library automatically manages the complexities of mixed-precision training. It handles the casting of tensors to the appropriate precision and ensures that the critical parts of the computation are done in FP32 to maintain accuracy.\n",
    "\n",
    "Hardware Requirements: To effectively use mixed-precision training, you need a GPU that supports it. NVIDIA GPUs with Tensor Cores (like the V100, T4, A100, and newer models) are designed to efficiently perform FP16 calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4f28cf81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='192' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  3/192 02:09 < 6:47:11, 0.01 it/s, Epoch 0.02/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 45\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Custom training loop to compute and store metrics after each epoch\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(training_args\u001b[38;5;241m.\u001b[39mnum_train_epochs):\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;66;03m# Training\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m     \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;66;03m# Evaluation\u001b[39;00m\n\u001b[0;32m     48\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mevaluate(eval_dataset\u001b[38;5;241m=\u001b[39mval_dataset)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\trainer.py:1555\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1553\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   1554\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1555\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1556\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1557\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1558\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1559\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1560\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\trainer.py:1860\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1857\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m   1859\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[1;32m-> 1860\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1862\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   1863\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   1864\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[0;32m   1865\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   1866\u001b[0m ):\n\u001b[0;32m   1867\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   1868\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\trainer.py:2734\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   2732\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m   2733\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2734\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2736\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\accelerate\\accelerator.py:1989\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[1;34m(self, loss, **kwargs)\u001b[0m\n\u001b[0;32m   1987\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1988\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1989\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    491\u001b[0m     )\n\u001b[1;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\autograd\\__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_dataset_size = len(train_dataset)\n",
    "\n",
    "N = train_dataset_size\n",
    "batch_size = 2\n",
    "\n",
    "steps_per_epoch = N// batch_size\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=steps_per_epoch,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    #gradient_accumulation_steps=4,  # Adjust this based on your desired effective batch size\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy= \"no\",\n",
    "    save_total_limit= 0,\n",
    "#     fp16= False,  # Mixed precision training\n",
    ")\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,  \n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    optimizers=(AdamW(model.parameters(), lr=1e-4), None)\n",
    ")\n",
    "\n",
    "\n",
    "# List to store metrics after each epoch\n",
    "epoch_metrics = []\n",
    "\n",
    "# Custom training loop to compute and store metrics after each epoch\n",
    "for epoch in range(training_args.num_train_epochs):\n",
    "    # Training\n",
    "    trainer.train()\n",
    "    \n",
    "    # Evaluation\n",
    "    metrics = trainer.evaluate(eval_dataset=val_dataset)\n",
    "    epoch_metrics.append(metrics)\n",
    "    print(f\"Epoch {epoch + 1} Metrics: {metrics}\")\n",
    "# # Start training\n",
    "# trainer.train()\n",
    "# Plotting\n",
    "training_logs = trainer.state.log_history\n",
    "\n",
    "training_loss = [log['loss'] for log in training_logs if 'loss' in log]\n",
    "validation_loss = [log['eval_loss'] for log in training_logs if 'eval_loss' in log]\n",
    "epochs = range(1, len(training_loss) + 1)\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(epochs, training_loss, 'bo-', label='Training loss')\n",
    "plt.plot(epochs, validation_loss, 'ro-', label='Validation loss')\n",
    "plt.title('Training and Validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# # Evaluate the model on the test dataset\n",
    "# evaluation_results = trainer.evaluate(eval_dataset=val_dataset)\n",
    "\n",
    "# # Store metrics in a list for each epoch\n",
    "# metrics_list = []\n",
    "# for log in trainer.state.log_history:\n",
    "#     if \"eval_loss\" in log:\n",
    "#         metrics_list.append(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6da509b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test dataset\n",
    "evaluation_results = trainer.evaluate(eval_dataset=test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9effaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "## To find the errors, running a simple code avoiding the gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bcec19b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import LongT5ForConditionalGeneration, T5Tokenizer, Trainer, TrainingArguments\n",
    "from datasets import load_metric\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google/long-t5-local-base\")\n",
    "\n",
    "# Define the dataset class\n",
    "class EmailSummarizationDataset(Dataset):\n",
    "    def __init__(self, tokenizer, dataframe, max_input_length=1500, max_target_length=150):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = df\n",
    "        self.max_input_length = max_input_length\n",
    "        self.max_target_length = max_target_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data_row = self.data.iloc[index]\n",
    "\n",
    "        source_encoding = tokenizer(\n",
    "            data_row['Preprocessed_Body'], \n",
    "            max_length=self.max_input_length, \n",
    "            padding='max_length', \n",
    "            truncation=True, \n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        target_encoding = tokenizer(\n",
    "            data_row['Preprocessed_Summary'], \n",
    "            max_length=self.max_target_length, \n",
    "            padding='max_length', \n",
    "            truncation=True, \n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        labels = target_encoding.input_ids\n",
    "        labels[labels == 0] = -100\n",
    "\n",
    "        return dict(\n",
    "            input_ids=source_encoding.input_ids.flatten(),\n",
    "            attention_mask=source_encoding.attention_mask.flatten(),\n",
    "            labels=labels.flatten()\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "21821a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your data into a DataFrame (replace 'your_dataframe' with your actual DataFrame)\n",
    "\n",
    "\n",
    "# Split the data\n",
    "train_df, temp_df = train_test_split(df, test_size=0.3, random_state=42)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "# Prepare the datasets\n",
    "train_dataset = EmailSummarizationDataset(tokenizer, train_df)\n",
    "val_dataset = EmailSummarizationDataset(tokenizer, val_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d81c3709",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LongT5ForConditionalGeneration were not initialized from the model checkpoint at google/long-t5-local-base and are newly initialized: ['lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = LongT5ForConditionalGeneration.from_pretrained(\"google/long-t5-local-base\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "300aae2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"epoch\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "87e3bb1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\764883\\AppData\\Local\\Temp\\ipykernel_28544\\3432290279.py:1: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  rouge = load_metric(\"rouge\")\n"
     ]
    }
   ],
   "source": [
    "rouge = load_metric(\"rouge\")\n",
    "bertscore = load_metric(\"bertscore\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    # Replace -100 in the labels as we can't decode them\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # ROUGE\n",
    "    rouge_results = rouge.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    rouge_results = {key: value.mid.fmeasure * 100 for key, value in rouge_results.items()}\n",
    "\n",
    "    # BERTScore\n",
    "    bertscore_results = bertscore.compute(predictions=decoded_preds, references=decoded_labels, lang=\"en\")\n",
    "    bertscore_results = {f\"bertscore_{key}\": value.mean().item() * 100 for key, value in bertscore_results.items()}\n",
    "\n",
    "    return {**rouge_results, **bertscore_results}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "76a2d8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1fbe4dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\764883\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='240' max='717' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [240/717 57:45 < 1:55:45, 0.07 it/s, Epoch 1/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='151' max='239' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [151/239 17:22 < 10:11, 0.14 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\trainer.py:1555\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1553\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   1554\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1555\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1556\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1557\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1558\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1559\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1560\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\trainer.py:1937\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1934\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_training_stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1936\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_epoch_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m-> 1937\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1939\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m DebugOption\u001b[38;5;241m.\u001b[39mTPU_METRICS_DEBUG \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdebug:\n\u001b[0;32m   1940\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_torch_tpu_available():\n\u001b[0;32m   1941\u001b[0m         \u001b[38;5;66;03m# tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\trainer.py:2271\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[1;34m(self, tr_loss, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2269\u001b[0m         metrics\u001b[38;5;241m.\u001b[39mupdate(dataset_metrics)\n\u001b[0;32m   2270\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2271\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2272\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_report_to_hp_search(trial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step, metrics)\n\u001b[0;32m   2274\u001b[0m \u001b[38;5;66;03m# Run delayed LR scheduler now that metrics are populated\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\trainer.py:3011\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[1;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   3008\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m   3010\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[1;32m-> 3011\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3012\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3013\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEvaluation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3014\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[0;32m   3015\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[0;32m   3016\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   3017\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3018\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3019\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3021\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[0;32m   3022\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_jit_compilation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\trainer.py:3229\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[1;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   3226\u001b[0m     preds_host \u001b[38;5;241m=\u001b[39m logits \u001b[38;5;28;01mif\u001b[39;00m preds_host \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m nested_concat(preds_host, logits, padding_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[0;32m   3228\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 3229\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgather_for_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3230\u001b[0m     labels_host \u001b[38;5;241m=\u001b[39m labels \u001b[38;5;28;01mif\u001b[39;00m labels_host \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m nested_concat(labels_host, labels, padding_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[0;32m   3232\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_prediction_step(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\accelerate\\accelerator.py:2186\u001b[0m, in \u001b[0;36mAccelerator.gather_for_metrics\u001b[1;34m(self, input_data)\u001b[0m\n\u001b[0;32m   2155\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2156\u001b[0m \u001b[38;5;124;03m    Gather the values in *tensor* across all processes and concatenate them on the first dimension. Useful to\u001b[39;00m\n\u001b[0;32m   2157\u001b[0m \u001b[38;5;124;03m    regroup the predictions from all processes when doing evaluation.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2182\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[0;32m   2183\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   2184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m gather(tensor)\n\u001b[1;32m-> 2186\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgather_for_metrics\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_data):\n\u001b[0;32m   2187\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2188\u001b[0m \u001b[38;5;124;03m    Gathers `input_data` and potentially drops duplicates in the last batch if on a distributed system. Should be\u001b[39;00m\n\u001b[0;32m   2189\u001b[0m \u001b[38;5;124;03m    used for gathering the inputs and targets for metric calculation.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2209\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[0;32m   2210\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   2212\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f7a68964",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='72' max='72' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [72/72 00:14]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test dataset\n",
    "evaluation_results = trainer.evaluate(eval_dataset=test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ee8af387",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': nan,\n",
       " 'eval_rouge1': 0.0,\n",
       " 'eval_rouge2': 0.0,\n",
       " 'eval_rougeL': 0.0,\n",
       " 'eval_rougeLsum': 0.0,\n",
       " 'eval_precision': 0.0,\n",
       " 'eval_recall': 0.0,\n",
       " 'eval_f1': 0.0,\n",
       " 'eval_hashcode': 0.0,\n",
       " 'eval_runtime': 18.139,\n",
       " 'eval_samples_per_second': 7.939,\n",
       " 'eval_steps_per_second': 3.969,\n",
       " 'epoch': 1.0}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f9bc97e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='335' max='6680' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 335/6680 00:58 < 18:27, 5.73 it/s, Epoch 1/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='72' max='72' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [72/72 00:25]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "'list' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 31\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Custom training loop\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(training_args\u001b[38;5;241m.\u001b[39mnum_train_epochs):\n\u001b[1;32m---> 31\u001b[0m     \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[0;32m     32\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mevaluate(eval_dataset\u001b[38;5;241m=\u001b[39mval_dataset)  \u001b[38;5;66;03m# Evaluate model\u001b[39;00m\n\u001b[0;32m     33\u001b[0m     epoch_metrics\u001b[38;5;241m.\u001b[39mappend(metrics)  \u001b[38;5;66;03m# Store metrics\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\trainer.py:1555\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1553\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   1554\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1555\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1556\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1557\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1558\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1559\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1560\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\trainer.py:1937\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1934\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_training_stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1936\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_epoch_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m-> 1937\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1939\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m DebugOption\u001b[38;5;241m.\u001b[39mTPU_METRICS_DEBUG \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdebug:\n\u001b[0;32m   1940\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_torch_tpu_available():\n\u001b[0;32m   1941\u001b[0m         \u001b[38;5;66;03m# tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\trainer.py:2271\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[1;34m(self, tr_loss, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2269\u001b[0m         metrics\u001b[38;5;241m.\u001b[39mupdate(dataset_metrics)\n\u001b[0;32m   2270\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2271\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2272\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_report_to_hp_search(trial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step, metrics)\n\u001b[0;32m   2274\u001b[0m \u001b[38;5;66;03m# Run delayed LR scheduler now that metrics are populated\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\trainer.py:3011\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[1;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   3008\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m   3010\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[1;32m-> 3011\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3012\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3013\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEvaluation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3014\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[0;32m   3015\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[0;32m   3016\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   3017\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3018\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3019\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3021\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[0;32m   3022\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_jit_compilation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\trainer.py:3304\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[1;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   3300\u001b[0m         metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_metrics(\n\u001b[0;32m   3301\u001b[0m             EvalPrediction(predictions\u001b[38;5;241m=\u001b[39mall_preds, label_ids\u001b[38;5;241m=\u001b[39mall_labels, inputs\u001b[38;5;241m=\u001b[39mall_inputs)\n\u001b[0;32m   3302\u001b[0m         )\n\u001b[0;32m   3303\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3304\u001b[0m         metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mEvalPrediction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_preds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_labels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3305\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3306\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m {}\n",
      "\u001b[1;31mTypeError\u001b[0m: 'list' object is not callable"
     ]
    }
   ],
   "source": [
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=20,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=len(train_dataloader),\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    #gradient_accumulation_steps=4,  # Uncomment and adjust as needed\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    fp16=True,  # Ensure your GPU supports FP16\n",
    "    # Add model saving strategy if needed\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,  \n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=[custom_eval_callback],\n",
    "    optimizers=(AdamW(model.parameters(), lr=5e-5), None)\n",
    ")\n",
    "\n",
    "# Custom training loop\n",
    "for epoch in range(training_args.num_train_epochs):\n",
    "    trainer.train()  # Train model\n",
    "    metrics = trainer.evaluate(eval_dataset=val_dataset)  # Evaluate model\n",
    "    epoch_metrics.append(metrics)  # Store metrics\n",
    "    print(f\"Epoch {epoch + 1} Metrics: {metrics}\")\n",
    "\n",
    "# Plot training and validation loss after training is complete\n",
    "training_logs = trainer.state.log_history\n",
    "training_loss = [log['loss'] for log in training_logs if 'loss' in log]\n",
    "validation_loss = [log['eval_loss'] for log in training_logs if 'eval_loss' in log]\n",
    "epochs = range(1, len(training_loss) + 1)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(epochs, training_loss, 'bo-', label='Training loss')\n",
    "plt.plot(epochs, validation_loss, 'ro-', label='Validation loss')\n",
    "plt.title('Training and Validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Evaluate the model on the test dataset\n",
    "evaluation_results = trainer.evaluate(eval_dataset=test_dataset)\n",
    "\n",
    "# Store and handle metrics as needed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3caf0f07",
   "metadata": {},
   "source": [
    "## Training longt5 with maximum tokens set to 3000 and different learning rates, batch size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "0fc51f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CUDA devices: 1\n",
      "\n",
      "Device: cuda:0\n",
      "Total Memory: 24.00 GB\n",
      "Allocated Memory: 36.85 GB\n",
      "Cached Memory: 37.48 GB\n",
      "Free Memory: -50.33 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device_count = torch.cuda.device_count()\n",
    "    print(f\"Number of CUDA devices: {device_count}\")\n",
    "\n",
    "    for device_id in range(device_count):\n",
    "        device = torch.device(f\"cuda:{device_id}\")\n",
    "        total_mem = torch.cuda.get_device_properties(device).total_memory\n",
    "        allocated_mem = torch.cuda.memory_allocated(device)\n",
    "        cached_mem = torch.cuda.memory_reserved(device)\n",
    "        free_mem = total_mem - (allocated_mem + cached_mem)\n",
    "\n",
    "        print(f\"\\nDevice: {device}\")\n",
    "        print(f\"Total Memory: {total_mem / 1024**3:.2f} GB\")\n",
    "        print(f\"Allocated Memory: {allocated_mem / 1024**3:.2f} GB\")\n",
    "        print(f\"Cached Memory: {cached_mem / 1024**3:.2f} GB\")\n",
    "        print(f\"Free Memory: {free_mem / 1024**3:.2f} GB\")\n",
    "else:\n",
    "    print(\"CUDA is not available\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "c85d099c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleared cached memory.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Clearing CUDA cache\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"Cleared cached memory.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "ff23ecf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Some weights of LongT5ForConditionalGeneration were not initialized from the model checkpoint at google/long-t5-local-base and are newly initialized: ['lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LongT5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 768)\n",
       "  (encoder): LongT5Stack(\n",
       "    (embed_tokens): Embedding(32128, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): LongT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): LongT5LayerLocalSelfAttention(\n",
       "            (LocalSelfAttention): LongT5LocalAttention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): LongT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): LongT5LayerFF(\n",
       "            (DenseReluDense): LongT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): LongT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x LongT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): LongT5LayerLocalSelfAttention(\n",
       "            (LocalSelfAttention): LongT5LocalAttention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): LongT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): LongT5LayerFF(\n",
       "            (DenseReluDense): LongT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): LongT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LongT5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): LongT5Stack(\n",
       "    (embed_tokens): Embedding(32128, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): LongT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): LongT5LayerSelfAttention(\n",
       "            (SelfAttention): LongT5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): LongT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): LongT5LayerCrossAttention(\n",
       "            (EncDecAttention): LongT5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): LongT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): LongT5LayerFF(\n",
       "            (DenseReluDense): LongT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): LongT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x LongT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): LongT5LayerSelfAttention(\n",
       "            (SelfAttention): LongT5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): LongT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): LongT5LayerCrossAttention(\n",
       "            (EncDecAttention): LongT5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): LongT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): LongT5LayerFF(\n",
       "            (DenseReluDense): LongT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): LongT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LongT5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load your data\n",
    "data = df\n",
    "data = data[['Preprocessed_Body', 'Summary_human']].dropna()\n",
    "data = data.sample(frac=1, random_state=0)\n",
    "data = HFDataset.from_pandas(data)\n",
    "\n",
    "# Splitting the data\n",
    "data = data.train_test_split(test_size=0.3)\n",
    "data[\"test\"] = data[\"test\"].train_test_split(test_size=0.5)\n",
    "\n",
    "# Prepare the data\n",
    "train_source = data[\"train\"][\"Preprocessed_Body\"]\n",
    "train_target = data[\"train\"][\"Summary_human\"]\n",
    "val_source = data[\"test\"][\"train\"][\"Preprocessed_Body\"]\n",
    "val_target = data[\"test\"][\"train\"][\"Summary_human\"]\n",
    "test_source = data[\"test\"][\"test\"][\"Preprocessed_Body\"]\n",
    "test_target = data[\"test\"][\"test\"][\"Summary_human\"]\n",
    "\n",
    "# Initialize tokenizer for LongT5\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/long-t5-local-base\")\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Tokenize the data\n",
    "train_encodings = tokenizer(train_source, truncation=True, max_length=3000, padding=True)\n",
    "train_decodings = tokenizer(train_target, truncation=True, padding=True)\n",
    "test_encodings = tokenizer(test_source, truncation=True, max_length=3000, padding=True)\n",
    "test_decodings = tokenizer(test_target, truncation=True, padding=True)\n",
    "\n",
    "# Convert to PyTorch tensors and create DataLoader\n",
    "def create_data_loader(encodings, decodings, batch_size=8):\n",
    "    input_ids = torch.tensor(encodings[\"input_ids\"])\n",
    "    attention_masks = torch.tensor(encodings['attention_mask'])\n",
    "    labels = torch.tensor(decodings['input_ids'])\n",
    "    data = TensorDataset(input_ids, attention_masks, labels)\n",
    "    sampler = RandomSampler(data)\n",
    "    return DataLoader(data, sampler=sampler, batch_size=batch_size)\n",
    "\n",
    "train_dataloader = create_data_loader(train_encodings, train_decodings)\n",
    "test_dataloader = create_data_loader(test_encodings, test_decodings)\n",
    "device = 'cpu'\n",
    "# Load LongT5 model\n",
    "model = LongT5ForConditionalGeneration.from_pretrained(\"google/long-t5-local-base\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "dd71fbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load metrics\n",
    "rouge_metric = load_metric(\"rouge\")\n",
    "bert_metric = load_metric(\"bertscore\")\n",
    "\n",
    "def validate_and_calculate_metrics(model, dataloader, tokenizer, device):\n",
    "    model.eval()\n",
    "    rouge_scores = []\n",
    "    bert_scores = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Validating\"):\n",
    "            input_ids, attention_masks, labels = [t.to(device) for t in batch]\n",
    "            generated_ids = model.generate(input_ids, attention_mask=attention_masks, max_length=150)\n",
    "            preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
    "            refs = [tokenizer.decode(l, skip_special_tokens=True, clean_up_tokenization_spaces=True) for l in labels]\n",
    "\n",
    "            # Calculate ROUGE\n",
    "            rouge_scores.append(rouge_metric.compute(predictions=preds, references=refs))\n",
    "            \n",
    "            # Calculate BERTScore\n",
    "            P, R, F1 = bert_score(preds, refs, lang=\"en\", rescale_with_baseline=True)\n",
    "            bert_scores.append({\"precision\": P.mean().item(), \"recall\": R.mean().item(), \"f1\": F1.mean().item()})\n",
    "\n",
    "    # Aggregate scores\n",
    "    rouge_final = {key: sum(score[key].mid.fmeasure for score in rouge_scores) / len(rouge_scores) for key in rouge_scores[0]}\n",
    "    bert_final = {\n",
    "        \"precision\": sum(score[\"precision\"] for score in bert_scores) / len(bert_scores),\n",
    "        \"recall\": sum(score[\"recall\"] for score in bert_scores) / len(bert_scores),\n",
    "        \"f1\": sum(score[\"f1\"] for score in bert_scores) / len(bert_scores)\n",
    "    }\n",
    "\n",
    "    return rouge_final, bert_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92eb8754",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LongT5ForConditionalGeneration.from_pretrained(\"google/long-t5-tglobal-base\")\n",
    "\n",
    "epochs = 2\n",
    "device = 'cpu'\n",
    "# device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = 1e-4)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(str(epoch)+\"\\n\")\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    with tqdm(train_dataloader, unit=\"batch\") as tepoch:    \n",
    "        for batch in tepoch:\n",
    "            tepoch.set_description(f\"Epoch {epoch}\")\n",
    "            input_ids, attention_masks, labels = tuple(t.to(device) for t in batch)\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_masks, labels=labels)\n",
    "\n",
    "            loss = outputs.loss\n",
    "            train_loss += loss.item()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            tepoch.set_postfix(loss= loss.item())\n",
    "        train_loss /= len(train_dataloader)\n",
    "        sleep(0.1)\n",
    "    print(loss)\n",
    "\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        with tqdm(test_dataloader, unit=\"batch\") as tepoch:\n",
    "            for batch in tepoch:\n",
    "                tepoch.set_description(f\"Epoch {epoch}\")\n",
    "                input_ids, attention_masks, labels = tuple(t.to(device) for t in batch)\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_masks, labels=labels)\n",
    "                loss = outputs.loss\n",
    "                test_loss += loss.item()\n",
    "                tepoch.set_postfix(loss= loss.item())\n",
    "\n",
    "            # Compute the average testing loss for the epoch\n",
    "            test_loss /= len(test_dataloader)\n",
    "    print(loss)\n",
    "    print(f'train_loss = {train_loss}\\n test_loss = {test_loss}')\n",
    "    # Validation and Metric Calculation\n",
    "    rouge_scores, bert_scores = validate_and_calculate_metrics(model, val_dataloader, tokenizer, device)\n",
    "    print(f\"Epoch {epoch} Validation Scores: ROUGE: {rouge_scores}, BERTScore: {bert_scores}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c317d2",
   "metadata": {},
   "source": [
    "## Training with 20 epochs and 5e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324d99b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LongT5ForConditionalGeneration.from_pretrained(\"google/long-t5-tglobal-base\")\n",
    "\n",
    "epochs = 2\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = 5e-4)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(str(epoch)+\"\\n\")\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    with tqdm(train_dataloader, unit=\"batch\") as tepoch:    \n",
    "        for batch in tepoch:\n",
    "            tepoch.set_description(f\"Epoch {epoch}\")\n",
    "            input_ids, attention_masks, labels = tuple(t.to(device) for t in batch)\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_masks, labels=labels)\n",
    "\n",
    "            loss = outputs.loss\n",
    "            train_loss += loss.item()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            tepoch.set_postfix(loss= loss.item())\n",
    "        train_loss /= len(train_dataloader)\n",
    "        sleep(0.1)\n",
    "    print(loss)\n",
    "\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        with tqdm(test_dataloader, unit=\"batch\") as tepoch:\n",
    "            for batch in tepoch:\n",
    "                tepoch.set_description(f\"Epoch {epoch}\")\n",
    "                input_ids, attention_masks, labels = tuple(t.to(device) for t in batch)\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_masks, labels=labels)\n",
    "                loss = outputs.loss\n",
    "                test_loss += loss.item()\n",
    "                tepoch.set_postfix(loss= loss.item())\n",
    "\n",
    "            # Compute the average testing loss for the epoch\n",
    "            test_loss /= len(test_dataloader)\n",
    "    print(loss)\n",
    "    print(f'train_loss = {train_loss}\\n test_loss = {test_loss}')\n",
    "    # Validation and Metric Calculation\n",
    "    rouge_scores, bert_scores = validate_and_calculate_metrics(model, val_dataloader, tokenizer, device)\n",
    "    print(f\"Epoch {epoch} Validation Scores: ROUGE: {rouge_scores}, BERTScore: {bert_scores}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c2845a",
   "metadata": {},
   "source": [
    "## Training with token limit set to 2000 and 20 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204daf42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your data\n",
    "data = df\n",
    "data = data[['Preprocessed_Body', 'Summary_human']].dropna()\n",
    "data = data.sample(frac=1, random_state=0)\n",
    "data = HFDataset.from_pandas(data)\n",
    "\n",
    "# Splitting the data\n",
    "data = data.train_test_split(test_size=0.3)\n",
    "data[\"test\"] = data[\"test\"].train_test_split(test_size=0.5)\n",
    "\n",
    "# Prepare the data\n",
    "train_source = data[\"train\"][\"Preprocessed_Body\"]\n",
    "train_target = data[\"train\"][\"Summary_human\"]\n",
    "val_source = data[\"test\"][\"train\"][\"Preprocessed_Body\"]\n",
    "val_target = data[\"test\"][\"train\"][\"Summary_human\"]\n",
    "test_source = data[\"test\"][\"test\"][\"Preprocessed_Body\"]\n",
    "test_target = data[\"test\"][\"test\"][\"Summary_human\"]\n",
    "\n",
    "# Initialize tokenizer for LongT5\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/long-t5-local-base\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Tokenize the data\n",
    "train_encodings = tokenizer(train_source, truncation=True, max_length=2000, padding=True)\n",
    "train_decodings = tokenizer(train_target, truncation=True, padding=True)\n",
    "test_encodings = tokenizer(test_source, truncation=True, max_length=2000, padding=True)\n",
    "test_decodings = tokenizer(test_target, truncation=True, padding=True)\n",
    "\n",
    "# Convert to PyTorch tensors and create DataLoader\n",
    "def create_data_loader(encodings, decodings, batch_size=32):\n",
    "    input_ids = torch.tensor(encodings[\"input_ids\"])\n",
    "    attention_masks = torch.tensor(encodings['attention_mask'])\n",
    "    labels = torch.tensor(decodings['input_ids'])\n",
    "    data = TensorDataset(input_ids, attention_masks, labels)\n",
    "    sampler = RandomSampler(data)\n",
    "    return DataLoader(data, sampler=sampler, batch_size=batch_size)\n",
    "\n",
    "train_dataloader = create_data_loader(train_encodings, train_decodings)\n",
    "test_dataloader = create_data_loader(test_encodings, test_decodings)\n",
    "\n",
    "# Load LongT5 model\n",
    "model = LongT5ForConditionalGeneration.from_pretrained(\"google/long-t5-local-base\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a606f41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load metrics\n",
    "rouge_metric = load_metric(\"rouge\")\n",
    "bert_metric = load_metric(\"bertscore\")\n",
    "\n",
    "def validate_and_calculate_metrics(model, dataloader, tokenizer, device):\n",
    "    model.eval()\n",
    "    rouge_scores = []\n",
    "    bert_scores = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Validating\"):\n",
    "            input_ids, attention_masks, labels = [t.to(device) for t in batch]\n",
    "            generated_ids = model.generate(input_ids, attention_mask=attention_masks, max_length=150)\n",
    "            preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
    "            refs = [tokenizer.decode(l, skip_special_tokens=True, clean_up_tokenization_spaces=True) for l in labels]\n",
    "\n",
    "            # Calculate ROUGE\n",
    "            rouge_scores.append(rouge_metric.compute(predictions=preds, references=refs))\n",
    "            \n",
    "            # Calculate BERTScore\n",
    "            P, R, F1 = bert_score(preds, refs, lang=\"en\", rescale_with_baseline=True)\n",
    "            bert_scores.append({\"precision\": P.mean().item(), \"recall\": R.mean().item(), \"f1\": F1.mean().item()})\n",
    "\n",
    "    # Aggregate scores\n",
    "    rouge_final = {key: sum(score[key].mid.fmeasure for score in rouge_scores) / len(rouge_scores) for key in rouge_scores[0]}\n",
    "    bert_final = {\n",
    "        \"precision\": sum(score[\"precision\"] for score in bert_scores) / len(bert_scores),\n",
    "        \"recall\": sum(score[\"recall\"] for score in bert_scores) / len(bert_scores),\n",
    "        \"f1\": sum(score[\"f1\"] for score in bert_scores) / len(bert_scores)\n",
    "    }\n",
    "\n",
    "    return rouge_final, bert_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e25724",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LongT5ForConditionalGeneration.from_pretrained(\"google/long-t5-tglobal-base\")\n",
    "\n",
    "epochs = 20\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = 1e-4)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(str(epoch)+\"\\n\")\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    with tqdm(train_dataloader, unit=\"batch\") as tepoch:    \n",
    "        for batch in tepoch:\n",
    "            tepoch.set_description(f\"Epoch {epoch}\")\n",
    "            input_ids, attention_masks, labels = tuple(t.to(device) for t in batch)\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_masks, labels=labels)\n",
    "\n",
    "            loss = outputs.loss\n",
    "            train_loss += loss.item()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            tepoch.set_postfix(loss= loss.item())\n",
    "        train_loss /= len(train_dataloader)\n",
    "        sleep(0.1)\n",
    "    print(loss)\n",
    "\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        with tqdm(test_dataloader, unit=\"batch\") as tepoch:\n",
    "            for batch in tepoch:\n",
    "                tepoch.set_description(f\"Epoch {epoch}\")\n",
    "                input_ids, attention_masks, labels = tuple(t.to(device) for t in batch)\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_masks, labels=labels)\n",
    "                loss = outputs.loss\n",
    "                test_loss += loss.item()\n",
    "                tepoch.set_postfix(loss= loss.item())\n",
    "\n",
    "            # Compute the average testing loss for the epoch\n",
    "            test_loss /= len(test_dataloader)\n",
    "    print(loss)\n",
    "    print(f'train_loss = {train_loss}\\n test_loss = {test_loss}')\n",
    "    # Validation and Metric Calculation\n",
    "    rouge_scores, bert_scores = validate_and_calculate_metrics(model, val_dataloader, tokenizer, device)\n",
    "    print(f\"Epoch {epoch} Validation Scores: ROUGE: {rouge_scores}, BERTScore: {bert_scores}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
